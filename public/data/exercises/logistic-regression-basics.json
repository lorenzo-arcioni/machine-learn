{
  "id": "logistic-regression-basics",
  "title": "Logistic Regression Implementation",
  "description": "Implementa la regressione logistica da zero usando NumPy. Impara la classificazione binaria e il gradient descent per modelli non lineari.",
  "category": "Supervised Learning",
  "tags": ["NumPy", "Classification", "Statistics"],
  "estimatedTime": "50 min",
  "difficulty": "Intermediate",
  "level": "Intermediate",
  "prerequisites": ["Python Basics", "NumPy", "Linear Regression"],
  
  "problemStatement": {
    "overview": "In questo esercizio implementerai un modello di regressione logistica da zero usando solo NumPy. Imparerai come gestire problemi di classificazione binaria e come ottimizzare parametri usando il gradient descent con la funzione sigmoide.",
    "objectives": [
      "Comprendere la differenza tra regressione lineare e logistica",
      "Implementare la funzione sigmoide e la sua derivata",
      "Codificare l'algoritmo di gradient descent per classificazione",
      "Valutare il modello usando metriche appropriate (accuracy, precision, recall)",
      "Visualizzare il decision boundary e comprendere il processo di classificazione"
    ],
    "context": "La regressione logistica è l'algoritmo fondamentale per la classificazione binaria. A differenza della regressione lineare, usa la funzione sigmoide per mappare qualsiasi valore reale in un range [0,1], permettendo di predire probabilità."
  },
  
  "documentation": {
    "theory": "## Fondamenti Matematici\n\nLa regressione logistica usa la funzione sigmoide per modellare la probabilità:\n\n**σ(z) = 1 / (1 + e^(-z))**\n\nDove z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n\n## Cost Function\nUsiamo la log-likelihood come funzione di costo:\n**J(β) = -(1/m) * Σ[y*log(h(x)) + (1-y)*log(1-h(x))]**\n\n## Gradient\nIl gradiente per l'aggiornamento dei parametri:\n**∇J = (1/m) * X^T * (h(x) - y)**",
    "implementation": "## Passi di Implementazione\n\n1. **Preparazione Dati**\n   - Carica e preprocessa il dataset\n   - Aggiungi il termine bias\n   - Dividi in training e test set\n\n2. **Funzione Sigmoide**\n   - Implementa la sigmoide in modo numericalmente stabile\n   - Gestisci l'overflow per valori grandi\n\n3. **Gradient Descent**\n   - Inizializza parametri casualmente\n   - Implementa la cost function\n   - Calcola i gradienti\n   - Aggiorna parametri iterativamente\n\n4. **Valutazione**\n   - Calcola predizioni e probabilità\n   - Computa metriche di classificazione\n   - Visualizza decision boundary",
    "tips": [
      "Usa np.clip per evitare overflow nella funzione sigmoide",
      "Monitora la cost function per verificare la convergenza",
      "Prova diversi learning rates per ottimizzare la convergenza",
      "Standardizza le features per migliorare le performance",
      "Visualizza sempre i risultati per capire il comportamento del modello"
    ]
  },
  
  "githubSolutionUrl": "https://github.com/lorenzo-arcioni/Personal-Learning-Hub/blob/main/Calculus/Functions.ipynb",
  
  "resources": [
    {
      "title": "Logistic Regression Theory",
      "type": "Theory",
      "url": "/theory/logistic-regression",
      "description": "Fondamenti matematici della regressione logistica e funzione sigmoide"
    },
    {
      "title": "Classification Metrics",
      "type": "Theory",
      "url": "/theory/classification-metrics", 
      "description": "Metriche per valutare modelli di classificazione: accuracy, precision, recall, F1-score"
    }
  ]
}
