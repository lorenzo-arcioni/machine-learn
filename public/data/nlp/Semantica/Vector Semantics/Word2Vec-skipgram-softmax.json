{
  "title": "Skip-gram con Softmax",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>Il modello <strong>Skip-gram</strong> di <em>word2vec</em> con softmax è una tecnica di apprendimento non supervisionato usata per generare vettori densi (embedding) che rappresentano parole in uno spazio continuo a dimensione $D$.</p>\n<p>Vediamo nel dettaglio tutti i passaggi e le componenti del modello.</p>\n<h2 id=\"parametri-da-apprendere-in-skip-gram-con-softmax\">Parametri da apprendere in Skip-Gram con Softmax</h2>\n<p>Nel modello Skip-Gram di word2vec, l&rsquo;obiettivo principale è imparare rappresentazioni dense (embedding) delle parole che catturino il loro significato in relazione al contesto in cui appaiono. Per fare ciò, dobbiamo definire e apprendere dei parametri, che rappresentano queste strutture vettoriali.</p>\n<h3 id=\"definizione-dei-parametri\">Definizione dei parametri</h3>\n<p>Sia $V$ il vocabolario di parole del modello, e sia $D$ la dimensione dello spazio di embedding, cioè il numero di componenti o caratteristiche usate per rappresentare ciascuna parola come un vettore numerico continuo. Ogni dimensione può essere interpretata come un &ldquo;tema&rdquo; o una caratteristica latente che cattura aspetti semantici o sintattici della parola.</p>\n<p>Indichiamo con:</p>\n$$\\large\n\\bm{\\theta} =\n\\begin{bmatrix}\n\\bm{\\theta}_W \\\\[0.3em] \\hline \\\\[-0.9em]\n\\bm{\\theta}_C\n\\end{bmatrix}\n\\quad\\text{con}\\quad\n\\bm{\\theta}_W \\in \\mathbb{R}^{|V| \\times D},\\quad\n\\bm{\\theta}_C \\in \\mathbb{R}^{|V| \\times D}\n$$\n<p>l&rsquo;insieme dei parametri del modello, suddiviso in due matrici principali:</p>\n<ul>\n<li>\n<p><strong>$\\bm{\\theta}_W$</strong> (matrice degli embedding delle parole centro):</p>\n<ul>\n<li><strong>Dimensione:</strong> $|V| \\times D$</li>\n<li>Ogni riga di $\\bm{\\theta}_W$ è un vettore che rappresenta una parola specifica <strong>nel ruolo di parola centrale</strong> all’interno di una finestra di contesto. Questo significa che il vettore codifica le proprietà della parola quando è il punto focale della previsione del modello.</li>\n<li>Il vettore di embedding in $\\bm{\\theta}_W$ viene usato dal modello per cercare di predire le parole di contesto che la circondano: ad esempio, dato un vettore centrale, il modello calcola la probabilità di ogni parola nel vocabolario come possibile parola di contesto.</li>\n<li>Questa rappresentazione è fondamentale perché permette al modello di apprendere relazioni tra parole basate sulle co-occorrenze: parole con significati simili o usi simili tendono ad avere vettori vicini nello spazio degli embedding.</li>\n<li>È importante notare che la stessa parola avrà vettori distinti in $\\bm{\\theta}_W$ e in $\\bm{\\theta}_C$, poiché il suo ruolo nel modello cambia (centro vs contesto). Questo permette una rappresentazione più ricca e flessibile del linguaggio.</li>\n</ul>\n</li>\n<li>\n<p><strong>$\\bm{\\theta}_C$</strong> (matrice degli embedding delle parole contesto):</p>\n<ul>\n<li><strong>Dimensione:</strong> $|V| \\times D$</li>\n<li>Ogni riga di $\\bm{\\theta}_C$ è un vettore che rappresenta una parola <strong>quando essa agisce come contesto</strong> di una parola centrale. In altre parole, questi vettori sono usati per modellare le parole che circondano la parola centrale nella finestra di contesto.</li>\n<li>La funzione di $\\bm{\\theta}_C$ è catturare le proprietà semantiche e sintattiche delle parole nel loro ruolo di contesto, cioè come &ldquo;indizi&rdquo; o segnali che aiutano a prevedere la parola centrale.</li>\n<li>Ad esempio, la parola &ldquo;delicious&rdquo; avrà un embedding in $\\bm{\\theta}_C$ che riflette il suo uso frequente vicino a parole legate al cibo, mentre la stessa parola avrà un embedding differente in $\\bm{\\theta}_W$ quando appare come parola centrale.</li>\n<li>Questa doppia rappresentazione consente al modello di distinguere come una parola si comporta quando è il fulcro della previsione (centro) rispetto a quando è un &ldquo;supporto&rdquo; per predire altre parole (contesto).</li>\n<li>Grazie a $\\bm{\\theta}_C$, il modello impara a riconoscere quali parole di contesto sono più probabili dati i vettori delle parole centrali, migliorando così la capacità di rappresentare le relazioni semantiche tra parole.</li>\n</ul>\n</li>\n</ul>\n<p>Questa suddivisione di parametri consente al modello di catturare dinamiche diverse, come il significato di una parola quando appare come centro o quando appare come contesto nella finestra di contesto.</p>\n<p><img src=\"/images/tikz/a24d8ddc52dc38d45e29d9fd9070e6a1.svg\" style=\"display: block; width: 100%; height: auto; max-height: 600px;\" class=\"tikz-svg\" /></p>\n<h3 id=\"perche-due-matrici-distinte\">Perché due matrici distinte?</h3>\n<ul>\n<li>\n<p><strong>Ruoli diversi</strong>: </p>\n</li>\n<li>\n<p>$\\bm{\\theta}_W$: embedding quando la parola è <strong>centro</strong> (target da cui si predice).  </p>\n</li>\n<li>\n<p>$\\bm{\\theta}_C$: embedding quando la parola è <strong>contesto</strong> (segnale per la previsione).</p>\n</li>\n<li>\n<p><strong>Esempio</strong> (“Il <strong>gatto</strong> nero dorme…”):</p>\n</li>\n<li>\n<p>“gatto” → $\\bm{\\theta}_W$ cattura come “gatto” governa il contesto (“nero”, “dorme”).  </p>\n</li>\n<li>“nero”, “dorme” → $\\bm{\\theta}_C$ catturano come questi agiscono da indizi per “gatto”.</li>\n</ul>\n<h3 id=\"problemi-con-un-singolo-embedding\">⚠️ Problemi con un singolo embedding</h3>\n<ol>\n<li>\n<p><strong>Ruolo funzionale perso</strong></p>\n</li>\n<li>\n<p>Ogni parola può comparire sia come <strong>centrale</strong> sia come <strong>di contesto</strong>.</p>\n</li>\n<li>\n<p>Esempio:</p>\n<ul>\n<li>“<strong>book</strong>” come centrale (es. <em>&ldquo;I read a book about history.&rdquo;</em>) → predice parole come <em>read</em>, <em>history</em>.</li>\n<li>“<strong>book</strong>” come contesto (es. <em>&ldquo;She put the book on the table.&rdquo;</em>) → aiuta a predire <em>put</em>, <em>table</em>.</li>\n</ul>\n</li>\n<li>\n<p>Se usiamo <strong>un solo embedding</strong>, non distinguiamo questi ruoli → perdiamo informazione funzionale importante.</p>\n</li>\n<li>\n<p><strong>Relazioni asimmetriche non modellate</strong></p>\n</li>\n<li>\n<p>Il significato delle relazioni cambia a seconda della direzione:</p>\n<ul>\n<li>“<strong>eat</strong>” → “<strong>food</strong>” = tipico: il verbo suggerisce l’oggetto (cosa si mangia).</li>\n<li>“<strong>food</strong>” → “<strong>eat</strong>” = più debole: “food” potrebbe comparire in molti altri contesti (buy, cook, smell…).</li>\n</ul>\n</li>\n<li>\n<p>Se usiamo lo stesso embedding per “food” in entrambi i ruoli, non possiamo catturare questa asimmetria.</p>\n</li>\n<li>\n<p>Due matrici permettono:</p>\n<ul>\n<li>$\\theta_W$(eat) → embedding ottimizzato per predire cibo.</li>\n<li>$\\theta_C$(food) → embedding ottimizzato per essere predetto da verbi come <em>eat</em>.</li>\n</ul>\n</li>\n<li>\n<p><strong>Embedding meno precisi</strong></p>\n</li>\n<li>\n<p>Un solo embedding deve essere &ldquo;tuttofare&rdquo; → media tra ruoli e significati.</p>\n</li>\n<li>Risultato: vettori <strong>più confusi, meno specializzati</strong>, e performance peggiori in downstream tasks.</li>\n<li>Due matrici aiutano a ottenere rappresentazioni <strong>più informative e discriminative</strong>.</li>\n</ol>\n<h3 id=\"numero-totale-di-parametri\">Numero totale di parametri</h3>\n<p>Il numero complessivo di parametri del modello è dato dalla somma degli elementi di entrambe le matrici:</p>\n$$\n2 \\cdot |V| \\times D\n$$\n<p>Ovvero:</p>\n<ul>\n<li>$|V| \\times D$ parametri per gli embedding come centro,</li>\n<li>$|V| \\times D$ parametri per gli embedding come contesto.</li>\n</ul>\n<h3 id=\"visualizzazione-intuitiva\">Visualizzazione intuitiva</h3>\n<p>Immagina il vocabolario come una lista di parole:</p>\n<table>\n<thead>\n<tr>\n<th>Indice</th>\n<th>Parola</th>\n<th>Embedding Centro ($\\bm{\\theta}_W$)</th>\n<th>Embedding Contesto ($\\bm{\\theta}_C$)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>&ldquo;lemon&rdquo;</td>\n<td>vettore in $\\mathbb{R}^D$</td>\n<td>vettore in $\\mathbb{R}^D$</td>\n</tr>\n<tr>\n<td>2</td>\n<td>&ldquo;tablespoon&rdquo;</td>\n<td>vettore in $\\mathbb{R}^D$</td>\n<td>vettore in $\\mathbb{R}^D$</td>\n</tr>\n<tr>\n<td>&hellip;</td>\n<td>&hellip;</td>\n<td>&hellip;</td>\n<td>&hellip;</td>\n</tr>\n<tr>\n<td>|V|</td>\n<td>&ldquo;jam&rdquo;</td>\n<td>vettore in $\\mathbb{R}^D$</td>\n<td>vettore in $\\mathbb{R}^D$</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Quando &ldquo;tablespoon&rdquo; è parola centro, useremo la riga 2 di $\\bm{\\theta}_W$.</li>\n<li>Quando &ldquo;tablespoon&rdquo; è nel contesto, useremo la riga 2 di $\\bm{\\theta}_C$.</li>\n</ul>\n<h3 id=\"perche-sono-vettori\">Perché sono vettori?</h3>\n<p>Rappresentare le parole come vettori in uno spazio continuo di dimensione $D$ consente al modello di apprendere relazioni semantiche e sintattiche tra parole, ad esempio:</p>\n<ul>\n<li>Parole con significati simili tendono ad avere vettori vicini nello spazio,</li>\n<li>Relazioni di analogia possono essere rappresentate come vettori differenza, es. vettore(&ldquo;re&rdquo;) - vettore(&ldquo;uomo&rdquo;) + vettore(&ldquo;donna&rdquo;) ≈ vettore(&ldquo;regina&rdquo;).</li>\n</ul>\n<h3 id=\"riassumendo\">Riassumendo:</h3>\n<ul>\n<li>$\\bm{\\theta}_W$ e $\\bm{\\theta}_C$ sono matrici di embedding distinte per parola centro e contesto.</li>\n<li>Entrambe hanno dimensione $|V| \\times D$.</li>\n<li>Complessivamente abbiamo $2 \\cdot |V| \\times D$ parametri da imparare.</li>\n<li>Questo doppio embedding è la chiave per modellare le relazioni tra parole in un modo più ricco e flessibile.</li>\n</ul>\n<p>Questa struttura di parametri sarà la base su cui il modello Skip-Gram costruirà la sua funzione di probabilità e la sua funzione di perdita durante l&rsquo;addestramento.</p>\n<h2 id=\"il-concetto-di-self-supervision-nello-skip-gram\">Il concetto di self-supervision nello Skip-gram</h2>\n<p>Il training si basa su un grande corpus di testo, ad esempio:<br />\n<code>... lemon, a tablespoon of apricot jam, a pinch ...</code></p>\n<p>Il modello considera una finestra di contesto di ampiezza $m$ (ad esempio $m=2$) centrata sulla parola al tempo $t$:</p>\n<ul>\n<li>La parola centrale è $w_t$, nel nostro esempio &ldquo;apricot&rdquo;.</li>\n<li>Le parole del contesto sono quelle all’interno della finestra di dimensione $2m$ intorno a $w_t$:</li>\n<li>$w_{t-2}$, $w_{t-1}$ a sinistra,</li>\n<li>$w_{t+1}$, $w_{t+2}$ a destra.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">~~lemon~~</th>\n<th style=\"text-align: center;\">~~a~~</th>\n<th style=\"text-align: center;\">[tablespoon</th>\n<th style=\"text-align: center;\">of</th>\n<th style=\"text-align: center;\"><strong>apricot</strong></th>\n<th style=\"text-align: center;\">jam</th>\n<th style=\"text-align: center;\">a]</th>\n<th style=\"text-align: center;\">~~pinch~~</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">$w_{t-2}$</td>\n<td style=\"text-align: center;\">$w_{t-1}$</td>\n<td style=\"text-align: center;\"><strong>$w_t$</strong></td>\n<td style=\"text-align: center;\">$w_{t+1}$</td>\n<td style=\"text-align: center;\">$w_{t+2}$</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n</tbody>\n</table>\n<p>È detto <strong>self-supervision</strong> perché non usa etichette esterne, ma sfrutta il contesto delle parole all’interno del testo come se fosse un’etichetta. 😃</p>\n<p>Il modello <strong>Skip-gram</strong> classico (e anche il CBOW) non cattura la posizione precisa delle parole nel contesto rispetto alla parola centrale, cioè non distingue se una parola del contesto sta a sinistra o a destra, o a quale distanza esatta.</p>\n<h2 id=\"obiettivo-del-modello\">Obiettivo del modello</h2>\n<p>Vogliamo modellare la probabilità congiunta di osservare le parole di contesto data la parola centrale $w_t$, ossia:</p>\n$$\\mathbb P(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2} \\mid w_t; \\bm{\\theta}) $$\n<p>Per semplicità si assume una <strong>forte indipendenza condizionata</strong> tra le parole di contesto dato il centro:</p>\n$$ \\mathbb P(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2} \\mid w_t; \\bm{\\theta}) \\approx \\prod_{j=-m, j \\neq 0}^{m} \\mathbb P(w_{t+j} \\mid w_t; \\bm{\\theta})$$\n<p>Questo significa che ogni parola di contesto è indipendente dalle altre data la parola centrale.</p>\n<h2 id=\"come-si-calcola-math_inline_117\">Come si calcola $\\mathbb P(w_{t+j}\\mid w_t)$?</h2>\n<p>Dato un centro $w_t$, vogliamo predire la parola di contesto $w_{t+j}$. Questa probabilità è modellata come una distribuzione categorica su tutto il vocabolario $V$.</p>\n<ol>\n<li>Prendiamo l&rsquo;embedding della parola centro: se $i$ è l&rsquo;indice di $w_t$ in $\\bm{\\theta}_W$, consideriamo il vettore riga $\\bm{\\theta}_W^i$ (di dimensione $1 \\times |D|$).</li>\n<li>Calcoliamo i punteggi (logits) per tutte le parole del vocabolario come prodotto scalare tra ogni vettore di contesto in $\\bm{\\theta}_C$ e l&rsquo;embedding del centro:</li>\n</ol>\n$$\n   \\underbrace{\\mathbf{z}_i}_{|V|\\times 1}=\\overbrace{\\underbrace{\\bm{\\theta}_C}_{|V|\\times D}}^{\\text{as context}}\\cdot\\overbrace{\\underbrace{{\\bm{\\theta}_{W}^i}^T}_{D\\times 1}}^{\\text{as center}}\n   $$\n<p>dove $\\mathbf{z}$ è un vettore di dimensione $|V|$, con ogni elemento che rappresenta la similarità (dot product) tra la parola centro e una possibile parola di contesto.</p>\n<ol>\n<li>Applichiamo la funzione <strong>softmax</strong> ai logits per ottenere una distribuzione di probabilità:</li>\n</ol>\n$$\n  \\mathbf{p}_i = \\text{softmax}(\\mathbf{z}_i) = \\begin{bmatrix}\n  p_1 \\\\\n  \\\\\n  \\vdots \\\\\n  \\\\\n  p_{|V|}\n  \\\\[0.45em]\n  \\end{bmatrix}= \\begin{bmatrix}\n  \\mathbb P(w_{t+j} = \\text{`apple`} | w_t = \\text{`apricot`}) \\\\\n  \\\\\n  \\vdots \\\\\n  \\\\\n  \\mathbb P(w_{t+j} = \\text{`zucchini`} | w_t = \\text{`apricot`})\n  \\end{bmatrix}\n  =  \n  \\Large\\begin{bmatrix}\n  \\frac{e^{z_1}}{\\sum_{i=1}^{|V|} e^{z_{i}}} \\\\\n  \\\\\n  \\vdots \\\\\n  \\\\\n  \\frac{e^{z_{|V|}}}{\\sum_{i=1}^{|V|} e^{z_{i}}}\n  \\end{bmatrix}\n  $$\n<p>Così otteniamo la probabilità di ogni parola del vocabolario come contesto dato il centro $w_t$.</p>\n<p><strong>Remark.</strong> L&rsquo;indice della parola $w_t$ nella matrice $\\bm{\\theta}_W$ è $i$.</p>\n<h2 id=\"massimizzazione-della-likelihood-su-tutta-la-finestra\">Massimizzazione della likelihood su tutta la finestra</h2>\n<p>Per ogni parola centrale $w_t$, la probabilità congiunta di osservare tutte le parole di contesto nella finestra è:</p>\n$$\n\\prod_{j=-m, j \\neq 0}^{m} \\mathbb P(w_{t+j} | w_t; \\bm{\\theta})\n$$\n<p>Il nostro obiettivo è trovare i parametri $\\bm{\\theta}$ che massimizzano la likelihood su tutto il corpus, ossia:</p>\n$$\n\\bm{\\theta}^* = \\arg\\max_{\\bm{\\theta}} \\prod_{t=1}^T \\prod_{j=-m, j \\neq 0}^m \\mathbb P(w_{t+j} | w_t; \\bm{\\theta})\n$$\n<h2 id=\"funzione-di-perdita-loss-derivata-dalla-likelihood\">Funzione di perdita (loss) derivata dalla likelihood</h2>\n<p>L’obiettivo dell&rsquo;addestramento è massimizzare la <strong>likelihood</strong> dei dati osservati, ovvero la probabilità di osservare le parole di contesto dato il centro, su tutto il corpus:</p>\n$$\nL(\\bm{\\theta}) = \\prod_{t=1}^T \\prod_{j=-m, j \\neq 0}^{m} \\mathbb P(w_{t+j} \\mid w_t; \\bm{\\theta})\n$$\n<p>Lavorare direttamente con la likelihood può essere numericamente instabile, quindi passiamo al <strong>logaritmo della likelihood</strong> (log-likelihood), che è una trasformazione monotona e rende il prodotto una somma:</p>\n$$\n\\log L(\\bm{\\theta}) = \\sum_{t=1}^T \\sum_{j=-m, j \\neq 0}^{m} \\log \\mathbb P(w_{t+j} \\mid w_t; \\bm{\\theta})\n$$\n<p>Il nostro obiettivo è quindi <strong>massimizzare</strong> questa log-likelihood:</p>\n$$\n\\bm{\\theta}^* = \\arg\\max_{\\bm{\\theta}} \\log L(\\bm{\\theta})\n$$\n<p>In pratica, però, gli algoritmi di ottimizzazione numerica (come la discesa del gradiente) lavorano meglio se formuliamo il problema come <strong>minimizzazione</strong>. Per questo motivo, definiamo la <strong>funzione di perdita</strong> come l&rsquo;opposto della log-likelihood:</p>\n$$\n\\mathcal{L}(\\bm{\\theta}) = - \\sum_{t=1}^T \\sum_{j=-m, j \\neq 0}^{m} \\log \\mathbb P(w_{t+j} \\mid w_t; \\bm{\\theta})\n$$\n<p>Così facendo, possiamo minimizzare la funzione $\\mathcal {L}$ per ottenere i parametri $\\bm{\\theta}^*$ che massimizzano la log-likelihood.</p>\n<p>Possiamo ora esplicitare $\\mathbb P(w_{t+j} \\mid w_t)$ usando la softmax, come visto in precedenza. Supponiamo che:\n- $\\mathbf u_{w_t}$ sia l&rsquo;embedding della parola centrale $w_t$, quindi la riga corrispondente a $w_t$ della matrice $\\bm{\\theta}_W$\n- $\\mathbf v_{w_{t+j}}$ sia l&rsquo;embedding della parola di contesto $w_{t+j}$, quindi la riga corrispondente a $w_{t+j}$ della matrice $\\bm{\\theta}_C$</p>\n<p>Allora la probabilità predetta dal modello è:</p>\n$$\n\\mathbb P(w_{t+j} \\mid w_t)\n= \\frac{\n    \\exp\\!\\bigl(\\mathbf{v}_{\\,w_{t+j}}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n  }{\n    \\displaystyle \\sum_{w' \\in V}\n      \\exp\\!\\bigl(\\mathbf{v}_{\\,w'}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n  }\n$$\n<p>Sostituendo nella funzione di perdita otteniamo:</p>\n$$\n\\mathcal{L}(\\bm{\\theta})\n= - \\sum_{t=1}^{T} \\sum_{\\substack{j=-m \\\\ j \\neq 0}}^{m}\n    \\log\n    \\frac{\n      \\exp\\!\\bigl(\\mathbf{v}_{\\,w_{t+j}}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n    }{\n      \\displaystyle \\sum_{w' \\in V}\n        \\exp\\!\\bigl(\\mathbf{v}_{\\,w'}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n    }\n$$\n<p>Applicando le proprietà del logaritmo, la loss per una singola coppia $(w_t, w_{t+j})$ diventa:</p>\n$$\n\\mathcal{L}(w_{t+j}, w_t; \\bm{\\theta}) = - \\log\n    \\frac{\n      \\exp\\!\\bigl(\\mathbf{v}_{\\,w_{t+j}}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n    }{\n      \\displaystyle \\sum_{w' \\in V}\n        \\exp\\!\\bigl(\\mathbf{v}_{\\,w'}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n    }\n$$\n<p>che si può riscrivere come:</p>\n$$\n\\mathcal{L}(w_{t+j}, w_t; \\bm{\\theta})\n= -\\,\\underbrace{\\mathbf{v}_{\\,w_{t+j}}^\\top \\,\\mathbf{u}_{\\,w_t}}_\\text{Similarità contesto-parola}\n  \\;+\\;\n  \\underbrace{\\log\n  \\sum_{w' \\in V}\n    \\exp\\!\\bigl(\\mathbf{v}_{\\,w'}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)}_\\text{Similarità di tutti gli altri contesti con la stessa parola}\n$$\n<p>Questa formula evidenzia il trade-off tra massimizzare la similarità centro-contesto della parola corretta e normalizzare le probabilità su tutto il vocabolario.</p>\n<p>Infine, la <strong>loss media</strong> su tutto il corpus è:</p>\n$$\n\\mathcal{L}(\\bm{\\theta})\n= -\\frac{1}{T}\n  \\sum_{t=1}^{T} \\sum_{\\substack{j=-m \\\\ j \\neq 0}}^{m}\n    \\log \\mathbb P(w_{t+j} \\mid w_t; \\bm{\\theta})\n= -\\frac{1}{T}\n  \\sum_{t=1}^{T} \\sum_{\\substack{j=-m \\\\ j \\neq 0}}^{m}\n    \\log\n    \\frac{\n      \\exp\\!\\bigl(\\mathbf{v}_{\\,w_{t+j}}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n    }{\n      \\displaystyle \\sum_{w' \\in V}\n        \\exp\\!\\bigl(\\mathbf{v}_{\\,w'}^\\top \\,\\mathbf{u}_{\\,w_t}\\bigr)\n    }\n$$\n<p>che coincide con la cross-entropy fra la distribuzione softmax predetta e la distribuzione one-hot vera.</p>\n<h2 id=\"ottimizzazione-tramite-sgd\">Ottimizzazione tramite SGD</h2>\n<p>🧠 <em>Prima di continuare, vedi la nota dedicata sul funzionamento dello SGD: <a href=\"/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente\" class=\"text-blue-600 hover:underline\">Discesa del Gradiente</a>.</em></p>\n<p>L’addestramento del modello Skip-gram con softmax consiste nell’ottimizzare i parametri $\\bm{\\theta} = \\begin{bmatrix} \\bm{\\theta}_W \\\\ \\bm{\\theta}_C \\end{bmatrix}$ per massimizzare la probabilità delle parole di contesto osservate, dato ciascun centro $w_t$ nel corpus.</p>\n<p>L’obiettivo è <strong>minimizzare la loss media</strong> dei dati, ovvero la somma della log-probabilità dei contesti osservati dato ogni parola centrale, moltiplicata per $-\\frac{1}{T}$. Formalmente:</p>\n$$\n\\mathcal{L}(\\bm{\\theta}) = -\\frac{1}{T} \\sum_{t=1}^T \\sum_{\\substack{j = -m \\\\ j \\ne 0}}^m \\log \\mathbb{P}(w_{t+j} \\mid w_t; \\bm{\\theta})\n$$\n<p>dove:</p>\n<ul>\n<li>$T$ è il numero totale di parole nel corpus,</li>\n<li>$m$ è l&rsquo;ampiezza della finestra di contesto,</li>\n<li>$\\mathbb{P}(w_{t+j} \\mid w_t; \\bm{\\theta})$ è la probabilità (softmax) di osservare $w_{t+j}$ dato il centro $w_t$, definita come:</li>\n</ul>\n$$\n\\mathbb{P}(w_{t+j} \\mid w_t; \\bm{\\theta}) = \\frac{\\exp\\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} \\right)}{\\sum_{k=1}^{|V|} \\exp\\left( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} \\right)}\n$$\n<p>con:</p>\n<ul>\n<li>$w_t$: parola centrale (indice $c$),</li>\n<li>$w_{t+j}$: parola di contesto (indice $o$),</li>\n<li>$\\mathbf u_{w_t} \\in \\mathbb{R}^D$: vettore embedding della parola centro $w_t$,</li>\n<li>$\\mathbf v_{w_{t+j}} \\in \\mathbb{R}^D$: vettore embedding della parola contesto $w_{t+j}$.</li>\n</ul>\n<h3 id=\"come-si-ottimizza\">Come si ottimizza?</h3>\n<p>Poiché la somma al denominatore del softmax scorre su tutto il vocabolario ($|V|$ è molto grande), il calcolo diretto è troppo costoso. Tuttavia, per ora assumiamo di usare il <strong>softmax esatto</strong>, per chiarezza.</p>\n<p>Il modello viene ottimizzato tramite <strong>Stochastic Gradient Descent (SGD)</strong>, cioè:</p>\n<ol>\n<li>Si considera una coppia $(w_t, w_{t+j})$ (parola centro + parola di contesto osservata),</li>\n<li>Si calcola la <strong>loss negativa log-likelihood</strong> per quella coppia:</li>\n</ol>\n$$\n\\mathcal{L}(w_{t+j}, w_t; \\bm{\\theta}) = -\\log \\mathbb{P}(w_{t+j} \\mid w_t; \\bm{\\theta}) = -\\log \\frac{\\exp\\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} \\right)}{\\sum_{k=1}^{|V|} \\exp\\left( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} \\right)}\n$$\n<ol>\n<li>Si calcola il <strong>gradiente</strong> della loss rispetto a $\\bm{\\theta}$,</li>\n<li>Si aggiorna $\\bm{\\theta}$ secondo la regola standard dello SGD:</li>\n</ol>\n$$\n\\bm{\\theta} \\leftarrow \\bm{\\theta} - \\eta \\cdot \\nabla_{\\bm{\\theta}}\\mathcal{L}(w_{t+j}, w_t; \\bm{\\theta})\n$$\n<p>dove $\\eta$ è il learning rate.</p>\n<h3 id=\"calcolo-del-gradiente\">Calcolo del gradiente</h3>\n<p>Calcoliamo ora il gradiente della funzione di loss rispetto ai vettori di embedding coinvolti, assumendo sempre l’uso del softmax esatto.</p>\n<p>Fissiamo una singola coppia $(w_t, w_{t+j})$, cioè una parola centrale e una parola di contesto. La loss associata a questa coppia è:</p>\n$$\n\\underbrace{\\mathcal{L}_{(t,j)}}_{\\mathcal{L}(w_{t+j}, w_t; \\bm{\\theta})} = -\\log \\mathbb{P}(w_{t+j} \\mid w_t; \\bm{\\theta})\n= -\\log \\left( \\frac{\\exp\\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} \\right)}{\\sum_{k=1}^{|V|} \\exp\\left( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} \\right)} \\right)\n$$\n<p>Dove:</p>\n<ul>\n<li>$\\mathbf u_{w_t} \\in \\mathbb{R}^D$: vettore della parola <strong>centro</strong> (da $\\bm{\\theta}_W$),</li>\n<li>$\\mathbf v_{w_k} \\in \\mathbb{R}^D$: vettori delle parole <strong>contesto</strong> (da $\\bm{\\theta}_C$),</li>\n<li>$|V|$: dimensione del vocabolario.</li>\n</ul>\n<h4 id=\"gradiente-rispetto-al-vettore-della-parola-centro-math_inline_174\">Gradiente rispetto al vettore della parola centro $\\mathbf u_{w_t}$</h4>\n<p>Vogliamo calcolare il gradiente della loss rispetto al vettore centro $\\mathbf u_{w_t}$ per la coppia $(w_t, w_{t+j})$:</p>\n$$\n\\nabla_{\\mathbf u_{w_t}} \\mathcal{L}_{(t,j)} =\n- \\nabla_{\\mathbf u_{w_t}} \\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t}\n- \\log \\sum_{k=1}^{|V|} \\exp\\left( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} \\right) \\right)\n$$\n<ol>\n<li><strong>Derivata del primo termine</strong> (prodotto scalare):</li>\n</ol>\n$$\n  \\nabla_{\\mathbf u_{w_t}} \\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} \\right)\n  = \\mathbf v_{w_{t+j}}\n  $$\n<p>Motivo: la derivata di un prodotto scalare $\\mathbf a^\\top \\mathbf x$ rispetto a $\\mathbf x$ è $\\mathbf a$.</p>\n<ol>\n<li><strong>Derivata del secondo termine</strong> (log-somma-esponenziali + chain rule):</li>\n</ol>\n$$\n  \\nabla_{\\mathbf u_{w_t}} \\left( \\log \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\right)\n  $$\n<p><strong>Passo 1</strong> – Applichiamo la derivata del logaritmo:</p>\n$$\n  \\nabla_{\\mathbf u_{w_t}} \\log f(\\mathbf u_{w_t}) = \\frac{1}{f(\\mathbf u_{w_t})} \\cdot \\nabla_{\\mathbf u_{w_t}} f(\\mathbf u_{w_t})\n  $$\n<p>Dove $f(\\mathbf u_{w_t}) = \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} )$</p>\n<p><strong>Passo 2</strong> – Derivata della somma:</p>\n$$\n  \\nabla_{\\mathbf u_{w_t}} \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) = \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf v_{w_k}\n  $$\n<p><strong>Passo 3</strong> – Mettiamo tutto insieme:</p>\n$$\n  \\nabla_{\\mathbf u_{w_t}} \\log \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} )\n  = \\frac{ \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf v_{w_k} }\n  { \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) }\n  = \\sum_{k=1}^{|V|} \\mathbb{P}(w_k \\mid w_t) \\cdot \\mathbf v_{w_k}\n  $$\n<p>dove:</p>\n$$\n  \\mathbb{P}(w_k \\mid w_t) = \\frac{\\exp(\\mathbf v_{w_k} \\cdot \\mathbf u_{w_t})}{\\sum_{j=1}^{|V|} \\exp(\\mathbf v_{w_j} \\cdot \\mathbf u_{w_t})}\n  $$\n<p><strong>Combinazione</strong> dei due termini:</p>\n$$\n  \\nabla_{\\mathbf u_{w_t}} \\mathcal{L}_{(t,j)}\n  = - \\left( \\mathbf v_{w_{t+j}} - \\sum_{k=1}^{|V|} \\mathbb{P}(w_k \\mid w_t) \\cdot \\mathbf v_{w_k} \\right)\n  $$\n<h4 id=\"gradiente-rispetto-al-vettore-contesto-corretto-math_inline_181-con-math_inline_182\">Gradiente rispetto al vettore contesto corretto $\\mathbf v_{w_{k}}$ con $k = t + j$</h4>\n<p>Calcoliamo:</p>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\mathcal{L}_{(t,j)} =\n- \\nabla_{\\mathbf v_{w_{t+j}}} \\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t}\n- \\log \\sum_{i=1}^{|V|} \\exp( \\mathbf v_{w_i} \\cdot \\mathbf u_{w_t} ) \\right)\n$$\n<ol>\n<li><strong>Derivata del primo termine</strong>:</li>\n</ol>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\left( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} \\right)\n= \\mathbf u_{w_t}\n$$\n<ol>\n<li><strong>Derivata del secondo termine</strong>:</li>\n</ol>\n<p>Solo il termine $k = t+j$ dipende da $\\mathbf v_{w_{t+j}}$, ma deriviamo comunque la somma intera, trattando ogni termine:</p>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\log \\left( \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\right)\n= \\frac{1}{\\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} )} \\cdot \n\\nabla_{\\mathbf v_{w_{t+j}}} \\left( \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\right)\n$$\n<p>Solo il termine $k = t+j$ sopravvive:</p>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\left( \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\right) \n= \\nabla_{\\mathbf v_{w_{t+j}}} \\left( \\exp( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) \\right)\n= \\exp( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf u_{w_t}.\n$$\n<p>Mettendo quindi tutto insieme otteniamo:</p>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\log \\left( \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) \\right)\n=\n\\frac{ \\exp( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf u_{w_t} }\n{ \\sum_{k=1}^{|V|} \\exp( \\mathbf v_{w_k} \\cdot \\mathbf u_{w_t} ) }\n= \\mathbb{P}(w_{t+j} \\mid w_t) \\cdot \\mathbf u_{w_t}\n$$\n<p><strong>Combinazione</strong> dei due termini:</p>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\mathcal{L}_{(t,j)}\n= - \\left( \\mathbf u_{w_t} - \\mathbb{P}(w_{t+j} \\mid w_t) \\cdot \\mathbf u_{w_t} \\right)\n= \\left( \\mathbb{P}(w_{t+j} \\mid w_t) - 1 \\right) \\cdot \\mathbf u_{w_t}\n$$\n<h4 id=\"gradiente-rispetto-agli-altri-vettori-contesto-math_inline_186-con-math_inline_187\">Gradiente rispetto agli altri vettori contesto $\\mathbf v_{w_k}$ con $k \\ne t+j$</h4>\n<p>Sia la loss per la coppia $(w_t, w_{t+j})$:</p>\n$$\n\\mathcal{L}_{(t,j)}\n= -\\Bigl(\\mathbf v_{w_{t+j}}\\!\\cdot\\!\\mathbf u_{w_t}\\Bigr)\n  + \\log \\sum_{i=1}^{|V|} \\exp\\!\\bigl(\\mathbf v_{w_i}\\!\\cdot\\!\\mathbf u_{w_t}\\bigr).\n$$\n<p>Vogliamo calcolare \n$\\nabla_{\\mathbf v_{w_k}} \\mathcal{L}_{(t,j)}$\nper un indice $k\\neq t+j$.</p>\n<ol>\n<li><strong>Derivata del primo termine</strong>  </li>\n</ol>\n<p>Il <strong>primo termine</strong> dipende <strong>solo</strong> da $\\mathbf v_{w_{t+j}}$, non da $\\mathbf v_{w_k}$ quando $k\\ne t+j$.  </p>\n$$\n   \\nabla_{\\mathbf v_{w_k}}\n   \\bigl(\\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t}\\bigr)\n   = 0\n   \\quad\\text{per }k \\ne t+j.\n   $$\n<ol>\n<li><strong>Derivata del secondo termine</strong>  </li>\n</ol>\n<p>Il <strong>secondo termine</strong> è\n   $$\n   F(\\mathbf v_{w_i})\n   = \\log \\sum_{i=1}^{|V|} \\exp\\!\\bigl(\\mathbf v_{w_i} \\cdot \\mathbf u_{w_t}\\bigr).\n   $$</p>\n<ul>\n<li>\n<p><strong>Passo 2.1</strong>: applichiamo la derivata del logaritmo:\n     $$\n     \\nabla_{\\mathbf v_{w_k}}\\,F\n     = \\frac{1}{\\displaystyle \\sum_{i=1}^{|V|} \\exp(\\mathbf v_{w_i}\\!\\cdot\\!\\mathbf u_{w_t})}\n       \\;\\nabla_{\\mathbf v_{w_k}}\n       \\sum_{i=1}^{|V|} \\exp(\\mathbf v_{w_i}\\!\\cdot\\!\\mathbf u_{w_t}).\n     $$</p>\n</li>\n<li>\n<p><strong>Passo 2.2</strong>: derivata della somma di esponenziali. In questa somma, ogni termine indice $i$ è\n     $\\exp(\\mathbf v_{w_i}\\!\\cdot\\!\\mathbf u_{w_t})$. Solo quando $i=k$ l’esponenziale dipende da $\\mathbf v_{w_k}$.  </p>\n$$\n     \\nabla_{\\mathbf v_{w_k}}\n     \\sum_{i=1}^{|V|} \\exp(\\mathbf v_{w_i}\\!\\cdot\\!\\mathbf u_{w_t})\n     = \\nabla_{\\mathbf v_{w_k}}\n       \\exp(\\mathbf v_{w_k}\\!\\cdot\\!\\mathbf u_{w_t})\n     = \\exp(\\mathbf v_{w_k}\\!\\cdot\\!\\mathbf u_{w_t}) \\;\\mathbf u_{w_t}.\n     $$\n</li>\n<li>\n<p><strong>Passo 2.3</strong>: sostituiamo nella regola del log:\n     $$\n     \\nabla_{\\mathbf v_{w_k}}\\,F\n     = \\frac{\\exp(\\mathbf v_{w_k}\\!\\cdot\\!\\mathbf u_{w_t}) \\;\\mathbf u_{w_t}}\n            {\\displaystyle \\sum_{i=1}^{|V|} \\exp(\\mathbf v_{w_i}\\!\\cdot\\!\\mathbf u_{w_t})}\n     = \\mathbb{P}(w_k \\mid w_t)\\;\\mathbf u_{w_t}.\n     $$</p>\n</li>\n<li>\n<p><strong>Combinazione dei termini</strong>  </p>\n</li>\n</ul>\n<p>Sommando le due derivazioni (primo termine zero + secondo termine):</p>\n$$\n   \\nabla_{\\mathbf v_{w_k}} \\mathcal{L}_{(t,j)}\n   = 0 + \\mathbb{P}(w_k \\mid w_t)\\;\\mathbf u_{w_t}\n   = \\mathbb{P}(w_k \\mid w_t)\\;\\mathbf u_{w_t}.\n   $$\n<h4 id=\"riassunto-aggiornamenti\">Riassunto aggiornamenti</h4>\n<p>Per ogni coppia $(w_t, w_{t+j})$, aggiorniamo:</p>\n<ul>\n<li>Il vettore <strong>centro</strong> $\\mathbf u_{w_t}$ secondo:</li>\n</ul>\n$$\n  \\mathbf u_{w_t} \\leftarrow \\mathbf u_{w_t} - \\eta \\cdot \\nabla_{\\mathbf u_{w_t}} \\mathcal{L}_{(t,j)}\n  $$\n<ul>\n<li>Il vettore <strong>contesto corretto</strong> $\\mathbf v_{w_{t+j}}$ secondo:</li>\n</ul>\n$$\n  \\mathbf v_{w_{t+j}} \\leftarrow \\mathbf v_{w_{t+j}} - \\eta \\cdot \\nabla_{\\mathbf v_{w_{t+j}}} \\mathcal{L}_{(t,j)}\n  $$\n<ul>\n<li>Gli altri vettori <strong>contesto</strong> $\\mathbf v_{w_k}$ con $k \\ne t+j$, opzionalmente:</li>\n</ul>\n$$\n  \\mathbf v_{w_k} \\leftarrow \\mathbf v_{w_k} - \\eta \\cdot \\nabla_{\\mathbf v_{w_k}} \\mathcal{L}_{(t,j)}\n  $$\n<p>In pratica, si usa <strong>Negative Sampling</strong> per evitare l&rsquo;aggiornamento su tutto il vocabolario.</p>\n<h3 id=\"negative-sampling\">Negative Sampling</h3>\n<p>L’obiettivo del <strong>Negative Sampling</strong> è approssimare in modo efficiente la funzione di perdita originale basata sulla softmax, che richiede una somma su tutto il vocabolario $|V|$ — troppo costosa per vocabolari grandi.</p>\n<p>Invece di calcolare la probabilità normalizzata per tutte le parole, si trasforma il problema in una <strong>serie di classificazioni binarie</strong>.</p>\n<h4 id=\"strategia\">Strategia</h4>\n<ul>\n<li>La coppia <strong>positiva</strong> $(w_t, w_{t+j})$ (parola centrale e parola di contesto reale) è trattata come un esempio <strong>positivo</strong>, con <strong>target = 1</strong>.</li>\n<li>Si campionano $K$ parole <strong>negative</strong> $w_1', \\dots, w_K'$ da una distribuzione rumorosa (noise distribution), e si trattano come esempi <strong>negativi</strong>, con <strong>target = 0</strong>.</li>\n</ul>\n<h4 id=\"notazione\">Notazione</h4>\n<ul>\n<li>$\\mathbf u_{w_t}$: vettore embedding della parola centrale (input)</li>\n<li>$\\mathbf v_{w_{t+j}}$: embedding della parola di contesto positiva (output)</li>\n<li>$\\mathbf v_{w_k'}$: embedding delle parole negative</li>\n<li>$\\sigma(x) = \\frac{1}{1 + e^{-x}}$: funzione sigmoide</li>\n</ul>\n<h4 id=\"loss-per-una-singola-coppia-math_inline_211-e-math_inline_212-parole-negative\">Loss per una singola coppia $(w_t, w_{t+j})$ e $K$ parole negative:</h4>\n$$\n\\mathcal{L}_{\\text{NS}}^{(t,j)} =\n- \\log \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} )\n- \\sum_{k=1}^K \\log \\sigma( - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} )\n$$\n<ul>\n<li>Il primo termine spinge $\\mathbf u_{w_t}$ e $\\mathbf v_{w_{t+j}}$ ad avere un <strong>prodotto scalare alto</strong>, quindi un’alta probabilità.</li>\n<li>Il secondo termine penalizza $\\mathbf u_{w_t}$ e i vettori negativi $\\mathbf v_{w_k'}$ se il loro prodotto scalare è troppo alto.</li>\n</ul>\n<h3 id=\"calcolo-dei-gradienti\">Calcolo dei Gradienti</h3>\n<p>La loss per una singola coppia $(w_t, w_{t+j})$ e $K$ parole negative è:</p>\n$$\n\\mathcal{L}_{\\text{NS}}^{(t,j)} =\n- \\log \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} )\n- \\sum_{k=1}^K \\log \\sigma( - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} )\n$$\n<p>Dove:\n- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n- La derivata della sigmoide: $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$</p>\n<h4 id=\"1-derivata-rispetto-a-math_inline_221-embedding-della-parola-centrale\">1. Derivata rispetto a $\\mathbf u_{w_t}$ (embedding della parola centrale)</h4>\n<p>Partiamo dalla derivata della loss rispetto a $\\mathbf u_{w_t}$:</p>\n$$\n\\nabla_{\\mathbf u_{w_t}} \\mathcal{L}_{\\text{NS}}^{(t,j)} =\n\\frac{\\partial}{\\partial \\mathbf u_{w_t}} \\left(\n- \\log \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} )\n- \\sum_{k=1}^K \\log \\sigma( - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} )\n\\right)\n$$\n<h5 id=\"primo-termine-positivo\">Primo termine (positivo):</h5>\n$$\n\\frac{\\partial}{\\partial \\mathbf u_{w_t}} \\left[ - \\log \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) \\right]\n$$\n<p>Applichiamo la chain rule:</p>\n<ol>\n<li>$x = \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t}$  </li>\n<li>\n<p>$\\frac{d}{dx}[-\\log \\sigma(x)] = - \\frac{\\sigma'(x)}{\\sigma(x)} = - (1 - \\sigma(x))$</p>\n</li>\n<li>\n<p>$\\frac{\\partial x}{\\partial \\mathbf u_{w_t}} = \\mathbf v_{w_{t+j}}$</p>\n</li>\n</ol>\n<p>Quindi:</p>\n$$\n= - (1 - \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} )) \\cdot \\mathbf v_{w_{t+j}} \n= ( \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) - 1 ) \\cdot \\mathbf v_{w_{t+j}}\n$$\n<h5 id=\"secondo-termine-negativi\">Secondo termine (negativi):</h5>\n<p>Ogni termine nella somma:</p>\n$$\n\\frac{\\partial}{\\partial \\mathbf u_{w_t}} \\left[ - \\log \\sigma( - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\right]\n$$\n<p>Applichiamo la regola della catena:</p>\n<ol>\n<li>$x = - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t}$  </li>\n<li>$\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))$  </li>\n<li>$\\frac{d}{dx}[-\\log \\sigma(x)] = - (1 - \\sigma(x))$</li>\n</ol>\n<p>Ma attenzione: deriviamo rispetto a $\\mathbf u_{w_t}$, quindi:</p>\n$$\n\\frac{\\partial}{\\partial \\mathbf u_{w_t}} \\left[ - \\log \\sigma( - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\right]\n= \\sigma( \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf v_{w_k'}\n$$\n<p>Perché:</p>\n<ul>\n<li>$\\sigma(-x) = 1 - \\sigma(x)$</li>\n<li>$\\frac{d}{dx}[-\\log(1 - \\sigma(x))] = \\sigma(x)$</li>\n</ul>\n<p>Sommiamo su $k$:</p>\n$$\n\\sum_{k=1}^K \\sigma( \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf v_{w_k'}\n$$\n<h5 id=\"totale\">Totale:</h5>\n$$\n\\nabla_{\\mathbf u_{w_t}} \\mathcal{L}_{\\text{NS}}^{(t,j)} =\n( \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) - 1 ) \\cdot \\mathbf v_{w_{t+j}} +\n\\sum_{k=1}^K \\sigma( \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf v_{w_k'}\n$$\n<h4 id=\"2-derivata-rispetto-a-math_inline_233-embedding-del-contesto-positivo\">2. Derivata rispetto a $\\mathbf v_{w_{t+j}}$ (embedding del contesto positivo)</h4>\n$$\n\\frac{\\partial}{\\partial \\mathbf v_{w_{t+j}}} \\left[ - \\log \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) \\right]\n$$\n<p>Stesso ragionamento:</p>\n<ol>\n<li>$x = \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t}$</li>\n<li>$\\frac{d}{dx}[-\\log \\sigma(x)] = - (1 - \\sigma(x))$</li>\n</ol>\n<p>Derivata rispetto a $\\mathbf v_{w_{t+j}}$:</p>\n$$\n= ( \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) - 1 ) \\cdot \\mathbf u_{w_t}\n$$\n<p>Nella somma non compare l&rsquo;embedding del contesto positivo. Quindi la derivata rispetto a $\\mathbf v_{w_{t+j}}$ è nulla.</p>\n<h4 id=\"3-derivata-rispetto-ad-ogni-math_inline_238-embedding-delle-parole-negative\">3. Derivata rispetto ad ogni $\\mathbf v_{w_k'}$ (embedding delle parole negative)</h4>\n$$\n\\frac{\\partial}{\\partial \\mathbf v_{w_k'}} \\left[ - \\log \\sigma( - \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\right]\n$$\n<p>Come sopra:</p>\n<ul>\n<li>$\\sigma(-x)' = - \\sigma(x)(1 - \\sigma(x))$</li>\n<li>$- \\log \\sigma(-x) = - \\log (1 - \\sigma(x))$</li>\n</ul>\n<p>Quindi:</p>\n$$\n= \\sigma( \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf u_{w_t}\n$$\n<h4 id=\"riepilogo-dei-gradienti\">Riepilogo dei Gradienti</h4>\n<ul>\n<li><strong>Parola centrale $\\mathbf u_{w_t}$</strong>:</li>\n</ul>\n$$\n\\nabla_{\\mathbf u_{w_t}} \\mathcal{L}_{\\text{NS}}^{(t,j)}=\n( \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) - 1 ) \\cdot \\mathbf v_{w_{t+j}} \n+ \\sum_{k=1}^K \\sigma( \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf v_{w_k'}\n$$\n<ul>\n<li><strong>Parola positiva $\\mathbf v_{w_{t+j}}$</strong>:</li>\n</ul>\n$$\n\\nabla_{\\mathbf v_{w_{t+j}}} \\mathcal{L}_{\\text{NS}}^{(t,j)} =\n( \\sigma( \\mathbf v_{w_{t+j}} \\cdot \\mathbf u_{w_t} ) - 1 ) \\cdot \\mathbf u_{w_t}\n$$\n<ul>\n<li><strong>Parola negativa $\\mathbf v_{w_k'}$</strong>:</li>\n</ul>\n$$\n\\nabla_{\\mathbf v_{w_k'}} \\mathcal{L}_{\\text{NS}}^{(t,j)} =\n\\sigma( \\mathbf v_{w_k'} \\cdot \\mathbf u_{w_t} ) \\cdot \\mathbf u_{w_t}\n$$\n<h3 id=\"intuizione-finale\">Intuizione Finale</h3>\n<ul>\n<li>Il gradiente rispetto alla parola <strong>positiva</strong> cerca di <strong>avvicinare</strong> gli embedding $\\mathbf u_{w_t}$ e $\\mathbf v_{w_{t+j}}$.</li>\n<li>I gradienti rispetto alle parole <strong>negative</strong> cercano di <strong>allontanare</strong> $\\mathbf u_{w_t}$ da $\\mathbf v_{w_k'}$, se sono troppo simili.</li>\n<li>In questo modo, la rete impara a <strong>distinguerle</strong> — creando spazi semantici utili per rappresentare significato e contesto.</li>\n</ul>\n<h3 id=\"perche-il-negative-sampling-funziona\">Perché il Negative Sampling funziona?</h3>\n<p>In un training tradizionale con softmax, ogni parola nel vocabolario è considerata in ogni update: inefficiente e inutile, perché la maggior parte delle parole <strong>non sono rilevanti</strong> nel contesto dato.</p>\n<p>Il negative sampling funziona bene <strong>anche campionando solo poche parole negative</strong> perché:</p>\n<ul>\n<li>La maggior parte delle parole nel vocabolario <strong>non appaiono nel contesto locale</strong>. È sufficiente penalizzarne alcune per rappresentare questo &ldquo;mare di parole irrilevanti&rdquo;.</li>\n<li>L’aggiornamento stocastico su $K$ parole negative scelte a caso <strong>approssima il gradiente medio</strong> su tutte le parole negative.</li>\n<li>Il modello impara a <strong>differenziare le parole &ldquo;giuste&rdquo; da quelle &ldquo;sbagliate&rdquo;</strong>, non a predire ogni parola nel vocabolario.</li>\n<li>Inoltre, campionando le negative da una distribuzione “disturbata” (es. proporzionale a $P(w)^{3/4}$), si aumenta l’efficacia dei campioni più informativi.</li>\n</ul>\n<p>➡️ <strong>In sintesi</strong>: invece di imparare su tutto il vocabolario, impariamo da un campione ben scelto. L&rsquo;efficienza migliora enormemente senza perdita significativa in qualità. Perché, in effetti, ad ogni iterazione ci interessa molto di più la relazione tra parola <strong>centro</strong> e <strong>contesto</strong> che quella tra parola <strong>centro</strong> e parole <strong>non-contesto</strong>.</p>\n<h3 id=\"negative-sampling-vs-softmax-differenze-chiave\">Negative Sampling vs Softmax: differenze chiave</h3>\n<p>Con il <strong>Negative Sampling</strong> non si calcola più una vera distribuzione di probabilità normalizzata su tutto il vocabolario, come avviene con la softmax classica.</p>\n<ul>\n<li>\n<p><strong>Softmax classico:</strong><br />\n  Calcola la probabilità che una parola sia nel contesto dato il centro, considerando <em>tutte</em> le parole del vocabolario. Questo è costoso ma produce una distribuzione completa.</p>\n</li>\n<li>\n<p><strong>Negative Sampling:</strong><br />\n  Trasforma il problema in una serie di classificazioni binarie:  </p>\n</li>\n<li>Le coppie (parola centro, parola contesto reale) sono esempi positivi.  </li>\n<li>Le coppie con parole negative campionate casualmente sono esempi negativi.  </li>\n</ul>\n<p>Il modello impara a distinguere parole di contesto “vere” da parole “false”, ma non produce una distribuzione completa su tutte le parole.</p>\n<p>Con Negative Sampling, il modello cerca invece una funzione che spinge gli embeddings di coppie (centro, positivo) a essere simili (dot product alto) e quelli (centro, negativi) a essere dissimili (dot product basso).</p>\n<p><strong>In pratica:</strong><br />\nNegative Sampling ottimizza l’efficienza concentrandosi solo su alcune parole negative per update, ma non fornisce probabilità normalizzate su tutto il vocabolario come la softmax.</p>\n<p>Se servono probabilità vere, si usano softmax o sue varianti (hierarchical softmax), ma a costo computazionale maggiore.</p>\n<h3 id=\"effetto-dellottimizzazione\">Effetto dell’ottimizzazione</h3>\n<p>Iterando su molte coppie $(w_t, w_{t+j})$ osservate dal corpus, il modello:</p>\n<ul>\n<li>rafforza le associazioni tra centri e contesti frequenti (es. “eat” → “food”),</li>\n<li>indebolisce associazioni tra parole che non co-occorrono.</li>\n</ul>\n<p>Alla convergenza, gli embedding $\\bm{\\theta}_W$ e $\\bm{\\theta}_C$ riflettono <strong>strutture semantiche</strong> e <strong>sintattiche</strong> apprese dai dati: parole con significati simili finiscono in regioni vicine dello spazio vettoriale.</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>Il modello <strong>Skip-gram con softmax</strong> rappresenta un approccio fondamentale nell&rsquo;ambito dell&rsquo;apprendimento non supervisionato per la rappresentazione distribuita delle parole. Utilizzando due matrici distinte — una per le parole <em>centro</em> e una per le parole <em>contesto</em> — il modello riesce a catturare in modo più preciso le relazioni semantiche e sintattiche nel linguaggio naturale.</p>\n<p>Questa separazione consente di modellare efficacemente le <strong>asimmetrie</strong> e i <strong>ruoli funzionali</strong> delle parole, migliorando la qualità degli embedding e le prestazioni in numerosi compiti downstream come il POS tagging, il parsing o il semantic similarity.</p>\n<p>La formulazione probabilistica basata su <strong>softmax</strong> permette di interpretare le previsioni come distribuzioni categoriali su tutto il vocabolario, sebbene a un costo computazionale elevato. Questo ha motivato lo sviluppo di tecniche più efficienti come il <strong>Negative Sampling</strong> e la <strong>Hierarchical Softmax</strong>, che estendono il framework Skip-gram per corpus di grandi dimensioni.</p>\n<h3 id=\"risorse-utili-e-approfondimenti\">Risorse utili e approfondimenti</h3>\n<ul>\n<li>\n<p>Dan Jurafsky &amp; James H. Martin, <em>Speech and Language Processing</em>, 3rd Edition (draft):<br />\n  https://web.stanford.edu/~jurafsky/slp3/  </p>\n</li>\n<li>\n<p>Mikolov et al. (2013), <em>Efficient Estimation of Word Representations in Vector Space</em><br />\n<a href=\"https://arxiv.org/abs/1301.3781\">https://arxiv.org/abs/1301.3781</a></p>\n</li>\n<li>\n<p>Goldberg &amp; Levy (2014), <em>word2vec Explained: Deriving Mikolov et al.&rsquo;s Negative-Sampling Word-Embedding Method</em><br />\n<a href=\"https://arxiv.org/abs/1402.3722\">https://arxiv.org/abs/1402.3722</a></p>\n</li>\n<li>\n<p>TensorFlow Tutorial: <em>Word2Vec Skip-gram</em><br />\n<a href=\"https://www.tensorflow.org/tutorials/text/word2vec\">https://www.tensorflow.org/tutorials/text/word2vec</a></p>\n</li>\n<li>\n<p>Chris McCormick, <em>Word2Vec Tutorial</em> (con codice e spiegazioni passo-passo)<br />\n<a href=\"https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></p>\n</li>\n<li>\n<p>Blog di Jay Alammar, <em>The Illustrated Word2Vec</em><br />\n<a href=\"https://jalammar.github.io/illustrated-word2vec/\">https://jalammar.github.io/illustrated-word2vec/</a></p>\n</li>\n</ul>\n<p>Questa panoramica costituisce la base concettuale per affrontare estensioni più sofisticate e ottimizzazioni del modello, fondamentali per lavorare con corpus molto ampi o con vocabolari di grandi dimensioni.</p>"
}