{
  "title": "Interpolazione Lineare nei Modelli Linguistici",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>L&rsquo;interpolazione lineare è una tecnica utilizzata per affrontare il problema degli $n$-grammi a frequenza zero. Se stiamo cercando di calcolare $P(w_n \\mid w_{n-2}\\, w_{n-1})$ e non abbiamo esempi per il trigramma $w_{n-2}\\, w_{n-1}\\, w_n$, possiamo stimarne la probabilità utilizzando quella del bigramma $P(w_n \\mid w_{n-1})$. Analogamente, se non disponiamo di conteggi per il bigramma, possiamo ricorrere alla probabilità dell&rsquo;unigramma $P(w_n)$. In altre parole, talvolta utilizzare meno contesto può aiutarci a generalizzare in modo più efficace per quei contesti di cui il modello ha appreso meno.</p>\n<h2 id=\"interpolazione-lineare\">Interpolazione Lineare</h2>\n<p>La maniera più comune per sfruttare questa gerarchia di $n$-grammi è definita <strong>interpolazione</strong>: una nuova probabilità viene calcolata combinando (pesando e sommando) le probabilità ottenute dai trigrammi, bigrammi e unigrammi.</p>\n<h3 id=\"interpolazione-lineare-semplice\">Interpolazione Lineare Semplice</h3>\n<p>In un&rsquo;interpolazione lineare semplice, le probabilità dei modelli di ordine inferiore vengono combinate linearmente. La stima della probabilità del trigramma viene espressa come:</p>\n$$\n\\hat{P}(w_n \\mid w_{n-2}\\, w_{n-1}) = \\lambda_1\\, P(w_n) + \\lambda_2\\, P(w_n \\mid w_{n-1}) + \\lambda_3\\, P(w_n \\mid w_{n-2}\\, w_{n-1})\n$$\n<p>dove i pesi $\\lambda_1$, $\\lambda_2$ e $\\lambda_3$ devono soddisfare la condizione:</p>\n$$\n\\lambda_1 + \\lambda_2 + \\lambda_3 = 1.\n$$\n<p>Questo rende l&rsquo;equazione equivalente a una media pesata.</p>\n<h3 id=\"interpolazione-lineare-condizionata-sul-contesto\">Interpolazione Lineare Condizionata sul Contesto</h3>\n<p>In questo approccio, i pesi dell&rsquo;interpolazione non sono costanti ma diventano funzioni del contesto, cioè delle parole precedenti. Per un modello trigramma, la stima della probabilità del termine $w_n$ data la sequenza $w_{n-2}, w_{n-1}$ è data da:</p>\n$$\n\\hat{P}(w_n \\mid w_{n-2}, w_{n-1}) = \\lambda_1(w_{n-2}, w_{n-1})\\, P(w_n) + \\lambda_2(w_{n-2}, w_{n-1})\\, P(w_n \\mid w_{n-1}) + \\lambda_3(w_{n-2}, w_{n-1})\\, P(w_n \\mid w_{n-2}, w_{n-1})\n$$\n<p>dove:\n- <strong>$P(w_n)$</strong> è la probabilità unigrama del termine $w_n$,\n- <strong>$P(w_n \\mid w_{n-1})$</strong> è la probabilità condizionata data dal modello bigramma,\n- <strong>$P(w_n \\mid w_{n-2}, w_{n-1})$</strong> è la probabilità condizionata data dal modello trigramma.</p>\n<h4 id=\"proprieta-delle-funzioni-di-peso-math_inline_27\">Proprietà delle Funzioni di Peso $\\lambda_i$</h4>\n<p>Le funzioni di peso, $\\lambda_1$, $\\lambda_2$ e $\\lambda_3$, devono soddisfare le seguenti proprietà:</p>\n<ol>\n<li>\n<p><strong>Normalizzazione</strong>:<br />\n   Per ogni contesto $(w_{n-2}, w_{n-1})$, la somma dei pesi deve essere 1:\n   $$\n   \\lambda_1(w_{n-2}, w_{n-1}) + \\lambda_2(w_{n-2}, w_{n-1}) + \\lambda_3(w_{n-2}, w_{n-1}) = 1.\n   $$</p>\n</li>\n<li>\n<p><strong>Non-negatività</strong>:<br />\n   Per ogni contesto, ogni peso è non negativo:\n   $$\n   \\lambda_i(w_{n-2}, w_{n-1}) \\geq 0 \\quad \\text{per } i = 1, 2, 3.\n   $$</p>\n</li>\n<li>\n<p><strong>Adattamento al Contesto</strong>:<br />\n   I pesi variano in base all’affidabilità delle stime fornite da ciascun modello nel contesto specifico. Ad esempio:</p>\n</li>\n<li>Se il contesto $w_{n-2}, w_{n-1}$ è frequente e quindi il trigramma è ben rappresentato, è preferibile che $\\lambda_3(w_{n-2}, w_{n-1})$ sia elevato.</li>\n<li>Se il trigramma è scarso o assente, il modello potrebbe ridurre il peso di $\\lambda_3$ e aumentare quello di $\\lambda_2$ (bigramma) o di $\\lambda_1$ (unigramma).</li>\n</ol>\n<h4 id=\"possibili-schemi-per-la-definizione-delle-math_inline_37\">Possibili Schemi per la Definizione delle $\\lambda_i$</h4>\n<p>I pesi condizionati sul contesto possono essere definiti mediante formule che tengono conto dei conteggi relativi ai contesti. Ad esempio, si possono usare delle funzioni basate sulla frequenza osservata dei contesti:</p>\n<ul>\n<li><strong>Ponderazione Basata sui Conteggi</strong>:</li>\n</ul>\n<p>Un possibile schema per impostare $\\lambda_3$, che rappresenta il peso del trigramma, è:\n  $$\n  \\lambda_3(w_{n-2}, w_{n-1}) = \\frac{N(w_{n-2}, w_{n-1})}{N(w_{n-2}, w_{n-1}) + \\gamma},\n  $$\n  dove:\n  - $N(w_{n-2}, w_{n-1})$ è il numero di occorrenze del contesto trigramma,\n  - $\\gamma > 0$ è un iperparametro che regola la sensibilità al conteggio.</p>\n<ul>\n<li><strong>Distribuzione dei Pesi Rimanenti</strong>:</li>\n</ul>\n<p>Dopo aver determinato $\\lambda_3$, i pesi per il bigramma e l&rsquo;unigramma possono essere assegnati in modo proporzionale all’affidabilità dei loro contesti:\n  $$\n  \\lambda_2(w_{n-2}, w_{n-1}) = \\left(1 - \\lambda_3(w_{n-2}, w_{n-1})\\right) \\frac{N(w_{n-1})}{N(w_{n-1}) + \\delta},\n  $$\n  e\n  $$\n  \\lambda_1(w_{n-2}, w_{n-1}) = 1 - \\lambda_3(w_{n-2}, w_{n-1}) - \\lambda_2(w_{n-2}, w_{n-1}),\n  $$\n  dove:\n  - $N(w_{n-1})$ è il numero di occorrenze del contesto bigramma (ossia la parola precedente $w_{n-1}$),\n  - $\\delta > 0$ è un ulteriore iperparametro.</p>\n<p>Queste formulazioni sono solo esempi e possono essere adattate o raffinate in base alla specifica applicazione o alla quantità e qualità del corpus a disposizione.</p>\n<p>In sintesi, l&rsquo;interpolazione lineare condizionata sul contesto permette di adattare in maniera dinamica e flessibile l&rsquo;importanza delle stime ottenute dai modelli di diversa granularità, sfruttando informazioni specifiche del contesto. Ciò contribuisce a migliorare la robustezza del modello, soprattutto quando i dati sono scarsi o la frequenza dei contesti varia significativamente.</p>\n<h2 id=\"apprendimento-dei-pesi-math_inline_45-interpolazione-lineare-semplice\">Apprendimento dei Pesi $\\lambda_i$ (Interpolazione Lineare Semplice)</h2>\n<p>I valori dei pesi $\\lambda_i$ sono generalmente appresi da un <strong>corpus di validazione</strong> (detto anche corpus &ldquo;held-out&rdquo; o di tuning). Il procedimento è il seguente:</p>\n<ul>\n<li><strong>Fissare le probabilità degli $n$-grammi:</strong> Vengono calcolate sulle sole parti del corpus di training.</li>\n<li>\n<p><strong>Ottimizzare i pesi:</strong> Si cercano i valori dei $\\lambda_i$ che massimizzano la probabilità del corpus di validazione, ovvero:</p>\n</li>\n<li>\n<p>Si fissa il modello (le probabilità degli $n$-grammi) calcolato dal training set.</p>\n</li>\n<li>Si utilizzano algoritmi come l&rsquo;<strong>EM (Expectation-Maximization)</strong> per trovare i valori $\\lambda_i$ ottimali che, una volta inseriti in una formula come quella sopra, danno il massimo della verosimiglianza al corpus di tuning.</li>\n</ul>\n<p>Questo approccio permette di apprendere in modo efficace quanto peso assegnare a ciascun ordine di $n$-grammi, garantendo al contempo una migliore generalizzazione per eventi rari o mai osservati.</p>\n<h3 id=\"obiettivo-massimizzare-la-verosimiglianza\">Obiettivo: Massimizzare la Verosimiglianza</h3>\n<p>L’obiettivo è trovare i pesi $\\lambda_1, \\lambda_2, \\lambda_3$ tali che:</p>\n$$\n\\lambda_1 + \\lambda_2 + \\lambda_3 = 1 \\quad \\text{e} \\quad \\lambda_i \\geq 0\n$$\n<p>e che massimizzano la log-verosimiglianza del corpus di validazione (held-out set):</p>\n$$\n\\mathcal{L}(\\boldsymbol{\\lambda}) = \\sum_{\\text{trigrammi } w_{n-2}^{n}} c(w_{n-2}^{n}) \\cdot \\log\\left( \\lambda_1\\, P(w_n) + \\lambda_2\\, P(w_n \\mid w_{n-1}) + \\lambda_3\\, P(w_n \\mid w_{n-2}\\, w_{n-1}) \\right)\n$$\n<p>dove $c(w_{n-2}^{n})$ è la frequenza del trigramma nel corpus di validazione.</p>\n<h3 id=\"vincoli\">Vincoli</h3>\n<p>L&rsquo;ottimizzazione deve rispettare:\n- $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$\n- $\\lambda_i \\geq 0$ per ogni $i$</p>\n<h3 id=\"tecniche-di-ottimizzazione\">Tecniche di Ottimizzazione</h3>\n<h4 id=\"1-grid-search\">1. Grid Search</h4>\n<p>Metodo semplice ma computazionalmente costoso:\n- Si esplora uno spazio discreto di valori $\\lambda_i$ che soddisfano i vincoli.\n- Si calcola la verosimiglianza per ciascuna combinazione.\n- Si sceglie la combinazione con log-verosimiglianza massima.</p>\n<h4 id=\"2-algoritmo-em-expectation-maximization\">2. Algoritmo EM (Expectation-Maximization)</h4>\n<p>L&rsquo;EM è un metodo iterativo che converge verso un massimo locale della verosimiglianza. Nel contesto dell&rsquo;interpolazione:</p>\n<p><strong>E-step:</strong><br />\nPer ogni trigramma osservato, calcolare la responsabilità (cioè la probabilità che ciascun modello abbia generato il trigramma):</p>\n$$\nr_i(w_{n-2}^{n}) = \\frac{\\lambda_i\\, P_i(w_n \\mid \\cdot)}{\\sum_{j=1}^{3} \\lambda_j\\, P_j(w_n \\mid \\cdot)}\n$$\n<p><strong>M-step:</strong><br />\nAggiornare ciascun $\\lambda_i$:</p>\n$$\n\\lambda_i^{(t+1)} = \\frac{\\sum_{w_{n-2}^{n}} c(w_{n-2}^{n})\\, r_i(w_{n-2}^{n})}{\\sum_{w_{n-2}^{n}} c(w_{n-2}^{n})}\n$$\n<p>Iterare fino a convergenza.</p>\n<h4 id=\"3-ottimizzazione-vincolata-es-lagrangiana\">3. Ottimizzazione Vincolata (es. Lagrangiana)</h4>\n<p>Si può anche porre il problema come ottimizzazione vincolata:</p>\n<ul>\n<li><strong>Funzione obiettivo:</strong> $\\mathcal{L}(\\boldsymbol{\\lambda})$ come sopra</li>\n<li><strong>Vincoli:</strong> </li>\n<li>$\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$</li>\n<li>$\\lambda_i \\geq 0$</li>\n</ul>\n<p>Questo si risolve tramite:\n- <strong>Metodo dei moltiplicatori di Lagrange</strong>\n- <strong>Algoritmi di programmazione convessa</strong> (es. <code>scipy.optimize.minimize</code> con vincoli)</p>\n<h3 id=\"output\">Output</h3>\n<p>L’ottimizzazione fornisce i pesi $\\lambda_i$ da usare nel modello interpolato, migliorando l’accuratezza predittiva del modello linguistico sui dati non visti.</p>\n<h2 id=\"considerazioni-finali\">Considerazioni Finali</h2>\n<p>L&rsquo;interpolazione lineare consente di:\n- Combinare le informazioni provenienti da diverse granualità di contesto.\n- Mitigare il problema degli $n$-grammi a frequenza zero sfruttando le stime di ordine inferiore.\n- Personalizzare l&rsquo;importanza dei diversi ordini di $n$-grammi in base alla qualità e alla quantità dei dati disponibili, mediante l&rsquo;apprendimento dei pesi $\\lambda_i$ da un corpus di validazione.</p>\n<p>Questa tecnica rappresenta una strategia fondamentale nella modellazione del linguaggio, in particolare per applicazioni in cui la robustezza e la generalizzazione sono cruciali.</p>"
}