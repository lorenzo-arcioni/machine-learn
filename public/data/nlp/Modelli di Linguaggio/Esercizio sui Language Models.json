{
  "title": "🧠 Esercizio: Costruzione di un Modello Bigramma",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>Considera il seguente corpus $C$ composto da 5 frasi:</p>\n<ol>\n<li><code>I am Sam</code>  </li>\n<li><code>Sam I am</code>  </li>\n<li><code>Sam I like</code>  </li>\n<li><code>Sam I do like</code>  </li>\n<li><code>do I like Sam</code></li>\n</ol>\n<h2 id=\"domanda-1-definire-il-modello-di-bigrammi-sul-corpus-math_inline_17\">🔸 Domanda 1: Definire il Modello di Bigrammi sul Corpus $C$</h2>\n<h3 id=\"step-1-aggiunta-dei-tag-di-inizio-e-fine-frase\">Step 1: Aggiunta dei tag di inizio e fine frase</h3>\n<p>A ogni frase si aggiungono i tag speciali <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code>:</p>\n<ul>\n<li><code>&lt;s&gt; I am Sam &lt;/s&gt;</code>  </li>\n<li><code>&lt;s&gt; Sam I am &lt;/s&gt;</code>  </li>\n<li><code>&lt;s&gt; Sam I like &lt;/s&gt;</code>  </li>\n<li><code>&lt;s&gt; Sam I do like &lt;/s&gt;</code>  </li>\n<li><code>&lt;s&gt; do I like Sam &lt;/s&gt;</code></li>\n</ul>\n<p><strong>Vocabolario (closed vocabulary):</strong> {<code>&lt;s&gt;</code>, <code>I</code>, <code>am</code>, <code>Sam</code>, <code>like</code>, <code>do</code>, <code>&lt;/s&gt;</code>}</p>\n<h3 id=\"step-2-costruzione-del-modello-di-bigramma\">Step 2: Costruzione del Modello di Bigramma</h3>\n<p>Prima di tutto, realizziamo il vettore di conteggio degli unigrammi:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><code>&lt;s&gt;</code></th>\n<th>I</th>\n<th>am</th>\n<th>Sam</th>\n<th>like</th>\n<th>do</th>\n<th><code>&lt;/s&gt;</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>conteggio</strong></td>\n<td>5</td>\n<td>5</td>\n<td>2</td>\n<td>5</td>\n<td>3</td>\n<td>2</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<p>Creiamo ora la matrice di conteggio dei bigrammi. In un bigramma $w_{i-1} w_i$, ogni riga rappresenta $w_{i-1}$ mentre ogni colonna rappresenta $w_i$.</p>\n<table>\n<thead>\n<tr>\n<th>$w_{i-1} \\backslash w_i$</th>\n<th><code>&lt;s&gt;</code></th>\n<th>I</th>\n<th>am</th>\n<th>Sam</th>\n<th>like</th>\n<th>do</th>\n<th><code>&lt;/s&gt;</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt;</code></td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>3</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>I</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n<td>0</td>\n<td>2</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>am</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Sam</td>\n<td>0</td>\n<td>3</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>like</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>do</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>Per ottenere la matrice di probabilità condizionata $P(w_i \\mid w_{i-1})$, normalizziamo ciascun conteggio di bigramma per il totale degli unigrammi nella riga corrispondente (cioè $c(w_{i-1})$):</p>\n$$\nP(w_i \\mid w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n$$\n<table>\n<thead>\n<tr>\n<th></th>\n<th><code>&lt;s&gt;</code></th>\n<th>I</th>\n<th>am</th>\n<th>Sam</th>\n<th>like</th>\n<th>do</th>\n<th><code>&lt;/s&gt;</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt;</code></td>\n<td>$0$</td>\n<td>$\\frac{1}{5}$</td>\n<td>$0$</td>\n<td>$\\frac{3}{5}$</td>\n<td>$0$</td>\n<td>$\\frac{1}{5}$</td>\n<td>$0$</td>\n</tr>\n<tr>\n<td>I</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{2}{5}$</td>\n<td>$0$</td>\n<td>$\\frac{2}{5}$</td>\n<td>$\\frac{1}{5}$</td>\n<td>$0$</td>\n</tr>\n<tr>\n<td>am</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{1}{2}$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{1}{2}$</td>\n</tr>\n<tr>\n<td>Sam</td>\n<td>$0$</td>\n<td>$\\frac{3}{5}$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{2}{5}$</td>\n</tr>\n<tr>\n<td>like</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{1}{3}$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{2}{3}$</td>\n</tr>\n<tr>\n<td>do</td>\n<td>$0$</td>\n<td>$\\frac{1}{2}$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$\\frac{1}{2}$</td>\n<td>$0$</td>\n<td>$0$</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n<td>$0$</td>\n</tr>\n</tbody>\n</table>\n<p>📌 <strong>Nota:</strong> Ogni riga rappresenta la distribuzione di probabilità $P(w_i \\mid w_{i-1})$, e la somma dei valori di ogni riga (dove $c(w_{i-1}) > 0$) è pari a 1.</p>\n<p>Abbiamo ora un modello completo di bigrammi normalizzato che possiamo utilizzare per:</p>\n<ul>\n<li><strong>Generare frasi</strong> (es. a partire da <code>&lt;s&gt;</code>)</li>\n<li><strong>Calcolare la probabilità</strong> di una sequenza di parole</li>\n<li><strong>Confrontare frasi</strong> in base alla loro probabilità</li>\n</ul>\n<h2 id=\"domanda-2-qual-e-la-parola-piu-probabile-che-segue-la-sequenza\">🔸 Domanda 2: Qual è la parola più probabile che segue la sequenza?</h2>\n<p>Date le seuguenti sequenze (🥶),</p>\n<ul>\n<li><code>&lt;s&gt; Sam ...</code>  </li>\n<li><code>&lt;s&gt; Sam I do ...</code>  </li>\n<li><code>&lt;s&gt; Sam I am Sam ...</code></li>\n<li><code>&lt;s&gt; do I like ...</code></li>\n</ul>\n<p>trovare la parola successiva più probabile.</p>\n<p>Per ogni sequenza, analizziamo l’ultimo bigramma osservabile e calcoliamo quale parola ha la massima probabilità condizionata dato il contesto.</p>\n<h3 id=\"sequenza-s-sam\">🔹 Sequenza: <code>&lt;s&gt; Sam ...</code></h3>\n<p>L’ultima parola è <code>Sam</code>. Guardiamo la riga corrispondente nella matrice di probabilità:</p>\n<table>\n<thead>\n<tr>\n<th>Successore</th>\n<th>Probabilità $P(w_i \\mid \\text{Sam})$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>I</td>\n<td>$\\frac{3}{5}$ ✅ (massima)</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>$\\frac{2}{5}$</td>\n</tr>\n</tbody>\n</table>\n<p><strong>👉 Parola più probabile:</strong> <code>I</code></p>\n<h3 id=\"sequenza-s-sam-i-do\">🔹 Sequenza: <code>&lt;s&gt; Sam I do ...</code></h3>\n<p>L’ultima parola è <code>do</code>. Guardiamo la riga per <code>do</code>:</p>\n<table>\n<thead>\n<tr>\n<th>Successore</th>\n<th>Probabilità $P(w_i \\mid \\text{do})$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>I</td>\n<td>$\\frac{1}{2}$ ✅ (massima pari)</td>\n</tr>\n<tr>\n<td>like</td>\n<td>$\\frac{1}{2}$ ✅ (massima pari)</td>\n</tr>\n</tbody>\n</table>\n<p><strong>👉 Parole più probabili:</strong> <code>I</code> o <code>like</code> (probabilità uguale)</p>\n<h3 id=\"sequenza-s-sam-i-am-sam\">🔹 Sequenza: <code>&lt;s&gt; Sam I am Sam ...</code></h3>\n<p>L’ultima parola è <code>Sam</code>. Guardiamo di nuovo la riga per <code>Sam</code>:</p>\n<table>\n<thead>\n<tr>\n<th>Successore</th>\n<th>Probabilità $P(w_i \\mid \\text{Sam})$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>I</td>\n<td>$\\frac{3}{5}$ ✅</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>$\\frac{2}{5}$</td>\n</tr>\n</tbody>\n</table>\n<p><strong>👉 Parola più probabile:</strong> <code>I</code></p>\n<h3 id=\"sequenza-s-do-i-like\">🔹 Sequenza: <code>&lt;s&gt; do I like ...</code></h3>\n<p>L’ultima parola è <code>like</code>. Guardiamo la riga per <code>like</code>:</p>\n<table>\n<thead>\n<tr>\n<th>Successore</th>\n<th>Probabilità $P(w_i \\mid \\text{like})$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sam</td>\n<td>$\\frac{1}{3}$</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>$\\frac{2}{3}$ ✅</td>\n</tr>\n</tbody>\n</table>\n<p><strong>👉 Parola più probabile:</strong> <code>&lt;/s&gt;</code></p>\n<h3 id=\"conclusione\">✅ Conclusione</h3>\n<table>\n<thead>\n<tr>\n<th>Sequenza</th>\n<th>Parola più probabile</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt; Sam</code></td>\n<td><code>I</code></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; Sam I do</code></td>\n<td><code>I</code> o <code>like</code></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; Sam I am Sam</code></td>\n<td><code>I</code></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; do I like</code></td>\n<td><code>&lt;/s&gt;</code></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"domanda-3-calcolare-la-probabilita-delle-seguenti-sequenze-di-parole\">🔸 Domanda 3: Calcolare la probabilità delle seguenti sequenze di parole</h2>\n<ul>\n<li><code>&lt;s&gt; Sam do like &lt;/s&gt;</code></li>\n<li><code>&lt;s&gt; Sam I am &lt;/s&gt;</code></li>\n<li><code>&lt;s&gt; I am Sam &lt;/s&gt;</code></li>\n</ul>\n<p>Dobbiamo calcolare la probabilità totale delle sequenze usando il modello di bigrammi, ovvero:</p>\n$$\nP(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_{i-1})\n$$\n<h3 id=\"sequenza-1-s-sam-do-like-s\">🔹 Sequenza 1: <code>&lt;s&gt; Sam do like &lt;/s&gt;</code></h3>\n<p>Bigrammi:<br />\n- P(Sam | <code>&lt;s&gt;</code>) = 3/5<br />\n- P(do | Sam) = 0<br />\n- P(like | do) = 1/2<br />\n- P(<code>&lt;/s&gt;</code> | like) = 2/3  </p>\n<p><strong>Probabilità totale:</strong>\n$$\nP(\\langle s\\rangle\\ Sam\\ do\\ like\\ \\langle/s\\rangle) = \\frac{3}{5} \\cdot 0 \\cdot \\frac{1}{2} \\cdot \\frac{2}{3} = 0\n$$</p>\n<h3 id=\"sequenza-2-s-sam-i-am-s\">🔹 Sequenza 2: <code>&lt;s&gt; Sam I am &lt;/s&gt;</code></h3>\n<p>Bigrammi:<br />\n- P(Sam | <code>&lt;s&gt;</code>) = 3/5<br />\n- P(I | Sam) = 3/5<br />\n- P(am | I) = 2/5<br />\n- P(<code>&lt;/s&gt;</code> | am) = 1/2  </p>\n<p><strong>Probabilità totale:</strong>\n$$\nP(\\langle s\\rangle\\ Sam\\ I\\ am\\ \\langle/s\\rangle) = \\frac{3}{5} \\cdot \\frac{3}{5} \\cdot \\frac{2}{5} \\cdot \\frac{1}{2} = \\frac{18}{250} = 0.072\n$$</p>\n<h3 id=\"sequenza-3-s-i-am-sam-s\">🔹 Sequenza 3: <code>&lt;s&gt; I am Sam &lt;/s&gt;</code></h3>\n<p>Bigrammi:<br />\n- P(I | <code>&lt;s&gt;</code>) = 1/5<br />\n- P(am | I) = 2/5<br />\n- P(Sam | am) = 1/2<br />\n- P(<code>&lt;/s&gt;</code> | Sam) = 2/5  </p>\n<p><strong>Probabilità totale:</strong>\n$$\nP(\\langle s\\rangle\\ I\\ am\\ Sam\\ \\langle/s\\rangle) = \\frac{1}{5} \\cdot \\frac{2}{5} \\cdot \\frac{1}{2} \\cdot \\frac{2}{5} = \\frac{4}{250} = 0.016\n$$</p>\n<h3 id=\"riepilogo\">✅ Riepilogo</h3>\n<table>\n<thead>\n<tr>\n<th>Sequenza</th>\n<th>Probabilità</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt; Sam do like &lt;/s&gt;</code></td>\n<td><strong>0.000</strong></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; Sam I am &lt;/s&gt;</code></td>\n<td><strong>0.072</strong></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; I am Sam &lt;/s&gt;</code></td>\n<td><strong>0.016</strong></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"domanda-4-applica-il-laplace-smoothing-al-modello\">🔸 Domanda 4: Applica il Laplace Smoothing al Modello</h2>\n<p>Applichiamo il <strong>Laplace Smoothing</strong> (add-one smoothing) al modello di bigrammi per evitare probabilità nulle. La formula modificata è:</p>\n$$\nP(w_i \\mid w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i) + 1}{\\text{count}(w_{i-1}) + |V|}\n$$\n<p>dove $|V|$ è la cardinalità del vocabolario.</p>\n<h3 id=\"dati\">📌 Dati:</h3>\n<ul>\n<li><strong>Vocabolario chiuso</strong>: $\\{ \\langle s\\rangle,\\ I,\\ am,\\ Sam,\\ like,\\ do,\\ \\langle/s\\rangle \\}$  </li>\n<li>$\\Rightarrow |V| = 7$</li>\n</ul>\n<p>Applichiamo il Laplace smoothing a tutti i bigrammi della matrice.</p>\n<ul>\n<li>$|V| = 7$</li>\n<li>Per ogni riga (cioè per ogni parola $w_{i-1}$), si usa:<br />\n  $$\n  P(w_i \\mid w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i) + 1}{\\text{count}(w_{i-1}) + 7}\n  $$</li>\n<li>Vettore dei conteggi degli unigrammi:</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><code>&lt;s&gt;</code></th>\n<th>I</th>\n<th>am</th>\n<th>Sam</th>\n<th>like</th>\n<th>do</th>\n<th><code>&lt;/s&gt;</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>conteggio</strong></td>\n<td>5</td>\n<td>5</td>\n<td>2</td>\n<td>5</td>\n<td>3</td>\n<td>2</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Matrice dei conteggi dei bigrammi:</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>$w_{i-1} \\backslash w_i$</th>\n<th><code>&lt;s&gt;</code></th>\n<th>I</th>\n<th>am</th>\n<th>Sam</th>\n<th>like</th>\n<th>do</th>\n<th><code>&lt;/s&gt;</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt;</code></td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>3</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>I</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n<td>0</td>\n<td>2</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>am</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Sam</td>\n<td>0</td>\n<td>3</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>like</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>do</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"matrice-di-probabilita-smoothed-math_inline_93\">📋 Matrice di Probabilità Smoothed $P(w_i \\mid w_{i-1})$</h3>\n<table>\n<thead>\n<tr>\n<th>$w_{i-1} \\backslash w_i$</th>\n<th><code>&lt;s&gt;</code></th>\n<th>I</th>\n<th>am</th>\n<th>Sam</th>\n<th>like</th>\n<th>do</th>\n<th><code>&lt;/s&gt;</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt;</code></td>\n<td>$\\frac{0+1}{5+7} = \\frac{1}{12}$</td>\n<td>$\\frac{1+1}{12} = \\frac{2}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{3+1}{12} = \\frac{4}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{1+1}{12} = \\frac{2}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n</tr>\n<tr>\n<td>I</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{2+1}{12} = \\frac{3}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{2+1}{12} = \\frac{3}{12}$</td>\n<td>$\\frac{1+1}{12} = \\frac{2}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n</tr>\n<tr>\n<td>am</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{1+1}{9} = \\frac{2}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{1+1}{9} = \\frac{2}{9}$</td>\n</tr>\n<tr>\n<td>Sam</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{3+1}{12} = \\frac{4}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{0+1}{12} = \\frac{1}{12}$</td>\n<td>$\\frac{2+1}{12} = \\frac{3}{12}$</td>\n</tr>\n<tr>\n<td>like</td>\n<td>$\\frac{0+1}{10} = \\frac{1}{10}$</td>\n<td>$\\frac{0+1}{10} = \\frac{1}{10}$</td>\n<td>$\\frac{0+1}{10} = \\frac{1}{10}$</td>\n<td>$\\frac{1+1}{10} = \\frac{2}{10}$</td>\n<td>$\\frac{0+1}{10} = \\frac{1}{10}$</td>\n<td>$\\frac{0+1}{10} = \\frac{1}{10}$</td>\n<td>$\\frac{2+1}{10} = \\frac{3}{10}$</td>\n</tr>\n<tr>\n<td>do</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{1+1}{9} = \\frac{2}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{1+1}{9} = \\frac{2}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n<td>$\\frac{0+1}{9} = \\frac{1}{9}$</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>—</td>\n<td>—</td>\n<td>—</td>\n<td>—</td>\n<td>—</td>\n<td>—</td>\n<td>— (nessuna transizione in uscita)</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"nota\">📌 Nota:</h3>\n<ul>\n<li>Ogni riga rappresenta la distribuzione $P(w_i \\mid w_{i-1})$ con smoothing.</li>\n<li>La somma di ogni riga = 1</li>\n<li>La riga <code>&lt;/s&gt;</code> non è definita perché <code>&lt;/s&gt;</code> è sempre terminale (non ha bigrammi in uscita).</li>\n</ul>\n<h3 id=\"calcolo-delle-probabilita-con-smoothing\">🔹 Calcolo delle probabilità con smoothing</h3>\n<h4 id=\"esempio-calcoliamo-math_inline_138\">🔸 Esempio: Calcoliamo $P(\\text{do} \\mid \\text{Sam})$</h4>\n<p>Senza smoothing:\n- $\\text{count}(\\text{Sam}, \\text{do}) = 0$\n- $\\text{count}(\\text{Sam}) = 5$</p>\n<p>Con smoothing:\n$$\nP(\\text{do} \\mid \\text{Sam}) = \\frac{0 + 1}{5 + 7} = \\frac{1}{12} \\approx 0.083\n$$</p>\n<h3 id=\"conclusioni\">Conclusioni</h3>\n<ul>\n<li><strong>Vantaggio</strong>: Elimina gli zeri (es. $P(\\text{do} | \\text{Sam}) = \\frac{1}{12}$).  </li>\n<li><strong>Svantaggio</strong>: Appiattimento delle probabilità ($P(\\text{I} | \\text{Sam})$ scende da $\\frac{3}{5}$ a $\\frac{4}{12}$).</li>\n</ul>\n<h2 id=\"domanda-5-quali-sono-le-nuove-probabilita-delle-seguenti-parole\">🔸 Domanda 5: Quali sono le nuove probabilità delle seguenti parole?</h2>\n<ul>\n<li><code>&lt;s&gt; Sam do like &lt;/s&gt;</code></li>\n<li><code>&lt;s&gt; Sam I am &lt;/s&gt;</code></li>\n<li><code>&lt;s&gt; I am Sam &lt;/s&gt;</code></li>\n</ul>\n<h4 id=\"sequenza-1-s-sam-do-like-s_1\">🔹 Sequenza 1: <code>&lt;s&gt; Sam do like &lt;/s&gt;</code></h4>\n<p>Bigrammi (con smoothing):\n- $P(\\text{Sam} \\mid \\langle s\\rangle) = \\frac{3 + 1}{5 + 7} = \\frac{4}{12}$\n- $P(\\text{do} \\mid \\text{Sam}) = \\frac{0 + 1}{5 + 7} = \\frac{1}{12}$\n- $P(\\text{like} \\mid \\text{do}) = \\frac{1 + 1}{2 + 7} = \\frac{2}{9}$\n- $P(\\langle/s\\rangle \\mid \\text{like}) = \\frac{2 + 1}{3 + 7} = \\frac{3}{10}$</p>\n<p><strong>Probabilità totale:</strong>\n$$\nP(\\langle s\\rangle\\ Sam\\ do\\ like\\ \\langle/s\\rangle) = \\frac{4}{12} \\cdot \\frac{1}{12} \\cdot \\frac{2}{9} \\cdot \\frac{3}{10} = \\frac{24}{12960} = \\frac{1}{540} \\approx \\mathbf{0.00185}\n$$</p>\n<h4 id=\"sequenza-2-s-sam-i-am-s_1\">🔹 Sequenza 2: <code>&lt;s&gt; Sam I am &lt;/s&gt;</code></h4>\n<p>Bigrammi (con smoothing):\n- $P(\\text{Sam} \\mid \\langle s\\rangle) = \\frac{3 + 1}{5 + 7} = \\frac{4}{12}$\n- $P(\\text{I} \\mid \\text{Sam}) = \\frac{3 + 1}{5 + 7} = \\frac{4}{12}$\n- $P(\\text{am} \\mid \\text{I}) = \\frac{2 + 1}{5 + 7} = \\frac{3}{12}$\n- $P(\\langle/s\\rangle \\mid \\text{am}) = \\frac{1 + 1}{2 + 7} = \\frac{2}{9}$</p>\n<p><strong>Probabilità totale:</strong>\n$$\nP(\\langle s\\rangle\\ Sam\\ I\\ am\\ \\langle/s\\rangle) = \\frac{4}{12} \\cdot \\frac{4}{12} \\cdot \\frac{3}{12} \\cdot \\frac{2}{9} = \\frac{96}{15552} = \\frac{2}{324} \\approx \\mathbf{0.00617}\n$$</p>\n<h4 id=\"sequenza-3-s-i-am-sam-s_1\">🔹 Sequenza 3: <code>&lt;s&gt; I am Sam &lt;/s&gt;</code></h4>\n<p>Bigrammi (con smoothing):\n- $P(\\text{I} \\mid \\langle s\\rangle) = \\frac{1 + 1}{5 + 7} = \\frac{2}{12}$\n- $P(\\text{am} \\mid \\text{I}) = \\frac{2 + 1}{5 + 7} = \\frac{3}{12}$\n- $P(\\text{Sam} \\mid \\text{am}) = \\frac{1 + 1}{2 + 7} = \\frac{2}{9}$\n- $P(\\langle/s\\rangle \\mid \\text{Sam}) = \\frac{2 + 1}{5 + 7} = \\frac{3}{12}$</p>\n<p><strong>Probabilità totale:</strong>\n$$\nP(\\langle s\\rangle\\ I\\ am\\ Sam\\ \\langle/s\\rangle) = \\frac{2}{12} \\cdot \\frac{3}{12} \\cdot \\frac{2}{9} \\cdot \\frac{3}{12} = \\frac{36}{15552} = \\frac{1}{432} \\approx \\mathbf{0.00231}\n$$</p>\n<h3 id=\"riepilogo-con-laplace-smoothing\">✅ Riepilogo (con Laplace smoothing)</h3>\n<table>\n<thead>\n<tr>\n<th>Sequenza</th>\n<th>Probabilità (con smoothing)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt; Sam do like &lt;/s&gt;</code></td>\n<td><strong>0.00185</strong></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; Sam I am &lt;/s&gt;</code></td>\n<td><strong>0.00617</strong></td>\n</tr>\n<tr>\n<td><code>&lt;s&gt; I am Sam &lt;/s&gt;</code></td>\n<td><strong>0.00231</strong></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"domanda-6-definire-una-catena-di-markov-per-un-modello-bigramma\">🔸 Domanda 6: Definire una Catena di Markov per un Modello Bigramma</h2>\n<h3 id=\"componenti-della-catena-di-markov\">Componenti della Catena di Markov:</h3>\n<ol>\n<li>\n<p><strong>Insieme degli stati $Q$</strong>:\n   $$\n   Q = \\{ \\langle s\\rangle,\\ \\text{I},\\ \\text{am},\\ \\text{Sam},\\ \\text{like},\\ \\text{do},\\ \\langle/s\\rangle \\}\n   $$\n   Ogni stato corrisponde a una parola del vocabolario, incluso il token di fine e inizio frase.</p>\n</li>\n<li>\n<p><strong>Matrice di transizione $A$</strong>:\n   Le probabilità di transizione $a_{ij} = P(w_j | w_i)$ sono definite dalla tabella sottostante (valori semplificati per chiarezza):</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Stato $w_i$</th>\n<th>Transizioni $w_j$ (probabilità semplificate)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>&lt;s&gt;</code></td>\n<td>I: $\\frac{2}{12}$, Sam: $\\frac{4}{12}$, do: $\\frac{2}{12}$, <code>&lt;s&gt;</code>/am/like/<code>&lt;/s&gt;</code>: $\\frac{1}{12}$</td>\n</tr>\n<tr>\n<td>I</td>\n<td>am: $\\frac{3}{12}$, like: $\\frac{3}{12}$, do: $\\frac{2}{12}$, altri: $\\frac{1}{12}$</td>\n</tr>\n<tr>\n<td>am</td>\n<td>Sam: $\\frac{2}{9}$, <code>&lt;/s&gt;</code>: $\\frac{2}{9}$, altri: $\\frac{1}{9}$</td>\n</tr>\n<tr>\n<td>Sam</td>\n<td>I: $\\frac{4}{12}$, <code>&lt;/s&gt;</code>: $\\frac{3}{12}$, altri: $\\frac{1}{12}$</td>\n</tr>\n<tr>\n<td>like</td>\n<td>Sam: $\\frac{2}{10}$, <code>&lt;/s&gt;</code>: $\\frac{3}{10}$, altri: $\\frac{1}{10}$</td>\n</tr>\n<tr>\n<td>do</td>\n<td>I: $\\frac{2}{9}$, like: $\\frac{2}{9}$, altri: $\\frac{1}{9}$</td>\n</tr>\n<tr>\n<td><code>&lt;/s&gt;</code></td>\n<td>Nessuna transizione (stato terminale)</td>\n</tr>\n</tbody>\n</table>\n<ol>\n<li><strong>Distribuzione iniziale $\\pi$</strong>:\n   $$\n   \\pi_{\\langle s\\rangle} = 1, \\quad \\pi_{\\text{I}} = \\pi_{\\text{am}} = \\pi_{\\text{Sam}} = \\pi_{\\text{like}} = \\pi_{\\text{do}} = \\pi_{\\langle/s\\rangle} = 0\n   $$\n   Tutte le frasi iniziano con <code>&lt;s&gt;</code>.</li>\n</ol>\n<h3 id=\"diagramma-della-catena-di-markov\">Diagramma della Catena di Markov:</h3>\n<p><img src=\"/images/tikz/d9a8452a4780687bb03acb8df4eef9ef.svg\" style=\"display: block; width: 100%; height: auto; max-height: 600px;\" class=\"tikz-svg\" /></p>\n<h2 id=\"conclusioni-finali\">🔷 Conclusioni Finali</h2>\n<h3 id=\"1-confronto-smoothing-vs-non-smoothed\">1. <strong>Confronto Smoothing vs. Non-Smoothed</strong></h3>\n<ul>\n<li><strong>Senza smoothing</strong>:  </li>\n<li>✔️ Accurate per dati osservati.  </li>\n<li>❌ Fragile: la presenza di probabilità nulle compromette la modellazione di sequenze nuove.  </li>\n<li><strong>Con Laplace</strong>:  </li>\n<li>✔️ Robusto: ogni transizione ha probabilità &gt; 0.  </li>\n<li>❌ Distorsione: ad es. $P(\\text{I} | \\text{Sam})$ si riduce del <strong>47%</strong> → effetto negativo su transizioni frequenti.</li>\n</ul>\n<h3 id=\"2-scelta-del-metodo-di-smoothing\">2. <strong>Scelta del Metodo di Smoothing</strong></h3>\n<ul>\n<li><strong>Laplace</strong>:  </li>\n<li>✅ Semplice da implementare.  </li>\n<li>❌ Uniforma eccessivamente → penalizza le distribuzioni reali.  </li>\n<li><strong>Altri metodi (Katz Backoff, Kneser-Ney)</strong>:  </li>\n<li>✅ Più sofisticati.  </li>\n<li>✅ Scontano in modo adattivo e preservano meglio le frequenze originali.  </li>\n</ul>\n<h3 id=\"3-impatto-del-corpus\">3. <strong>Impatto del Corpus</strong></h3>\n<ul>\n<li><strong>Dimensione</strong>:  </li>\n<li>Corpus molto piccolo ($N = 5$ frasi) → alta <strong>sparsità</strong>.  </li>\n<li><strong>Pattern ripetitivi</strong>:  </li>\n<li>Sequenze come “Sam I” o “I am” dominano le transizioni → il modello rischia di <strong>overfittare</strong> su co-occorrenze casuali.  </li>\n<li><strong>Copertura lessicale</strong>:  </li>\n<li>Vocabolario ristretto ($|V| = 6$ parole) → le tecniche di smoothing hanno effetto amplificato.</li>\n</ul>\n<h3 id=\"4-modello-come-catena-di-markov\">4. <strong>Modello come Catena di Markov</strong></h3>\n<ul>\n<li>Il modello $n$-gramma (qui bigramma) equivale a una <strong>catena di Markov di ordine 1</strong>:<br />\n  $$ P(w_i | w_1^{i-1}) \\approx P(w_i | w_{i-1}) $$</li>\n<li>Vantaggi:  </li>\n<li>✅ Rappresentazione semplice e visualizzabile con automi.  </li>\n<li>Limiti:  </li>\n<li>❌ Perde dipendenze a lungo termine.  </li>\n<li>❌ La memoria limitata (solo lo stato precedente) può causare ambiguità in sequenze complesse.</li>\n</ul>\n<h3 id=\"5-equazioni-chiave\">5. <strong>Equazioni Chiave</strong></h3>\n<ul>\n<li><strong>MLE (Massima Verosimiglianza)</strong>:<br />\n  $$ P_{\\text{MLE}}(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})} $$</li>\n<li><strong>Laplace Smoothing</strong>:<br />\n  $$ P_{\\text{smooth}}(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i) + 1}{\\text{count}(w_{i-1}) + |V|} $$</li>\n</ul>\n<h3 id=\"conclusione-generale\">✅ Conclusione Generale:</h3>\n<p>I modelli $n$-gramma, pur essendo una base utile per la modellazione del linguaggio, <strong>necessitano di smoothing avanzati</strong> per affrontare la <strong>sparsità</strong> dei dati.<br />\nQuando interpretati come catene di Markov, evidenziano chiaramente il <strong>trade-off tra semplicità e potere predittivo</strong>.</p>"
}