{
  "title": "Principal Component Analysis (PCA)",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"definizione\">üìò Definizione</h2>\n<p>La <strong>Principal Component Analysis (PCA)</strong> √® una tecnica di riduzione della dimensionalit√†, che si basa sulla feature extraction, utilizzata per comprimere i dati preservando la maggior parte delle informazioni rilevanti. Viene ampiamente applicata in machine learning, analisi dei dati e statistica.</p>\n<p>L&rsquo;obiettivo principale della PCA √®:</p>\n<blockquote>\n<p>Trovare una rappresentazione dello spazio originale dei dati in un sistema di coordinate trasformato, chiamato &ldquo;componenti principali&rdquo;, che massimizzi la varianza dei dati.</p>\n</blockquote>\n<p>√à strettamente collegata all&rsquo;ipotesi dei <a href=\"/theory/unsupervised-learning/Dimensionality Reduction/Manifold Hypothesis\" class=\"text-blue-600 hover:underline\">Manifold</a>.</p>\n<hr />\n<h2 id=\"obiettivi-principali\">üéØ Obiettivi principali</h2>\n<ol>\n<li><strong>Riduzione della dimensionalit√†</strong>: Ridurre il numero di variabili per semplificare l&rsquo;analisi.</li>\n<li><strong>Rimozione della collinearit√†</strong>: Ridurre la ridondanza tra le variabili.</li>\n<li><strong>Visualizzazione</strong>: Rappresentare dati complessi in 2D o 3D per comprenderne la struttura.</li>\n</ol>\n<hr />\n<h2 id=\"dataset-di-partenza\">üìä Dataset di partenza</h2>\n<p>Consideriamo un dataset $D = \\{\\vec{x}_i\\}_{i=1}^N$, con $\\vec{x}_i \\in \\mathbb{R}^d$, dove:\n- $N$: numero di osservazioni\n- $d$: numero di variabili (dimensioni)</p>\n<p>Scritto in forma di matrice, $D$ pu√≤ essere rappresentato come:\n$$\nD = \\begin{bmatrix}\nx_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\nx_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{N,1} & x_{N,2} & \\cdots & x_{N,d}\n\\end{bmatrix}\n$$</p>\n<p>Dove $x_{i,j}$ rappresenta il valore della $j$-esima variabile per il punto $\\vec{x}_i$.</p>\n<hr />\n<h2 id=\"procedura-dettagliata\">üõ†Ô∏è Procedura dettagliata</h2>\n<h3 id=\"1-standardizzazione-del-dataset\">1Ô∏è‚É£ <strong>Standardizzazione del dataset</strong></h3>\n<p>Per ogni variabile $j \\in \\{1, 2, \\dots, d\\}$, calcoliamo:\n1. La media:\n   $$\n   \\mu_j = \\frac{1}{N} \\sum_{i=1}^N x_{i,j}\n   $$\n2. La deviazione standard:\n   $$\n   \\sigma_j = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_{i,j} - \\mu_j)^2}\n   $$</p>\n<p>Per ogni punto $\\vec{x}_i = [x_{i,1}, x_{i,2}, \\dots, x_{i,d}]$, standardizziamo ogni dimensione:\n$$\nz_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}\n$$</p>\n<p>Il nuovo dataset standardizzato diventa:\n$$\nZ = \\begin{bmatrix}\nz_{1,1} & z_{1,2} & \\cdots & z_{1,d} \\\\\nz_{2,1} & z_{2,2} & \\cdots & z_{2,d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nz_{N,1} & z_{N,2} & \\cdots & z_{N,d}\n\\end{bmatrix}\n$$</p>\n<hr />\n<h3 id=\"2-calcolo-della-matrice-di-covarianza\">2Ô∏è‚É£ <strong>Calcolo della matrice di covarianza</strong></h3>\n<p>La <a href=\"/theory/math-for-ml/Probabilit√†/Covarianza\" class=\"text-blue-600 hover:underline\">matrice di covarianza</a> $\\Sigma$ misura la relazione lineare tra le variabili del dataset standardizzato $Z$:\n$$\n\\Sigma = \\frac{1}{N-1} Z^\\top Z\n$$\nDove $Z^\\top$ √® la trasposta della matrice $Z$.</p>\n<p>Ogni elemento $\\Sigma_{j,k}$ della matrice di covarianza rappresenta la covarianza tra le variabili $j$ e $k$.</p>\n<p>Notiamo inoltre l&rsquo;applicazione della <a href=\"/theory/math-for-ml/Probabilit√†/Correzione di Bessel\" class=\"text-blue-600 hover:underline\">Correzione di Bessel</a> nella formula della covarianza.</p>\n<hr />\n<h3 id=\"3-calcolo-di-autovalori-e-autovettori\">3Ô∏è‚É£ <strong>Calcolo di autovalori e autovettori</strong></h3>\n<p>Calcoliamo gli <strong>autovalori</strong> $\\lambda_1, \\lambda_2, \\dots, \\lambda_d$ e gli <strong>autovettori</strong> $\\vec{v}_1, \\vec{v}_2, \\dots, \\vec{v}_d$ della matrice di covarianza $\\Sigma$:\n$$\n\\Sigma \\vec{v}_j = \\lambda_j \\vec{v}_j\n$$\n- Gli autovalori $\\lambda_j$ indicano la (quantit√† di) varianza catturata dal componente principale associato. Infatti, se sommiamo tutti gli autovalori, otteniamo la varianza totale.\n- Gli autovettori $\\vec{v}_j$ definiscono la direzione dei componenti principali.</p>\n<p>L&rsquo;unico caso in cui possiamo ottenere una soluzione non banale dell&rsquo;equazione (i.e. $\\vec v_j = \\vec 0$) √® quando la matrice $(\\Sigma - \\lambda_j I)$ non √® invertibile, infatti se lo fosse\n$$\\begin{align}\n\\Sigma \\vec{v}_j &= \\lambda_j \\vec{v}_j\\\\\n\\Sigma \\vec{v}_j - \\lambda_j \\vec{v}_j &= \\vec 0\\\\\n(\\Sigma - \\lambda_j I) \\vec{v}_j &= \\vec 0\\\\\n(\\Sigma - \\lambda_j I)^{-1} (\\Sigma - \\lambda_j I) \\vec{v}_j &= (\\Sigma - \\lambda_j I)^{-1} \\vec 0\\\\\n\\vec v_j &= \\vec 0.\n\\end{align}\n$$\navremmo la soluzione banale. Quindi dobbiamo assumere per forza che \n$$\ndet[(\\Sigma - \\lambda_j I)] = 0.\n$$\nDato che questa equazione √® un polinomio di grado al massimo $d$, pu√≤ avere al massimo $d$ soluzioni, e quindi non pi√π di $d$ autovettori e autovalori.</p>\n<p>Una volta trovati gli autovalori, non ci resta altro che sostituirli nell&rsquo;equazione\n$$\n(\\Sigma - \\lambda_j I) \\vec{v}_j = \\vec 0\n$$\nper ottenere i rispettivi autovettori. Infine, per ottenere i versori dagli autovettori, ci baster√† dividerli per la loro norma $L^2$, in modo da ottenere dei vettori di lunghezza unitaria.</p>\n<hr />\n<h3 id=\"4-selezione-dei-componenti-principali\">4Ô∏è‚É£ <strong>Selezione dei componenti principali</strong></h3>\n<p>Ordiniamo gli autovalori in ordine decrescente:\n$$\n\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_d\n$$\nSelezioniamo i primi $k$ componenti principali che spiegano la maggior parte della varianza, cio√®:\n$$\n\\text{Varianza spiegata} = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^d \\lambda_j}\n$$</p>\n<hr />\n<h3 id=\"5-proiezione-dei-dati\">5Ô∏è‚É£ <strong>Proiezione dei dati</strong></h3>\n<p>Proiettiamo il dataset standardizzato $Z$ nello spazio dei $k$ componenti principali selezionati. La matrice proiettata √® definita come:</p>\n$$\nZ_{\\text{PCA}} = Z V_k\n$$\n<p>Dove:\n- $Z$ √® la matrice del dataset standardizzato di dimensione $N \\times d$ (con $N$ osservazioni e $p$ variabili),\n- $V_k$ √® una matrice $d \\times k$, i cui $k$ autovettori (o &ldquo;vettori propri&rdquo;) corrispondono ai $k$ autovalori maggiori della matrice di covarianza.</p>\n<h4 id=\"perche-questa-operazione-funziona\">üîç <strong>Perch√© questa operazione funziona?</strong></h4>\n<p>La proiezione $Z V_k$ riduce la dimensione del dataset da $d$ a $k$ variabili, ma conserva la maggior parte dell&rsquo;informazione originale (ovvero la varianza). Questo avviene per i seguenti motivi:</p>\n<ol>\n<li>\n<p><strong>Gli autovettori definiscono una nuova base</strong>:\n   Gli autovettori di $Z$ (calcolati dalla matrice di covarianza o di correlazione) rappresentano le direzioni principali lungo cui i dati variano maggiormente. In termini geometrici, queste direzioni sono ortogonali tra loro e formano una nuova base dello spazio originale.</p>\n</li>\n<li>\n<p><strong>Proiezione lungo le direzioni principali</strong>:\n   Molte variabili del dataset originale possono essere fortemente correlate tra loro, il che significa che i dati &ldquo;occupano&rdquo; solo una porzione ridotta dello spazio ad alta dimensionalit√†. Gli autovettori identificano le direzioni in cui i dati sono pi√π sparsi. Proiettando $Z$ sugli autovettori principali, ci &ldquo;spostiamo&rdquo; dal sistema di coordinate originale a uno nuovo, allineato con queste direzioni principali.</p>\n</li>\n<li>\n<p><strong>Selezione dei $k$ componenti principali</strong>:\n   Scegliendo i primi $k$ autovettori (che corrispondono ai $k$ autovalori pi√π grandi), ci limitiamo a considerare le direzioni che catturano la maggior parte della varianza nei dati. Le altre direzioni (con varianza pi√π piccola) vengono ignorate perch√© contribuiscono meno all&rsquo;informazione complessiva.</p>\n</li>\n</ol>\n<h4 id=\"cosa-succede-a-livello-geometrico\">üìè <strong>Cosa succede a livello geometrico?</strong></h4>\n<ul>\n<li>Il dataset standardizzato $Z$ √® rappresentato come un insieme di punti in uno spazio $d$-dimensionale.</li>\n<li>Gli autovettori $V_k$ definiscono un sotto-spazio $k$-dimensionale in cui i dati sono &ldquo;schiacciati&rdquo; (proiettati).</li>\n<li>La proiezione $Z V_k$ consiste nel prendere i punti originali e calcolare le loro coordinate rispetto a questo nuovo sistema di riferimento ridotto. Geometricamente, stiamo spostando i dati su un &ldquo;piano&rdquo; o &ldquo;sottospazio&rdquo; che meglio rappresenta la struttura del dataset.</li>\n</ul>\n<p>Il risultato della proiezione, $Z_{\\text{PCA}}$, √® un dataset ridotto di dimensione $N \\times k$. Questo dataset:\n- Mantiene la maggior parte della varianza originale (se $k$ √® scelto correttamente),\n- Elimina la ridondanza e il rumore presente nelle dimensioni con varianza bassa,\n- √à pi√π compatto e facilmente interpretabile rispetto al dataset originale.</p>\n<hr />\n<h2 id=\"vantaggi\">üîó Vantaggi</h2>\n<p>La PCA offre numerosi vantaggi, sia in termini di efficienza che di qualit√† dei risultati in analisi dei dati e machine learning. Di seguito, ogni vantaggio √® spiegato in dettaglio:</p>\n<h3 id=\"1-riduzione-del-rumore\">1Ô∏è‚É£ <strong>Riduzione del rumore</strong></h3>\n<ul>\n<li><strong>Cosa significa</strong>: Durante il processo di PCA, le dimensioni del dataset associate a una varianza bassa (spesso causate da rumore o variabili irrilevanti) vengono scartate. Questo aiuta a concentrarsi sulle componenti che rappresentano la struttura &ldquo;vera&rdquo; dei dati.</li>\n<li><strong>Perch√© √® importante</strong>: I dati reali spesso contengono rumore dovuto a misurazioni imprecise, errori di raccolta dati o informazioni non significative. Rimuovendo queste dimensioni, il dataset risultante √® pi√π pulito e meno soggetto a interpretazioni errate.</li>\n</ul>\n<hr />\n<h3 id=\"2-efficienza-computazionale\">2Ô∏è‚É£ <strong>Efficienza computazionale</strong></h3>\n<ul>\n<li><strong>Cosa significa</strong>: La riduzione del numero di dimensioni del dataset porta a una significativa riduzione dei costi computazionali, sia per il calcolo che per il successivo utilizzo nei modelli di machine learning.</li>\n<li><strong>Perch√© √® importante</strong>:</li>\n<li>Molti algoritmi di machine learning, come regressione logistica, SVM e reti neurali, funzionano pi√π velocemente quando il numero di feature √® minore.</li>\n<li>Inoltre, la complessit√† computazionale degli algoritmi di ottimizzazione cresce con l&rsquo;aumentare delle dimensioni del dataset.</li>\n</ul>\n<hr />\n<h3 id=\"3-rappresentazione-compatta\">3Ô∏è‚É£ <strong>Rappresentazione compatta</strong></h3>\n<ul>\n<li><strong>Cosa significa</strong>: La PCA comprime le informazioni pi√π importanti del dataset originale in un numero inferiore di dimensioni, chiamate componenti principali. Questo consente una rappresentazione pi√π semplice e comprensibile dei dati.</li>\n<li><strong>Perch√© √® importante</strong>:</li>\n<li>Riducendo le dimensioni, il dataset diventa pi√π facile da esplorare e visualizzare. Ad esempio, possiamo rappresentare un dataset multidimensionale in 2D o 3D, facilitando la comprensione dei pattern nei dati.</li>\n<li>Conserva la maggior parte della varianza (cio√® l‚Äôinformazione significativa) anche dopo la riduzione.</li>\n</ul>\n<hr />\n<h3 id=\"4-eliminazione-della-collinearita\">4Ô∏è‚É£ <strong>Eliminazione della collinearit√†</strong></h3>\n<ul>\n<li><strong>Cosa significa</strong>: La PCA trasforma le variabili originali in componenti principali che sono ortogonali tra loro (non correlate). Questo risolve il problema della multi-collinearit√†, dove due o pi√π variabili sono fortemente correlate.</li>\n<li><strong>Perch√© √® importante</strong>:</li>\n<li>La multi-collinearit√† pu√≤ causare problemi in molti algoritmi, come regressione lineare, rendendo difficile stimare accuratamente i coefficienti.</li>\n<li>Le componenti principali, essendo ortogonali, eliminano automaticamente questa problematica.</li>\n<li><strong>Esempio pratico</strong>:</li>\n<li>In un dataset economico, reddito e spese potrebbero essere fortemente correlati. La PCA pu√≤ combinare queste due variabili in una nuova componente principale che rappresenta il &ldquo;livello economico&rdquo; senza ridondanza.</li>\n</ul>\n<hr />\n<h3 id=\"5-facilita-linterpretazione-e-la-visualizzazione-dei-dati\">5Ô∏è‚É£ <strong>Facilita l&rsquo;interpretazione e la visualizzazione dei dati</strong></h3>\n<ul>\n<li><strong>Cosa significa</strong>: Riducendo il numero di dimensioni, diventa pi√π semplice rappresentare i dati graficamente o interpretarne la struttura.</li>\n<li><strong>Perch√© √® importante</strong>:</li>\n<li>In dataset ad alta dimensionalit√†, √® difficile comprendere relazioni tra variabili o identificare pattern.</li>\n<li>Dopo la PCA, possiamo spesso visualizzare i dati in 2D o 3D, rendendo pi√π intuitiva l&rsquo;analisi esplorativa.</li>\n</ul>\n<hr />\n<h2 id=\"svantaggi\">‚ö†Ô∏è Svantaggi</h2>\n<ul>\n<li><strong>Perdita di interpretabilit√†</strong>: I componenti principali non sono facili da interpretare.</li>\n<li><strong>Lineare</strong>: La PCA √® limitata a relazioni lineari tra le variabili, quindi se i dati vivono in un sotto-spazio (i.e. <a href=\"/theory/unsupervised-learning/Dimensionality Reduction/Manifold Hypothesis\" class=\"text-blue-600 hover:underline\">manifold</a>) non lineare, la PCA pu√≤ non catturare tutta l&rsquo;informazione. </li>\n<li><strong>Sensibile alla scala</strong>: I dati devono essere standardizzati.</li>\n<li><strong>Complessit√† computazionale</strong>: La PCA √® abbastanza complessa dal punto di vista computazionale, perch√© richiede vari calcoli che coinvolgono matrici potenzialmente molto grandi. Quindi pu√≤ essere abbastanza costoso eseguire la PCA su dataset di grandi dimensioni.</li>\n</ul>\n<hr />\n<h2 id=\"conclusione\">üöÄ <strong>Conclusione</strong></h2>\n<p>Grazie a questi vantaggi, la PCA √® un&rsquo;operazione fondamentale in molti ambiti dell&rsquo;analisi dei dati, dalla riduzione delle dimensioni alla preparazione dei dati per modelli di machine learning. √à particolarmente utile quando si lavora con dataset ad alta dimensionalit√†, in cui √® cruciale ridurre la complessit√† senza perdere informazione significativa.</p>"
}