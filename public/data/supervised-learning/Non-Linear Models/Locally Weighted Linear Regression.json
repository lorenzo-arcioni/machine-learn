{
  "title": "Local Weighted Linear Regression (LWLR)",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>La <strong>Local Weighted Linear Regression</strong> (LWLR), nota anche come <strong>Locally Weighted Regression</strong> o <strong>LOESS</strong> (Locally Estimated Scatterplot Smoothing), è un&rsquo;estensione non parametrica della regressione lineare che adatta un modello lineare localmente per ogni punto di query. A differenza della regressione lineare classica che trova un unico modello globale, LWLR costruisce un modello diverso per ogni previsione, dando più peso ai punti di training vicini al punto di query.</p>\n<h2 id=\"1-motivazione-e-intuizione\"><strong>1. Motivazione e Intuizione</strong></h2>\n<p>Nella regressione lineare tradizionale, tutti i punti del training set contribuiscono ugualmente alla determinazione dei parametri del modello. Tuttavia, in molte applicazioni reali, la relazione tra variabili può variare localmente. LWLR risolve questo problema:</p>\n<ul>\n<li><strong>Adattandosi localmente</strong> alla struttura dei dati</li>\n<li><strong>Pesando maggiormente</strong> i punti vicini al punto di query</li>\n<li><strong>Riducendo l&rsquo;influenza</strong> dei punti lontani</li>\n<li><strong>Catturando pattern non lineari</strong> attraverso approssimazioni lineari locali</li>\n</ul>\n<h3 id=\"11-esempio-intuitivo\"><strong>1.1. Esempio Intuitivo</strong></h3>\n<p>Consideriamo una relazione non lineare tra temperatura e vendite di gelato. Un modello lineare globale potrebbe non catturare bene le variazioni stagionali, mentre LWLR può adattarsi localmente: in estate darà più peso ai dati estivi vicini, in inverno ai dati invernali, catturando così meglio la variabilità locale.</p>\n<h2 id=\"2-formulazione-matematica\"><strong>2. Formulazione Matematica</strong></h2>\n<h3 id=\"21-caso-univariato-forma-vettoriale\"><strong>2.1. Caso Univariato (Forma Vettoriale)</strong></h3>\n<p>Consideriamo un dataset di training $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^m$ dove $x_i \\in \\mathbb{R}$ e $y_i \\in \\mathbb{R}$.</p>\n<p>Per un punto di query $x_q$, LWLR risolve il seguente problema di ottimizzazione pesato:</p>\n$$\n\\min_{\\theta_0, \\theta_1} \\sum_{i=1}^{m} w_i(x_q) \\left( y_i - \\theta_0 - \\theta_1 x_i \\right)^2\n$$\n<p>Dove:\n- $\\theta_0$ è l&rsquo;intercetta (bias) del modello locale\n- $\\theta_1$ è il coefficiente angolare del modello locale\n- $w_i(x_q)$ è il peso assegnato al punto $i$-esimo in funzione della sua distanza da $x_q$</p>\n<h3 id=\"22-funzione-peso-kernel\"><strong>2.2. Funzione Peso (Kernel)</strong></h3>\n<p>Il peso $w_i(x_q)$ è tipicamente definito usando un kernel gaussiano:</p>\n$$\nw_i(x_q) = \\exp\\left(-\\frac{(x_i - x_q)^2}{2\\tau^2}\\right)\n$$\n<p>Dove:\n- $\\tau > 0$ è il <strong>bandwidth parameter</strong> che controlla la &ldquo;larghezza&rdquo; della finestra locale\n- $\\tau$ piccolo → finestra stretta → modello più &ldquo;wiggly&rdquo; (alta varianza, basso bias)\n- $\\tau$ grande → finestra larga → modello più smooth (bassa varianza, alto bias)</p>\n<h3 id=\"23-formulazione-matriciale-generale\"><strong>2.3. Formulazione Matriciale Generale</strong></h3>\n<p>Per il caso multivariato con dataset $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^m$ dove $\\mathbf{x}_i \\in \\mathbb{R}^d$ e $y_i \\in \\mathbb{R}$.</p>\n<h4 id=\"notazione-matriciale\"><strong>Notazione Matriciale</strong></h4>\n<ul>\n<li>\n<p>$\\mathbf{X} \\in \\mathbb{R}^{m \\times (d+1)}$ è la matrice di design con bias:\n  $$\n  \\mathbf{X} = \\begin{bmatrix}\n  1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n  1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & x_{m,1} & x_{m,2} & \\cdots & x_{m,d}\n  \\end{bmatrix}\n  $$</p>\n</li>\n<li>\n<p>$\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$ è il vettore delle variabili target:\n  $$\n  \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\n  $$</p>\n</li>\n<li>\n<p>$\\boldsymbol{\\theta} \\in \\mathbb{R}^{(d+1) \\times 1}$ è il vettore dei parametri locali:\n  $$\n  \\boldsymbol{\\theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{bmatrix}\n  $$</p>\n</li>\n<li>\n<p>$\\mathbf{W}(\\mathbf{x}_q) \\in \\mathbb{R}^{m \\times m}$ è la matrice diagonale dei pesi:\n  $$\n  \\mathbf{W}(\\mathbf{x}_q) = \\begin{bmatrix}\n  w_1(\\mathbf{x}_q) & 0 & \\cdots & 0 \\\\\n  0 & w_2(\\mathbf{x}_q) & \\cdots & 0 \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 & 0 & \\cdots & w_m(\\mathbf{x}_q)\n  \\end{bmatrix}\n  $$</p>\n</li>\n</ul>\n<h4 id=\"funzione-peso-multivariata\"><strong>Funzione Peso Multivariata</strong></h4>\n$$\nw_i(\\mathbf{x}_q) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_q\\|_2^2}{2\\tau^2}\\right)\n$$\n<h4 id=\"problema-di-ottimizzazione\"><strong>Problema di Ottimizzazione</strong></h4>\n$$\n\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{m} w_i(\\mathbf{x}_q) \\left( y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} \\right)^2\n$$\n<p>In forma matriciale:\n$$\n\\min_{\\boldsymbol{\\theta}} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T \\mathbf{W}(\\mathbf{x}_q) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\n$$</p>\n<h2 id=\"3-soluzione-analitica-weighted-least-squares\"><strong>3. Soluzione Analitica (Weighted Least Squares)</strong></h2>\n<h3 id=\"31-derivazione-della-soluzione-ottimale\"><strong>3.1. Derivazione della Soluzione Ottimale</strong></h3>\n<p>La funzione obiettivo da minimizzare è:\n$$\nJ(\\boldsymbol{\\theta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T \\mathbf{W}(\\mathbf{x}_q) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\n$$</p>\n<p>Espandendo:\n$$\nJ(\\boldsymbol{\\theta}) = \\mathbf{y}^T \\mathbf{W} \\mathbf{y} - 2\\mathbf{y}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta}\n$$</p>\n<h3 id=\"32-calcolo-del-gradiente\"><strong>3.2. Calcolo del Gradiente</strong></h3>\n<p>Il gradiente rispetto a $\\boldsymbol{\\theta}$ è:\n$$\n\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -2\\mathbf{X}^T \\mathbf{W} \\mathbf{y} + 2\\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta}\n$$</p>\n<p><strong>Derivazione dettagliata:</strong>\n- $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}(\\mathbf{y}^T \\mathbf{W} \\mathbf{y}) = \\mathbf{0}$ (non dipende da $\\boldsymbol{\\theta}$)\n- $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}(-2\\mathbf{y}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta}) = -2\\mathbf{X}^T \\mathbf{W} \\mathbf{y}$\n- $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta}) = 2\\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta}$</p>\n<h3 id=\"33-soluzione-ottimale\"><strong>3.3. Soluzione Ottimale</strong></h3>\n<p>Ponendo il gradiente uguale a zero:\n$$\n-2\\mathbf{X}^T \\mathbf{W} \\mathbf{y} + 2\\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\theta} = \\mathbf{0}\n$$</p>\n<p>Risolvendo per $\\boldsymbol{\\theta}$:\n$$\n\\boldsymbol{\\theta}^*(\\mathbf{x}_q) = (\\mathbf{X}^T \\mathbf{W}(\\mathbf{x}_q) \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}(\\mathbf{x}_q) \\mathbf{y}\n$$</p>\n<h3 id=\"34-predizione\"><strong>3.4. Predizione</strong></h3>\n<p>La predizione per il punto di query $\\mathbf{x}_q$ è:\n$$\n\\hat{y}_q = \\mathbf{x}_q^T \\boldsymbol{\\theta}^*(\\mathbf{x}_q)\n$$</p>\n<p>Sostituendo la soluzione ottimale:\n$$\n\\hat{y}_q = \\mathbf{x}_q^T (\\mathbf{X}^T \\mathbf{W}(\\mathbf{x}_q) \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}(\\mathbf{x}_q) \\mathbf{y}\n$$</p>\n<h3 id=\"35-implementazione-in-forma-chiusa\"><strong>3.5. Implementazione in Forma Chiusa</strong></h3>\n<p>L&rsquo;implementazione della soluzione analitica richiede alcuni passaggi fondamentali:</p>\n<h4 id=\"algoritmo\"><strong>Algoritmo:</strong></h4>\n<ol>\n<li><strong>Preparazione dei dati:</strong></li>\n<li>Assicurarsi che $\\mathbf{y}$ sia un vettore colonna $(m \\times 1)$</li>\n<li>Aggiungere colonna di 1&rsquo;s a $\\mathbf{X}$ per il bias: $\\mathbf{X}_{\\text{aug}} \\in \\mathbb{R}^{m \\times (d+1)}$</li>\n<li>\n<p>Estendere il punto query: $\\mathbf{x}_{q,\\text{aug}} \\in \\mathbb{R}^{1 \\times (d+1)}$</p>\n</li>\n<li>\n<p><strong>Calcolo dei pesi:</strong></p>\n</li>\n<li>Calcolare le distanze: $\\text{diff}_i = \\mathbf{x}_i - \\mathbf{x}_q$ per $i = 1,\\ldots,m$</li>\n<li>Calcolare i pesi: $w_i = \\exp\\left(-\\frac{\\|\\text{diff}_i\\|^2}{2\\tau^2}\\right)$</li>\n<li>\n<p>Costruire la matrice diagonale: $\\mathbf{W} = \\text{diag}(w_1, w_2, \\ldots, w_m)$</p>\n</li>\n<li>\n<p><strong>Risoluzione del sistema:</strong></p>\n</li>\n<li>Calcolare $\\mathbf{X}^T \\mathbf{W}$</li>\n<li>Calcolare $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$</li>\n<li>\n<p>Risolvere: $\\boldsymbol{\\theta}^* = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}$</p>\n</li>\n<li>\n<p><strong>Predizione:</strong></p>\n</li>\n<li>$\\hat{y}_q = \\mathbf{x}_{q,\\text{aug}}^T \\boldsymbol{\\theta}^*$</li>\n</ol>\n<h4 id=\"implementazione\"><strong>Implementazione:</strong></h4>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">lwlr_CF</span><span class=\"p\">(</span><span class=\"n\">x_q</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Locally Weighted Linear Regression in forma chiusa</span>\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        x_q: punto query (1, d)</span>\n<span class=\"sd\">        X: training data (m, d)  </span>\n<span class=\"sd\">        y: training labels (m,) o (m, 1)</span>\n<span class=\"sd\">        t: bandwidth parameter</span>\n<span class=\"sd\">    Returns:</span>\n<span class=\"sd\">        y_pred: predizione per x_q</span>\n<span class=\"sd\">        theta_weights: coefficienti delle features  </span>\n<span class=\"sd\">        theta_bias: intercetta</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n\n    <span class=\"c1\"># Assicuriamoci che y sia un vettore colonna</span>\n    <span class=\"k\">if</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">dim</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># (m, 1)</span>\n\n    <span class=\"c1\"># Aggiungi colonna di 1 per il bias</span>\n    <span class=\"n\">X_aug</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cat</span><span class=\"p\">([</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)],</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># (m, d+1)</span>\n    <span class=\"n\">x_q_aug</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cat</span><span class=\"p\">([</span><span class=\"n\">x_q</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)],</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># (1, d+1)</span>\n\n    <span class=\"c1\"># Calcolo dei pesi</span>\n    <span class=\"n\">diff</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">-</span> <span class=\"n\">x_q</span>  <span class=\"c1\"># (m, d)</span>\n    <span class=\"n\">weights</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">diff</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">t</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">))</span>  <span class=\"c1\"># (m,)</span>\n    <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"p\">)</span>  <span class=\"c1\"># (m, m)</span>\n\n    <span class=\"c1\"># Soluzione in forma chiusa: θ = (X^T W X)^(-1) X^T W y</span>\n    <span class=\"n\">XTW</span> <span class=\"o\">=</span> <span class=\"n\">X_aug</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">W</span>  <span class=\"c1\"># (d+1, m)</span>\n    <span class=\"n\">XTWX</span> <span class=\"o\">=</span> <span class=\"n\">XTW</span> <span class=\"o\">@</span> <span class=\"n\">X_aug</span>  <span class=\"c1\"># (d+1, d+1)</span>\n    <span class=\"n\">theta</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">inverse</span><span class=\"p\">(</span><span class=\"n\">XTWX</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">XTW</span> <span class=\"o\">@</span> <span class=\"n\">y</span>  <span class=\"c1\"># (d+1, 1)</span>\n\n    <span class=\"c1\"># Predizione</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_q_aug</span> <span class=\"o\">@</span> <span class=\"n\">theta</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">theta</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<h4 id=\"considerazioni-numeriche\"><strong>Considerazioni Numeriche:</strong></h4>\n<ol>\n<li><strong>Stabilità dell&rsquo;inversione:</strong> La matrice $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ può essere mal condizionata se:</li>\n<li>I punti sono quasi collineari nell&rsquo;intorno locale</li>\n<li>Alcuni pesi sono molto piccoli (vicini a zero)</li>\n<li>\n<p>Il bandwidth $\\tau$ è troppo piccolo</p>\n</li>\n<li>\n<p><strong>Alternative numericamente stabili:</strong></p>\n</li>\n<li>Usare la <strong>decomposizione SVD</strong>: $\\boldsymbol{\\theta}^* = \\mathbf{V} \\boldsymbol{\\Sigma}^{-1} \\mathbf{U}^T \\mathbf{W} \\mathbf{y}$</li>\n<li>Usare <strong>pseudo-inversa di Moore-Penrose</strong>: $\\boldsymbol{\\theta}^* = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{\\dagger} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}$</li>\n<li>Aggiungere <strong>regolarizzazione Ridge</strong>: $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{I})^{-1}$</li>\n</ol>\n<h3 id=\"36-vantaggi-e-svantaggi-della-forma-chiusa\"><strong>3.6. Vantaggi e Svantaggi della Forma Chiusa</strong></h3>\n<h4 id=\"vantaggi\"><strong>Vantaggi:</strong></h4>\n<ul>\n<li><strong>Convergenza garantita</strong> in una sola iterazione</li>\n<li><strong>Soluzione esatta</strong> (modulo errori numerici)</li>\n<li><strong>Deterministica</strong> - risultati riproducibili</li>\n<li><strong>Veloce</strong> per piccoli dataset</li>\n</ul>\n<h4 id=\"svantaggi\"><strong>Svantaggi:</strong></h4>\n<ul>\n<li><strong>Complessità computazionale</strong> $O(d^3)$ per l&rsquo;inversione della matrice</li>\n<li><strong>Instabilità numerica</strong> quando $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ è mal condizionata</li>\n<li><strong>Memoria</strong> richiesta per memorizzare la matrice dei pesi</li>\n<li><strong>Non scalabile</strong> per dataset molto grandi</li>\n</ul>\n<h2 id=\"4-implementazioni-con-gradient-descent\"><strong>4. Implementazioni con Gradient Descent</strong></h2>\n<p>Quando la soluzione analitica è computazionalmente proibitiva o numericamente instabile, si può utilizzare il gradient descent per ottimizzare i parametri.</p>\n<h3 id=\"41-stochastic-gradient-descent-sgd\"><strong>4.1. Stochastic Gradient Descent (SGD)</strong></h3>\n<p>Nel SGD, i parametri vengono aggiornati per ogni singolo punto di training.</p>\n<h4 id=\"algoritmo-sgd-per-lwlr\"><strong>Algoritmo SGD per LWLR:</strong></h4>\n<ol>\n<li><strong>Inizializzazione:</strong> $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}, b^{(0)} = 0$</li>\n<li><strong>Per ogni epoca $t = 1, \\ldots, T$:</strong></li>\n<li><strong>Per ogni campione $i = 1, \\ldots, m$:</strong><ul>\n<li>Calcola peso: $w_i = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_q\\|^2}{2\\tau^2}\\right)$</li>\n<li>Calcola predizione: $\\hat{y}_i = \\mathbf{x}_i^T \\boldsymbol{\\theta} + b$</li>\n<li>Calcola errore: $e_i = y_i - \\hat{y}_i$</li>\n<li>Calcola loss pesata: $L_i = \\frac{1}{2} w_i e_i^2$</li>\n<li><strong>Aggiorna parametri:</strong><ul>\n<li>$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha \\cdot w_i \\cdot e_i \\cdot \\mathbf{x}_i$</li>\n<li>$b \\leftarrow b + \\alpha \\cdot w_i \\cdot e_i$</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"derivazione-dei-gradienti-sgd\"><strong>Derivazione dei Gradienti SGD:</strong></h4>\n<p>Per un singolo campione $i$, la loss pesata è:\n$$\nL_i = \\frac{1}{2} w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} - b)^2\n$$</p>\n<p>I gradienti sono:\n$$\n\\frac{\\partial L_i}{\\partial \\boldsymbol{\\theta}} = -w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} - b) \\mathbf{x}_i = -w_i e_i \\mathbf{x}_i\n$$</p>\n$$\n\\frac{\\partial L_i}{\\partial b} = -w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} - b) = -w_i e_i\n$$\n<p>Gli aggiornamenti dei parametri sono quindi:\n$$\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha \\frac{\\partial L_i}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{\\theta} + \\alpha w_i e_i \\mathbf{x}_i\n$$</p>\n$$\nb \\leftarrow b - \\alpha \\frac{\\partial L_i}{\\partial b} = b + \\alpha w_i e_i\n$$\n<h4 id=\"implementazione-sgd\"><strong>Implementazione SGD:</strong></h4>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">lwlr_SGD</span><span class=\"p\">(</span><span class=\"n\">x_q</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Locally Weighted Linear Regression usando SGD</span>\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        x_q: punto query (1, d)</span>\n<span class=\"sd\">        X: training data (m, d)</span>\n<span class=\"sd\">        y: training labels (m, 1)</span>\n<span class=\"sd\">        t: bandwidth parameter</span>\n<span class=\"sd\">        lr: learning rate</span>\n<span class=\"sd\">        epochs: numero di epochs</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">theta</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Assicuriamoci che y sia della forma corretta</span>\n    <span class=\"k\">if</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">dim</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n        <span class=\"n\">total_loss</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n            <span class=\"n\">xi</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]</span>  <span class=\"c1\"># Mantieni dimensione (1, d)</span>\n            <span class=\"n\">yi</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">]</span>  <span class=\"c1\"># Mantieni dimensione (1, 1)</span>\n\n            <span class=\"c1\"># Calcolo del peso</span>\n            <span class=\"n\">diff</span> <span class=\"o\">=</span> <span class=\"n\">xi</span> <span class=\"o\">-</span> <span class=\"n\">x_q</span>  <span class=\"c1\"># (1, d)</span>\n            <span class=\"n\">wi</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span> <span class=\"n\">diff</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">()</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">t</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n\n            <span class=\"c1\"># Predizione</span>\n            <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">xi</span> <span class=\"o\">@</span> <span class=\"n\">theta</span> <span class=\"o\">+</span> <span class=\"n\">bias</span>  <span class=\"c1\"># (1, 1)</span>\n\n            <span class=\"c1\"># Loss pesata</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">wi</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">yi</span> <span class=\"o\">-</span> <span class=\"n\">y_pred</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n            <span class=\"n\">total_loss</span> <span class=\"o\">+=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n            <span class=\"c1\"># Backward pass</span>\n            <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n            <span class=\"c1\"># Update parameters</span>\n            <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n                <span class=\"k\">if</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                    <span class=\"n\">theta</span> <span class=\"o\">-=</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span>\n                <span class=\"k\">if</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                    <span class=\"n\">bias</span> <span class=\"o\">-=</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span>\n\n            <span class=\"c1\"># Zero gradients</span>\n            <span class=\"k\">if</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">zero_</span><span class=\"p\">()</span>\n            <span class=\"k\">if</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">zero_</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Predizione finale</span>\n    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n        <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_q</span> <span class=\"o\">@</span> <span class=\"n\">theta</span> <span class=\"o\">+</span> <span class=\"n\">bias</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">detach</span><span class=\"p\">(),</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"42-batch-gradient-descent-bgd\"><strong>4.2. Batch Gradient Descent (BGD)</strong></h3>\n<p>Nel BGD, i parametri vengono aggiornati utilizzando tutti i punti di training simultaneamente.</p>\n<h4 id=\"algoritmo-bgd-per-lwlr\"><strong>Algoritmo BGD per LWLR:</strong></h4>\n<ol>\n<li><strong>Inizializzazione:</strong> $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}, b^{(0)} = 0$</li>\n<li><strong>Per ogni epoca $t = 1, \\ldots, T$:</strong></li>\n<li>Calcola tutti i pesi: $w_i = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_q\\|^2}{2\\tau^2}\\right), \\forall i$</li>\n<li>Calcola tutte le predizioni: $\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\theta} + b\\mathbf{1}$</li>\n<li>Calcola errori: $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$</li>\n<li>Calcola loss pesata: $L = \\frac{1}{2m} \\sum_{i=1}^m w_i e_i^2$</li>\n<li><strong>Aggiorna parametri:</strong><ul>\n<li>$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\frac{\\alpha}{m} \\sum_{i=1}^m w_i e_i \\mathbf{x}_i$</li>\n<li>$b \\leftarrow b + \\frac{\\alpha}{m} \\sum_{i=1}^m w_i e_i$</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"derivazione-dei-gradienti-bgd\"><strong>Derivazione dei Gradienti BGD:</strong></h4>\n<p>La loss totale pesata è:\n$$\nL = \\frac{1}{2m} \\sum_{i=1}^m w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} - b)^2\n$$</p>\n<p>I gradienti sono:\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{\\theta}} = -\\frac{1}{m} \\sum_{i=1}^m w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} - b) \\mathbf{x}_i = -\\frac{1}{m} \\sum_{i=1}^m w_i e_i \\mathbf{x}_i\n$$</p>\n$$\n\\frac{\\partial L}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^m w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta} - b) = -\\frac{1}{m} \\sum_{i=1}^m w_i e_i\n$$\n<p>In forma matriciale:\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{\\theta}} = -\\frac{1}{m} \\mathbf{X}^T \\mathbf{W} \\mathbf{e}\n$$</p>\n$$\n\\frac{\\partial L}{\\partial b} = -\\frac{1}{m} \\mathbf{1}^T \\mathbf{W} \\mathbf{e}\n$$\n<p>Dove $\\mathbf{W} = \\text{diag}(w_1, w_2, \\ldots, w_m)$ e $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta} - b\\mathbf{1}$.</p>\n<h4 id=\"implementazione-bgd\"><strong>Implementazione BGD:</strong></h4>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">lwlr_BGD</span><span class=\"p\">(</span><span class=\"n\">x_q</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Locally Weighted Linear Regression con Batch Gradient Descent</span>\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        x_q: punto query (1, d)</span>\n<span class=\"sd\">        X: training data (m, d)</span>\n<span class=\"sd\">        y: training labels (m,) o (m, 1)</span>\n<span class=\"sd\">        t: bandwidth parameter</span>\n<span class=\"sd\">        lr: learning rate</span>\n<span class=\"sd\">        epochs: numero di epoche</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">theta</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Assicuriamoci che y sia colonna</span>\n    <span class=\"k\">if</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">dim</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># (m,1)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Differenze rispetto al punto query</span>\n        <span class=\"n\">diff</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">-</span> <span class=\"n\">x_q</span>  <span class=\"c1\"># (m,d)</span>\n        <span class=\"n\">wi</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span> <span class=\"n\">diff</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">t</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">))</span>  <span class=\"c1\"># (m,)</span>\n\n        <span class=\"c1\"># Predizioni su tutti i dati</span>\n        <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">@</span> <span class=\"n\">theta</span> <span class=\"o\">+</span> <span class=\"n\">bias</span>  <span class=\"c1\"># (m,1)</span>\n\n        <span class=\"c1\"># Loss pesata globale</span>\n        <span class=\"n\">residuals</span> <span class=\"o\">=</span> <span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">y_pred</span>  <span class=\"c1\"># (m,1)</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">wi</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">residuals</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"mi\">2</span>\n\n        <span class=\"c1\"># Backward</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Aggiornamento parametri</span>\n        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n            <span class=\"k\">if</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                <span class=\"n\">theta</span> <span class=\"o\">-=</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span>\n            <span class=\"k\">if</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                <span class=\"n\">bias</span> <span class=\"o\">-=</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span>\n\n        <span class=\"c1\"># Azzera gradienti</span>\n        <span class=\"k\">if</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">zero_</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">zero_</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Predizione finale sul punto query</span>\n    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n        <span class=\"n\">y_pred_q</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_q</span> <span class=\"o\">@</span> <span class=\"n\">theta</span> <span class=\"o\">+</span> <span class=\"n\">bias</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">y_pred_q</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">detach</span><span class=\"p\">(),</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<h2 id=\"5-proprieta-e-caratteristiche-di-lwlr\"><strong>5. Proprietà e Caratteristiche di LWLR</strong></h2>\n<h3 id=\"51-vantaggi\"><strong>5.1. Vantaggi</strong></h3>\n<ol>\n<li><strong>Non parametrico</strong>: Non assume una forma specifica per la funzione target</li>\n<li><strong>Flessibilità locale</strong>: Si adatta alle caratteristiche locali dei dati</li>\n<li><strong>Robustezza</strong>: Meno sensibile agli outliers globali</li>\n<li><strong>Interpretabilità</strong>: Ogni previsione ha un modello lineare locale interpretabile</li>\n</ol>\n<h3 id=\"52-svantaggi\"><strong>5.2. Svantaggi</strong></h3>\n<ol>\n<li><strong>Computazione costosa</strong>: Richiede riaddestramento per ogni query</li>\n<li><strong>Maledizione della dimensionalità</strong>: Performance degrada in alta dimensionalità</li>\n<li><strong>Scelta del bandwidth</strong>: Richiede tuning del parametro $\\tau$</li>\n<li><strong>Memoria</strong>: Deve memorizzare tutto il training set</li>\n</ol>\n<h3 id=\"53-bias-variance-tradeoff\"><strong>5.3. Bias-Variance Tradeoff</strong></h3>\n<p>Il parametro $\\tau$ (bandwidth) controlla il tradeoff bias-varianza:</p>\n<ul>\n<li><strong>$\\tau$ piccolo</strong> (finestra stretta):</li>\n<li><strong>Alto bias</strong>: Il modello potrebbe non catturare il pattern locale</li>\n<li><strong>Bassa varianza</strong>: Più sensibile ai cambiamenti nei dati</li>\n<li>\n<p><strong>Overfitting</strong>: Modello molto &ldquo;wiggly&rdquo;</p>\n</li>\n<li>\n<p><strong>$\\tau$ grande</strong> (finestra larga):</p>\n</li>\n<li><strong>Basso bias</strong>: Si avvicina alla regressione lineare globale</li>\n<li><strong>Alta varianza</strong>: Più stabile rispetto ai cambiamenti nei dati</li>\n<li><strong>Underfitting</strong>: Modello troppo smooth</li>\n</ul>\n<h3 id=\"54-scelta-ottimale-del-bandwidth\"><strong>5.4. Scelta Ottimale del Bandwidth</strong></h3>\n<p>La scelta di $\\tau$ può essere effettuata attraverso:</p>\n<ol>\n<li><strong>Cross-validation</strong>: Minimizzare l&rsquo;errore di validazione</li>\n<li><strong>Leave-one-out CV</strong>: Particolarmente efficiente per LWLR</li>\n<li><strong>Regola del pollice</strong>: $\\tau \\approx \\frac{\\text{range dei dati}}{5}$</li>\n<li><strong>Grid search</strong>: Testare diversi valori e scegliere il migliore</li>\n</ol>\n<h2 id=\"6-estensioni-e-varianti\"><strong>6. Estensioni e Varianti</strong></h2>\n<h3 id=\"61-kernel-alternativi\"><strong>6.1. Kernel Alternativi</strong></h3>\n<p>Oltre al kernel gaussiano, si possono utilizzare:</p>\n<ol>\n<li>\n<p><strong>Tricube Kernel</strong>:\n   $$\n   w(u) = \\begin{cases}\n   (1 - |u|^3)^3 & \\text{se } |u| \\leq 1 \\\\\n   0 & \\text{altrimenti}\n   \\end{cases}\n   $$</p>\n</li>\n<li>\n<p><strong>Epanechnikov Kernel</strong>:\n   $$\n   w(u) = \\begin{cases}\n   \\frac{3}{4}(1 - u^2) & \\text{se } |u| \\leq 1 \\\\\n   0 & \\text{altrimenti}\n   \\end{cases}\n   $$</p>\n</li>\n</ol>\n<h3 id=\"62-bandwidth-adattivo\"><strong>6.2. Bandwidth Adattivo</strong></h3>\n<p>Il bandwidth può variare localmente:\n$$\n\\tau_i = \\tau_0 \\cdot k\\text{-th nearest neighbor distance}\n$$</p>\n<h3 id=\"63-lwlr-con-regolarizzazione\"><strong>6.3. LWLR con Regolarizzazione</strong></h3>\n<p>Si può aggiungere regolarizzazione Ridge:\n$$\n\\min_{\\boldsymbol{\\theta}} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T \\mathbf{W}(\\mathbf{x}_q) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}) + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2\n$$</p>\n<p>La soluzione diventa:\n$\n\\boldsymbol{\\theta}^* = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$</p>\n<h2 id=\"7-confronto-dei-metodi\"><strong>7. Confronto dei Metodi</strong></h2>\n<h3 id=\"71-confronto-sgd-vs-bgd-vs-forma-chiusa\"><strong>7.1. Confronto SGD vs BGD vs Forma Chiusa</strong></h3>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Forma Chiusa</th>\n<th>SGD</th>\n<th>BGD</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Convergenza</strong></td>\n<td>Istantanea</td>\n<td>Stocastica</td>\n<td>Deterministica</td>\n</tr>\n<tr>\n<td><strong>Velocità per epoca</strong></td>\n<td>N/A</td>\n<td>Più veloce</td>\n<td>Più lenta</td>\n</tr>\n<tr>\n<td><strong>Memoria</strong></td>\n<td>Alta ($O(m^2)$ per $\\mathbf{W}$)</td>\n<td>Minore</td>\n<td>Intermedia</td>\n</tr>\n<tr>\n<td><strong>Parallelizzazione</strong></td>\n<td>Limitata</td>\n<td>Difficile</td>\n<td>Facile</td>\n</tr>\n<tr>\n<td><strong>Stabilità numerica</strong></td>\n<td>Dipende da cond($\\mathbf{X}^T\\mathbf{W}\\mathbf{X}$)</td>\n<td>Robusta</td>\n<td>Intermedia</td>\n</tr>\n<tr>\n<td><strong>Controllo ottimizzazione</strong></td>\n<td>Nessuno</td>\n<td>Massimo</td>\n<td>Intermedio</td>\n</tr>\n<tr>\n<td><strong>Riproducibilità</strong></td>\n<td>Perfetta</td>\n<td>Richiede seed</td>\n<td>Perfetta</td>\n</tr>\n<tr>\n<td><strong>Scalabilità</strong></td>\n<td>$O(d^3)$</td>\n<td>$O(mde)$</td>\n<td>$O(mde)$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"72-quando-usare-ogni-metodo\"><strong>7.2. Quando Usare Ogni Metodo</strong></h3>\n<p><strong>Forma Chiusa:</strong>\n- Dataset piccolo-medio (&lt; 1000 punti)\n- Matrice ben condizionata\n- Serve la soluzione esatta\n- Riproducibilità è critica</p>\n<p><strong>SGD:</strong>\n- Dataset grande\n- Problemi di condizionamento numerico\n- Serve controllo fine dell&rsquo;ottimizzazione\n- Memoria limitata</p>\n<p><strong>BGD:</strong>\n- Compromesso tra forma chiusa e SGD\n- Convergenza stabile prioritaria\n- Dataset di medie dimensioni</p>\n<h2 id=\"8-esempio-comparativo-pratico\"><strong>8. Esempio Comparativo Pratico</strong></h2>\n<p>Per illustrare le differenze tra le varie implementazioni di LWLR e confrontarle con la regressione lineare classica, consideriamo un esempio con dati sintetici che presentano pattern non lineari.</p>\n<h3 id=\"81-dataset-sintetico\"><strong>8.1. Dataset Sintetico</strong></h3>\n<p>Generiamo un dataset con una relazione non lineare:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n<span class=\"n\">data_points</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">data_points</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># (50, 1)</span>\n\n<span class=\"c1\"># Trend lineare + componente sinusoidale</span>\n<span class=\"n\">y_true</span> <span class=\"o\">=</span> <span class=\"mf\">3.5</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mf\">1.5</span>\n<span class=\"n\">amplitude</span> <span class=\"o\">=</span> <span class=\"mf\">4.0</span>\n<span class=\"n\">frequency</span> <span class=\"o\">=</span> <span class=\"mf\">2.0</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y_true</span> <span class=\"o\">+</span> <span class=\"n\">amplitude</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">frequency</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>  <span class=\"c1\"># Pattern non lineare</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"82-confronto-dei-metodi\"><strong>8.2. Confronto dei Metodi</strong></h3>\n<p>Confrontiamo quattro approcci:</p>\n<ol>\n<li><strong>Regressione Lineare Classica (forma chiusa)</strong>: Trova un&rsquo;unica retta per tutti i dati</li>\n<li><strong>Regressione Lineare Classica (SGD)</strong>: Stessa retta ma ottenuta iterativamente</li>\n<li><strong>LWLR (forma chiusa)</strong>: Adattamento locale ottimale</li>\n<li><strong>LWLR (SGD)</strong>: Adattamento locale iterativo</li>\n</ol>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Dataset di test</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">25</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">y_true_test</span> <span class=\"o\">=</span> <span class=\"mf\">3.5</span> <span class=\"o\">*</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mf\">1.5</span>\n<span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">y_true_test</span> <span class=\"o\">+</span> <span class=\"n\">amplitude</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">frequency</span> <span class=\"o\">*</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Predizioni LWLR (forma chiusa)</span>\n<span class=\"n\">y_preds_lwlr_cf</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">x_q</span> <span class=\"ow\">in</span> <span class=\"n\">X_test</span><span class=\"p\">:</span>\n    <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">lwlr_CF</span><span class=\"p\">(</span><span class=\"n\">x_q</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n    <span class=\"n\">y_preds_lwlr_cf</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Predizioni LWLR (SGD) </span>\n<span class=\"n\">y_preds_lwlr_sgd</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">x_q</span> <span class=\"ow\">in</span> <span class=\"n\">X_test</span><span class=\"p\">:</span>\n    <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">lwlr_SGD</span><span class=\"p\">(</span><span class=\"n\">x_q</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">25</span><span class=\"p\">)</span>\n    <span class=\"n\">y_preds_lwlr_sgd</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"83-visualizzazione-dei-risultati\"><strong>8.3. Visualizzazione dei Risultati</strong></h3>\n<p>Un tipico confronto mostra che:</p>\n<ul>\n<li><strong>Regressione Lineare</strong>: Produce una singola retta che rappresenta il trend medio globale, ma non cattura la variabilità locale</li>\n<li><strong>LWLR (CF)</strong>: Si adatta perfettamente ai pattern locali, seguendo la curvatura dei dati</li>\n<li><strong>LWLR (SGD)</strong>: Produce risultati molto simili alla forma chiusa, ma con leggere variazioni dovute alla natura stocastica dell&rsquo;ottimizzazione</li>\n</ul>\n<h3 id=\"84-analisi-dellerrore\"><strong>8.4. Analisi dell&rsquo;Errore</strong></h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Calcolo MSE per ciascun metodo</span>\n<span class=\"n\">mse_linear</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">y_pred_linear</span> <span class=\"o\">-</span> <span class=\"n\">y_test</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">mse_lwlr_cf</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">y_preds_lwlr_cf</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">y_test</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span> \n<span class=\"n\">mse_lwlr_sgd</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">y_preds_lwlr_sgd</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">y_test</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;MSE Linear Regression: </span><span class=\"si\">{</span><span class=\"n\">mse_linear</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;MSE LWLR (Closed Form): </span><span class=\"si\">{</span><span class=\"n\">mse_lwlr_cf</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>  \n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;MSE LWLR (SGD): </span><span class=\"si\">{</span><span class=\"n\">mse_lwlr_sgd</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><strong>Risultati tipici:</strong>\n- <strong>Linear Regression</strong>: MSE più alto, non cattura la non linearità\n- <strong>LWLR (CF)</strong>: MSE più basso, adattamento ottimale\n- <strong>LWLR (SGD)</strong>: MSE simile alla forma chiusa, dipende da learning rate ed epoche</p>\n<h3 id=\"85-effetto-del-bandwidth\"><strong>8.5. Effetto del Bandwidth</strong></h3>\n<p>Il parametro $\\tau$ ha un impatto significativo sulle prestazioni:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Test con diversi valori di bandwidth</span>\n<span class=\"n\">bandwidths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">]</span>\n<span class=\"n\">mse_values</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"k\">for</span> <span class=\"n\">tau</span> <span class=\"ow\">in</span> <span class=\"n\">bandwidths</span><span class=\"p\">:</span>\n    <span class=\"n\">y_preds</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">x_q</span> <span class=\"ow\">in</span> <span class=\"n\">X_test</span><span class=\"p\">:</span>\n        <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">lwlr_CF</span><span class=\"p\">(</span><span class=\"n\">x_q</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"o\">=</span><span class=\"n\">tau</span><span class=\"p\">)</span>\n        <span class=\"n\">y_preds</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n\n    <span class=\"n\">mse</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">y_preds</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">y_test</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">mse_values</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mse</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Il bandwidth ottimale minimizza l&#39;MSE sul test set</span>\n<span class=\"n\">optimal_tau</span> <span class=\"o\">=</span> <span class=\"n\">bandwidths</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">argmin</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">mse_values</span><span class=\"p\">))]</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><strong>Osservazioni:</strong>\n- <strong>$\\tau$ troppo piccolo</strong> ($< 0.5$): Overfitting, modello troppo &ldquo;nervoso&rdquo;\n- <strong>$\\tau$ troppo grande</strong> ($> 2.0$): Underfitting, si avvicina alla regressione lineare\n- <strong>$\\tau$ ottimale</strong> ($\\approx 0.5-1.0$): Bilancia bias e varianza</p>\n<h3 id=\"86-considerazioni-pratiche\"><strong>8.6. Considerazioni Pratiche</strong></h3>\n<ol>\n<li><strong>Tempo di calcolo</strong>: La forma chiusa è più veloce per singole predizioni, ma SGD può essere parallelizzato</li>\n<li><strong>Stabilità numerica</strong>: SGD è più robusto quando la matrice è mal condizionata  </li>\n<li><strong>Controllo fine</strong>: SGD permette di monitorare la convergenza e implementare early stopping</li>\n<li><strong>Riproducibilità</strong>: La forma chiusa è deterministica, SGD richiede seed fissato</li>\n</ol>\n<h2 id=\"9-complessita-computazionale\"><strong>9. Complessità Computazionale</strong></h2>\n<h3 id=\"91-analisi-della-complessita\"><strong>9.1. Analisi della Complessità</strong></h3>\n<p><strong>Soluzione Analitica (Forma Chiusa):</strong>\n- <strong>Calcolo pesi</strong>: $O(md)$ \n- <strong>Costruzione matrice $\\mathbf{W}$</strong>: $O(m^2)$\n- <strong>Prodotti matriciali</strong>: $O(md^2 + d^3)$\n- <strong>Inversione matrice</strong>: $O(d^3)$\n- <strong>Complessità totale per query</strong>: $O(m^2 + md^2 + d^3)$\n- <strong>Spazio</strong>: $O(m^2 + md)$</p>\n<p><strong>SGD:</strong>\n- <strong>Complessità per epoca</strong>: $O(md)$\n- <strong>Complessità totale</strong>: $O(mdE)$ dove $E$ è il numero di epoche\n- <strong>Spazio</strong>: $O(md)$</p>\n<p><strong>BGD:</strong>\n- <strong>Complessità per epoca</strong>: $O(md)$ \n- <strong>Complessità totale</strong>: $O(mdE)$ dove $E$ è il numero di epoche\n- <strong>Spazio</strong>: $O(md)$</p>\n<h3 id=\"92-scalabilita\"><strong>9.2. Scalabilità</strong></h3>\n<ul>\n<li><strong>Forma chiusa</strong>: Non scala bene per $m$ o $d$ grandi</li>\n<li><strong>SGD/BGD</strong>: Scalano linearmente con $m$ e $d$</li>\n</ul>\n<h2 id=\"10-applicazioni-pratiche\"><strong>10. Applicazioni Pratiche</strong></h2>\n<p>LWLR è particolarmente utile in:</p>\n<ol>\n<li><strong>Time series forecasting</strong>: Pattern temporali che cambiano localmente</li>\n<li><strong>Robotics</strong>: Controllo adattivo e apprendimento di traiettorie</li>\n<li><strong>Computer vision</strong>: Interpolazione di immagini, tracking</li>\n<li><strong>Bioinformatica</strong>: Analisi di dati genomici con pattern locali</li>\n<li><strong>Economia</strong>: Modelli che si adattano a regimi economici diversi</li>\n<li><strong>Meteorologia</strong>: Previsioni che si adattano alle condizioni locali</li>\n</ol>\n<h2 id=\"11-limitazioni-e-considerazioni\"><strong>11. Limitazioni e Considerazioni</strong></h2>\n<h3 id=\"111-maledizione-della-dimensionalita\"><strong>11.1. Maledizione della Dimensionalità</strong></h3>\n<p>In alta dimensionalità ($d > 10-20$):\n- I punti diventano equidistanti\n- Tutti i pesi tendono a essere simili\n- Il concetto di &ldquo;località&rdquo; perde significato\n- Performance si degrada rapidamente</p>\n<h3 id=\"112-problemi-numerici\"><strong>11.2. Problemi Numerici</strong></h3>\n<ol>\n<li><strong>Matrice singolare</strong>: Quando $\\mathbf{X}^T\\mathbf{W}\\mathbf{X}$ non è invertibile</li>\n<li><strong>Condizionamento numerico</strong>: Alto numero di condizione causa instabilità</li>\n<li><strong>Underflow</strong>: Pesi estremamente piccoli possono causare problemi</li>\n</ol>\n<h3 id=\"113-soluzioni\"><strong>11.3. Soluzioni</strong></h3>\n<ul>\n<li><strong>Regolarizzazione</strong>: Aggiungere $\\lambda \\mathbf{I}$ alla matrice</li>\n<li><strong>SVD</strong>: Usare decomposizione SVD invece dell&rsquo;inversione diretta</li>\n<li><strong>Thresholding</strong>: Impostare soglia minima per i pesi</li>\n</ul>\n<h2 id=\"12-conclusioni\"><strong>12. Conclusioni</strong></h2>\n<p>LWLR rappresenta un potente compromesso tra la semplicità della regressione lineare e la flessibilità dei metodi non parametrici. La sua capacità di adattarsi localmente ai dati lo rende particolarmente adatto per problemi dove la relazione target varia spazialmente o temporalmente.</p>\n<h3 id=\"121-linee-guida-per-la-scelta-del-metodo\"><strong>12.1. Linee Guida per la Scelta del Metodo</strong></h3>\n<p><strong>Usa la forma chiusa quando:</strong>\n- Dataset piccolo-medio (&lt; 1000 punti)\n- Dimensionalità bassa ($d < 20$)\n- Matrice ben condizionata\n- Serve la soluzione esatta\n- Riproducibilità è critica</p>\n<p><strong>Usa SGD quando:</strong>\n- Dataset grande ($m > 10000$)\n- Problemi di condizionamento numerico\n- Memoria limitata\n- Serve controllo fine dell&rsquo;ottimizzazione\n- Possibilità di parallelizzazione</p>\n<p><strong>Usa BGD quando:</strong>\n- Serve compromesso tra forma chiusa e SGD\n- Convergenza stabile è prioritaria\n- Dataset di medie dimensioni\n- Si vuole evitare il rumore del SGD</p>\n<h3 id=\"122-considerazioni-finali\"><strong>12.2. Considerazioni Finali</strong></h3>\n<p>Le tre implementazioni offrono diversi vantaggi:\n- <strong>Forma chiusa</strong>: Soluzione esatta e veloce per problemi piccoli\n- <strong>SGD</strong>: Scalabilità e robustezza numerica\n- <strong>BGD</strong>: Stabilità e controllo della convergenza</p>\n<p>La scelta dipende dalle caratteristiche del problema: dimensionalità dei dati, dimensione del dataset, requisiti di accuratezza e risorse computazionali disponibili.</p>\n<p>LWLR rimane uno strumento fondamentale nell&rsquo;arsenal del machine learning, particolarmente efficace quando i pattern nei dati variano localmente e si necessita di un approccio che bilanci interpretabilità e flessibilità.</p>"
}