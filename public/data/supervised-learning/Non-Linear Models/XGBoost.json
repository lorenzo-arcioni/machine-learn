{
  "title": "XGBoost: Un Sistema Scalabile di Tree Boosting",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"indice\">Indice</h2>\n<ol>\n<li><a href=\"#introduzione\">Introduzione</a></li>\n<li><a href=\"#fondamenti\">Fondamenti del Tree Boosting</a></li>\n<li><a href=\"#obiettivo\">Obiettivo Regolarizzato</a></li>\n<li><a href=\"#split-finding\">Algoritmi di Split Finding</a></li>\n<li><a href=\"#system-design\">Design del Sistema</a></li>\n<li><a href=\"#ottimizzazioni\">Ottimizzazioni Avanzate</a></li>\n<li><a href=\"#risultati\">Risultati Sperimentali</a></li>\n</ol>\n<hr />\n<h2 id=\"introduzione\">1. Introduzione</h2>\n<h3 id=\"cose-xgboost\">Cos&rsquo;è XGBoost?</h3>\n<p>XGBoost (eXtreme Gradient Boosting) è un sistema di machine learning altamente efficiente e scalabile basato sul <strong>gradient tree boosting</strong>. Sviluppato da Tianqi Chen e Carlos Guestrin all&rsquo;Università di Washington, è diventato lo strumento di riferimento per competizioni di machine learning e applicazioni industriali.</p>\n<h3 id=\"perche-xgboost-e-importante\">Perché XGBoost è importante?</h3>\n<p>Nel 2015, <strong>17 delle 29 soluzioni vincenti</strong> su Kaggle hanno utilizzato XGBoost. Questa predominanza dimostra l&rsquo;efficacia del sistema in una vasta gamma di problemi:\n- Classificazione di eventi in fisica delle alte energie\n- Previsione di vendite\n- Click-through rate prediction\n- Classificazione di malware\n- Rilevamento del movimento\n- E molti altri</p>\n<h3 id=\"principali-innovazioni\">Principali Innovazioni</h3>\n<p>XGBoost introduce quattro innovazioni fondamentali:</p>\n<ol>\n<li><strong>Algoritmo sparsity-aware</strong>: gestisce in modo efficiente dati sparsi (missing values, encoding one-hot)</li>\n<li><strong>Weighted quantile sketch</strong>: permette l&rsquo;apprendimento approssimato con garanzie teoriche</li>\n<li><strong>Struttura a blocchi cache-aware</strong>: ottimizza l&rsquo;accesso alla memoria</li>\n<li><strong>Out-of-core computation</strong>: gestisce dataset che non entrano in memoria</li>\n</ol>\n<hr />\n<h2 id=\"fondamenti\">2. Fondamenti del Tree Boosting</h2>\n<h3 id=\"cose-il-gradient-boosting\">Cos&rsquo;è il Gradient Boosting?</h3>\n<p>Il gradient boosting è una tecnica di <strong>ensemble learning</strong> che combina più modelli deboli (tipicamente alberi di decisione) per creare un modello forte. L&rsquo;idea chiave è aggiungere iterativamente nuovi modelli che correggono gli errori dei modelli precedenti.</p>\n<h3 id=\"il-modello-ensemble\">Il Modello Ensemble</h3>\n<p>Dato un dataset $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}$ con $n$ esempi e $m$ features ($|\\mathcal{D}| = n$, $\\mathbf{x}_i \\in \\mathbb{R}^m$, $y_i \\in \\mathbb{R}$), XGBoost usa un modello ensemble di $K$ alberi:</p>\n$$\\hat{y}_i = \\phi(\\mathbf{x}_i) = \\sum_{k=1}^K f_k(\\mathbf{x}_i), \\quad f_k \\in \\mathcal{F}$$\n<p>dove $\\mathcal{F} = \\{f(\\mathbf{x}) = w_{q(\\mathbf{x})}\\}$ è lo spazio degli alberi di regressione (CART - Classification And Regression Trees).</p>\n<h4 id=\"componenti-di-un-albero\">Componenti di un Albero</h4>\n<p>Ogni funzione $f_k$ è definita da:\n- <strong>Struttura $q$</strong>: mappa un esempio a un indice di foglia ($q: \\mathbb{R}^m \\rightarrow T$)\n- <strong>Pesi delle foglie $w$</strong>: vettore di $T$ valori reali ($w \\in \\mathbb{R}^T$)\n- $T$ è il numero di foglie nell&rsquo;albero</p>\n<p><strong>Differenza con alberi di decisione classici</strong>: A differenza degli alberi di decisione che assegnano categorie, gli alberi di regressione assegnano un <strong>punteggio continuo</strong> $w_i$ a ogni foglia.</p>\n<hr />\n<h2 id=\"obiettivo\">3. Obiettivo Regolarizzato</h2>\n<h3 id=\"funzione-obiettivo\">Funzione Obiettivo</h3>\n<p>XGBoost minimizza un obiettivo <strong>regolarizzato</strong> che bilancia accuratezza e complessità del modello:</p>\n$$\\mathcal{L}(\\phi) = \\sum_{i} l(\\hat{y}_i, y_i) + \\sum_{k} \\Omega(f_k)$$\n<p>dove:\n- $l$ è una <strong>funzione di loss differenziabile e convessa</strong> (es. errore quadratico, log-loss)\n- $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$ è il <strong>termine di regolarizzazione</strong></p>\n<h4 id=\"interpretazione-del-termine-di-regolarizzazione\">Interpretazione del Termine di Regolarizzazione</h4>\n$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$$\n<ul>\n<li>$\\gamma T$: penalizza il <strong>numero di foglie</strong> (favorisce alberi più semplici)</li>\n<li>$\\frac{1}{2}\\lambda \\|w\\|^2$: penalizza <strong>pesi grandi</strong> (regolarizzazione L2, simile a Ridge)</li>\n</ul>\n<p><strong>Perché la regolarizzazione è importante?</strong>\n- Previene l&rsquo;overfitting\n- Favorisce modelli più semplici e interpretabili\n- Migliora la generalizzazione</p>\n<p>Quando $\\lambda = \\gamma = 0$, l&rsquo;obiettivo si riduce al gradient boosting tradizionale.</p>\n<h3 id=\"apprendimento-additivo\">Apprendimento Additivo</h3>\n<p>Poiché ottimizzare direttamente $K$ alberi è intrattabile, XGBoost usa un approccio <strong>greedy additivo</strong>:</p>\n<ol>\n<li>Inizia con $\\hat{y}_i^{(0)} = 0$</li>\n<li>A ogni iterazione $t$, aggiungi un albero $f_t$ che minimizza:</li>\n</ol>\n$$\\mathcal{L}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$$\n<h3 id=\"approssimazione-di-taylor-del-secondo-ordine\">Approssimazione di Taylor del Secondo Ordine</h3>\n<p>Per ottimizzare velocemente l&rsquo;obiettivo, XGBoost usa l&rsquo;<strong>espansione di Taylor</strong> del secondo ordine:</p>\n$$\\mathcal{L}^{(t)} \\simeq \\sum_{i=1}^n \\left[l(y_i, \\hat{y}^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2}h_i f_t^2(\\mathbf{x}_i)\\right] + \\Omega(f_t)$$\n<p>dove:\n- $g_i = \\frac{\\partial l(y_i, \\hat{y}^{(t-1)})}{\\partial \\hat{y}^{(t-1)}}$ è il <strong>gradiente primo</strong>\n- $h_i = \\frac{\\partial^2 l(y_i, \\hat{y}^{(t-1)})}{\\partial (\\hat{y}^{(t-1)})^2}$ è il <strong>gradiente secondo</strong> (Hessiana)</p>\n<p><strong>Perché il secondo ordine?</strong>\n- Convergenza più rapida (come il metodo di Newton vs. gradient descent)\n- Migliore approssimazione locale della funzione di loss\n- Maggiore stabilità numerica</p>\n<p>Rimuovendo i termini costanti:</p>\n$$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[g_i f_t(\\mathbf{x}_i) + \\frac{1}{2}h_i f_t^2(\\mathbf{x}_i)\\right] + \\Omega(f_t)$$\n<h3 id=\"calcolo-dei-pesi-ottimali\">Calcolo dei Pesi Ottimali</h3>\n<p>Definiamo $I_j = \\{i | q(\\mathbf{x}_i) = j\\}$ come l&rsquo;insieme di esempi assegnati alla foglia $j$.</p>\n<p>Riscriviamo l&rsquo;obiettivo raggruppando per foglie:</p>\n$$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{j=1}^T \\left[\\left(\\sum_{i \\in I_j} g_i\\right)w_j + \\frac{1}{2}\\left(\\sum_{i \\in I_j} h_i + \\lambda\\right)w_j^2\\right] + \\gamma T$$\n<p>Questa è una <strong>funzione quadratica</strong> in $w_j$. Per ogni foglia $j$, il peso ottimale è:</p>\n$$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$$\n<p><strong>Interpretazione</strong>:\n- Numeratore: somma dei gradienti (direzione di &ldquo;correzione&rdquo;)\n- Denominatore: somma delle Hessiane + regolarizzazione (scala di &ldquo;confidenza&rdquo;)</p>\n<p>Sostituendo $w_j^*$ nell&rsquo;obiettivo, otteniamo il <strong>punteggio di qualità</strong> della struttura:</p>\n$$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2}\\sum_{j=1}^T \\frac{\\left(\\sum_{i \\in I_j} g_i\\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T$$\n<p>Questo punteggio misura quanto è &ldquo;buona&rdquo; una particolare struttura di albero $q$.</p>\n<h3 id=\"valutazione-degli-split\">Valutazione degli Split</h3>\n<p>Per decidere dove dividere un nodo, calcoliamo il <strong>guadagno</strong> dello split:</p>\n<p>Sia $I = I_L \\cup I_R$ l&rsquo;insieme di esempi prima dello split, e $I_L$, $I_R$ gli insiemi dopo. Il guadagno è:</p>\n$$\\mathcal{L}_{split} = \\frac{1}{2}\\left[\\frac{\\left(\\sum_{i \\in I_L} g_i\\right)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{\\left(\\sum_{i \\in I_R} g_i\\right)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{\\left(\\sum_{i \\in I} g_i\\right)^2}{\\sum_{i \\in I} h_i + \\lambda}\\right] - \\gamma$$\n<p><strong>Interpretazione</strong>:\n- Primi due termini: qualità dei due nodi figli\n- Terzo termine: qualità del nodo padre\n- $-\\gamma$: penalità per aver aggiunto una foglia</p>\n<p>Uno split è vantaggioso se $\\mathcal{L}_{split} > 0$.</p>\n<h3 id=\"tecniche-aggiuntive-di-regolarizzazione\">Tecniche Aggiuntive di Regolarizzazione</h3>\n<h4 id=\"1-shrinkage-learning-rate\">1. Shrinkage (Learning Rate)</h4>\n<p>Dopo ogni iterazione, i pesi dei nuovi alberi sono scalati di un fattore $\\eta$ (tipicamente 0.01-0.3):</p>\n$$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta f_t(\\mathbf{x}_i)$$\n<p><strong>Effetto</strong>: riduce l&rsquo;influenza di ogni singolo albero, lasciando spazio agli alberi futuri per miglioramenti incrementali.</p>\n<h4 id=\"2-column-subsampling\">2. Column Subsampling</h4>\n<p>A ogni iterazione (o split), si seleziona casualmente una frazione delle features (tipicamente 50-80%).</p>\n<p><strong>Vantaggi</strong>:\n- Previene l&rsquo;overfitting (simile a Random Forest)\n- Accelera il calcolo\n- Secondo il feedback degli utenti, è più efficace del row subsampling</p>\n<hr />\n<h2 id=\"split-finding\">4. Algoritmi di Split Finding</h2>\n<h3 id=\"41-exact-greedy-algorithm\">4.1 Exact Greedy Algorithm</h3>\n<p>L&rsquo;algoritmo <strong>exact greedy</strong> enumera tutti i possibili split per ogni feature:</p>\n<p><strong>Procedimento</strong>:\n1. Per ogni feature $k = 1, \\ldots, m$:\n   - Ordina gli esempi per il valore della feature $k$\n   - Scansiona linearmente per calcolare le statistiche di gradiente cumulative\n   - Valuta tutti i possibili split usando la formula del guadagno</p>\n<p><strong>Complessità</strong>: $O(n \\cdot m \\cdot \\log n)$ per albero (dominato dall&rsquo;ordinamento)</p>\n<p><strong>Vantaggi</strong>:\n- Trova sempre lo split ottimale\n- Semplice da implementare</p>\n<p><strong>Svantaggi</strong>:\n- Non scalabile per dati enormi\n- Problematico quando i dati non entrano in memoria</p>\n<h3 id=\"42-approximate-algorithm\">4.2 Approximate Algorithm</h3>\n<p>Per dataset grandi, XGBoost propone un algoritmo <strong>approssimato</strong> basato su quantili:</p>\n<p><strong>Idea chiave</strong>: Invece di considerare tutti i possibili valori di split, proponi un insieme di $l$ <strong>candidati</strong> $S_k = \\{s_{k1}, s_{k2}, \\ldots, s_{kl}\\}$ per ogni feature $k$.</p>\n<p><strong>Varianti</strong>:</p>\n<ol>\n<li><strong>Global proposal</strong>: i candidati sono proposti una sola volta all&rsquo;inizio e usati per tutti i livelli dell&rsquo;albero</li>\n<li><strong>Local proposal</strong>: i candidati sono ri-proposti dopo ogni split (più costoso ma più accurato per alberi profondi)</li>\n</ol>\n<p><strong>Procedimento</strong>:\n1. Proponi candidati basati sui percentili della distribuzione\n2. Mappa i valori continui in bucket definiti dai candidati\n3. Aggrega le statistiche per bucket\n4. Trova il miglior split tra i candidati</p>\n<p><strong>Parametro chiave</strong>: $\\epsilon$ (precisione dell&rsquo;approssimazione)\n- Approssimativamente $1/\\epsilon$ candidati per feature\n- $\\epsilon = 0.1$ → circa 10 bucket per feature\n- $\\epsilon$ più piccolo → più accurato ma più costoso</p>\n<h3 id=\"43-weighted-quantile-sketch\">4.3 Weighted Quantile Sketch</h3>\n<p><strong>Problema</strong>: Come selezionare i candidati in modo che rappresentino bene i dati?</p>\n<p>XGBoost usa i <strong>quantili pesati</strong> invece dei semplici percentili.</p>\n<h4 id=\"perche-pesati\">Perché Pesati?</h4>\n<p>Riscriviamo l&rsquo;obiettivo approssimato come:</p>\n$$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\frac{1}{2}h_i\\left(f_t(\\mathbf{x}_i) - \\frac{g_i}{h_i}\\right)^2 + \\Omega(f_t) + \\text{const}$$\n<p>Questa è una <strong>weighted squared loss</strong> con:\n- Etichette: $g_i / h_i$\n- Pesi: $h_i$</p>\n<p>Quindi gli esempi con Hessiana maggiore dovrebbero avere più influenza nella selezione dei candidati.</p>\n<h4 id=\"rank-function\">Rank Function</h4>\n<p>Definiamo la <strong>rank function pesata</strong> per la feature $k$:</p>\n$$r_k(z) = \\frac{1}{\\sum_{(x,h) \\in \\mathcal{D}_k} h} \\sum_{(x,h) \\in \\mathcal{D}_k, x < z} h$$\n<p>dove $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2), \\ldots, (x_{nk}, h_n)\\}$.</p>\n<p><strong>Obiettivo</strong>: Trovare candidati $\\{s_{k1}, s_{k2}, \\ldots, s_{kl}\\}$ tali che:</p>\n$$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon$$\n<p>con $s_{k1} = \\min_i x_{ik}$ e $s_{kl} = \\max_i x_{ik}$.</p>\n<p><strong>Innovazione</strong>: XGBoost introduce un nuovo algoritmo distribuibile con garanzie teoriche per calcolare questi quantili pesati (descritto nell&rsquo;appendix del paper).</p>\n<h3 id=\"44-sparsity-aware-split-finding\">4.4 Sparsity-Aware Split Finding</h3>\n<p><strong>Problema</strong>: I dati reali sono spesso <strong>sparsi</strong> a causa di:\n- Missing values\n- Zero frequenti nelle statistiche\n- Feature engineering (es. one-hot encoding)</p>\n<p><strong>Soluzione</strong>: Algoritmo che impara una <strong>direzione di default</strong> per ogni split.</p>\n<h4 id=\"come-funziona\">Come Funziona</h4>\n<p>Per ogni split candidato:\n1. Assegna gli esempi <strong>non-missing</strong> ai nodi sinistro/destro normalmente\n2. Prova <strong>due scenari</strong>:\n   - Esempi missing vanno a <strong>sinistra</strong>\n   - Esempi missing vanno a <strong>destra</strong>\n3. Scegli la direzione che massimizza il guadagno</p>\n<p><strong>Vantaggi</strong>:\n- Complessità <strong>lineare</strong> nel numero di valori non-missing: $O(\\|x\\|_0)$ invece di $O(n)$\n- Gestisce naturalmente tutti i pattern di sparsità\n- 50x più veloce su dati sparsi (come dimostrato su Allstate dataset)</p>\n<p><strong>Dettaglio tecnico</strong>: Durante la scansione, consideriamo solo $I_k = \\{i \\in I | x_{ik} \\neq \\text{missing}\\}$.</p>\n<hr />\n<h2 id=\"system-design\">5. Design del Sistema</h2>\n<h3 id=\"51-column-block-structure\">5.1 Column Block Structure</h3>\n<p><strong>Problema</strong>: L&rsquo;ordinamento dei dati è l&rsquo;operazione più costosa nel tree learning.</p>\n<p><strong>Soluzione</strong>: Struttura a <strong>blocchi</strong> con dati pre-ordinati.</p>\n<h4 id=\"caratteristiche\">Caratteristiche</h4>\n<ul>\n<li>Dati memorizzati in blocchi in-memory</li>\n<li>Formato <strong>Compressed Sparse Column (CSC)</strong></li>\n<li>Ogni colonna ordinata per valore della feature</li>\n<li>Ordinamento fatto <strong>una sola volta</strong> prima del training</li>\n</ul>\n<h4 id=\"vantaggi\">Vantaggi</h4>\n<ol>\n<li><strong>Exact greedy</strong>: un singolo blocco con scan lineare</li>\n<li><strong>Approximate</strong>: più blocchi distribuibili o su disco</li>\n<li><strong>Parallelizzazione</strong>: ogni colonna può essere processata in parallelo</li>\n<li><strong>Column subsampling</strong>: facile selezionare subset di colonne</li>\n</ol>\n<h4 id=\"complessita-temporale\">Complessità Temporale</h4>\n<p><strong>Senza block structure</strong>:\n- Exact greedy: $O(Kd\\|\\mathbf{x}\\|_0 \\log n)$ dove:\n  - $K$ = numero di alberi\n  - $d$ = profondità massima\n  - $\\|\\mathbf{x}\\|_0$ = numero di entry non-missing</p>\n<p><strong>Con block structure</strong>:\n- Exact greedy: $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log n)$\n- Approximate: $O(Kd\\|\\mathbf{x}\\|_0 + \\|\\mathbf{x}\\|_0 \\log B)$ dove $B$ = max righe per blocco</p>\n<p><strong>Risparmio</strong>: fattore $\\log n$ (o $\\log q$ per approximate), significativo per $n$ grande.</p>\n<h3 id=\"52-cache-aware-access\">5.2 Cache-Aware Access</h3>\n<p><strong>Problema</strong>: La struttura a blocchi richiede accessi alla memoria <strong>non contigui</strong> per le statistiche di gradiente.</p>\n<h4 id=\"pattern-di-accesso\">Pattern di Accesso</h4>\n<p>Durante lo split finding:\n1. Le features sono accedute in ordine (dal blocco ordinato)\n2. Gli indici di riga sono <strong>sparsi</strong> e non sequenziali\n3. Le statistiche di gradiente ($g_i$, $h_i$) sono accedute per indice di riga\n4. Questo crea <strong>cache miss</strong> frequenti</p>\n<p><strong>Dipendenza read/write immediata</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Per ogni valore in colonna ordinata:\n    Leggi indice riga i (non contiguo)\n    Fetch g_i, h_i (cache miss!)\n    Accumula statistiche (scrittura)\n</code></pre></div>\n</div>\n</details>\n\n<h4 id=\"soluzione-prefetching\">Soluzione: Prefetching</h4>\n<p><strong>Algoritmo cache-aware</strong>:\n1. Alloca un <strong>buffer interno</strong> per ogni thread\n2. Fetch delle statistiche di gradiente in <strong>mini-batch</strong>\n3. Accumula su batch invece che elemento per elemento</p>\n<p><strong>Effetto</strong>: trasforma la dipendenza read/write da immediata a più lunga, riducendo l&rsquo;overhead.</p>\n<p><strong>Risultati</strong>: 2x più veloce su dataset grandi (10M+ esempi).</p>\n<h4 id=\"scelta-della-block-size\">Scelta della Block Size</h4>\n<p>Per l&rsquo;algoritmo approssimato, la dimensione del blocco è critica:</p>\n<ul>\n<li><strong>Troppo piccola</strong>: parallelizzazione inefficiente</li>\n<li><strong>Troppo grande</strong>: cache miss (le statistiche non entrano in cache)</li>\n</ul>\n<p><strong>Scelta ottimale</strong>: $2^{16}$ (65,536) esempi per blocco\n- Bilancia cache e parallelizzazione\n- Confermato empiricamente su diversi dataset</p>\n<h3 id=\"53-out-of-core-computation\">5.3 Out-of-Core Computation</h3>\n<p><strong>Obiettivo</strong>: Processare dataset che non entrano in RAM.</p>\n<h4 id=\"strategia-base\">Strategia Base</h4>\n<ol>\n<li>Dividi i dati in <strong>multipli blocchi</strong></li>\n<li>Memorizza ogni blocco su disco</li>\n<li>Usa un <strong>thread indipendente</strong> per prefetch dei blocchi in memoria</li>\n<li>Computa mentre leggi da disco (overlapping I/O e computation)</li>\n</ol>\n<p><strong>Problema</strong>: Il disk I/O domina il tempo di calcolo.</p>\n<h4 id=\"ottimizzazione-1-block-compression\">Ottimizzazione 1: Block Compression</h4>\n<p><strong>Tecnica</strong>:\n- Comprimi ogni blocco per colonne\n- Decomprimi on-the-fly con thread indipendente durante il caricamento</p>\n<p><strong>Compressione utilizzata</strong>:\n- Feature values: algoritmo general-purpose\n- Row indices: offset a 16-bit ($2^{16}$ esempi per blocco)</p>\n<p><strong>Risultati</strong>:\n- <strong>26-29% compression ratio</strong> sui dataset testati\n- Trade-off: computation (decompressione) vs. disk reading</p>\n<h4 id=\"ottimizzazione-2-block-sharding\">Ottimizzazione 2: Block Sharding</h4>\n<p><strong>Tecnica</strong>:\n- Distribuisci i dati su <strong>multipli dischi</strong> in modo alternato\n- Thread di prefetch dedicato per ogni disco\n- Thread di training legge alternativamente da ogni buffer</p>\n<p><strong>Vantaggi</strong>:\n- Aumenta il <strong>throughput</strong> di lettura\n- Parallelizza l&rsquo;I/O su più dispositivi</p>\n<p><strong>Risultati combinati</strong>:\n- Compression: 3x speedup\n- Sharding (2 dischi): ulteriori 2x speedup\n- <strong>6x più veloce</strong> dell&rsquo;approccio base</p>\n<hr />\n<h2 id=\"ottimizzazioni\">6. Ottimizzazioni Avanzate</h2>\n<h3 id=\"61-confronto-con-altri-sistemi\">6.1 Confronto con Altri Sistemi</h3>\n<p>XGBoost è l&rsquo;unico sistema che combina tutte le seguenti capacità:</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>XGBoost</th>\n<th>pGBRT</th>\n<th>Spark MLLib</th>\n<th>H2O</th>\n<th>scikit-learn</th>\n<th>R gbm</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Exact greedy</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n<td>✗</td>\n<td>✓</td>\n<td>✓</td>\n</tr>\n<tr>\n<td>Approximate global</td>\n<td>✓</td>\n<td>✗</td>\n<td>✓</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Approximate local</td>\n<td>✓</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Out-of-core</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n<td>✗</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Sparsity-aware</td>\n<td>✓</td>\n<td>✗</td>\n<td>Partial</td>\n<td>Partial</td>\n<td>✗</td>\n<td>Partial</td>\n</tr>\n<tr>\n<td>Parallel</td>\n<td>✓</td>\n<td>✓</td>\n<td>✓</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"62-integrazione-e-portabilita\">6.2 Integrazione e Portabilità</h3>\n<p>XGBoost è disponibile in molteplici ecosistemi:</p>\n<p><strong>Linguaggi</strong>:\n- Python (con integrazione scikit-learn)\n- R\n- Julia\n- Java/Scala</p>\n<p><strong>Piattaforme distribuite</strong>:\n- Hadoop\n- Apache Spark\n- Apache Flink\n- MPI\n- Sun Grid Engine</p>\n<p><strong>Libreria base</strong>: rabit per operazioni allreduce distribuite</p>\n<hr />\n<h2 id=\"risultati\">7. Risultati Sperimentali</h2>\n<h3 id=\"71-dataset-utilizzati\">7.1 Dataset Utilizzati</h3>\n<h4 id=\"allstate-insurance-10m\">Allstate Insurance (10M)</h4>\n<ul>\n<li><strong>Task</strong>: Classificazione probabilità di claim assicurativo</li>\n<li><strong>Features</strong>: 4,227 (principalmente sparse da one-hot encoding)</li>\n<li><strong>Uso</strong>: Valutare sparsity-aware algorithm</li>\n</ul>\n<h4 id=\"higgs-boson-10m\">Higgs Boson (10M)</h4>\n<ul>\n<li><strong>Task</strong>: Classificazione eventi fisici</li>\n<li><strong>Features</strong>: 28 (21 cinetiche + 7 derivate)</li>\n<li><strong>Uso</strong>: Confronti con baseline, cache evaluation</li>\n</ul>\n<h4 id=\"yahoo-ltrc-473k\">Yahoo LTRC (473K)</h4>\n<ul>\n<li><strong>Task</strong>: Learning to rank</li>\n<li><strong>Features</strong>: 700</li>\n<li><strong>Query</strong>: ~20K con ~22 documenti ciascuna</li>\n<li><strong>Uso</strong>: Confronto con pGBRT</li>\n</ul>\n<h4 id=\"criteo-17b\">Criteo (1.7B)</h4>\n<ul>\n<li><strong>Task</strong>: Click-through rate prediction</li>\n<li><strong>Features</strong>: 67 (13 integer + 26 CTR stats + 26 counts)</li>\n<li><strong>Dimensione</strong>: &gt; 1 TB in formato LibSVM</li>\n<li><strong>Uso</strong>: Scalabilità distribuita e out-of-core</li>\n</ul>\n<h3 id=\"72-risultati-di-classificazione\">7.2 Risultati di Classificazione</h3>\n<p><strong>Higgs-1M (500 alberi)</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Tempo per albero (sec)</th>\n<th>Test AUC</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>XGBoost</td>\n<td>0.684</td>\n<td>0.8304</td>\n</tr>\n<tr>\n<td>XGBoost (colsample=0.5)</td>\n<td>0.640</td>\n<td>0.8245</td>\n</tr>\n<tr>\n<td>scikit-learn</td>\n<td>28.51</td>\n<td>0.8302</td>\n</tr>\n<tr>\n<td>R gbm</td>\n<td>1.032</td>\n<td>0.6224</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Osservazioni</strong>:\n- XGBoost è <strong>10x più veloce</strong> di scikit-learn con accuratezza simile\n- Column subsampling migliora velocità con minima perdita di accuratezza\n- R gbm è più veloce ma molto meno accurato (usa greedy one-side)</p>\n<h3 id=\"73-learning-to-rank\">7.3 Learning to Rank</h3>\n<p><strong>Yahoo LTRC (500 alberi)</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Tempo per albero (sec)</th>\n<th>NDCG@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>XGBoost</td>\n<td>0.826</td>\n<td>0.7892</td>\n</tr>\n<tr>\n<td>XGBoost (colsample=0.5)</td>\n<td>0.506</td>\n<td>0.7913</td>\n</tr>\n<tr>\n<td>pGBRT</td>\n<td>2.576</td>\n<td>0.7915</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Osservazioni</strong>:\n- XGBoost exact greedy batte pGBRT approximate in velocità\n- Column subsampling <strong>migliora</strong> leggermente l&rsquo;accuratezza (previene overfitting)</p>\n<h3 id=\"74-impatto-sparsity-aware\">7.4 Impatto Sparsity-Aware</h3>\n<p><strong>Allstate-10K</strong>:\n- Sparsity-aware algorithm: <strong>50x più veloce</strong> della versione naive\n- Conferma l&rsquo;importanza critica dell&rsquo;ottimizzazione per dati sparsi</p>\n<h3 id=\"75-out-of-core-performance\">7.5 Out-of-Core Performance</h3>\n<p><strong>Criteo (subsets crescenti su singola macchina)</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>100M</th>\n<th>200M</th>\n<th>400M</th>\n<th>1.7B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Basic</td>\n<td>OK</td>\n<td>OK</td>\n<td>Slow</td>\n<td>OOM</td>\n</tr>\n<tr>\n<td>+Compression</td>\n<td>3x faster</td>\n<td>3x faster</td>\n<td>OK</td>\n<td>OK</td>\n</tr>\n<tr>\n<td>+Sharding (2 disks)</td>\n<td>6x faster</td>\n<td>6x faster</td>\n<td>2x faster</td>\n<td>OK</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Macchina</strong>: AWS c3.8xlarge (32 vcore, 2x320GB SSD, 60GB RAM)</p>\n<p><strong>Osservazioni</strong>:\n- Compression da sola: 3x speedup\n- Sharding aggiunge ulteriore 2x\n- Gestisce 1.7B esempi su singola macchina desktop</p>\n<h3 id=\"76-performance-distribuita\">7.6 Performance Distribuita</h3>\n<p><strong>Criteo completo (32 nodi EC2 m3.2xlarge, 10 iterazioni)</strong>:</p>\n<h4 id=\"end-to-end-time-incluso-data-loading\">End-to-end time (incluso data loading):</h4>\n<ul>\n<li><strong>Spark MLLib</strong>: Out of memory a 400M</li>\n<li><strong>H2O</strong>: Molto lento nel loading, out of memory a 800M</li>\n<li><strong>XGBoost</strong>: Scala linearmente fino a 1.7B</li>\n</ul>\n<h4 id=\"per-iteration-time\">Per-iteration time:</h4>\n<ul>\n<li>XGBoost è <strong>10x più veloce di Spark</strong> per iterazione</li>\n<li>XGBoost è <strong>2.2x più veloce di H2O</strong> per iterazione</li>\n</ul>\n<h4 id=\"scaling-con-numero-di-macchine-dataset-completo-17b\">Scaling con numero di macchine (dataset completo 1.7B):</h4>\n<ul>\n<li><strong>4 macchine</strong>: Gestisce l&rsquo;intero dataset</li>\n<li><strong>8 macchine</strong>: ~1.8x speedup</li>\n<li><strong>16 macchine</strong>: ~3.5x speedup</li>\n<li><strong>32 macchine</strong>: ~6.5x speedup</li>\n</ul>\n<p>Scaling leggermente super-lineare grazie a maggiore file cache disponibile.</p>\n<hr />\n<h2 id=\"8-dettagli-implementativi-importanti\">8. Dettagli Implementativi Importanti</h2>\n<h3 id=\"81-calcolo-delle-statistiche-di-gradiente\">8.1 Calcolo delle Statistiche di Gradiente</h3>\n<p>Per ogni loss function, dobbiamo calcolare:</p>\n<p><strong>Loss quadratica</strong> $l(y, \\hat{y}) = (y - \\hat{y})^2$:\n- $g_i = -2(y_i - \\hat{y}_i^{(t-1)})$\n- $h_i = 2$</p>\n<p><strong>Log-loss</strong> (classificazione binaria) $l(y, \\hat{y}) = y\\log(1 + e^{-\\hat{y}}) + (1-y)\\log(1 + e^{\\hat{y}})$:\n- $g_i = p_i - y_i$ dove $p_i = 1/(1 + e^{-\\hat{y}_i^{(t-1)}})$\n- $h_i = p_i(1 - p_i)$</p>\n<h3 id=\"82-gestione-di-missing-values\">8.2 Gestione di Missing Values</h3>\n<p>L&rsquo;algoritmo sparsity-aware gestisce tre scenari:</p>\n<ol>\n<li><strong>Valore realmente mancante</strong>: impara direzione ottimale</li>\n<li><strong>Zero in sparse matrix</strong>: trattato come missing, impara direzione</li>\n<li><strong>Valore specifico dell&rsquo;utente</strong>: può essere configurato</li>\n</ol>\n<h3 id=\"83-parallelizzazione\">8.3 Parallelizzazione</h3>\n<p><strong>Level di parallelizzazione</strong>:\n1. <strong>Feature-level</strong>: ogni feature processata da thread diverso\n2. <strong>Tree-level</strong>: multipli alberi costruiti in parallelo (meno comune)\n3. <strong>Data-level</strong>: dati partizionati tra nodi (distributed)</p>\n<p><strong>Sincronizzazione</strong>: AllReduce per aggregare statistiche di gradiente</p>\n<hr />\n<h2 id=\"9-confronto-con-random-forest\">9. Confronto con Random Forest</h2>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>XGBoost</th>\n<th>Random Forest</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Costruzione alberi</strong></td>\n<td>Sequenziale (boosting)</td>\n<td>Parallela (bagging)</td>\n</tr>\n<tr>\n<td><strong>Dipendenza</strong></td>\n<td>Ogni albero dipende dai precedenti</td>\n<td>Alberi indipendenti</td>\n</tr>\n<tr>\n<td><strong>Profondità</strong></td>\n<td>Tipicamente limitata (3-10)</td>\n<td>Alberi completi</td>\n</tr>\n<tr>\n<td><strong>Prediction</strong></td>\n<td>Somma pesata</td>\n<td>Media semplice</td>\n</tr>\n<tr>\n<td><strong>Overfitting</strong></td>\n<td>Controllato con regolarizzazione</td>\n<td>Controllato con randomness</td>\n</tr>\n<tr>\n<td><strong>Interpretabilità</strong></td>\n<td>Difficile (ensemble sequenziale)</td>\n<td>Media (aggregazione)</td>\n</tr>\n<tr>\n<td><strong>Performance</strong></td>\n<td>Generalmente migliore su strutturati</td>\n<td>Buona baseline</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"10-best-practices-e-tuning\">10. Best Practices e Tuning</h2>\n<h3 id=\"101-parametri-chiave\">10.1 Parametri Chiave</h3>\n<p><strong>Struttura albero</strong>:\n- <code>max_depth</code>: 3-10 (default 6)\n- <code>min_child_weight</code>: sum of instance weight needed in a child\n- <code>gamma</code>: minimum loss reduction required for split</p>\n<p><strong>Regolarizzazione</strong>:\n- <code>lambda</code>: L2 reg on weights (default 1)\n- <code>alpha</code>: L1 reg on weights (default 0)\n- <code>eta</code>: learning rate 0.01-0.3 (default 0.3)</p>\n<p><strong>Sampling</strong>:\n- <code>subsample</code>: row sampling ratio 0.5-1.0\n- <code>colsample_bytree</code>: column sampling per tree 0.5-1.0\n- <code>colsample_bylevel</code>: column sampling per level</p>\n<h3 id=\"102-strategia-di-tuning\">10.2 Strategia di Tuning</h3>\n<p><strong>Step 1</strong>: Fissa parametri conservativi\n- <code>max_depth</code>: 6\n- <code>eta</code>: 0.3\n- <code>subsample</code>, <code>colsample_bytree</code>: 0.8</p>\n<p><strong>Step 2</strong>: Ottimizza struttura albero\n- Varia <code>max_depth</code> (3-10)\n- Varia <code>min_child_weight</code> (1-6)\n- Usa cross-validation</p>\n<p><strong>Step 3</strong>: Aggiungi regolarizzazione\n- Aumenta <code>lambda</code> se overfitting\n- Aggiungi <code>alpha</code> per feature selection</p>\n<p><strong>Step 4</strong>: Riduci learning rate e aumenta alberi\n- Riduci <code>eta</code> a 0.01-0.1\n- Aumenta numero alberi di conseguenza\n- Early stopping per evitare overfitting</p>\n<h3 id=\"103-segni-di-overfitting\">10.3 Segni di Overfitting</h3>\n<ul>\n<li>Train error molto &lt; validation error</li>\n<li>Validation error aumenta mentre train error diminuisce</li>\n<li>Performance degrada su test set</li>\n</ul>\n<p><strong>Soluzioni</strong>:\n1. Aumenta <code>lambda</code>, <code>alpha</code>\n2. Riduci <code>max_depth</code>\n3. Aumenta <code>min_child_weight</code>\n4. Usa <code>subsample</code>, <code>colsample_bytree</code> &lt; 1.0\n5. Riduci <code>eta</code> e usa early stopping</p>\n<hr />\n<h2 id=\"11-vantaggi-e-limitazioni\">11. Vantaggi e Limitazioni</h2>\n<h3 id=\"111-vantaggi-di-xgboost\">11.1 Vantaggi di XGBoost</h3>\n<p><strong>Performance</strong>:\n- State-of-the-art su dati tabulari\n- Velocità superiore ai competitor\n- Scala a miliardi di esempi</p>\n<p><strong>Flessibilità</strong>:\n- Supporta custom loss functions\n- Gestisce missing values nativamente\n- Funziona con dati sparsi</p>\n<p><strong>Robustezza</strong>:\n- Regolarizzazione built-in\n- Meno propenso a overfitting\n- Gestisce outliers bene</p>\n<p><strong>Praticità</strong>:\n- Pochi hyperparameter da tunare\n- Feature importance automatica\n- Integrazione in molti ecosistemi</p>\n<h3 id=\"112-limitazioni\">11.2 Limitazioni</h3>\n<p><strong>Interpretabilità</strong>:\n- Modelli complessi difficili da interpretare\n- Ensemble di centinaia di alberi\n- Non lineare e non parametrico</p>\n<p><strong>Dati non strutturati</strong>:\n- Non ottimale per immagini, testo, audio\n- Deep learning preferibile per questi domini\n- Richiede feature engineering manuale</p>\n<p><strong>Memoria</strong>:\n- Richiede tutto il dataset in memoria (o out-of-core)\n- Alberi memorizzati interamente\n- Può essere memory-intensive</p>\n<p><strong>Training time</strong>:\n- Più lento di linear models\n- Sequenziale per natura (boosting)\n- Non parallelizzabile quanto Random Forest</p>\n<p><strong>Extrapolation</strong>:\n- Non estrapolano bene fuori dal range di training\n- Previsioni limitate ai valori visti\n- Problematico per serie temporali con trend</p>\n<hr />\n<h2 id=\"12-applicazioni-pratiche\">12. Applicazioni Pratiche</h2>\n<h3 id=\"121-quando-usare-xgboost\">12.1 Quando Usare XGBoost</h3>\n<p><strong>Ideale per</strong>:\n- Dati tabulari strutturati\n- Classification e regression\n- Ranking problems\n- Competizioni Kaggle\n- Feature importance analysis\n- Dataset medi-grandi (1K - 100M+ rows)</p>\n<p><strong>Esempi di successo</strong>:\n- Click-through rate prediction (advertising)\n- Credit scoring (finanza)\n- Fraud detection (sicurezza)\n- Customer churn prediction (business)\n- Medical diagnosis (healthcare)\n- Energy consumption forecasting</p>\n<h3 id=\"122-quando-non-usare-xgboost\">12.2 Quando NON Usare XGBoost</h3>\n<p><strong>Alternative migliori</strong>:\n- <strong>Immagini/Video</strong>: CNN (Convolutional Neural Networks)\n- <strong>Testo/NLP</strong>: Transformers (BERT, GPT)\n- <strong>Audio</strong>: RNN, WaveNet\n- <strong>Dati piccoli (&lt; 1000 rows)</strong>: Linear models, SVM\n- <strong>Real-time inference critico</strong>: Linear models, decision trees singoli\n- <strong>Interpretabilità critica</strong>: Linear models, GAMs, single decision trees</p>\n<hr />\n<h2 id=\"13-innovazioni-tecniche-dettagliate\">13. Innovazioni Tecniche Dettagliate</h2>\n<h3 id=\"131-weighted-quantile-sketch\">13.1 Weighted Quantile Sketch</h3>\n<p><strong>Problema formale</strong>:\nDato un multi-set $\\mathcal{D}_k = \\{(x_{1k}, h_1), (x_{2k}, h_2), \\ldots, (x_{nk}, h_n)\\}$, definire:</p>\n<p>$r_k(z) = \\frac{1}{\\sum_{(x,h) \\in \\mathcal{D}_k} h} \\sum_{(x,h) \\in \\mathcal{D}_k, x < z} h$</p>\n<p><strong>Obiettivo</strong>: Trovare $\\{s_{k1}, s_{k2}, \\ldots, s_{kl}\\}$ tale che:</p>\n<p>$|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\epsilon$</p>\n<p><strong>Proprietà chiave</strong>:\n- <strong>Merge operation</strong>: Combina due summary con errore $\\max(\\epsilon_1, \\epsilon_2)$\n- <strong>Prune operation</strong>: Riduce elementi mantenendo garanzie di errore\n- <strong>Distribuibile</strong>: Può essere calcolato in parallelo</p>\n<p><strong>Algoritmo</strong>:\n1. Costruisci summary locale per ogni partizione\n2. Merge dei summary con garanzie di errore\n3. Prune per ridurre dimensione mantenendo precisione</p>\n<h3 id=\"132-column-block-dettagli-implementativi\">13.2 Column Block - Dettagli Implementativi</h3>\n<p><strong>Struttura dati</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Block = {\n    feature_columns: [Column_1, Column_2, ..., Column_m]\n    row_indices: sorted per ogni colonna\n    gradient_stats: [g_1, g_2, ..., g_n], [h_1, h_2, ..., h_n]\n}\n</code></pre></div>\n</div>\n</details>\n\n<p><strong>Processo di costruzione</strong>:\n1. Per ogni feature $k$:\n   - Crea coppie $(valore_{ik}, indice_i)$\n   - Ordina per valore\n   - Memorizza in formato compresso\n2. Un solo ordinamento iniziale\n3. Riutilizzato per tutte le iterazioni</p>\n<p><strong>Accesso durante split finding</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Per ogni feature k:\n    Per ogni valore v in column k (già ordinato):\n        i = get_row_index(v)\n        g_L += g[i]\n        h_L += h[i]\n        Calcola gain per split\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"133-gestione-missing-values-algoritmo-completo\">13.3 Gestione Missing Values - Algoritmo Completo</h3>\n<p><strong>Procedura per ogni split candidato</strong>:</p>\n<ol>\n<li>\n<p><strong>Raccogli statistiche non-missing</strong>:\n   $G_{present} = \\sum_{i: x_{ik} \\neq missing} g_i$\n   $H_{present} = \\sum_{i: x_{ik} \\neq missing} h_i$</p>\n</li>\n<li>\n<p><strong>Scenario A - Missing vanno a sinistra</strong>:</p>\n</li>\n<li>Scansiona valori in ordine crescente</li>\n<li>\n<p>Per ogni threshold $t$:</p>\n<ul>\n<li>$G_L = G_{missing} + \\sum_{i: x_{ik} < t} g_i$</li>\n<li>$G_R = \\sum_{i: x_{ik} \\geq t} g_i$</li>\n<li>Calcola gain</li>\n</ul>\n</li>\n<li>\n<p><strong>Scenario B - Missing vanno a destra</strong>:</p>\n</li>\n<li>Scansiona valori in ordine decrescente</li>\n<li>\n<p>Per ogni threshold $t$:</p>\n<ul>\n<li>$G_R = G_{missing} + \\sum_{i: x_{ik} \\geq t} g_i$</li>\n<li>$G_L = \\sum_{i: x_{ik} < t} g_i$</li>\n<li>Calcola gain</li>\n</ul>\n</li>\n<li>\n<p><strong>Scegli direzione ottimale</strong>: quella che massimizza il gain</p>\n</li>\n</ol>\n<p><strong>Complessità</strong>: $O(|I_k|)$ dove $I_k$ sono gli esempi non-missing (invece di $O(|I|)$)</p>\n<hr />\n<h2 id=\"14-matematica-avanzata\">14. Matematica Avanzata</h2>\n<h3 id=\"141-derivazione-completa-dellobiettivo\">14.1 Derivazione Completa dell&rsquo;Obiettivo</h3>\n<p>Partiamo dalla funzione obiettivo generale:</p>\n<p>$\\mathcal{L}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t)$</p>\n<p><strong>Espansione di Taylor al secondo ordine</strong> intorno a $\\hat{y}_i^{(t-1)}$:</p>\n<p>$l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) \\approx l(y_i, \\hat{y}_i^{(t-1)}) + \\frac{\\partial l}{\\partial \\hat{y}_i^{(t-1)}} f_t(\\mathbf{x}_i) + \\frac{1}{2} \\frac{\\partial^2 l}{\\partial (\\hat{y}_i^{(t-1)})^2} f_t^2(\\mathbf{x}_i)$</p>\n<p>Definendo:\n- $g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i^{(t-1)}}$\n- $h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{\\partial (\\hat{y}_i^{(t-1)})^2}$</p>\n<p>Otteniamo:</p>\n<p>$\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\Omega(f_t)$</p>\n<p>Rimuovendo il termine costante $l(y_i, \\hat{y}_i^{(t-1)})$:</p>\n<p>$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n [g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t^2(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$</p>\n<p><strong>Raggruppando per foglie</strong> con $I_j = \\{i | q(\\mathbf{x}_i) = j\\}$ e $f_t(\\mathbf{x}_i) = w_{q(\\mathbf{x}_i)}$:</p>\n<p>$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{j=1}^T \\left[\\left(\\sum_{i \\in I_j} g_i\\right) w_j + \\frac{1}{2}\\left(\\sum_{i \\in I_j} h_i + \\lambda\\right) w_j^2\\right] + \\gamma T$</p>\n<p><strong>Minimizzazione rispetto a $w_j$</strong>:</p>\n<p>$\\frac{\\partial \\tilde{\\mathcal{L}}^{(t)}}{\\partial w_j} = \\sum_{i \\in I_j} g_i + \\left(\\sum_{i \\in I_j} h_i + \\lambda\\right) w_j = 0$</p>\n<p>$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$</p>\n<p><strong>Sostituendo nell&rsquo;obiettivo</strong>:</p>\n<p>$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T$</p>\n<h3 id=\"142-interpretazione-geometrica\">14.2 Interpretazione Geometrica</h3>\n<p>Il termine di regolarizzazione può essere visto come:</p>\n<p>$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2$</p>\n<p><strong>Interpretazione bayesiana</strong>:\n- Prior gaussiano sui pesi: $w_j \\sim \\mathcal{N}(0, 1/\\lambda)$\n- Penalità sul numero di foglie: preference per alberi semplici\n- MAP estimation invece di MLE</p>\n<p><strong>Interpretazione MDL</strong> (Minimum Description Length):\n- $\\gamma T$: costo di codifica della struttura\n- $\\frac{1}{2}\\lambda \\|w\\|^2$: costo di codifica dei pesi\n- Minimizzare $\\mathcal{L}$ = minimizzare lunghezza descrizione</p>\n<hr />\n<h2 id=\"15-confronto-con-altre-tecniche-di-boosting\">15. Confronto con Altre Tecniche di Boosting</h2>\n<h3 id=\"151-adaboost-vs-xgboost\">15.1 AdaBoost vs XGBoost</h3>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>AdaBoost</th>\n<th>XGBoost</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Loss</strong></td>\n<td>Exponential loss</td>\n<td>Generic differentiable loss</td>\n</tr>\n<tr>\n<td><strong>Ottimizzazione</strong></td>\n<td>Peso esempi</td>\n<td>Gradient descent in function space</td>\n</tr>\n<tr>\n<td><strong>Weak learners</strong></td>\n<td>Tipicamente stumps</td>\n<td>Regression trees con depth</td>\n</tr>\n<tr>\n<td><strong>Regolarizzazione</strong></td>\n<td>Implicita</td>\n<td>Esplicita (L1, L2, gamma)</td>\n</tr>\n<tr>\n<td><strong>Robustezza</strong></td>\n<td>Sensibile a outliers</td>\n<td>Più robusto</td>\n</tr>\n<tr>\n<td><strong>Flessibilità</strong></td>\n<td>Limitata</td>\n<td>Alta (custom objectives)</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"152-gradient-boosting-sklearn-vs-xgboost\">15.2 Gradient Boosting (sklearn) vs XGBoost</h3>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>sklearn GBM</th>\n<th>XGBoost</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Regolarizzazione</strong></td>\n<td>Solo max_depth, min_samples</td>\n<td>L1, L2, gamma, alpha</td>\n</tr>\n<tr>\n<td><strong>Secondo ordine</strong></td>\n<td>No</td>\n<td>Sì (Newton boosting)</td>\n</tr>\n<tr>\n<td><strong>Parallelizzazione</strong></td>\n<td>No</td>\n<td>Sì</td>\n</tr>\n<tr>\n<td><strong>Missing values</strong></td>\n<td>No (errore)</td>\n<td>Sì (learn direction)</td>\n</tr>\n<tr>\n<td><strong>Sparsità</strong></td>\n<td>Ignora</td>\n<td>Ottimizzato</td>\n</tr>\n<tr>\n<td><strong>Cache optimization</strong></td>\n<td>No</td>\n<td>Sì</td>\n</tr>\n<tr>\n<td><strong>Out-of-core</strong></td>\n<td>No</td>\n<td>Sì</td>\n</tr>\n<tr>\n<td><strong>Velocità</strong></td>\n<td>1x</td>\n<td>10-40x</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"153-lightgbm-vs-xgboost\">15.3 LightGBM vs XGBoost</h3>\n<p><strong>LightGBM</strong> (Microsoft, 2017):</p>\n<p><strong>Vantaggi di LightGBM</strong>:\n- <strong>Leaf-wise</strong> growth invece di level-wise (più veloce)\n- <strong>GOSS</strong> (Gradient-based One-Side Sampling)\n- <strong>EFB</strong> (Exclusive Feature Bundling)\n- Più veloce su dataset molto grandi\n- Usa meno memoria</p>\n<p><strong>Vantaggi di XGBoost</strong>:\n- Più maturo e testato\n- Level-wise growth più conservativo (meno overfitting)\n- Migliore supporto distribuito\n- Più opzioni di regolarizzazione</p>\n<p><strong>Quando usare quale</strong>:\n- <strong>XGBoost</strong>: Dataset medio-grandi, bisogno di stabilità\n- <strong>LightGBM</strong>: Dataset enormi (&gt; 10M rows), bisogno di velocità</p>\n<h3 id=\"154-catboost-vs-xgboost\">15.4 CatBoost vs XGBoost</h3>\n<p><strong>CatBoost</strong> (Yandex, 2017):</p>\n<p><strong>Vantaggi di CatBoost</strong>:\n- <strong>Categorical features</strong> gestite nativamente (senza encoding)\n- <strong>Ordered boosting</strong> riduce overfitting\n- <strong>Symmetric trees</strong> più veloci in inference\n- Meno tuning richiesto (buoni defaults)</p>\n<p><strong>Vantaggi di XGBoost</strong>:\n- Più veloce su dati non categorici\n- Migliore per dati sparsi\n- Ecosistema più maturo\n- Più flessibile</p>\n<hr />\n<h2 id=\"16-feature-importance-in-xgboost\">16. Feature Importance in XGBoost</h2>\n<h3 id=\"161-metriche-di-importanza\">16.1 Metriche di Importanza</h3>\n<p><strong>1. Gain (default)</strong>:\n- Media del guadagno di loss quando feature è usata per split\n- Formula: $\\frac{1}{K} \\sum_{k=1}^K \\sum_{\\text{splits on feature}} \\mathcal{L}_{split}$\n- <strong>Pro</strong>: Misura impatto reale sulla loss\n- <strong>Contro</strong>: Bias verso features con molti possibili split</p>\n<p><strong>2. Weight (Frequency)</strong>:\n- Numero di volte che feature appare negli split\n- <strong>Pro</strong>: Semplice, intuitivo\n- <strong>Contro</strong>: Non considera qualità degli split</p>\n<p><strong>3. Cover</strong>:\n- Media del numero di esempi affetti da split sulla feature\n- Formula: $\\frac{1}{K} \\sum_{k=1}^K \\sum_{\\text{splits on feature}} |I|$\n- <strong>Pro</strong>: Considera distribuzione dei dati\n- <strong>Contro</strong>: Bias verso features comuni</p>\n<h3 id=\"162-shap-values-per-xgboost\">16.2 SHAP Values per XGBoost</h3>\n<p><strong>TreeSHAP</strong> (Lundberg &amp; Lee, 2017):</p>\n<p>Calcola contributi Shapley per ogni feature:</p>\n<p>$\\phi_j(x) = \\sum_{S \\subseteq F \\setminus \\{j\\}} \\frac{|S|!(|F| - |S| - 1)!}{|F|!} [f_{S \\cup \\{j\\}}(x_S) - f_S(x_S)]$</p>\n<p><strong>Vantaggi</strong>:\n- Garanzie teoriche (unico metodo con proprietà desiderabili)\n- Additive: $\\sum_j \\phi_j(x) = f(x) - E[f(X)]$\n- Consistenza: se feature contribuisce di più, ha SHAP maggiore</p>\n<p><strong>Implementazione efficiente</strong> per trees:\n- Complexity $O(TLD^2)$ invece di $O(TL2^M)$\n  - $T$ = numero alberi\n  - $L$ = max numero foglie\n  - $D$ = max profondità\n  - $M$ = numero features</p>\n<hr />\n<h2 id=\"17-ottimizzazioni-pratiche\">17. Ottimizzazioni Pratiche</h2>\n<h3 id=\"171-early-stopping\">17.1 Early Stopping</h3>\n<p><strong>Strategia</strong>:\n1. Dividi dati in train/validation\n2. Monitora metrica su validation set\n3. Ferma training se metrica non migliora per $n$ rounds</p>\n<p><strong>Implementazione</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Parametri:\n- early_stopping_rounds: numero iterazioni senza miglioramento\n- eval_metric: metrica da monitorare\n- eval_set: validation set\n\nProcesso:\nFor each iteration t:\n    Construisci albero f_t su train\n    Valuta su eval_set\n    Se metrica non migliora per n rounds:\n        STOP e return best iteration\n</code></pre></div>\n</div>\n</details>\n\n<p><strong>Benefici</strong>:\n- Previene overfitting automaticamente\n- Riduce tempo training\n- Trova numero ottimale alberi</p>\n<h3 id=\"172-cross-validation\">17.2 Cross-Validation</h3>\n<p><strong>CV distribuito in XGBoost</strong>:\n- Ogni fold processato in parallelo\n- Stratified CV per classification\n- Supporto per custom folds</p>\n<p><strong>Utilizzo ottimale</strong>:\n1. CV per hyperparameter tuning\n2. Train finale su tutti i dati\n3. Early stopping su separate validation</p>\n<h3 id=\"173-monotonic-constraints\">17.3 Monotonic Constraints</h3>\n<p><strong>Problema</strong>: In alcuni domini, relazioni devono essere monotone\n- Es: Credit scoring - income ↑ → credit score ↑\n- Es: Insurance - age ↑ → premium ↑</p>\n<p><strong>Soluzione in XGBoost</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Parametro monotone_constraints:\n- +1: relazione crescente\n- -1: relazione decrescente\n-  0: nessun constraint\n</code></pre></div>\n</div>\n</details>\n\n<p><strong>Implementazione</strong>:\n- Durante split finding, considera solo split che rispettano constraint\n- Garantisce monotonia globale del modello</p>\n<hr />\n<h2 id=\"18-casi-duso-avanzati\">18. Casi d&rsquo;Uso Avanzati</h2>\n<h3 id=\"181-custom-objective-functions\">18.1 Custom Objective Functions</h3>\n<p>XGBoost permette di definire loss custom fornendo:</p>\n<p><strong>Gradient</strong>:\n$g_i = \\frac{\\partial l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}$</p>\n<p><strong>Hessian</strong>:\n$h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}$</p>\n<p><strong>Esempio - Huber Loss</strong> (robusto a outliers):</p>\n<p>$l(y, \\hat{y}) = \\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n\\end{cases}$</p>\n<p>$g = \\begin{cases}\n\\hat{y} - y & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta \\cdot \\text{sign}(\\hat{y} - y) & \\text{otherwise}\n\\end{cases}$</p>\n<p>$h = \\begin{cases}\n1 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n0 & \\text{otherwise}\n\\end{cases}$</p>\n<h3 id=\"182-multi-output-regression\">18.2 Multi-output Regression</h3>\n<p>Per predire $K$ output simultaneamente:</p>\n<p><strong>Approccio 1 - Modelli separati</strong>:\n- Train $K$ modelli XGBoost indipendenti\n- <strong>Pro</strong>: Semplice, parallelizzabile\n- <strong>Contro</strong>: Ignora correlazioni tra output</p>\n<p><strong>Approccio 2 - Multi-task learning</strong>:\n- Custom objective che considera tutti $K$ output\n- <strong>Pro</strong>: Sfrutta correlazioni\n- <strong>Contro</strong>: Più complesso, richiede tuning</p>\n<h3 id=\"183-imbalanced-classification\">18.3 Imbalanced Classification</h3>\n<p><strong>Problema</strong>: Classi sbilanciate (es. fraud: 0.1% positivi)</p>\n<p><strong>Soluzioni</strong>:</p>\n<p><strong>1. Scale_pos_weight</strong>:\n$\\text{scale_pos_weight} = \\frac{\\text{count}(y=0)}{\\text{count}(y=1)}$\n- Aumenta peso degli esempi positivi</p>\n<p><strong>2. Custom weighted loss</strong>:\n- Penalizza errori su classe minoritaria di più</p>\n<p><strong>3. Threshold tuning</strong>:\n- XGBoost produce probabilità\n- Ottimizza threshold per F1, precision, recall</p>\n<p><strong>4. Focal Loss</strong>:\n$FL(p_t) = -(1-p_t)^\\gamma \\log(p_t)$\n- Focalizza su esempi difficili\n- $\\gamma$ controlla focus (tipicamente 2)</p>\n<hr />\n<h2 id=\"19-limitazioni-teoriche-e-pratiche\">19. Limitazioni Teoriche e Pratiche</h2>\n<h3 id=\"191-limiti-teorici\">19.1 Limiti Teorici</h3>\n<p><strong>1. Capacità di Approssimazione</strong>:\n- Alberi CART possono approssimare funzioni continue (teorema di approssimazione universale)\n- MA richiedono numero esponenziale di foglie per alcune funzioni\n- Particolarmente inefficienti per funzioni molto smooth</p>\n<p><strong>2. Sample Complexity</strong>:\n- Numero di esempi necessari cresce con complessità della funzione target\n- Per funzioni con interazioni di alto ordine, serve $O(2^d)$ esempi\n- Non efficiente quanto metodi parametrici quando assumptions sono corretti</p>\n<p><strong>3. Generalizzazione</strong>:\n- Bound di generalizzazione dipendono da:\n  - Numero alberi $K$\n  - Profondità $d$\n  - Numero esempi $n$\n- Trade-off bias-variance esplicito</p>\n<h3 id=\"192-limiti-pratici\">19.2 Limiti Pratici</h3>\n<p><strong>1. Tempo di Training</strong>:\n- $O(nmd)$ per iterazione nel caso base\n- Con $K$ alberi: $O(Knmd)$\n- Non scala bene con numero di features $m$</p>\n<p><strong>2. Consumo Memoria</strong>:\n- Block structure richiede $O(nm)$ memoria\n- Gradient stats: $O(n)$ per iterazione\n- Alberi: $O(KT)$ dove $T$ = foglie medie\n- Total: $O(nm + Kn + KT)$</p>\n<p><strong>3. Hyperparameter Sensitivity</strong>:\n- Molti parametri da tunare\n- Interazioni complesse tra parametri\n- Richiede expertise per tuning ottimale</p>\n<p><strong>4. Interpretabilità</strong>:\n- Ensemble di centinaia di alberi\n- Impossibile visualizzare completamente\n- SHAP values aiutano ma sono computazionalmente costosi</p>\n<hr />\n<h2 id=\"20-conclusioni-e-direzioni-future\">20. Conclusioni e Direzioni Future</h2>\n<h3 id=\"201-contributi-chiave-di-xgboost\">20.1 Contributi Chiave di XGBoost</h3>\n<ol>\n<li><strong>Sistema end-to-end</strong> che combina algoritmi e ottimizzazioni di sistema</li>\n<li><strong>Scalabilità</strong> senza precedenti (billions di esempi, single machine)</li>\n<li><strong>Innovazioni algoritmiche</strong>: sparsity-aware, weighted quantile sketch</li>\n<li><strong>Innovazioni di sistema</strong>: cache-aware, out-of-core, block structure</li>\n<li><strong>Impatto pratico</strong>: dominanza in competizioni ML e industry adoption</li>\n</ol>\n<h3 id=\"202-lezioni-apprese\">20.2 Lezioni Apprese</h3>\n<p><strong>Design di Sistemi ML</strong>:\n- <strong>Co-design</strong> algoritmo-sistema è cruciale\n- Ottimizzazioni hardware (cache, I/O) fanno differenza enorme\n- Scalabilità richiede attenzione a ogni livello dello stack</p>\n<p><strong>Machine Learning Pratico</strong>:\n- Regolarizzazione è essenziale per generalizzazione\n- Gestione sparsità e missing values deve essere first-class\n- Flessibilità (custom objectives) abilita nuove applicazioni</p>\n<h3 id=\"203-sviluppi-post-paper\">20.3 Sviluppi Post-Paper</h3>\n<p><strong>LightGBM</strong> (2017):\n- Leaf-wise growth\n- Histogram-based learning\n- Più veloce su dataset enormi</p>\n<p><strong>CatBoost</strong> (2017):\n- Ordered boosting\n- Categorical features native\n- Symmetric trees</p>\n<p><strong>XGBoost Improvements</strong>:\n- GPU acceleration\n- Federated learning support\n- Categorical features support (2022)\n- Distributed training migliorato</p>\n<h3 id=\"204-quando-xgboost-rimane-la-scelta-migliore\">20.4 Quando XGBoost Rimane la Scelta Migliore</h3>\n<p><strong>Nel 2024-2026</strong>:\n- <strong>Dati tabulari strutturati</strong>: ancora SOTA\n- <strong>Interpretabilità importante</strong>: con SHAP\n- <strong>Risorse limitate</strong>: efficiente su singola macchina\n- <strong>Production stability</strong>: maturo e testato\n- <strong>Integrazione ecosistema</strong>: supporto ovunque</p>\n<p><strong>Alternative emergenti</strong>:\n- <strong>TabNet, FT-Transformer</strong>: neural networks per tabular data\n- <strong>AutoML</strong>: automated feature engineering + XGBoost\n- <strong>Deep learning</strong>: quando dati non-strutturati o huge scale</p>\n<h3 id=\"205-direzioni-di-ricerca\">20.5 Direzioni di Ricerca</h3>\n<p><strong>Aperte</strong>:\n1. <strong>Theoretical understanding</strong>: perché boosting funziona così bene?\n2. <strong>Automated tuning</strong>: come ridurre necessità di expertise?\n3. <strong>Neural-symbolic hybrid</strong>: combinare trees e neural nets?\n4. <strong>Causal inference</strong>: usare trees per causal discovery?\n5. <strong>Online learning</strong>: incremental boosting efficiente?</p>\n<hr />\n<h2 id=\"appendice-formule-di-riferimento-rapido\">Appendice: Formule di Riferimento Rapido</h2>\n<h3 id=\"loss-functions-comuni\">Loss Functions Comuni</h3>\n<p><strong>Regression</strong>:\n- Square loss: $l(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$\n  - $g = \\hat{y} - y$, $h = 1$</p>\n<p><strong>Binary Classification</strong>:\n- Logistic loss: $l(y, \\hat{y}) = y\\log(1 + e^{-\\hat{y}}) + (1-y)\\log(1 + e^{\\hat{y}})$\n  - $p = 1/(1 + e^{-\\hat{y}})$\n  - $g = p - y$, $h = p(1-p)$</p>\n<p><strong>Multi-class</strong> (softmax):\n- $l = -\\sum_k y_k \\log(p_k)$ dove $p_k = \\frac{e^{\\hat{y}_k}}{\\sum_j e^{\\hat{y}_j}}$\n  - $g_k = p_k - y_k$\n  - $h_k = p_k(1 - p_k)$</p>\n<h3 id=\"formule-chiave\">Formule Chiave</h3>\n<p><strong>Peso ottimale foglia</strong>:\n$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$</p>\n<p><strong>Score struttura</strong>:\n$\\text{Score}(q) = -\\frac{1}{2}\\sum_{j=1}^T \\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T$</p>\n<p><strong>Gain split</strong>:\n$\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda}\\right] - \\gamma$</p>\n<p>dove $G = \\sum_i g_i$, $H = \\sum_i h_i$ per il rispettivo set.</p>\n<hr />\n<h2 id=\"riferimenti-e-risorse\">Riferimenti e Risorse</h2>\n<h3 id=\"paper-originale\">Paper Originale</h3>\n<ul>\n<li>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD &lsquo;16.</li>\n</ul>\n<h3 id=\"risorse-online\">Risorse Online</h3>\n<ul>\n<li>Documentazione ufficiale: https://xgboost.readthedocs.io/</li>\n<li>Repository GitHub: https://github.com/dmlc/xgboost</li>\n<li>Tutorial: https://xgboost.readthedocs.io/en/latest/tutorials/</li>\n</ul>\n<h3 id=\"paper-correlati\">Paper Correlati</h3>\n<ul>\n<li>Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine</li>\n<li>Friedman, J. H. (2002). Stochastic gradient boosting</li>\n<li>Breiman, L. (2001). Random forests</li>\n<li>Ke, G., et al. (2017). LightGBM: A highly efficient gradient boosting decision tree</li>\n<li>Prokhorenkova, L., et al. (2018). CatBoost: unbiased boosting with categorical features</li>\n</ul>\n<hr />\n<p><strong>Fine della Nota Completa su XGBoost</strong></p>"
}