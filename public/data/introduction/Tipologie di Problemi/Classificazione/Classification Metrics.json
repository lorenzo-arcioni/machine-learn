{
  "title": "Metriche di Valutazione per Classificazione in Machine Learning",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"indice\">Indice</h2>\n<ol>\n<li><a href=\"#1-fondamenti-teorici\">Fondamenti Teorici</a></li>\n<li><a href=\"#2-matrice-di-confusione\">Matrice di Confusione</a></li>\n<li><a href=\"#3-teoria-delle-decisioni-e-loss-functions\">Teoria delle Decisioni e Loss Functions</a></li>\n<li><a href=\"#4-metriche-fondamentali\">Metriche Fondamentali</a></li>\n<li><a href=\"#5-curve-roc-e-analisi-delle-performance\">Curve ROC e Analisi delle Performance</a></li>\n<li><a href=\"#6-curve-precision-recall\">Curve Precision-Recall</a></li>\n<li><a href=\"#7-metriche-avanzate-e-robuste\">Metriche Avanzate e Robuste</a></li>\n<li><a href=\"#8-valutazione-probabilistica-e-calibrazione\">Valutazione Probabilistica e Calibrazione</a></li>\n<li><a href=\"#9-classificazione-multi-classe\">Classificazione Multi-Classe</a></li>\n<li><a href=\"#10-guida-pratica-alla-scelta-delle-metriche\">Guida Pratica alla Scelta delle Metriche</a></li>\n</ol>\n<h2 id=\"1-fondamenti-teorici\">1. Fondamenti Teorici</h2>\n<h3 id=\"11-introduzione\">1.1 Introduzione</h3>\n<p>La valutazione di modelli di classificazione è un problema fondamentale nel machine learning. Non esiste una singola metrica universale: la scelta dipende dal problema specifico, dalla distribuzione dei dati, e dai costi associati ai diversi tipi di errore.</p>\n<p>Questo documento presenta una trattazione rigorosa e completa delle principali metriche di valutazione, partendo dai fondamenti della teoria delle decisioni bayesiane fino alle applicazioni pratiche.</p>\n<h3 id=\"12-il-framework-della-teoria-delle-decisioni-bayesiane\">1.2 Il Framework della Teoria delle Decisioni Bayesiane</h3>\n<p>Nel contesto della teoria delle decisioni, un problema di classificazione può essere formalizzato come un <strong>gioco contro la natura</strong>:</p>\n<ol>\n<li><strong>La natura</strong> sceglie uno stato (label) $y \\in \\mathcal{Y}$, sconosciuto a noi</li>\n<li><strong>La natura</strong> genera un&rsquo;osservazione $x \\in \\mathcal{X}$, che possiamo osservare</li>\n<li><strong>Noi</strong> scegliamo un&rsquo;azione $a$ da uno spazio di azioni $\\mathcal{A}$</li>\n<li><strong>Incorriamo</strong> in una perdita $L(y, a)$ che misura la discrepanza tra stato reale e azione scelta</li>\n</ol>\n<h4 id=\"objective-decision-rule-ottimale\">Objective: Decision Rule Ottimale</h4>\n<p>L&rsquo;obiettivo è trovare una <strong>decision rule</strong> (o <strong>policy</strong>) $\\delta: \\mathcal{X} \\rightarrow \\mathcal{A}$ che minimizzi la perdita attesa:</p>\n$$\\delta^*(x) = \\arg\\min_{a \\in \\mathcal{A}} \\mathbb{E}_{p(y|x)}[L(y, a)]$$\n<p>Nell&rsquo;approccio <strong>bayesiano</strong>, dopo aver osservato $x$, l&rsquo;azione ottimale è quella che minimizza la <strong>perdita attesa a posteriori</strong> (posterior expected loss):</p>\n$$\\rho(a|x) = \\mathbb{E}_{p(y|x)}[L(y, a)] = \\sum_{y \\in \\mathcal{Y}} L(y, a) \\cdot p(y|x)$$\n<p>Quindi, il <strong>Bayes estimator</strong> (o <strong>Bayes decision rule</strong>) è:</p>\n$$\\delta^*(x) = \\arg\\min_{a \\in \\mathcal{A}} \\rho(a|x) = \\arg\\min_{a \\in \\mathcal{A}} \\sum_{y \\in \\mathcal{Y}} L(y, a) \\cdot p(y|x)$$\n<p><strong>Interpretazione intuitive</strong>: Il Bayes estimator ci dice: &ldquo;Data l&rsquo;osservazione $x$, scegli l&rsquo;azione che minimizza la perdita media che ti aspetti di subire, pesando ogni possibile stato reale $y$ per la sua probabilità a posteriori $p(y|x)$.&rdquo;</p>\n<h4 id=\"principio-di-utilita-attesa-massima\">Principio di Utilità Attesa Massima</h4>\n<p>In economia, è più comune parlare di <strong>funzione di utilità</strong> $U(y, a) = -L(y, a)$. Il problema diventa:</p>\n$$\\delta^*(x) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E}_{p(y|x)}[U(y, a)]$$\n<p>Questo è il <strong>principio di utilità attesa massima</strong>, che costituisce la base del comportamento razionale in condizioni di incertezza.</p>\n<h3 id=\"13-rischio-e-generalizzazione\">1.3 Rischio e Generalizzazione</h3>\n<p>Il <strong>rischio</strong> (o rischio atteso) di una decision rule $\\delta$ è la perdita media sulla distribuzione dei dati:</p>\n$$R(\\delta) = \\mathbb{E}_{(X,Y) \\sim p(x,y)}[L(Y, \\delta(X))] = \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} L(y, \\delta(x)) \\, p(x,y) \\, dy \\, dx$$\n<p>Dobbiamo distinguere tre concetti fondamentali:</p>\n<p><strong>Rischio Empirico</strong> (Training Risk):\n$$\\hat{R}_{\\text{train}}(\\delta) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, \\delta(x_i))$$</p>\n<p>È la perdita media calcolata sul training set. Tende a <strong>sottostimare</strong> il vero rischio (overfitting).</p>\n<p><strong>Rischio di Generalizzazione</strong> (True Risk):\n$$R_{\\text{true}}(\\delta) = \\mathbb{E}_{(X,Y) \\sim p_{\\text{true}}(x,y)}[L(Y, \\delta(X))]$$</p>\n<p>È il vero rischio sulla distribuzione sottostante (sconosciuta). È quello che vogliamo davvero minimizzare.</p>\n<p><strong>Rischio Empirico su Test Set</strong>:\n$$\\hat{R}_{\\text{test}}(\\delta) = \\frac{1}{m} \\sum_{j=1}^{m} L(y_j^{\\text{test}}, \\delta(x_j^{\\text{test}}))$$</p>\n<p>È una stima non distorta di $R_{\\text{true}}(\\delta)$ se il test set è indipendente dal training.</p>\n<p><strong>Principio Fondamentale</strong>: Minimizziamo $\\hat{R}_{\\text{train}}(\\delta)$ durante il training, ma valutiamo su $\\hat{R}_{\\text{test}}(\\delta)$ per stimare $R_{\\text{true}}(\\delta)$.</p>\n<h2 id=\"2-matrice-di-confusione\">2. Matrice di Confusione</h2>\n<h3 id=\"21-definizione-e-struttura\">2.1 Definizione e Struttura</h3>\n<p>La <strong>matrice di confusione</strong> (confusion matrix) è la struttura fondamentale per calcolare tutte le metriche di classificazione binaria. Essa organizza le predizioni in base alla classe reale e alla classe predetta.</p>\n<p>Per un problema di classificazione binaria, con:\n- $y = 1$: classe <strong>positiva</strong>\n- $y = 0$: classe <strong>negativa</strong></p>\n<p>La matrice di confusione ha questa struttura:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th><strong>Predetto Positivo</strong> ($\\hat{y}=1$)</th>\n<th><strong>Predetto Negativo</strong> ($\\hat{y}=0$)</th>\n<th><strong>Totale</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Reale Positivo</strong> ($y=1$)</td>\n<td><strong>TP</strong> (True Positive)</td>\n<td><strong>FN</strong> (False Negative)</td>\n<td>$P = TP + FN$</td>\n</tr>\n<tr>\n<td><strong>Reale Negativo</strong> ($y=0$)</td>\n<td><strong>FP</strong> (False Positive)</td>\n<td><strong>TN</strong> (True Negative)</td>\n<td>$N = TN + FP$</td>\n</tr>\n<tr>\n<td><strong>Totale</strong></td>\n<td>$P^* = TP + FP$</td>\n<td>$N^* = TN + FN$</td>\n<td>$n = P + N$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"22-definizioni-rigorose\">2.2 Definizioni Rigorose</h3>\n<p>Dato un dataset di $n$ esempi $\\{(x_i, y_i)\\}_{i=1}^n$ e un classificatore che produce predizioni $\\hat{y}_i$, definiamo:</p>\n<p><strong>True Positive (TP)</strong>:\n$$TP = |\\{i : y_i = 1 \\land \\hat{y}_i = 1\\}|$$\nNumero di istanze positive correttamente classificate come positive.</p>\n<p><strong>True Negative (TN)</strong>:\n$$TN = |\\{i : y_i = 0 \\land \\hat{y}_i = 0\\}|$$\nNumero di istanze negative correttamente classificate come negative.</p>\n<p><strong>False Positive (FP)</strong>:\n$$FP = |\\{i : y_i = 0 \\land \\hat{y}_i = 1\\}|$$\nNumero di istanze negative erroneamente classificate come positive (<strong>Errore di Tipo I</strong>).</p>\n<p><strong>False Negative (FN)</strong>:\n$$FN = |\\{i : y_i = 1 \\land \\hat{y}_i = 0\\}|$$\nNumero di istanze positive erroneamente classificate come negative (<strong>Errore di Tipo II</strong>).</p>\n<p><strong>Mnemonico</strong>: La prima lettera (T/F) indica se la predizione è corretta (True) o errata (False). La seconda lettera (P/N) indica cosa ha predetto il classificatore.</p>\n<h3 id=\"23-relazioni-fondamentali\">2.3 Relazioni Fondamentali</h3>\n<p>Dalla matrice di confusione derivano identità fondamentali:</p>\n<p><strong>Totale esempi</strong>:\n$$n = TP + TN + FP + FN$$</p>\n<p><strong>Esempi positivi reali</strong>:\n$$P = TP + FN$$</p>\n<p><strong>Esempi negativi reali</strong>:\n$$N = TN + FP$$</p>\n<p><strong>Esempi predetti come positivi</strong>:\n$$P^* = TP + FP$$</p>\n<p><strong>Esempi predetti come negativi</strong>:\n$$N^* = TN + FN$$</p>\n<p><strong>Prevalenza</strong> (proporzione di positivi):\n$$\\pi = \\frac{P}{n} = \\frac{TP + FN}{n}$$</p>\n<h3 id=\"24-interpretazione-probabilistica\">2.4 Interpretazione Probabilistica</h3>\n<p>Possiamo interpretare i conteggi della matrice di confusione in termini probabilistici. Definiamo:</p>\n<p><strong>Ipotesi</strong>:\n- $H_0$: L&rsquo;istanza appartiene alla classe negativa ($y=0$)\n- $H_1$: L&rsquo;istanza appartiene alla classe positiva ($y=1$)</p>\n<p><strong>Decisioni</strong>:\n- $D_0$: Classificare come negativo ($\\hat{y}=0$)\n- $D_1$: Classificare come positivo ($\\hat{y}=1$)</p>\n<p>Allora possiamo scrivere le probabilità condizionate:</p>\n<p><strong>True Positive Rate (TPR)</strong>:\n$$\\text{TPR} = P(D_1 | H_1) = P(\\hat{y}=1 | y=1) = \\frac{TP}{P}$$\nProbabilità di classificare correttamente un positivo.</p>\n<p><strong>False Positive Rate (FPR)</strong>:\n$$\\text{FPR} = P(D_1 | H_0) = P(\\hat{y}=1 | y=0) = \\frac{FP}{N}$$\nProbabilità di classificare erroneamente un negativo come positivo (Errore di Tipo I).</p>\n<p><strong>True Negative Rate (TNR)</strong>:\n$$\\text{TNR} = P(D_0 | H_0) = P(\\hat{y}=0 | y=0) = \\frac{TN}{N}$$\nProbabilità di classificare correttamente un negativo.</p>\n<p><strong>False Negative Rate (FNR)</strong>:\n$$\\text{FNR} = P(D_0 | H_1) = P(\\hat{y}=0 | y=1) = \\frac{FN}{P}$$\nProbabilità di classificare erroneamente un positivo come negativo (Errore di Tipo II).</p>\n<p><strong>Relazioni complementari</strong>:\n$$\\text{TPR} + \\text{FNR} = 1$$\n$$\\text{TNR} + \\text{FPR} = 1$$</p>\n<h3 id=\"25-esempio-rilevamento-di-malattie-della-tiroide\">2.5 Esempio: Rilevamento di Malattie della Tiroide</h3>\n<p>Consideriamo il problema di rilevare malattie della tiroide usando un dataset con 3428 pazienti nel test set, di cui 250 hanno una malattia tiroidea.</p>\n<p><strong>Prima configurazione</strong> (soglia di default $\\tau = 0.5$):</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Predetto Normale</th>\n<th>Predetto Malato</th>\n<th>Totale</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Realmente Normale</td>\n<td>3177</td>\n<td>1</td>\n<td>3178</td>\n</tr>\n<tr>\n<td>Realmente Malato</td>\n<td>237</td>\n<td>13</td>\n<td>250</td>\n</tr>\n<tr>\n<td>Totale</td>\n<td>3414</td>\n<td>14</td>\n<td>3428</td>\n</tr>\n</tbody>\n</table>\n<p>Analisi:\n- <strong>Accuracy</strong>: $(3177 + 13) / 3428 = 93.1\\%$ (sembra buona!)\n- <strong>Recall</strong>: $13 / 250 = 5.2\\%$ (pessimo! Perdiamo il 95% dei malati)\n- <strong>Precision</strong>: $13 / 14 = 92.9\\%$ (alta, ma poche predizioni positive)</p>\n<p>Questo classificatore è <strong>praticamente inutile</strong>: un modello &ldquo;dummy&rdquo; che predice sempre &ldquo;normale&rdquo; otterrebbe accuracy del $92.7\\%$, quasi identica!</p>\n<p><strong>Seconda configurazione</strong> (soglia abbassata a $\\tau = 0.15$):</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Predetto Normale</th>\n<th>Predetto Malato</th>\n<th>Totale</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Realmente Normale</td>\n<td>3067</td>\n<td>111</td>\n<td>3178</td>\n</tr>\n<tr>\n<td>Realmente Malato</td>\n<td>165</td>\n<td>85</td>\n<td>250</td>\n</tr>\n<tr>\n<td>Totale</td>\n<td>3232</td>\n<td>196</td>\n<td>3428</td>\n</tr>\n</tbody>\n</table>\n<p>Analisi:\n- <strong>Accuracy</strong>: $(3067 + 85) / 3428 = 91.9\\%$ (leggermente diminuita)\n- <strong>Recall</strong>: $85 / 250 = 34\\%$ (migliorato significativamente!)\n- <strong>Precision</strong>: $85 / 196 = 43.4\\%$ (diminuita, ma accettabile)</p>\n<p><strong>Conclusione</strong>: Il secondo modello è probabilmente più utile in pratica, nonostante l&rsquo;accuracy leggermente inferiore. Questo esempio illustra perché l&rsquo;accuracy da sola è insufficiente per problemi sbilanciati.</p>\n<h2 id=\"3-teoria-delle-decisioni-e-loss-functions\">3. Teoria delle Decisioni e Loss Functions</h2>\n<h3 id=\"31-loss-functions-e-bayes-estimators\">3.1 Loss Functions e Bayes Estimators</h3>\n<p>La scelta della <strong>loss function</strong> $L(y, a)$ determina quale azione è ottimale. Diverse loss functions portano a diversi estimatori ottimali.</p>\n<h4 id=\"311-0-1-loss-e-stima-map\">3.1.1 0-1 Loss e Stima MAP</h4>\n<p>La <strong>0-1 loss</strong> è la più semplice e naturale:</p>\n$$L_{0-1}(y, a) = \\mathbb{I}(y \\neq a) = \\begin{cases} 0 & \\text{se } a = y \\\\ 1 & \\text{se } a \\neq y \\end{cases}$$\n<p><strong>Interpretazione</strong>: Penalizziamo ugualmente tutti gli errori, senza distinzione di tipo.</p>\n<p><strong>Teorema 3.1</strong> (Bayes Estimator per 0-1 Loss):\n<em>La 0-1 loss è minimizzata dalla stima MAP (Maximum A Posteriori).</em></p>\n<p><strong>Dimostrazione</strong>:</p>\n<p>La perdita attesa a posteriori per l&rsquo;azione $a$ è:</p>\n$$\\rho(a|x) = \\sum_{y \\in \\mathcal{Y}} L_{0-1}(y,a) \\cdot p(y|x) = \\sum_{y \\neq a} p(y|x) = 1 - p(a|x)$$\n<p>Per minimizzare $\\rho(a|x)$, dobbiamo massimizzare $p(a|x)$:</p>\n$$\\delta^*(x) = \\arg\\min_a \\rho(a|x) = \\arg\\max_a p(a|x) = \\arg\\max_{y \\in \\mathcal{Y}} p(y|x)$$\n<p>che è esattamente la <strong>stima MAP</strong>. $\\square$</p>\n<p><strong>Corollario</strong>: Per classificazione binaria con 0-1 loss, la regola ottimale è:</p>\n$$\\hat{y} = \\begin{cases} 1 & \\text{se } p(y=1|x) > 0.5 \\\\ 0 & \\text{altrimenti} \\end{cases}$$\n<h4 id=\"312-loss-asimmetrica-e-costi-differenziati\">3.1.2 Loss Asimmetrica e Costi Differenziati</h4>\n<p>Nella pratica, i diversi tipi di errore hanno spesso costi diversi. Rappresentiamo questo con una <strong>matrice di loss</strong>:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>$\\hat{y}=1$</th>\n<th>$\\hat{y}=0$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$y=1$</td>\n<td>$0$</td>\n<td>$L_{FN}$</td>\n</tr>\n<tr>\n<td>$y=0$</td>\n<td>$L_{FP}$</td>\n<td>$0$</td>\n</tr>\n</tbody>\n</table>\n<p>dove:\n- $L_{FN}$: costo di un False Negative (mancata rilevazione)\n- $L_{FP}$: costo di un False Positive (falso allarme)</p>\n<p><strong>Teorema 3.2</strong> (Regola di Decisione Ottimale con Costi Asimmetrici):\n<em>Sotto la matrice di loss asimmetrica, dovremmo classificare come positivo se e solo se:</em></p>\n$$\\frac{p(y=1|x)}{p(y=0|x)} > \\frac{L_{FP}}{L_{FN}}$$\n<p><strong>Dimostrazione</strong>:</p>\n<p>Le perdite attese per le due azioni sono:</p>\n$$\\rho(\\hat{y}=0|x) = L_{FN} \\cdot p(y=1|x) + 0 \\cdot p(y=0|x) = L_{FN} \\cdot p(y=1|x)$$\n$$\\rho(\\hat{y}=1|x) = 0 \\cdot p(y=1|x) + L_{FP} \\cdot p(y=0|x) = L_{FP} \\cdot p(y=0|x)$$\n<p>Scegliamo $\\hat{y}=1$ quando $\\rho(\\hat{y}=1|x) < \\rho(\\hat{y}=0|x)$:</p>\n$$L_{FP} \\cdot p(y=0|x) < L_{FN} \\cdot p(y=1|x)$$\n<p>Dividendo entrambi i lati per $p(y=0|x)$ e $L_{FN}$:</p>\n$$\\frac{L_{FP}}{L_{FN}} < \\frac{p(y=1|x)}{p(y=0|x)}$$\n<p>$\\square$</p>\n<p><strong>Corollario (Soglia Ottimale)</strong>:\nSe $L_{FN} = c \\cdot L_{FP}$ con $c > 0$, la regola diventa: classificare come positivo se $p(y=1|x) > \\tau^*$ dove:</p>\n$$\\tau^* = \\frac{1}{1 + c} = \\frac{L_{FP}}{L_{FP} + L_{FN}}$$\n<p><strong>Esempi numerici</strong>:</p>\n<ol>\n<li>\n<p><strong>Screening medico</strong>: $L_{FN} = 100$, $L_{FP} = 1$ (la mancata diagnosi è 100 volte più grave)\n   $$\\tau^* = \\frac{1}{101} \\approx 0.01$$\n   Soglia molto bassa → massimizziamo il recall.</p>\n</li>\n<li>\n<p><strong>Anti-spam</strong>: $L_{FN} = 1$, $L_{FP} = 10$ (eliminare email legittima è 10 volte peggio)\n   $$\\tau^* = \\frac{10}{11} \\approx 0.91$$\n   Soglia alta → massimizziamo la precision.</p>\n</li>\n</ol>\n<h4 id=\"313-reject-option\">3.1.3 Reject Option</h4>\n<p>In applicazioni ad alto rischio (medicina, finanza), può essere preferibile <strong>rifiutare</strong> di classificare esempi incerti piuttosto che rischiare errori gravi.</p>\n<p>Formalizziamo l&rsquo;azione di rifiuto come $a = \\text{reject}$ con costo $\\lambda_r$, mentre gli errori di classificazione hanno costo $\\lambda_s$ (substitution error).</p>\n<p><strong>Teorema 3.3</strong> (Regola con Reject Option):\n<em>L&rsquo;azione ottimale è:</em></p>\n$$\\delta^*(x) = \\begin{cases}\n\\arg\\max_c p(y=c|x) & \\text{se } \\max_c p(y=c|x) \\geq 1 - \\frac{\\lambda_r}{\\lambda_s} \\\\\n\\text{reject} & \\text{altrimenti}\n\\end{cases}$$\n<p><strong>Dimostrazione</strong> (sketch):</p>\n<p>Il costo atteso per classificare nella classe $c$ è:</p>\n$$\\rho(\\hat{y}=c|x) = \\lambda_s \\cdot P(\\text{errore}|x) = \\lambda_s \\cdot (1 - p(y=c|x))$$\n<p>Il costo per rifiutare è costante: $\\rho(\\text{reject}|x) = \\lambda_r$.</p>\n<p>Conviene classificare se:</p>\n$$\\lambda_s \\cdot (1 - p(y=c|x)) < \\lambda_r$$\n$$p(y=c|x) > 1 - \\frac{\\lambda_r}{\\lambda_s}$$\n<p>Scegliamo la classe con probabilità massima solo se supera questa soglia. $\\square$</p>\n<p><strong>Esempio</strong>: Se $\\lambda_s = 10$ (errore costa 10) e $\\lambda_r = 2$ (rifiuto costa 2):\n$$\\text{Soglia} = 1 - \\frac{2}{10} = 0.8$$</p>\n<p>Rifiutiamo di classificare se $\\max_c p(y=c|x) < 0.8$.</p>\n<h4 id=\"314-quadratic-loss-e-posterior-mean\">3.1.4 Quadratic Loss e Posterior Mean</h4>\n<p>Per problemi di regressione o quando lavoriamo con probabilità, la <strong>quadratic loss</strong> (o squared error) è naturale:</p>\n$$L_2(y, a) = (y - a)^2$$\n<p><strong>Teorema 3.4</strong> (Bayes Estimator per Quadratic Loss):\n<em>La $\\ell_2$ loss è minimizzata dalla media a posteriori.</em></p>\n<p><strong>Dimostrazione</strong>:</p>\n<p>La perdita attesa a posteriori è:</p>\n$$\\rho(a|x) = \\mathbb{E}[(y-a)^2|x] = \\mathbb{E}[y^2|x] - 2a\\mathbb{E}[y|x] + a^2$$\n<p>Deriviamo rispetto ad $a$ e poniamo uguale a zero:</p>\n$$\\frac{\\partial \\rho(a|x)}{\\partial a} = -2\\mathbb{E}[y|x] + 2a = 0$$\n$$\\Rightarrow a^* = \\mathbb{E}[y|x] = \\int y \\, p(y|x) \\, dy$$\n<p>Questa è la <strong>stima MMSE (Minimum Mean Squared Error)</strong>. $\\square$</p>\n<p><strong>Applicazione in regressione</strong>: Per un modello lineare $p(y|x,w) = \\mathcal{N}(y|w^Tx, \\sigma^2)$, l&rsquo;estimatore MMSE è:</p>\n$$\\hat{y}(x) = \\mathbb{E}[y|x, \\mathcal{D}] = x^T \\mathbb{E}[w|\\mathcal{D}]$$\n<p>Basta usare la media a posteriori dei parametri.</p>\n<h4 id=\"315-absolute-loss-e-posterior-median\">3.1.5 Absolute Loss e Posterior Median</h4>\n<p>La <strong>absolute loss</strong> (o $\\ell_1$ loss) è più robusta agli outlier:</p>\n$$L_1(y, a) = |y - a|$$\n<p><strong>Teorema 3.5</strong> (Bayes Estimator per Absolute Loss):\n<em>La $\\ell_1$ loss è minimizzata dalla mediana a posteriori.</em></p>\n<p><strong>Dimostrazione</strong>:</p>\n<p>La perdita attesa è:</p>\n$$\\rho(a|x) = \\int |y-a| p(y|x) dy = \\int_{-\\infty}^{a} (a-y) p(y|x) dy + \\int_{a}^{\\infty} (y-a) p(y|x) dy$$\n<p>Deriviamo rispetto ad $a$. Usando la regola di Leibniz:</p>\n$$\\frac{\\partial \\rho(a|x)}{\\partial a} = \\int_{-\\infty}^{a} p(y|x) dy - \\int_{a}^{\\infty} p(y|x) dy$$\n$$= P(y \\leq a|x) - P(y > a|x)$$\n<p>Ponendo uguale a zero:</p>\n$$P(y \\leq a|x) = P(y > a|x) = \\frac{1}{2}$$\n<p>che è la definizione di <strong>mediana</strong>. $\\square$</p>\n<p><strong>Perché la $\\ell_1$ è più robusta?</strong> La $\\ell_2$ loss penalizza quadraticamente le deviazioni, quindi un singolo outlier molto distante può dominare la loss. La $\\ell_1$ penalizza linearmente, riducendo l&rsquo;influenza degli outlier.</p>\n<h2 id=\"4-metriche-fondamentali\">4. Metriche Fondamentali</h2>\n<h3 id=\"41-accuracy-accuratezza\">4.1 Accuracy (Accuratezza)</h3>\n<p>L&rsquo;<strong>accuracy</strong> è la metrica più semplice e intuitiva: misura la proporzione di predizioni corrette.</p>\n<p><strong>Definizione</strong>:\n$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{TP + TN}{n}$$</p>\n<p><strong>Interpretazione probabilistica</strong>:\n$$\\text{Accuracy} = P(\\hat{y} = y)$$\nÈ la probabilità che una predizione casuale sia corretta.</p>\n<p><strong>Proprietà</strong>:\n- <strong>Range</strong>: $[0, 1]$, dove $1$ indica predizioni perfette\n- <strong>Simmetrica</strong> rispetto alle classi\n- <strong>Uguale peso</strong> a errori positivi e negativi</p>\n<p><strong>Limitazione critica: Dataset Sbilanciati</strong></p>\n<p>Consideriamo un problema di fraud detection dove solo l&lsquo;1% delle transazioni è fraudolenta ($\\pi = 0.01$).</p>\n<p>Un classificatore &ldquo;dummy&rdquo; che <strong>predice sempre negativo</strong> ottiene:\n$$\\text{Accuracy}_{\\text{dummy}} = \\frac{0 + 0.99n}{n} = 0.99$$</p>\n<p>Questo sembra eccellente, ma il modello è completamente inutile! Non rileva nessuna frode.</p>\n<p><strong>Teorema 4.1</strong> (Lower Bound su Accuracy per Classificatore Dummy):\n<em>Un classificatore che predice sempre la classe maggioritaria ottiene accuracy pari alla prevalenza della classe maggioritaria:</em></p>\n$$\\text{Accuracy}_{\\text{majority}} = \\max(\\pi, 1-\\pi)$$\n<p><strong>Conclusione</strong>: L&rsquo;accuracy è <strong>inadeguata per dataset sbilanciati</strong>. Dobbiamo usare metriche che distinguano tra i diversi tipi di errore.</p>\n<h3 id=\"42-precision-precisione\">4.2 Precision (Precisione)</h3>\n<p>La <strong>precision</strong> misura l&rsquo;affidabilità delle predizioni positive.</p>\n<p><strong>Definizione</strong>:\n$$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{TP}{P^*}$$</p>\n<p><strong>Interpretazione probabilistica</strong>:\n$$\\text{Precision} = P(y=1|\\hat{y}=1)$$\n&ldquo;Tra tutti i casi che ho predetto come positivi, qual è la probabilità che siano realmente positivi?&rdquo;</p>\n<p><strong>Interpretazione intuitiva</strong>: &ldquo;Quando il modello dice &lsquo;positivo&rsquo;, quanto possiamo fidarci?&rdquo;</p>\n<p><strong>Quando è critica</strong>: Scenari dove i <strong>falsi positivi sono costosi</strong>:</p>\n<ol>\n<li><strong>Spam detection</strong>: Classificare email legittime come spam può far perdere comunicazioni importanti</li>\n<li><strong>Diagnosi mediche aggressive</strong>: Prescrivere chemioterapia a pazienti sani</li>\n<li><strong>Raccomandazioni</strong>: Raccomandare prodotti irrilevanti irrita l&rsquo;utente</li>\n<li><strong>Allerte di sicurezza</strong>: Troppi falsi allarmi causano &ldquo;alarm fatigue&rdquo;</li>\n</ol>\n<p><strong>Complemento - False Discovery Rate (FDR)</strong>:\n$$\\text{FDR} = 1 - \\text{Precision} = \\frac{FP}{TP+FP}$$\nProporzione di &ldquo;scoperte&rdquo; che sono in realtà false.</p>\n<p><strong>Dipendenza dalla Prevalenza</strong></p>\n<p>La precision dipende fortemente dalla prevalenza $\\pi = P(y=1)$. Usando il teorema di Bayes:</p>\n$$\\text{Precision} = P(y=1|\\hat{y}=1) = \\frac{P(\\hat{y}=1|y=1) \\cdot P(y=1)}{P(\\hat{y}=1)}$$\n$$= \\frac{\\text{TPR} \\cdot \\pi}{\\text{TPR} \\cdot \\pi + \\text{FPR} \\cdot (1-\\pi)}$$\n<p><strong>Esempio</strong>: Con TPR = 0.9, FPR = 0.1:\n- Se $\\pi = 0.5$: Precision = $\\frac{0.9 \\cdot 0.5}{0.9 \\cdot 0.5 + 0.1 \\cdot 0.5} = 0.9$\n- Se $\\pi = 0.01$: Precision = $\\frac{0.9 \\cdot 0.01}{0.9 \\cdot 0.01 + 0.1 \\cdot 0.99} \\approx 0.08$</p>\n<p>Con prevalenza bassa, anche un FPR modesto degrada drasticamente la precision!</p>\n<h3 id=\"43-recall-sensibilita-true-positive-rate\">4.3 Recall (Sensibilità, True Positive Rate)</h3>\n<p>Il <strong>recall</strong> misura la capacità di identificare i positivi.</p>\n<p><strong>Definizione</strong>:\n$$\\text{Recall} = \\text{TPR} = \\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{TP}{P}$$</p>\n<p><strong>Interpretazione probabilistica</strong>:\n$$\\text{Recall} = P(\\hat{y}=1|y=1)$$\n&ldquo;Tra tutti i casi realmente positivi, quale proporzione riesco a identificare?&rdquo;</p>\n<p><strong>Interpretazione intuitiva</strong>: &ldquo;Quanto è completa la mia rilevazione dei positivi?&rdquo;</p>\n<p><strong>Quando è critico</strong>: Scenari dove i <strong>falsi negativi sono costosi</strong>:</p>\n<ol>\n<li><strong>Screening medico</strong>: Non diagnosticare un tumore è potenzialmente fatale</li>\n<li><strong>Rilevamento frodi</strong>: Non bloccare una transazione fraudolenta causa perdite economiche</li>\n<li><strong>Sistemi di sicurezza</strong>: Non rilevare un&rsquo;intrusione compromette la sicurezza</li>\n<li><strong>Information retrieval</strong>: Non trovare documenti rilevanti limita l&rsquo;utilità del sistema</li>\n</ol>\n<p><strong>Complemento - False Negative Rate (FNR)</strong>:\n$$\\text{FNR} = \\text{Miss Rate} = 1 - \\text{Recall} = \\frac{FN}{TP+FN}$$\nProporzione di positivi che &ldquo;perdiamo&rdquo; (manchiamo di rilevare).</p>\n<p><strong>Indipendenza dalla Prevalenza</strong></p>\n<p>A differenza della precision, il recall <strong>non dipende dalla prevalenza</strong> perché è condizionato sulla classe reale:</p>\n<p>$\\text{Recall} = P(\\hat{y}=1|y=1)$</p>\n<p>Questa è una probabilità condizionata su $y=1$, che dipende solo da $p(x|y=1)$ e dalla soglia di decisione, non da $P(y=1)$.</p>\n<h3 id=\"44-specificity-true-negative-rate\">4.4 Specificity (True Negative Rate)</h3>\n<p>La <strong>specificity</strong> è il &ldquo;recall per la classe negativa&rdquo;.</p>\n<p><strong>Definizione</strong>:\n$\\text{Specificity} = \\text{TNR} = \\frac{TN}{TN + FP} = \\frac{TN}{N}$</p>\n<p><strong>Interpretazione probabilistica</strong>:\n$\\text{Specificity} = P(\\hat{y}=0|y=0)$\n&ldquo;Tra tutti i casi realmente negativi, quale proporzione riesco a identificare correttamente?&rdquo;</p>\n<p><strong>Relazione con FPR</strong>:\n$\\text{FPR} = 1 - \\text{Specificity} = \\frac{FP}{FP + TN} = P(\\hat{y}=1|y=0)$</p>\n<p>Il FPR è la probabilità di <strong>falso allarme</strong> (Errore di Tipo I nel testing d&rsquo;ipotesi).</p>\n<p><strong>Importanza in medicina</strong>: In test diagnostici, specificity alta significa pochi falsi positivi, riducendo ansia ingiustificata e procedure invasive non necessarie.</p>\n<h3 id=\"45-trade-off-precision-vs-recall\">4.5 Trade-off Precision vs Recall</h3>\n<p>Precision e recall sono tipicamente in <strong>trade-off</strong>: migliorare una tende a peggiorare l&rsquo;altra.</p>\n<p><strong>Intuizione del trade-off</strong>:</p>\n<p>Consideriamo un classificatore probabilistico che produce $p(y=1|x)$ e una soglia $\\tau$:</p>\n<p>$\\hat{y} = \\begin{cases} 1 & \\text{se } p(y=1|x) > \\tau \\\\ 0 & \\text{altrimenti} \\end{cases}$</p>\n<p><strong>Abbassando la soglia</strong> $\\tau$ (classifichiamo più casi come positivi):\n- ✅ <strong>Recall aumenta</strong>: Catturiamo più veri positivi\n- ❌ <strong>Precision diminuisce</strong>: Includiamo anche più falsi positivi</p>\n<p><strong>Alzando la soglia</strong> $\\tau$ (siamo più selettivi):\n- ✅ <strong>Precision aumenta</strong>: Solo predizioni molto confidenti\n- ❌ <strong>Recall diminuisce</strong>: Perdiamo alcuni veri positivi &ldquo;border-line&rdquo;</p>\n<p><strong>Analisi formale</strong>:</p>\n<p>Al variare di $\\tau$:</p>\n<p>$\\tau \\to 0: \\quad \\begin{cases} \\text{Recall} \\to 1 \\\\ \\text{Precision} \\to \\pi \\end{cases} \\quad \\text{(tutto positivo)}$</p>\n<p>$\\tau \\to 1: \\quad \\begin{cases} \\text{Recall} \\to 0 \\\\ \\text{Precision} \\to 1 \\end{cases} \\quad \\text{(tutto negativo)}$</p>\n<p><strong>Esempio numerico</strong>:</p>\n<p>Dataset: 100 positivi, 900 negativi. Modello produce score da 0 a 1.</p>\n<table>\n<thead>\n<tr>\n<th>Soglia $\\tau$</th>\n<th>TP</th>\n<th>FP</th>\n<th>FN</th>\n<th>Precision</th>\n<th>Recall</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.9</td>\n<td>10</td>\n<td>5</td>\n<td>90</td>\n<td>0.67</td>\n<td>0.10</td>\n</tr>\n<tr>\n<td>0.7</td>\n<td>40</td>\n<td>50</td>\n<td>60</td>\n<td>0.44</td>\n<td>0.40</td>\n</tr>\n<tr>\n<td>0.5</td>\n<td>70</td>\n<td>200</td>\n<td>30</td>\n<td>0.26</td>\n<td>0.70</td>\n</tr>\n<tr>\n<td>0.3</td>\n<td>90</td>\n<td>500</td>\n<td>10</td>\n<td>0.15</td>\n<td>0.90</td>\n</tr>\n</tbody>\n</table>\n<p>Osserviamo chiaramente il trade-off: recall alta → precision bassa, e viceversa.</p>\n<h3 id=\"46-f-scores-armonizzare-precision-e-recall\">4.6 F-Scores: Armonizzare Precision e Recall</h3>\n<h4 id=\"461-f1-score-media-armonica\">4.6.1 F1-Score: Media Armonica</h4>\n<p>L&rsquo;<strong>F1-score</strong> combina precision e recall in una singola metrica bilanciata.</p>\n<p><strong>Definizione</strong>:\n$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}$</p>\n<p><strong>Perché la media armonica?</strong></p>\n<p>La media armonica di due numeri $a$ e $b$ è:\n$H(a,b) = \\frac{2ab}{a+b}$</p>\n<p>È più <strong>severa</strong> della media aritmetica quando i valori sono sbilanciati:\n$H(a,b) \\leq G(a,b) \\leq A(a,b)$\ndove $G$ è la media geometrica e $A$ la media aritmetica.</p>\n<p><strong>Esempio illustrativo</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Precision</th>\n<th>Recall</th>\n<th>Media Aritmetica</th>\n<th>F1 (Media Armonica)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.9</td>\n<td>0.9</td>\n<td>0.90</td>\n<td>0.90</td>\n</tr>\n<tr>\n<td>0.9</td>\n<td>0.5</td>\n<td>0.70</td>\n<td>0.64</td>\n</tr>\n<tr>\n<td>0.9</td>\n<td>0.1</td>\n<td>0.50</td>\n<td>0.18</td>\n</tr>\n<tr>\n<td>0.5</td>\n<td>0.5</td>\n<td>0.50</td>\n<td>0.50</td>\n</tr>\n</tbody>\n</table>\n<p>L&rsquo;F1 <strong>penalizza fortemente</strong> sistemi con una metrica molto bassa, anche se l&rsquo;altra è alta.</p>\n<p><strong>Proprietà matematiche</strong>:</p>\n<ol>\n<li><strong>Range</strong>: $F_1 \\in [0, 1]$</li>\n<li><strong>Massimo</strong>: $F_1 = 1$ se e solo se $\\text{Precision} = \\text{Recall} = 1$</li>\n<li><strong>Simmetria</strong>: $F_1(P, R) = F_1(R, P)$</li>\n<li><strong>Monotonia</strong>: $F_1$ cresce se aumentiamo sia $P$ che $R$</li>\n</ol>\n<p><strong>Derivazione alternativa</strong>:</p>\n<p>Possiamo scrivere:\n$F_1 = \\frac{1}{\\frac{1}{2}\\left(\\frac{1}{P} + \\frac{1}{R}\\right)}$</p>\n<p>L&rsquo;F1 è la media armonica perché è l&rsquo;inverso della media aritmetica degli inversi.</p>\n<h4 id=\"462-f-beta-score-peso-asimmetrico\">4.6.2 F-Beta Score: Peso Asimmetrico</h4>\n<p>Il <strong>F-beta score</strong> generalizza l&rsquo;F1 permettendo di pesare diversamente recall e precision.</p>\n<p><strong>Definizione</strong>:\n$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}$</p>\n<p><strong>Interpretazione del parametro $\\beta$</strong>:</p>\n<ul>\n<li>$\\beta < 1$: <strong>Precision pesa di più</strong> (enfasi su evitare falsi positivi)</li>\n<li>$\\beta = 1$: <strong>Peso uguale</strong> (F1-score standard)</li>\n<li>$\\beta > 1$: <strong>Recall pesa di più</strong> (enfasi su catturare tutti i positivi)</li>\n</ul>\n<p><strong>Valori comuni</strong>:</p>\n<p><strong>$F_{0.5}$</strong> (Precision vale il doppio):\n$F_{0.5} = 1.25 \\cdot \\frac{P \\cdot R}{0.25 \\cdot P + R}$\nUso: Spam detection, dove falsi positivi sono molto costosi.</p>\n<p><strong>$F_2$</strong> (Recall vale il doppio):\n$F_2 = 5 \\cdot \\frac{P \\cdot R}{4 \\cdot P + R}$\nUso: Screening medico, dove falsi negativi sono molto costosi.</p>\n<p><strong>Derivazione del peso di $\\beta$</strong>:</p>\n<p>Riscriviamo l&rsquo;F-beta in forma estesa:\n$F_\\beta = \\frac{(1+\\beta^2) \\cdot TP}{(1+\\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}$</p>\n<p>Notiamo che:\n- I falsi negativi (FN) sono pesati per $\\beta^2$\n- I falsi positivi (FP) sono pesati per $1$</p>\n<p>Quindi $\\beta^2$ è il <strong>rapporto di importanza</strong> tra recall e precision:\n$\\beta^2 = \\frac{\\text{Importanza del Recall}}{\\text{Importanza della Precision}}$</p>\n<p><strong>Esempio numerico</strong>:</p>\n<p>Consideriamo tre modelli con diverse caratteristiche:</p>\n<table>\n<thead>\n<tr>\n<th>Modello</th>\n<th>Precision</th>\n<th>Recall</th>\n<th>F1</th>\n<th>F0.5</th>\n<th>F2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>0.90</td>\n<td>0.50</td>\n<td>0.64</td>\n<td>0.75</td>\n<td>0.56</td>\n</tr>\n<tr>\n<td>B</td>\n<td>0.50</td>\n<td>0.90</td>\n<td>0.64</td>\n<td>0.56</td>\n<td>0.75</td>\n</tr>\n<tr>\n<td>C</td>\n<td>0.70</td>\n<td>0.70</td>\n<td>0.70</td>\n<td>0.70</td>\n<td>0.70</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><strong>Modello A</strong>: Alta precision, basso recall → F0.5 lo premia</li>\n<li><strong>Modello B</strong>: Bassa precision, alto recall → F2 lo premia  </li>\n<li><strong>Modello C</strong>: Bilanciato → performance costante su tutti gli F-score</li>\n</ul>\n<p><strong>Scelta di $\\beta$ in base al dominio</strong>:</p>\n<ol>\n<li><strong>Medicina (screening)</strong>: $\\beta = 2$ o superiore (priorità su recall)</li>\n<li><strong>Spam filtering</strong>: $\\beta = 0.5$ (priorità su precision)</li>\n<li><strong>Information retrieval</strong>: $\\beta = 1$ (bilanciamento)</li>\n<li><strong>Fraud detection</strong>: $\\beta = 1.5$ - $2$ (leggermente sbilanciato verso recall)</li>\n</ol>\n<h2 id=\"5-curve-roc-e-analisi-delle-performance\">5. Curve ROC e Analisi delle Performance</h2>\n<h3 id=\"51-introduzione-alle-curve-roc\">5.1 Introduzione alle Curve ROC</h3>\n<p>La <strong>curva ROC</strong> (Receiver Operating Characteristic) è uno strumento fondamentale per valutare classificatori binari indipendentemente dalla scelta della soglia.</p>\n<p><strong>Contesto storico</strong>: Le curve ROC furono sviluppate durante la Seconda Guerra Mondiale per analizzare segnali radar. Il nome &ldquo;Receiver Operating Characteristic&rdquo; deriva proprio dall&rsquo;analisi dei ricevitori radio.</p>\n<h3 id=\"52-costruzione-della-curva-roc\">5.2 Costruzione della Curva ROC</h3>\n<p><strong>Definizione formale</strong>:</p>\n<p>Data una famiglia di classificatori parametrizzati da una soglia $\\tau \\in [0,1]$:\n$\\hat{y}(\\tau) = \\mathbb{I}(p(y=1|x) > \\tau)$</p>\n<p>La curva ROC è il grafico dei punti:\n$\\text{ROC}(\\tau) = \\big(\\text{FPR}(\\tau), \\text{TPR}(\\tau)\\big)$</p>\n<p>al variare di $\\tau$ da 0 a 1.</p>\n<p><strong>Coordinate</strong>:\n- <strong>Asse X</strong>: False Positive Rate = $\\frac{FP}{N}$\n- <strong>Asse Y</strong>: True Positive Rate = $\\frac{TP}{P}$</p>\n<p><strong>Algoritmo di costruzione</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Input: Score s_i e label y_i per i = 1,...,n\n\n1. Ordina gli esempi per score decrescente: s_1 ≥ s_2 ≥ ... ≥ s_n\n2. Inizializza: TP = 0, FP = 0\n3. Per ogni soglia τ (prendendo s_i come soglie):\n   a. Se y_i = 1: TP++\n   b. Se y_i = 0: FP++\n   c. Calcola: TPR = TP/P, FPR = FP/N\n   d. Aggiungi punto (FPR, TPR) alla curva\n4. Collega i punti per formare la curva\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"53-interpretazione-e-punti-notevoli\">5.3 Interpretazione e Punti Notevoli</h3>\n<p><strong>Punti estremi</strong>:</p>\n<p>$(0, 0)$: <strong>Origine</strong> - Soglia $\\tau = 1$ → tutto classificato come negativo\n- TP = 0, FP = 0\n- Classifier &ldquo;always negative&rdquo;</p>\n<p>$(1, 1)$: <strong>Angolo in alto a destra</strong> - Soglia $\\tau = 0$ → tutto classificato come positivo\n- TP = P, FP = N\n- Classifier &ldquo;always positive&rdquo;</p>\n<p>$(0, 1)$: <strong>Angolo in alto a sinistra</strong> - Classificatore perfetto\n- TP = P, FP = 0\n- Nessun errore</p>\n<p><strong>Diagonale principale</strong> $y = x$: Classificatore casuale\n- Prediction random con $P(\\hat{y}=1) = p$\n- In media: $\\text{TPR} = p$, $\\text{FPR} = p$</p>\n<p><strong>Interpretazione geometrica</strong>:</p>\n<ul>\n<li><strong>Curve vicine all&rsquo;angolo (0,1)</strong>: Ottimo classificatore</li>\n<li>Alto TPR con basso FPR</li>\n<li>\n<p>Buona separazione tra classi</p>\n</li>\n<li>\n<p><strong>Curve vicine alla diagonale</strong>: Classificatore scarso</p>\n</li>\n<li>Nessun potere discriminante</li>\n<li>\n<p>Simile a indovinare casualmente</p>\n</li>\n<li>\n<p><strong>Curve sotto la diagonale</strong>: Classificatore &ldquo;invertito&rdquo;</p>\n</li>\n<li>Performance peggiore del caso</li>\n<li>Invertire le predizioni migliorerebbe il modello!</li>\n</ul>\n<p><strong>Proprietà di monotonia</strong>:</p>\n<p>La curva ROC è <strong>monotona crescente</strong>: muovendoci lungo la curva da sinistra a destra (abbassando $\\tau$), sia TPR che FPR aumentano (o restano costanti).</p>\n<p><strong>Teorema 5.1</strong> (Monotonia della Curva ROC):\n<em>Per un classificatore con score $s(x)$, se $\\tau_1 < \\tau_2$, allora:</em>\n$\\text{FPR}(\\tau_1) \\geq \\text{FPR}(\\tau_2) \\quad \\text{e} \\quad \\text{TPR}(\\tau_1) \\geq \\text{TPR}(\\tau_2)$</p>\n<p><strong>Dimostrazione</strong>: Abbassando la soglia, classifichiamo più esempi come positivi, quindi sia TP che FP possono solo aumentare (o restare costanti). $\\square$</p>\n<h3 id=\"54-area-under-the-curve-auc-roc\">5.4 Area Under the Curve (AUC-ROC)</h3>\n<p>L&rsquo;<strong>AUC</strong> (Area Under the ROC Curve) è una metrica scalare che riassume la performance complessiva del classificatore.</p>\n<p><strong>Definizione matematica</strong>:\n$\\text{AUC} = \\int_0^1 \\text{TPR}(t) \\, d(\\text{FPR}(t))$</p>\n<p>dove $t$ varia lungo la curva (parametro di soglia).</p>\n<p><strong>Range</strong>: $\\text{AUC} \\in [0, 1]$</p>\n<p><strong>Interpretazione dei valori</strong>:</p>\n<ul>\n<li>$\\text{AUC} = 1.0$: <strong>Perfetto</strong> - Separazione completa tra classi</li>\n<li>$\\text{AUC} = 0.9$: <strong>Eccellente</strong> - Ottima discriminazione</li>\n<li>$\\text{AUC} = 0.8$: <strong>Buono</strong> - Buona discriminazione</li>\n<li>$\\text{AUC} = 0.7$: <strong>Accettabile</strong> - Discriminazione discreta</li>\n<li>$\\text{AUC} = 0.5$: <strong>Nullo</strong> - Nessun potere discriminante (casuale)</li>\n<li>$\\text{AUC} < 0.5$: <strong>Invertito</strong> - Performance peggiore del caso</li>\n</ul>\n<p><strong>Teorema 5.2</strong> (Interpretazione Probabilistica dell&rsquo;AUC):\n<em>L&rsquo;AUC è la probabilità che un esempio positivo casuale abbia score maggiore di un esempio negativo casuale:</em></p>\n<p>$\\text{AUC} = P(s(X_+) > s(X_-))$</p>\n<p>dove $X_+ \\sim p(x|y=1)$ e $X_- \\sim p(x|y=0)$.</p>\n<p><strong>Dimostrazione</strong> (sketch):</p>\n<p>Consideriamo tutti i possibili confronti tra un esempio positivo e uno negativo. Per ogni soglia $\\tau$, contiamo:\n- Quante coppie $(x_+, x_-)$ hanno $s(x_+) > \\tau$ e $s(x_-) \\leq \\tau$</p>\n<p>La curva ROC traccia esattamente questa proporzione. L&rsquo;integrale accumula tutti questi confronti, dando la frazione totale di coppie ordinate correttamente.</p>\n<p>Formalmente, l&rsquo;AUC può essere calcolata come:\n$\\text{AUC} = \\frac{1}{P \\cdot N} \\sum_{i: y_i=1} \\sum_{j: y_j=0} \\mathbb{I}(s_i > s_j)$</p>\n<p>dove $P$ è il numero di positivi e $N$ di negativi. Questo è esattamente una stima di $P(s(X_+) > s(X_-))$. $\\square$</p>\n<p><strong>Corollario</strong>: L&rsquo;AUC equivale alla statistica U del test di Mann-Whitney-Wilcoxon:\n$\\text{AUC} = \\frac{U}{P \\cdot N}$</p>\n<p>dove $U$ è la statistica U di Mann-Whitney.</p>\n<p><strong>Calcolo pratico dell&rsquo;AUC</strong>:</p>\n<p><strong>Metodo 1</strong> (Regola del trapezio):\n$\\text{AUC} \\approx \\sum_{i=1}^{n-1} \\frac{1}{2}(\\text{TPR}_i + \\text{TPR}_{i+1}) \\cdot (\\text{FPR}_{i+1} - \\text{FPR}_i)$</p>\n<p><strong>Metodo 2</strong> (Conteggio di coppie concordanti):\n$\\text{AUC} = \\frac{\\#\\{(i,j): y_i=1, y_j=0, s_i > s_j\\}}{P \\cdot N}$</p>\n<h3 id=\"55-proprieta-fondamentali-dellauc\">5.5 Proprietà Fondamentali dell&rsquo;AUC</h3>\n<p><strong>Proprietà 5.1</strong> (Invarianza alla Scala):\nL&rsquo;AUC dipende solo dall&rsquo;<strong>ordinamento</strong> degli score, non dai valori assoluti.</p>\n<p>Se applichiamo una trasformazione monotona crescente $f$ agli score:\n$\\text{AUC}(f(s)) = \\text{AUC}(s)$</p>\n<p><strong>Implicazione</strong>: Possiamo confrontare modelli che producono score su scale diverse (e.g., probabilità vs logit vs distance).</p>\n<p><strong>Proprietà 5.2</strong> (Robustezza allo Sbilanciamento):\nL&rsquo;AUC <strong>non dipende dalla prevalenza</strong> della classe positiva.</p>\n<p>Se cambiamo la distribuzione di classe nel test set, l&rsquo;AUC rimane invariata (a patto che $p(x|y)$ non cambi).</p>\n<p><strong>Dimostrazione</strong>: TPR e FPR sono entrambi condizionati su $y$:\n$\\text{TPR} = P(\\hat{y}=1|y=1), \\quad \\text{FPR} = P(\\hat{y}=1|y=0)$</p>\n<p>Questi dipendono solo da $p(x|y=1)$, $p(x|y=0)$ e dalla soglia, non da $P(y)$. $\\square$</p>\n<p><strong>Proprietà 5.3</strong> (Interpretazione come Ranking Metric):\nL&rsquo;AUC misura la qualità del <strong>ranking</strong> prodotto dal classificatore:\n- Un buon ranking mette esempi positivi in cima\n- AUC alta → la maggior parte dei positivi è rankata sopra i negativi</p>\n<p><strong>Limitazioni dell&rsquo;AUC</strong>:</p>\n<ol>\n<li>\n<p><strong>Non fornisce informazioni sulla calibrazione</strong>: Due modelli con stesso AUC possono avere probabilità molto diverse</p>\n</li>\n<li>\n<p><strong>Aggregazione su tutte le soglie</strong>: Può mascherare performance scarse in regioni critiche</p>\n</li>\n<li>\n<p><strong>Ottimizza per ranking globale</strong>: Può non essere ottimale se ci interessa solo una specifica regione operativa (e.g., basso FPR)</p>\n</li>\n<li>\n<p><strong>Sensibilità ridotta</strong>: Cambiamenti in regioni di bassa densità hanno stesso peso di regioni ad alta densità</p>\n</li>\n</ol>\n<h3 id=\"56-operating-points-e-trade-offs\">5.6 Operating Points e Trade-offs</h3>\n<p><strong>Operating point</strong>: Un punto specifico sulla curva ROC corrispondente a una soglia $\\tau$.</p>\n<p><strong>Scelta dell&rsquo;operating point</strong>:</p>\n<p>La curva ROC mostra tutti i possibili trade-off, ma dobbiamo scegliere un punto operativo specifico basato su:</p>\n<ol>\n<li>\n<p><strong>Costi asimmetrici</strong>: Se $c = L_{FN}/L_{FP}$, cerchiamo il punto che minimizza:\n   $\\text{Cost}(\\tau) = c \\cdot \\text{FNR}(\\tau) + \\text{FPR}(\\tau)$</p>\n</li>\n<li>\n<p><strong>Vincoli operativi</strong>: </p>\n</li>\n<li>&ldquo;FPR deve essere $\\leq 0.05$&rdquo; → scegli il punto con FPR massimo 0.05 e TPR massimo</li>\n<li>\n<p>&ldquo;Recall deve essere $\\geq 0.9$&rdquo; → scegli il punto con TPR minimo 0.9 e FPR minimo</p>\n</li>\n<li>\n<p><strong>Youden&rsquo;s index</strong>: Massimizza la distanza dalla diagonale:\n   $J = \\text{TPR} - \\text{FPR} = \\text{Sensitivity} + \\text{Specificity} - 1$\n   Equivale a massimizzare l&rsquo;informedness.</p>\n</li>\n</ol>\n<h3 id=\"57-equal-error-rate-eer\">5.7 Equal Error Rate (EER)</h3>\n<p>L&rsquo;<strong>Equal Error Rate</strong> è il punto sulla curva ROC dove:\n$\\text{FPR}(\\tau^*) = \\text{FNR}(\\tau^*) = \\text{EER}$</p>\n<p>Equivalentemente, dove:\n$\\text{FPR}(\\tau^*) = 1 - \\text{TPR}(\\tau^*)$</p>\n<p><strong>Interpretazione geometrica</strong>: Intersezione della curva ROC con la linea $y = 1 - x$.</p>\n<p><strong>Proprietà</strong>:\n- Bilanciamento naturale tra i due tipi di errore\n- Utile quando non abbiamo informazioni sui costi relativi\n- EER basso indica performance migliore</p>\n<p><strong>Calcolo</strong>: Cercare la soglia dove $|\\text{FPR} - \\text{FNR}|$ è minimo.</p>\n<h3 id=\"58-confronto-tra-modelli-con-roc\">5.8 Confronto tra Modelli con ROC</h3>\n<p><strong>Dominanza</strong>: Il modello A <strong>domina</strong> il modello B se:\n$\\text{TPR}_A(\\tau) \\geq \\text{TPR}_B(\\tau) \\quad \\forall \\text{FPR}(\\tau)$</p>\n<p>In altre parole, la curva ROC di A è sempre sopra (o coincide con) quella di B.</p>\n<p>Se A domina B, allora certamente $\\text{AUC}_A \\geq \\text{AUC}_B$.</p>\n<p><strong>Curve che si intersecano</strong>: Se le curve ROC si intersecano, nessun modello domina l&rsquo;altro. La scelta dipende dalla regione operativa:\n- Se operiamo a basso FPR (alta specificità), scegliamo il modello migliore in quella regione\n- Se operiamo ad alto TPR (alta sensibilità), scegliamo il modello migliore in quella regione</p>\n<p><strong>Esempio</strong>:\n- Modello A: Migliore per FPR &lt; 0.1 (applicazioni dove FP sono molto costosi)\n- Modello B: Migliore per FPR &gt; 0.1 (applicazioni dove vogliamo alto recall)</p>\n<h2 id=\"6-curve-precision-recall\">6. Curve Precision-Recall</h2>\n<h3 id=\"61-motivazione-per-dataset-sbilanciati\">6.1 Motivazione per Dataset Sbilanciati</h3>\n<p>Quando la classe positiva è <strong>rara</strong> (e.g., $P(y=1) \\ll 0.5$), la curva ROC può essere <strong>poco informativa</strong>:</p>\n<p><strong>Problema con ROC per classi rare</strong>:</p>\n<ol>\n<li>Il numero di negativi $N$ è molto grande</li>\n<li>Anche un piccolo FPR corrisponde a <strong>molti falsi positivi</strong> in termini assoluti</li>\n<li>La maggior parte della curva ROC è compressa vicino all&rsquo;origine</li>\n<li>Variazioni importanti nella precision sono mascherate</li>\n</ol>\n<p><strong>Esempio numerico</strong>:</p>\n<p>Dataset: 10,000 esempi, 100 positivi (1%), 9,900 negativi.</p>\n<p>Due classificatori:\n- <strong>Modello A</strong>: TPR = 0.90, FPR = 0.02\n- <strong>Modello B</strong>: TPR = 0.90, FPR = 0.05</p>\n<p>Sulla curva ROC sembrano molto simili (stessa TPR, FPR simili).</p>\n<p>Ma calcoliamo la precision:</p>\n<p>$\\text{Precision}_A = \\frac{TP}{TP + FP} = \\frac{90}{90 + (0.02 \\times 9900)} = \\frac{90}{288} \\approx 0.31$</p>\n<p>$\\text{Precision}_B = \\frac{90}{90 + (0.05 \\times 9900)} = \\frac{90}{585} \\approx 0.15$</p>\n<p>La precision di B è <strong>metà</strong> di quella di A! Ma questo non è evidente nella curva ROC.</p>\n<p><strong>Soluzione</strong>: La curva <strong>Precision-Recall</strong> focalizza l&rsquo;attenzione sui positivi, rendendola più informativa per dataset sbilanciati.</p>\n<h3 id=\"62-definizione-della-curva-pr\">6.2 Definizione della Curva PR</h3>\n<p><strong>Definizione</strong>: La curva Precision-Recall plotta:\n$\\text{PR}(\\tau) = \\big(\\text{Recall}(\\tau), \\text{Precision}(\\tau)\\big)$</p>\n<p>al variare della soglia $\\tau$.</p>\n<p><strong>Coordinate</strong>:\n- <strong>Asse X</strong>: Recall = $\\frac{TP}{P}$\n- <strong>Asse Y</strong>: Precision = $\\frac{TP}{TP + FP}$</p>\n<p><strong>Costruzione</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Input: Score s_i e label y_i per i = 1,...,n\n\n1. Ordina per score decrescente: s_1 ≥ s_2 ≥ ... ≥ s_n\n2. Inizializza: TP = 0, FP = 0\n3. Per ogni soglia τ:\n   a. Aggiorna TP e FP\n   b. Calcola: Recall = TP/P, Precision = TP/(TP+FP)\n   c. Aggiungi punto (Recall, Precision)\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"63-interpretazione-e-comportamento\">6.3 Interpretazione e Comportamento</h3>\n<p><strong>Punti notevoli</strong>:</p>\n<p><strong>Alta soglia</strong> ($\\tau \\to 1$):\n- Poche predizioni positive (solo le più confidenti)\n- Recall basso, Precision alta\n- Punto in basso a destra della curva</p>\n<p><strong>Bassa soglia</strong> ($\\tau \\to 0$):\n- Molte predizioni positive\n- Recall alto, Precision bassa (≈ prevalenza)\n- Punto in alto a sinistra della curva</p>\n<p><strong>Baseline casuale</strong>:</p>\n<p>Un classificatore casuale che predice positivo con probabilità $p$ ottiene:\n$\\text{Precision}_{\\text{random}} = \\frac{P}{n} = \\pi$</p>\n<p>indipendentemente da $p$ (in media). Quindi la baseline è una <strong>linea orizzontale</strong> a $y = \\pi$.</p>\n<p><strong>Interpretazione</strong>: Una curva PR buona deve stare <strong>sopra</strong> questa baseline.</p>\n<p><strong>Forma tipica</strong>: La curva PR tende a decrescere muovendosi da sinistra a destra (aumentando recall). Questo riflette il trade-off precision-recall.</p>\n<h3 id=\"64-average-precision-ap\">6.4 Average Precision (AP)</h3>\n<p>L&rsquo;<strong>Average Precision</strong> riassume la curva PR in un singolo numero.</p>\n<p><strong>Definizione</strong> (interpolata):\n$\\text{AP} = \\sum_{k=1}^{n} (R_k - R_{k-1}) \\cdot P_k$</p>\n<p>dove $(P_k, R_k)$ sono precision e recall al $k$-esimo elemento rankat, ordinati per recall crescente.</p>\n<p><strong>Interpretazione</strong>: Approssimazione dell&rsquo;area sotto la curva PR, pesando ogni livello di recall per quanto è &ldquo;grande&rdquo; (quanto recall guadagniamo).</p>\n<p><strong>Definizione alternativa</strong> (usata in PASCAL VOC):\n$\\text{AP} = \\sum_{k=1}^{n} (R_k - R_{k-1}) \\cdot P_{\\text{interp}}(R_k)$</p>\n<p>dove:\n$P_{\\text{interp}}(R_k) = \\max_{R' \\geq R_k} P(R')$</p>\n<p>Questo usa la precision <strong>interpolata</strong> (massima raggiungibile per recall $\\geq R_k$), rendendo la curva monotona.</p>\n<p><strong>Relazione con Ranking</strong>:</p>\n<p>$\\text{AP} = \\frac{1}{P} \\sum_{k=1}^{n} P(k) \\cdot \\text{rel}(k)$</p>\n<p>dove:\n- $P(k)$ = precision at rank $k$\n- $\\text{rel}(k) = 1$ se l&rsquo;item al rank $k$ è positivo, 0 altrimenti\n- $P$ = numero totale di positivi</p>\n<p><strong>Interpretazione</strong>: L&rsquo;AP è la precision media su tutte le posizioni dove troviamo un positivo.</p>\n<h3 id=\"65-precisionk-e-recallk\">6.5 Precision@K e Recall@K</h3>\n<p>In information retrieval e ranking systems, spesso ci interessano solo i top-K risultati.</p>\n<p><strong>Precision@K</strong>:\n$P@K = \\frac{|\\{i \\in \\text{top-}K : y_i = 1\\}|}{K}$</p>\n<p>Frazione di positivi tra i primi K elementi rankati.</p>\n<p><strong>Recall@K</strong>:\n$R@K = \\frac{|\\{i \\in \\text{top-}K : y_i = 1\\}|}{P}$</p>\n<p>Frazione di tutti i positivi catturati nei primi K elementi.</p>\n<p><strong>Average Precision@K</strong>:\n$AP@K = \\frac{1}{\\min(m, K)} \\sum_{k=1}^{K} P(k) \\cdot \\text{rel}(k)$</p>\n<p>dove $m$ è il numero di positivi nel dataset.</p>\n<p><strong>Uso tipico</strong>: \n- Motori di ricerca: P@10 (prime 10 ricerche)\n- Sistemi di raccomandazione: P@20 (prime 20 raccomandazioni)\n- Object detection: mAP@IoU (mean Average Precision a diversi IoU threshold)</p>\n<h3 id=\"66-confronto-auc-roc-vs-auc-pr\">6.6 Confronto AUC-ROC vs AUC-PR</h3>\n<p><strong>Differenze fondamentali</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>AUC-ROC</th>\n<th>AUC-PR</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Assi</strong></td>\n<td>TPR vs FPR</td>\n<td>Precision vs Recall</td>\n</tr>\n<tr>\n<td><strong>Focus</strong></td>\n<td>Bilanciamento tra positivi e negativi</td>\n<td>Solo classe positiva</td>\n</tr>\n<tr>\n<td><strong>Baseline</strong></td>\n<td>Diagonale (0.5)</td>\n<td>Orizzontale (prevalenza $\\pi$)</td>\n</tr>\n<tr>\n<td><strong>Dipendenza da $\\pi$</strong></td>\n<td>Invariante</td>\n<td>Dipendente</td>\n</tr>\n<tr>\n<td><strong>Dataset bilanciati</strong></td>\n<td>Ottimo</td>\n<td>Equivalente</td>\n</tr>\n<tr>\n<td><strong>Dataset sbilanciati</strong></td>\n<td>Può essere misleading</td>\n<td>Più informativa</td>\n</tr>\n<tr>\n<td><strong>Interpretazione</strong></td>\n<td>Ranking globale</td>\n<td>Rilevanza dei positivi</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Teorema 6.1</strong> (Sensibilità al Prior):\n<em>Dato uno shift nella prevalenza da $\\pi_{\\text{train}}$ a $\\pi_{\\text{test}}$:</em>\n- <em>L&rsquo;AUC-ROC rimane invariante</em>\n- <em>L&rsquo;AUC-PR cambia proporzionalmente</em></p>\n<p><strong>Dimostrazione</strong>:</p>\n<p>ROC usa metriche condizionate su $y$:\n$\\text{TPR} = P(\\hat{y}=1|y=1), \\quad \\text{FPR} = P(\\hat{y}=1|y=0)$</p>\n<p>Queste dipendono solo da $p(x|y)$, non da $P(y)$.</p>\n<p>PR usa Precision che dipende esplicitamente dal prior:\n$\\text{Precision} = P(y=1|\\hat{y}=1) = \\frac{P(\\hat{y}=1|y=1) \\cdot P(y=1)}{P(\\hat{y}=1)}$</p>\n<p>Per Bayes:\n$\\text{Precision} = \\frac{\\text{TPR} \\cdot \\pi}{\\text{TPR} \\cdot \\pi + \\text{FPR} \\cdot (1-\\pi)}$</p>\n<p>Cambiando $\\pi$, la precision cambia, quindi cambia AUC-PR. $\\square$</p>\n<p><strong>Implicazione pratica</strong>: Se il test set ha prevalenza diversa dal training, AUC-PR sarà diversa anche con stesso modello. Questo rende AUC-PR più &ldquo;onesta&rdquo; per dataset molto sbilanciati.</p>\n<p><strong>Quando usare quale</strong>:</p>\n<ul>\n<li><strong>ROC-AUC</strong>: </li>\n<li>Dataset bilanciati o moderatamente sbilanciati</li>\n<li>Interessa ranking generale</li>\n<li>\n<p>Vogliamo confrontare modelli indipendentemente dalla prevalenza</p>\n</li>\n<li>\n<p><strong>PR-AUC</strong>:</p>\n</li>\n<li>Dataset fortemente sbilanciati ($\\pi < 0.1$ o $\\pi > 0.9$)</li>\n<li>Focus sulla classe positiva rara</li>\n<li>Information retrieval e detection tasks</li>\n</ul>\n<h2 id=\"7-metriche-avanzate-e-robuste\">7. Metriche Avanzate e Robuste</h2>\n<h3 id=\"71-matthews-correlation-coefficient-mcc\">7.1 Matthews Correlation Coefficient (MCC)</h3>\n<p>Il <strong>Matthews Correlation Coefficient</strong> è considerato una delle metriche più bilanciate e robuste per classificazione binaria.</p>\n<p><strong>Definizione</strong>:\n$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$</p>\n<p><strong>Derivazione</strong>: L&rsquo;MCC è il <strong>coefficiente di correlazione di Pearson</strong> $\\phi$ tra le variabili binarie $y$ (label reale) e $\\hat{y}$ (predizione).</p>\n<p>Per due variabili binarie, il coefficiente $\\phi$ è:\n$\\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{n_{1\\cdot}n_{0\\cdot}n_{\\cdot1}n_{\\cdot0}}}$</p>\n<p>dove $n_{ij}$ è la frequenza congiunta di $y=i$ e $\\hat{y}=j$. Sostituendo con la notazione della confusion matrix:\n- $n_{11} = TP$ (entrambi 1)\n- $n_{00} = TN$ (entrambi 0)\n- $n_{10} = FN$ ($y=1$, $\\hat{y}=0$)\n- $n_{01} = FP$ ($y=0$, $\\hat{y}=1$)\n- $n_{1\\cdot} = TP + FN$ (totale reali positivi)\n- $n_{0\\cdot} = TN + FP$ (totale reali negativi)\n- $n_{\\cdot1} = TP + FP$ (totale predetti positivi)\n- $n_{\\cdot0} = TN + FN$ (totale predetti negativi)</p>\n<p>Otteniamo esattamente la formula dell&rsquo;MCC.</p>\n<p><strong>Range e Interpretazione</strong>:</p>\n<p>$\\text{MCC} \\in [-1, +1]$</p>\n<ul>\n<li>$\\text{MCC} = +1$: <strong>Perfetto</strong> - Predizione completamente corretta</li>\n<li>$\\text{MCC} = 0$: <strong>Casuale</strong> - Performance non migliore del caso</li>\n<li>$\\text{MCC} = -1$: <strong>Inverso perfetto</strong> - Predizioni completamente sbagliate (ma consistentemente)</li>\n</ul>\n<p><strong>Interpretazione come correlazione</strong>:\n- MCC positivo: Associazione positiva tra predizioni e realtà\n- MCC vicino a 0: Nessuna associazione (predizioni random)\n- MCC negativo: Associazione negativa (predizioni sistematicamente inverse)</p>\n<p><strong>Proprietà Fondamentali</strong>:</p>\n<p><strong>Proprietà 7.1</strong> (Simmetria):\n$\\text{MCC}(y, \\hat{y}) = \\text{MCC}(\\neg y, \\neg \\hat{y})$</p>\n<p>L&rsquo;MCC è invariante rispetto allo scambio di classe (chiamare &ldquo;positivo&rdquo; quello che prima era &ldquo;negativo&rdquo;).</p>\n<p><strong>Dimostrazione</strong>: Sotto lo scambio $y \\leftrightarrow (1-y)$ e $\\hat{y} \\leftrightarrow (1-\\hat{y})$:\n- $TP \\leftrightarrow TN$\n- $FP \\leftrightarrow FN$</p>\n<p>Il numeratore diventa: $TN \\cdot TP - FN \\cdot FP = TP \\cdot TN - FP \\cdot FN$ (invariato).</p>\n<p>Il denominatore è simmetrico per costruzione. $\\square$</p>\n<p><strong>Proprietà 7.2</strong> (Robustezza allo Sbilanciamento):\nL&rsquo;MCC <strong>non favorisce la classe maggioritaria</strong> ed è considerato la metrica più affidabile per dataset sbilanciati.</p>\n<p><strong>Confronto con Accuracy su Dataset Sbilanciato</strong>:</p>\n<p>Esempio: 95 negativi, 5 positivi.</p>\n<p><strong>Classificatore Dummy</strong> (sempre negativo):\n- TP = 0, TN = 95, FP = 0, FN = 5\n- Accuracy = $95/100 = 0.95$ (sembra ottimo!)\n- MCC = $\\frac{0 \\cdot 95 - 0 \\cdot 5}{\\sqrt{0 \\cdot 5 \\cdot 95 \\cdot 100}} = \\frac{0}{0}$ (indefinito, o 0)</p>\n<p><strong>Classificatore Bilanciato</strong>:\n- TP = 4, TN = 90, FP = 5, FN = 1\n- Accuracy = $94/100 = 0.94$ (leggermente peggio)\n- MCC = $\\frac{4 \\cdot 90 - 5 \\cdot 1}{\\sqrt{9 \\cdot 5 \\cdot 95 \\cdot 91}} \\approx 0.60$ (molto meglio!)</p>\n<p>L&rsquo;MCC riconosce correttamente che il secondo classificatore è superiore.</p>\n<p><strong>Relazione con altre metriche</strong>:</p>\n<p>L&rsquo;MCC può essere espresso in termini di TPR, TNR, PPV (Precision), NPV:</p>\n<p>$\\text{MCC} = \\sqrt{\\text{TPR} \\cdot \\text{TNR} \\cdot \\text{PPV} \\cdot \\text{NPV}} - \\sqrt{\\text{FNR} \\cdot \\text{FPR} \\cdot \\text{FOR} \\cdot \\text{FDR}}$</p>\n<p>dove FOR = False Omission Rate, FDR = False Discovery Rate.</p>\n<p><strong>Nota computazionale</strong>: Quando uno qualsiasi dei termini nel denominatore è zero, l&rsquo;MCC è indefinito (divisione per zero). In pratica, si assegna MCC = 0 in questi casi.</p>\n<h3 id=\"72-cohens-kappa-math_inline_384\">7.2 Cohen&rsquo;s Kappa ($\\kappa$)</h3>\n<p>Il <strong>Cohen&rsquo;s Kappa</strong> misura l&rsquo;accordo tra predizioni e realtà, <strong>corretto per l&rsquo;accordo casuale</strong>.</p>\n<p><strong>Definizione</strong>:\n$\\kappa = \\frac{p_o - p_e}{1 - p_e}$</p>\n<p>dove:\n- $p_o = \\frac{TP + TN}{n}$ è l&rsquo;<strong>accuratezza osservata</strong>\n- $p_e$ è l&rsquo;<strong>accuratezza attesa per caso</strong></p>\n<p><strong>Calcolo di $p_e$</strong> (Accordo Casuale Atteso):</p>\n<p>Se $y$ e $\\hat{y}$ fossero <strong>indipendenti</strong> ma con le stesse distribuzioni marginali:</p>\n<p>$p_e = P(y = \\hat{y}|\\text{indipendenza})$</p>\n<p>$= P(y=1) \\cdot P(\\hat{y}=1) + P(y=0) \\cdot P(\\hat{y}=0)$</p>\n<p>$= \\frac{TP + FN}{n} \\cdot \\frac{TP + FP}{n} + \\frac{TN + FP}{n} \\cdot \\frac{TN + FN}{n}$</p>\n<p>$= \\frac{(TP+FN)(TP+FP) + (TN+FP)(TN+FN)}{n^2}$</p>\n<p><strong>Interpretazione</strong>:</p>\n<p>$\\kappa = \\frac{\\text{Accordo Osservato} - \\text{Accordo Casuale}}{1 - \\text{Accordo Casuale}}$</p>\n<ul>\n<li>Numeratore: Quanto l&rsquo;accordo osservato supera il caso</li>\n<li>Denominatore: Massimo miglioramento possibile rispetto al caso</li>\n</ul>\n<p><strong>Range</strong>:</p>\n<p>$\\kappa \\in [-1, 1]$</p>\n<p>ma tipicamente $\\kappa \\in [0, 1]$ per classificatori ragionevoli.</p>\n<p><strong>Scala di Landis e Koch</strong> (interpretazione classica):</p>\n<table>\n<thead>\n<tr>\n<th>Kappa</th>\n<th>Forza dell&rsquo;Accordo</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$< 0$</td>\n<td>Peggiore del caso</td>\n</tr>\n<tr>\n<td>$0.00 - 0.20$</td>\n<td>Lieve</td>\n</tr>\n<tr>\n<td>$0.21 - 0.40$</td>\n<td>Discreto</td>\n</tr>\n<tr>\n<td>$0.41 - 0.60$</td>\n<td>Moderato</td>\n</tr>\n<tr>\n<td>$0.61 - 0.80$</td>\n<td>Sostanziale</td>\n</tr>\n<tr>\n<td>$0.81 - 1.00$</td>\n<td>Quasi perfetto</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Esempio di calcolo</strong>:</p>\n<p>Dataset: 100 esempi, 60 positivi, 40 negativi.\nModello: TP = 50, TN = 30, FP = 10, FN = 10.</p>\n<p>$p_o = \\frac{50 + 30}{100} = 0.80$</p>\n<p>$p_e = \\frac{60 \\cdot 60}{100^2} + \\frac{40 \\cdot 40}{100^2} = \\frac{3600 + 1600}{10000} = 0.52$</p>\n<p>$\\kappa = \\frac{0.80 - 0.52}{1 - 0.52} = \\frac{0.28}{0.48} \\approx 0.58$</p>\n<p>Interpretazione: Accordo <strong>moderato</strong> (secondo Landis e Koch).</p>\n<p><strong>Relazione con MCC</strong>:</p>\n<p>Per problemi binari, MCC e Kappa sono correlati ma <strong>non identici</strong>. In generale:\n- MCC è preferito per la sua interpretazione come correlazione\n- MCC ha migliori proprietà matematiche\n- Kappa è più usato in ambito medico/statistico per inter-rater agreement</p>\n<p><strong>Differenza chiave</strong>: Kappa usa le distribuzioni marginali empiriche per calcolare $p_e$, mentre MCC è una pura misura di correlazione.</p>\n<h3 id=\"73-balanced-accuracy\">7.3 Balanced Accuracy</h3>\n<p>La <strong>balanced accuracy</strong> è particolarmente utile per dataset sbilanciati, dando peso uguale a ciascuna classe.</p>\n<p><strong>Definizione</strong>:\n$\\text{Balanced Accuracy} = \\frac{1}{2}\\left(\\frac{TP}{TP+FN} + \\frac{TN}{TN+FP}\\right) = \\frac{\\text{TPR} + \\text{TNR}}{2}$</p>\n<p><strong>Equivalente</strong>:\n$\\text{Balanced Accuracy} = \\frac{\\text{Sensitivity} + \\text{Specificity}}{2}$</p>\n<p><strong>Motivazione</strong>: L&rsquo;accuracy standard può essere dominata dalla classe maggioritaria. La balanced accuracy:\n- Calcola accuracy per ciascuna classe separatamente\n- Fa la media (non pesata) delle due</p>\n<p><strong>Esempio illustrativo</strong>:</p>\n<p>Dataset: 950 negativi, 50 positivi.</p>\n<p><strong>Classificatore A</strong> (sempre negativo):\n- Accuracy = $950/1000 = 0.95$\n- Balanced Accuracy = $\\frac{0 + 1}{2} = 0.50$ ← Rivela che è casuale!</p>\n<p><strong>Classificatore B</strong>:\n- TP = 40, TN = 900, FP = 50, FN = 10\n- Accuracy = $940/1000 = 0.94$\n- Balanced Accuracy = $\\frac{40/50 + 900/950}{2} = \\frac{0.8 + 0.947}{2} \\approx 0.87$</p>\n<p>La balanced accuracy rivela correttamente che B è molto migliore di A, anche se l&rsquo;accuracy semplice è simile.</p>\n<p><strong>Generalizzazione Multi-Classe</strong>:</p>\n<p>$\\text{Balanced Accuracy} = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{TP_c}{TP_c + FN_c}$</p>\n<p>dove $C$ è il numero di classi.</p>\n<p><strong>Proprietà</strong>:\n- Range: $[0, 1]$\n- Balanced Accuracy = 0.5 per classificatore casuale (binario)\n- Non favorisce alcuna classe\n- Più interpretabile dell&rsquo;MCC per utenti non tecnici</p>\n<p><strong>Confronto con Macro-F1</strong>:</p>\n<p>Entrambe danno peso uguale alle classi, ma:\n- Balanced Accuracy: Media di recall per classe\n- Macro-F1: Media di F1 per classe (combina precision e recall)</p>\n<h3 id=\"74-informedness-e-markedness\">7.4 Informedness e Markedness</h3>\n<p>Due metriche meno note ma teoricamente importanti.</p>\n<p><strong>Informedness</strong> (Bookmaker Informedness):\n$\\text{Informedness} = \\text{TPR} + \\text{TNR} - 1 = \\text{Sensitivity} + \\text{Specificity} - 1$</p>\n<p><strong>Interpretazione</strong>: \n- Quanto il classificatore è più informato del caso?\n- Probabilità di decisione informata vs casuale\n- Range: $[-1, 1]$ dove 0 = casuale</p>\n<p><strong>Relazione</strong>: \n$\\text{Informedness} = 2 \\cdot \\text{Balanced Accuracy} - 1$</p>\n<p><strong>Markedness</strong>:\n$\\text{Markedness} = \\text{PPV} + \\text{NPV} - 1 = \\text{Precision} + \\text{NPV} - 1$</p>\n<p>dove NPV (Negative Predictive Value) = $\\frac{TN}{TN+FN}$.</p>\n<p><strong>Interpretazione</strong>: Quanto sono &ldquo;marcate&rdquo; (affidabili) le predizioni?</p>\n<p><strong>Teorema 7.1</strong> (Relazione MCC con Informedness e Markedness):\n$\\text{MCC} = \\sqrt{\\text{Informedness} \\times \\text{Markedness}}$</p>\n<p>(quando tutti i termini sono definiti e non negativi)</p>\n<p><strong>Dimostrazione</strong> (sketch):\n$\\text{Informedness} = \\frac{TP}{P} + \\frac{TN}{N} - 1$</p>\n<p>$\\text{Markedness} = \\frac{TP}{P^*} + \\frac{TN}{N^*} - 1$</p>\n<p>Espandendo e semplificando usando le identità della confusion matrix, si ottiene che il loro prodotto geometrico è correlato a MCC². $\\square$</p>\n<h2 id=\"8-valutazione-probabilistica-e-calibrazione\">8. Valutazione Probabilistica e Calibrazione</h2>\n<h3 id=\"81-introduzione\">8.1 Introduzione</h3>\n<p>Molti classificatori producono <strong>probabilità</strong> $p(y=1|x)$ anziché solo label binari. È importante valutare:\n1. <strong>Discriminazione</strong>: Il modello separa bene le classi? (ROC, PR)\n2. <strong>Calibrazione</strong>: Le probabilità predette riflettono le vere frequenze?</p>\n<p>Un modello può avere ottima discriminazione (AUC alta) ma pessima calibrazione.</p>\n<p><strong>Esempio</strong>: Un modello che produce sempre $p=0.9$ per positivi e $p=0.1$ per negativi ha:\n- Perfetta discriminazione (AUC = 1)\n- Pessima calibrazione (le probabilità non riflettono l&rsquo;incertezza reale)</p>\n<h3 id=\"82-log-loss-cross-entropy-loss\">8.2 Log Loss (Cross-Entropy Loss)</h3>\n<p>La <strong>log loss</strong> valuta la qualità delle probabilità predette.</p>\n<p><strong>Definizione</strong> (Binaria):\n$\\mathcal{L}_{\\text{log}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]$</p>\n<p>dove $p_i = P(y_i=1|x_i)$ è la probabilità predetta.</p>\n<p><strong>Derivazione</strong>: La log loss è l&rsquo;<strong>entropia incrociata</strong> tra distribuzione empirica e predetta:</p>\n<p>$H(q, p) = -\\mathbb{E}_{y \\sim q}[\\log p(y|x)]$</p>\n<p>Per label binari deterministici: $q(y=1|x) = y$ (0 o 1):\n$H = -y \\log p - (1-y) \\log(1-p)$</p>\n<p><strong>Proprietà</strong>:</p>\n<ol>\n<li><strong>Range</strong>: $[0, +\\infty)$ dove 0 indica probabilità perfette</li>\n<li><strong>Penalizzazione logaritmica</strong>: Predizioni confidenti ma sbagliate sono penalizzate esponenzialmente</li>\n<li><strong>Proper scoring rule</strong>: Minimizzata dalle vere probabilità</li>\n</ol>\n<p><strong>Proper Scoring Rule</strong>: Una metrica è &ldquo;proper&rdquo; se è ottimizzata predicendo le vere probabilità:\n$\\mathbb{E}_{y \\sim p^*}[S(y, p)] \\geq \\mathbb{E}_{y \\sim p^*}[S(y, q)] \\quad \\forall q$</p>\n<p>con uguaglianza solo se $q = p^*$.</p>\n<p><strong>Esempi di penalizzazione</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Vera Classe</th>\n<th>Probabilità Predetta</th>\n<th>Log Loss</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$y=1$</td>\n<td>$p=0.99$</td>\n<td>$-\\log(0.99) \\approx 0.01$</td>\n</tr>\n<tr>\n<td>$y=1$</td>\n<td>$p=0.9$</td>\n<td>$-\\log(0.9) \\approx 0.11$</td>\n</tr>\n<tr>\n<td>$y=1$</td>\n<td>$p=0.5$</td>\n<td>$-\\log(0.5) \\approx 0.69$</td>\n</tr>\n<tr>\n<td>$y=1$</td>\n<td>$p=0.1$</td>\n<td>$-\\log(0.1) \\approx 2.30$</td>\n</tr>\n<tr>\n<td>$y=1$</td>\n<td>$p=0.01$</td>\n<td>$-\\log(0.01) \\approx 4.61$</td>\n</tr>\n</tbody>\n</table>\n<p>Notare la <strong>penalizzazione esponenziale</strong>: predire $p=0.01$ quando $y=1$ costa 460 volte più che predire $p=0.99$!</p>\n<p><strong>Collegamento con Maximum Likelihood</strong>:</p>\n<p>Minimizzare la log loss è equivalente a massimizzare la log-likelihood:\n$\\arg\\min_\\theta \\mathcal{L}_{\\text{log}} = \\arg\\max_\\theta \\sum_{i=1}^{n} \\log p(y_i|x_i, \\theta)$</p>\n<p>Questo è il principio del <strong>Maximum Likelihood Estimation (MLE)</strong>.</p>\n<p><strong>Multi-Classe</strong>:\n$\\mathcal{L}_{\\text{log}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{ic} \\log(p_{ic})$</p>\n<p>dove $y_{ic} = 1$ se $y_i = c$, altrimenti 0 (one-hot encoding).</p>\n<h3 id=\"83-brier-score\">8.3 Brier Score</h3>\n<p>Il <strong>Brier score</strong> misura l&rsquo;errore quadratico delle probabilità.</p>\n<p><strong>Definizione</strong> (Binaria):\n$\\text{BS} = \\frac{1}{n} \\sum_{i=1}^{n} (p_i - y_i)^2$</p>\n<p>dove $y_i \\in \\{0, 1\\}$.</p>\n<p><strong>Derivazione</strong>: È semplicemente il <strong>Mean Squared Error (MSE)</strong> tra probabilità predette e label binari.</p>\n<p><strong>Range</strong>: $[0, 1]$ dove 0 indica probabilità perfette.</p>\n<p><strong>Decomposizione di Murphy</strong>:</p>\n<p>Il Brier score può essere decomposto in tre componenti interpretabili:</p>\n<p>$\\text{BS} = \\text{Reliability} - \\text{Resolution} + \\text{Uncertainty}$</p>\n<p>dove:</p>\n<p><strong>Uncertainty</strong> (varianza intrinseca):\n$\\text{Uncertainty} = \\bar{y}(1 - \\bar{y})$\ndove $\\bar{y}$ è la prevalenza. Non controllabile dal modello.</p>\n<p><strong>Resolution</strong> (capacità di separare):\n$\\text{Resolution} = \\frac{1}{n} \\sum_{k=1}^{K} n_k(\\bar{y}_k - \\bar{y})^2$\nQuanto bene il modello separa esempi con diverse probabilità vere. Vogliamo massimizzarlo.</p>\n<p><strong>Reliability</strong> (calibrazione):\n$\\text{Reliability} = \\frac{1}{n} \\sum_{k=1}^{K} n_k(\\bar{y}_k - \\bar{p}_k)^2$\nDeviazione tra probabilità predette e frequenze osservate. Vogliamo minimizzarlo.</p>\n<p><strong>Interpretazione</strong>: \n- Un buon modello ha <strong>alta resolution</strong> (separa bene) e <strong>bassa reliability</strong> (ben calibrato)\n- BS basso indica entrambe le proprietà</p>\n<p><strong>Confronto Log Loss vs Brier Score</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Log Loss</th>\n<th>Brier Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Penalizzazione</strong></td>\n<td>Logaritmica (severa)</td>\n<td>Quadratica (moderata)</td>\n</tr>\n<tr>\n<td><strong>Range</strong></td>\n<td>$[0, \\infty)$</td>\n<td>$[0, 1]$</td>\n</tr>\n<tr>\n<td><strong>Proper scoring</strong></td>\n<td>Sì</td>\n<td>Sì</td>\n</tr>\n<tr>\n<td><strong>Interpretabilità</strong></td>\n<td>Meno intuitiva</td>\n<td>Più intuitiva (MSE)</td>\n</tr>\n<tr>\n<td><strong>Sensibilità a errori</strong></td>\n<td>Molto alta</td>\n<td>Moderata</td>\n</tr>\n<tr>\n<td><strong>Decomponibile</strong></td>\n<td>No (direttamente)</td>\n<td>Sì (Murphy)</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Quando usare quale</strong>:\n- <strong>Log Loss</strong>: Training di modelli (gradient-based), quando predizioni molto sbagliate devono essere evitate\n- <strong>Brier Score</strong>: Valutazione finale, quando vogliamo interpretabilità e decomposizione</p>\n<h3 id=\"84-calibrazione-delle-probabilita\">8.4 Calibrazione delle Probabilità</h3>\n<p>Un modello è <strong>ben calibrato</strong> (o <strong>affidabile</strong>) se le probabilità predette riflettono le vere frequenze:</p>\n<p><strong>Definizione Formale</strong>:\n$P(y=1 | p(y=1|x) = q) = q \\quad \\forall q \\in [0,1]$</p>\n<p><strong>Interpretazione</strong>: &ldquo;Tra tutti gli esempi a cui assegno probabilità $q$, una frazione $q$ dovrebbe essere effettivamente positiva.&rdquo;</p>\n<p><strong>Esempio</strong>:\n- Se predico $p=0.7$ per 100 esempi, circa 70 dovrebbero essere realmente positivi\n- Se predico $p=0.3$ per 50 esempi, circa 15 dovrebbero essere realmente positivi</p>\n<h4 id=\"841-reliability-diagram-calibration-plot\">8.4.1 Reliability Diagram (Calibration Plot)</h4>\n<p>Il <strong>reliability diagram</strong> visualizza la calibrazione.</p>\n<p><strong>Procedura</strong>:</p>\n<ol>\n<li>\n<p><strong>Binning</strong>: Dividi le predizioni in $B$ bin basati su $p_i$ (e.g., $B=10$ bin di ampiezza 0.1)</p>\n</li>\n<li>\n<p><strong>Per ogni bin $b$</strong>:</p>\n</li>\n<li>Calcola <strong>probabilità media predetta</strong>: $\\bar{p}_b = \\frac{1}{|B_b|} \\sum_{i \\in B_b} p_i$</li>\n<li>\n<p>Calcola <strong>frazione empirica di positivi</strong>: $\\bar{y}_b = \\frac{1}{|B_b|} \\sum_{i \\in B_b} y_i$</p>\n</li>\n<li>\n<p><strong>Plot</strong>: $\\bar{y}_b$ (asse Y) vs $\\bar{p}_b$ (asse X)</p>\n</li>\n</ol>\n<p><strong>Interpretazione</strong>:</p>\n<ul>\n<li><strong>Diagonale perfetta</strong> ($\\bar{y}_b = \\bar{p}_b$ per ogni bin): Calibrazione perfetta</li>\n<li><strong>Sopra la diagonale</strong>: Modello <strong>sotto-confidente</strong> (predice probabilità troppo basse)</li>\n<li><strong>Sotto la diagonale</strong>: Modello <strong>sovra-confidente</strong> (predice probabilità troppo alte)</li>\n<li><strong>Forma a S</strong>: Modello sovra-confidente alle estremità, sotto-confidente al centro</li>\n</ul>\n<p><strong>Esempio</strong>:</p>\n<p>Bin $[0.8, 0.9]$:\n- $\\bar{p}_b = 0.85$ (probabilità media predetta)\n- $\\bar{y}_b = 0.95$ (frazione reale di positivi)\n- Interpretazione: Il modello è sotto-confidente in questa regione</p>\n<h4 id=\"842-expected-calibration-error-ece\">8.4.2 Expected Calibration Error (ECE)</h4>\n<p>L&rsquo;<strong>ECE</strong> quantifica numericamente la deviazione dalla calibrazione perfetta.</p>\n<p><strong>Definizione</strong>:\n$\\text{ECE} = \\sum_{b=1}^{B} \\frac{|B_b|}{n} |\\bar{y}_b - \\bar{p}_b|$</p>\n<p>dove:\n- $B$ = numero di bin\n- $|B_b|$ = numero di esempi nel bin $b$\n- $\\bar{y}_b$ = frazione empirica di positivi nel bin\n- $\\bar{p}_b$ = probabilità media predetta nel bin</p>\n<p><strong>Interpretazione</strong>: Media pesata della deviazione assoluta dalla calibrazione perfetta.</p>\n<p><strong>Proprietà</strong>:\n- Range: $[0, 1]$\n- ECE = 0 indica calibrazione perfetta\n- Usa errore <strong>assoluto</strong> (più robusto del quadratico)</p>\n<p><strong>Maximum Calibration Error (MCE)</strong>:\n$\\text{MCE} = \\max_{b=1,\\ldots,B} |\\bar{y}_b - \\bar{p}_b|$</p>\n<p>Misura la <strong>peggiore</strong> deviazione locale dalla calibrazione.</p>\n<p><strong>Scelta del numero di bin</strong>: Tipicamente $B \\in \\{10, 15, 20\\}$. Troppo pochi → scarsa risoluzione. Troppi → bin con pochi esempi (stime instabili).</p>\n<h4 id=\"843-metodi-di-calibrazione\">8.4.3 Metodi di Calibrazione</h4>\n<p>Se un modello ha buona discriminazione ma scarsa calibrazione, possiamo <strong>post-processare</strong> le probabilità.</p>\n<p><strong>Platt Scaling</strong> (Regressione Logistica):</p>\n<p>Applica una trasformazione logistica agli score:\n$p_{\\text{calib}}(y=1|x) = \\frac{1}{1 + e^{-(a \\cdot s(x) + b)}}$</p>\n<p>dove:\n- $s(x)$ è lo score non calibrato del modello\n- $a, b$ sono parametri appresi su un <strong>validation set</strong></p>\n<p><strong>Procedura</strong>:\n1. Genera score $s_i$ per il validation set\n2. Fit regressione logistica: $y_i \\sim \\text{Logistic}(a \\cdot s_i + b)$\n3. Applica la trasformazione agli score futuri</p>\n<p><strong>Quando usare</strong>: Funziona bene quando la relazione score-probabilità è monotona e approssimativamente sigmoidale (comune per SVM, Naive Bayes).</p>\n<p><strong>Isotonic Regression</strong>:</p>\n<p>Apprende una funzione <strong>monotona crescente</strong> non-parametrica $f: \\mathbb{R} \\to [0,1]$:\n$p_{\\text{calib}}(y=1|x) = f(s(x))$</p>\n<p><strong>Procedura</strong>:\n1. Ordina validation set per score crescente\n2. Trova la funzione a gradini monotona che minimizza MSE con i label\n3. Applica $f$ agli score futuri</p>\n<p><strong>Quando usare</strong>: Più flessibile di Platt, funziona per relazioni non-sigmoidali. Richiede più dati per evitare overfitting.</p>\n<p><strong>Temperature Scaling</strong> (per Neural Networks):</p>\n<p>Scala i <strong>logit</strong> con un parametro temperatura $T$:\n$p_i^{\\text{calib}} = \\frac{e^{z_i/T}}{\\sum_{j=1}^{C} e^{z_j/T}}$</p>\n<p>dove $z_i$ sono i logit (output pre-softmax).</p>\n<p><strong>Effetto di $T$</strong>:\n- $T > 1$: <strong>&ldquo;Smoothing&rdquo;</strong> → probabilità meno confidenti (più disperse)\n- $T < 1$: <strong>&ldquo;Sharpening&rdquo;</strong> → probabilità più confidenti (più concentrate)\n- $T = 1$: Nessun cambiamento</p>\n<p><strong>Apprendimento</strong>: Trova $T$ che minimizza log loss sul validation set (tipicamente con grid search o gradient descent).</p>\n<p><strong>Vantaggi</strong>: Mantiene l&rsquo;ordinamento relativo delle classi, singolo parametro globale, preserva accuracy.</p>\n<p><strong>Confronto metodi</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Parametri</th>\n<th>Flessibilità</th>\n<th>Requisiti Dati</th>\n<th>Uso Tipico</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Platt</td>\n<td>2</td>\n<td>Bassa (sigmoid)</td>\n<td>Moderati</td>\n<td>SVM, Naive Bayes</td>\n</tr>\n<tr>\n<td>Isotonic</td>\n<td>Molti (piecewise)</td>\n<td>Alta</td>\n<td>Molti</td>\n<td>Alberi, ensemble</td>\n</tr>\n<tr>\n<td>Temperature</td>\n<td>1</td>\n<td>Bassa (scala)</td>\n<td>Pochi</td>\n<td>Neural networks</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"85-decisioni-ottimali-con-costi-asimmetrici\">8.5 Decisioni Ottimali con Costi Asimmetrici</h3>\n<h4 id=\"851-framework-del-rischio-bayesiano\">8.5.1 Framework del Rischio Bayesiano</h4>\n<p>Abbiamo visto nella Sezione 3 che la decision rule ottimale minimizza il rischio atteso. Approfondiamo ora come utilizzare questo framework in pratica.</p>\n<p><strong>Rischio Atteso</strong> per soglia $\\tau$:\n$$R(\\tau) = L_{FN} \\cdot \\text{FNR}(\\tau) \\cdot \\pi + L_{FP} \\cdot \\text{FPR}(\\tau) \\cdot (1-\\pi)$$</p>\n<p>dove $\\pi = P(Y=1)$ è la prevalenza.</p>\n<p><strong>Teorema 8.1</strong> (Soglia Ottimale per Costi Asimmetrici):\n<em>Data loss matrix con costi $L_{FP}$ e $L_{FN}$, la soglia ottimale è:</em></p>\n$$\\tau^* = \\frac{L_{FP} \\cdot (1-\\pi)}{L_{FP} \\cdot (1-\\pi) + L_{FN} \\cdot \\pi}$$\n<p><strong>Dimostrazione</strong>:</p>\n<p>Dal Teorema 3.2, classifichiamo come positivo quando:\n$$\\frac{p(y=1|x)}{p(y=0|x)} > \\frac{L_{FP}}{L_{FN}}$$</p>\n<p>Riscrivendo in termini di $p(y=1|x) = p$:\n$$\\frac{p}{1-p} > \\frac{L_{FP}}{L_{FN}}$$</p>\n<p>Risolvendo per $p$:\n$$p > \\frac{L_{FP}}{L_{FP} + L_{FN}}$$</p>\n<p>Questa è la soglia ottimale quando $\\pi = 0.5$. Per prevalenza arbitraria, la soglia diventa:\n$$\\tau^* = \\frac{L_{FP} \\cdot (1-\\pi)}{L_{FP} \\cdot (1-\\pi) + L_{FN} \\cdot \\pi}$$</p>\n<p>$\\square$</p>\n<p><strong>Casi speciali</strong>:</p>\n<ol>\n<li>\n<p><strong>Costi uguali</strong> ($L_{FP} = L_{FN} = 1$):\n   $$\\tau^* = \\frac{1-\\pi}{1-\\pi+\\pi} = 1-\\pi$$</p>\n</li>\n<li>\n<p><strong>Prevalenza bilanciata</strong> ($\\pi = 0.5$):\n   $$\\tau^* = \\frac{L_{FP}}{L_{FP} + L_{FN}}$$</p>\n</li>\n</ol>\n<p><strong>Esempio pratico</strong>:</p>\n<p>Screening medico: $L_{FN} = 1000$ (vita a rischio), $L_{FP} = 1$ (test aggiuntivo), $\\pi = 0.01$.</p>\n$$\\tau^* = \\frac{1 \\cdot 0.99}{1 \\cdot 0.99 + 1000 \\cdot 0.01} = \\frac{0.99}{10.99} \\approx 0.09$$\n<p>Soglia molto bassa → massimizziamo la sensibilità, accettando molti falsi positivi.</p>\n<h3 id=\"852-cost-sensitive-learning\">8.5.2 Cost-Sensitive Learning</h3>\n<p>Invece di ottimizzare la soglia post-hoc, possiamo integrare i costi <strong>durante il training</strong>.</p>\n<p><strong>Weighted Loss</strong>:\n$$\\mathcal{L}_{\\text{weighted}} = -\\frac{1}{n}\\sum_{i=1}^n \\left[w_1 \\cdot y_i\\log(p_i) + w_0 \\cdot (1-y_i)\\log(1-p_i)\\right]$$</p>\n<p>dove i pesi sono proporzionali ai costi:\n$$w_1 = L_{FN}, \\quad w_0 = L_{FP}$$</p>\n<p><strong>Class Rebalancing</strong>:\nAlternativamente, possiamo ri-pesare le classi per compensare lo sbilanciamento:\n$$w_c = \\frac{n}{C \\cdot n_c}$$\ndove $C$ è il numero di classi e $n_c$ il numero di esempi della classe $c$.</p>\n<h2 id=\"9-classificazione-multi-classe\">9. Classificazione Multi-Classe</h2>\n<h3 id=\"91-estensione-della-matrice-di-confusione\">9.1 Estensione della Matrice di Confusione</h3>\n<p>Per $C$ classi, la matrice di confusione è $C \\times C$:</p>\n$$\\text{CM}[i,j] = \\text{numero di esempi con classe reale } i \\text{ predetti come } j$$\n<p><strong>Diagonale</strong>: Predizioni corrette\n<strong>Fuori diagonale</strong>: Errori</p>\n<h3 id=\"92-metriche-per-classe\">9.2 Metriche Per-Classe</h3>\n<p>Per ogni classe $c$, definiamo:</p>\n<p><strong>Precision per classe $c$</strong>:\n$$\\text{Precision}_c = \\frac{TP_c}{TP_c + FP_c} = \\frac{\\text{CM}[c,c]}{\\sum_i \\text{CM}[i,c]}$$</p>\n<p><strong>Recall per classe $c$</strong>:\n$$\\text{Recall}_c = \\frac{TP_c}{TP_c + FN_c} = \\frac{\\text{CM}[c,c]}{\\sum_j \\text{CM}[c,j]}$$</p>\n<p><strong>F1 per classe $c$</strong>:\n$$F1_c = \\frac{2 \\cdot \\text{Precision}_c \\cdot \\text{Recall}_c}{\\text{Precision}_c + \\text{Recall}_c}$$</p>\n<h3 id=\"93-aggregazione-macro-vs-micro-vs-weighted\">9.3 Aggregazione: Macro vs Micro vs Weighted</h3>\n<p><strong>Macro-Average</strong> (media semplice):\n$$\\text{Macro-Precision} = \\frac{1}{C}\\sum_{c=1}^C \\text{Precision}_c$$</p>\n<p><strong>Interpretazione</strong>: Ogni classe ha peso uguale, indipendentemente dalla sua frequenza.\n<strong>Uso</strong>: Dataset bilanciati, tutte le classi sono ugualmente importanti.</p>\n<p><strong>Micro-Average</strong> (aggregazione globale):\n$$\\text{Micro-Precision} = \\frac{\\sum_{c=1}^C TP_c}{\\sum_{c=1}^C (TP_c + FP_c)}$$</p>\n<p><strong>Interpretazione</strong>: Ogni esempio ha peso uguale.\n<strong>Uso</strong>: Dataset sbilanciati, classi maggioritarie sono più importanti.</p>\n<p><strong>Weighted-Average</strong> (pesata per frequenza):\n$$\\text{Weighted-Precision} = \\sum_{c=1}^C \\frac{n_c}{n} \\cdot \\text{Precision}_c$$</p>\n<p><strong>Interpretazione</strong>: Peso proporzionale alla dimensione della classe.\n<strong>Uso</strong>: Compromesso tra macro e micro.</p>\n<p><strong>Esempio</strong>:</p>\n<p>3 classi: A (100 esempi), B (50 esempi), C (10 esempi)</p>\n<table>\n<thead>\n<tr>\n<th>Classe</th>\n<th>Precision</th>\n<th>Recall</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>0.90</td>\n<td>0.85</td>\n</tr>\n<tr>\n<td>B</td>\n<td>0.80</td>\n<td>0.75</td>\n</tr>\n<tr>\n<td>C</td>\n<td>0.50</td>\n<td>0.40</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><strong>Macro-Precision</strong> = $(0.90 + 0.80 + 0.50)/3 = 0.73$</li>\n<li><strong>Micro-Precision</strong> = $(90 + 40 + 5)/(100 + 50 + 10) = 0.84$</li>\n<li><strong>Weighted-Precision</strong> = $0.90 \\cdot \\frac{100}{160} + 0.80 \\cdot \\frac{50}{160} + 0.50 \\cdot \\frac{10}{160} = 0.85$</li>\n</ul>\n<h3 id=\"94-one-vs-rest-e-one-vs-one\">9.4 One-vs-Rest e One-vs-One</h3>\n<p><strong>One-vs-Rest (OvR)</strong>:\n- Per ogni classe $c$, creiamo un problema binario: classe $c$ vs tutte le altre\n- Calcoliamo metriche binarie per ciascun problema\n- Aggreghiamo con macro/micro/weighted</p>\n<p><strong>One-vs-One (OvO)</strong>:\n- Per ogni coppia di classi $(c_i, c_j)$, creiamo un classificatore binario\n- Totale: $\\binom{C}{2} = \\frac{C(C-1)}{2}$ classificatori\n- Utile per SVM multi-classe</p>\n<h3 id=\"95-matthews-correlation-coefficient-multi-classe\">9.5 Matthews Correlation Coefficient Multi-Classe</h3>\n<p>L&rsquo;MCC può essere esteso al caso multi-classe:</p>\n$$\\text{MCC} = \\frac{\\sum_{k,l,m} C_{kk}C_{lm} - C_{kl}C_{mk}}{\\sqrt{\\sum_k\\left(\\sum_l C_{kl}\\right)\\left(\\sum_{k'\\neq k}\\sum_{l'}C_{k'l'}\\right)} \\cdot \\sqrt{\\sum_k\\left(\\sum_l C_{lk}\\right)\\left(\\sum_{k'\\neq k}\\sum_{l'}C_{l'k'}\\right)}}$$\n<p>dove $C$ è la matrice di confusione.</p>\n<p><strong>Interpretazione</strong>: Generalizzazione del coefficiente di correlazione al caso multi-classe.</p>\n<p><strong>Range</strong>: $[-1, +1]$ come nel caso binario.</p>\n<h2 id=\"10-guida-pratica-alla-scelta-delle-metriche\">10. Guida Pratica alla Scelta delle Metriche</h2>\n<h3 id=\"101-albero-decisionale\">10.1 Albero Decisionale</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Dataset bilanciato?\n├─ Sì\n│  ├─ Interessa solo accuracy? → Accuracy, Balanced Accuracy\n│  └─ Serve probabilità? → Log Loss, Brier Score, Calibration\n│\n└─ No (sbilanciato)\n   ├─ Qual è la classe di interesse?\n   │  ├─ Classe rara (positiva)\n   │  │  ├─ FN molto costosi? → Recall, F2, PR-AUC\n   │  │  ├─ FP molto costosi? → Precision, F0.5\n   │  │  └─ Bilanciamento? → F1, MCC\n   │  │\n   │  └─ Entrambe le classi importanti → Balanced Accuracy, MCC, Cohen&#39;s Kappa\n   │\n   └─ Serve valutazione threshold-independent? → ROC-AUC (se moderatamente sbilanciato), PR-AUC (se molto sbilanciato)\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"102-raccomandazioni-per-dominio\">10.2 Raccomandazioni per Dominio</h3>\n<p><strong>Medicina (Screening)</strong>:\n- <strong>Primarie</strong>: Recall, Sensitivity, F2\n- <strong>Secondarie</strong>: Specificity, PR-AUC\n- <strong>Perché</strong>: FN (mancata diagnosi) sono critici</p>\n<p><strong>Medicina (Diagnostica Definitiva)</strong>:\n- <strong>Primarie</strong>: Balanced Accuracy, MCC, F1\n- <strong>Secondarie</strong>: Specificity, PPV\n- <strong>Perché</strong>: Bilanciamento tra evitare trattamenti inutili e non perdere malati</p>\n<p><strong>Fraud Detection</strong>:\n- <strong>Primarie</strong>: Precision@K, PR-AUC, F1.5\n- <strong>Secondarie</strong>: Recall, ROC-AUC\n- <strong>Perché</strong>: FN costosi (perdite economiche), ma serve precision ragionevole</p>\n<p><strong>Spam Filtering</strong>:\n- <strong>Primarie</strong>: Precision, F0.5\n- <strong>Secondarie</strong>: FPR, Specificity\n- <strong>Perché</strong>: FP (email legittime in spam) sono inaccettabili</p>\n<p><strong>Information Retrieval</strong>:\n- <strong>Primarie</strong>: MAP (Mean Average Precision), NDCG, Precision@K\n- <strong>Secondarie</strong>: Recall@K, F1\n- <strong>Perché</strong>: Focus su top-K risultati e qualità del ranking</p>\n<p><strong>Computer Vision (Classification)</strong>:\n- <strong>Bilanciato</strong>: Top-1 Accuracy, Top-5 Accuracy\n- <strong>Sbilanciato</strong>: Macro-F1, Per-class metrics\n- <strong>Perché</strong>: Dipende dal numero e bilanciamento delle classi</p>\n<p><strong>Sentiment Analysis / NLP</strong>:\n- <strong>Primarie</strong>: Macro-F1, Weighted-F1\n- <strong>Secondarie</strong>: Per-class Precision/Recall, Confusion Matrix\n- <strong>Perché</strong>: Classi spesso sbilanciate, tutte le sentiment importanti</p>\n<h3 id=\"103-checklist-di-valutazione\">10.3 Checklist di Valutazione</h3>\n<p>Prima di scegliere le metriche, rispondi a:</p>\n<ol>\n<li><strong>Dataset è bilanciato?</strong></li>\n<li>[ ] Sì (Accuracy OK)</li>\n<li>\n<p>[ ] No (evitare Accuracy)</p>\n</li>\n<li>\n<p><strong>Costi asimmetrici?</strong></p>\n</li>\n<li>[ ] FP più costosi → enfatizza Precision</li>\n<li>[ ] FN più costosi → enfatizza Recall</li>\n<li>\n<p>[ ] Bilanciati → F1, MCC</p>\n</li>\n<li>\n<p><strong>Soglia fissa o variabile?</strong></p>\n</li>\n<li>[ ] Fissa → metriche a soglia fissata (Precision, Recall, F1)</li>\n<li>\n<p>[ ] Variabile → curve (ROC, PR)</p>\n</li>\n<li>\n<p><strong>Serve calibrazione?</strong></p>\n</li>\n<li>[ ] Sì → Log Loss, Brier Score, ECE, Reliability Diagram</li>\n<li>\n<p>[ ] No → solo discriminazione</p>\n</li>\n<li>\n<p><strong>Multi-classe?</strong></p>\n</li>\n<li>[ ] Macro (classi ugualmente importanti)</li>\n<li>[ ] Micro (esempi ugualmente importanti)</li>\n<li>[ ] Weighted (compromesso)</li>\n</ol>\n<h3 id=\"104-metriche-da-riportare-sempre\">10.4 Metriche da Riportare Sempre</h3>\n<p><strong>Minimo indispensabile</strong>:\n1. Matrice di confusione (visualizzazione completa)\n2. Almeno 2 metriche complementari (e.g., Precision + Recall, o F1 + MCC)\n3. Curva appropriata (ROC o PR) con AUC</p>\n<p><strong>Report completo</strong>:\n1. Confusion matrix\n2. Precision, Recall, F1\n3. ROC curve + AUC-ROC\n4. PR curve + AUC-PR (se sbilanciato)\n5. MCC o Cohen&rsquo;s Kappa\n6. Calibration plot + ECE (se probabilistico)\n7. Per-class metrics (se multi-classe)</p>\n<h3 id=\"105-errori-comuni-da-evitare\">10.5 Errori Comuni da Evitare</h3>\n<p><strong>❌ Usare solo Accuracy su dataset sbilanciato</strong>\n- Un modello dummy può avere accuracy alta</p>\n<p><strong>❌ Ignorare la calibrazione</strong>\n- AUC alta non implica probabilità ben calibrate</p>\n<p><strong>❌ Ottimizzare solo una metrica</strong>\n- Trade-off impliciti possono nascondere problemi</p>\n<p><strong>❌ Non considerare i costi reali</strong>\n- FP e FN raramente hanno stesso costo</p>\n<p><strong>❌ Confrontare modelli con metriche diverse</strong>\n- Usare stesse metriche per confronti fair</p>\n<p><strong>❌ Dimenticare intervalli di confidenza</strong>\n- Report puntuale senza incertezza è fuorviante</p>\n<p><strong>❌ Usare test set per tuning</strong>\n- Porta a overfitting ottimistico</p>\n<p><strong>✅ Best Practices</strong>:\n1. Sempre riportare confusion matrix\n2. Usare multiple metriche complementari\n3. Considerare i costi del dominio applicativo\n4. Validare calibrazione se si usano probabilità\n5. Report con confidence intervals (bootstrap o cross-validation)\n6. Mantenere test set completamente holdout</p>\n<h2 id=\"riferimenti-e-risorse\">Riferimenti e Risorse</h2>\n<p><strong>Paper fondamentali</strong>:\n- Provost, F., Fawcett, T. (2001). &ldquo;Robust Classification for Imprecise Environments&rdquo;\n- Davis, J., Goadrich, M. (2006). &ldquo;The Relationship Between Precision-Recall and ROC Curves&rdquo;\n- Chicco, D., Jurman, G. (2020). &ldquo;The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation&rdquo;</p>\n<p><strong>Libri consigliati</strong>:\n- Murphy, K. P. (2022). &ldquo;Probabilistic Machine Learning: An Introduction&rdquo;\n- Hastie, T., Tibshirani, R., Friedman, J. (2009). &ldquo;The Elements of Statistical Learning&rdquo;\n- Bishop, C. M. (2006). &ldquo;Pattern Recognition and Machine Learning&rdquo;</p>\n<p><strong>Strumenti software</strong>:\n- <code>sklearn.metrics</code> (Python): Implementazione completa\n- <code>ROCR</code> (R): Visualizzazione ROC/PR\n- <code>calibration</code> (Python): Post-processing calibration</p>"
}