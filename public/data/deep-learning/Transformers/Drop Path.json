{
  "title": "Drop Path (Stochastic Depth)",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione\">Introduzione</h2>\n<p><strong>Drop Path</strong>, noto anche come <strong>Stochastic Depth</strong>, è una tecnica di regolarizzazione utilizzata nelle reti neurali profonde, particolarmente efficace nei modelli basati su architetture residuali come ResNet, Vision Transformer (ViT) e Swin Transformer.</p>\n<p>A differenza del classico <strong>Dropout</strong> che disattiva casualmente singoli neuroni durante il training, Drop Path disattiva interi <strong>percorsi residuali</strong> (residual paths) o <strong>layer interi</strong>, rendendo la rete più robusta e migliorando la generalizzazione.</p>\n<h2 id=\"motivazione\">Motivazione</h2>\n<h3 id=\"il-problema-delle-reti-profonde\">Il Problema delle Reti Profonde</h3>\n<p>Nelle architetture molto profonde con connessioni residuali, ogni layer contribuisce incrementalmente alla trasformazione dell&rsquo;input. Tuttavia, durante il training si possono verificare alcuni problemi:</p>\n<ol>\n<li><strong>Co-adaptation</strong>: I layer possono diventare eccessivamente dipendenti l&rsquo;uno dall&rsquo;altro</li>\n<li><strong>Redundancy</strong>: Alcuni layer potrebbero imparare trasformazioni ridondanti</li>\n<li><strong>Overfitting</strong>: La capacità totale della rete può portare a memorizzazione invece che generalizzazione</li>\n<li><strong>Gradient flow</strong>: In reti molto profonde, alcuni layer potrebbero ricevere gradienti molto piccoli</li>\n</ol>\n<h3 id=\"la-soluzione-stochastic-depth\">La Soluzione: Stochastic Depth</h3>\n<p>Drop Path affronta questi problemi rendendo la rete <strong>stocasticamente più shallow</strong> durante il training, forzando ogni layer a imparare rappresentazioni utili anche quando alcuni layer precedenti o successivi sono assenti.</p>\n<h2 id=\"formulazione-matematica\">Formulazione Matematica</h2>\n<h3 id=\"architettura-residuale-base\">Architettura Residuale Base</h3>\n<p>Consideriamo un blocco residuale standard:</p>\n$$\n\\mathbf{x}_{l+1} = \\mathbf{x}_l + \\mathcal{F}(\\mathbf{x}_l, \\mathbf{W}_l)\n$$\n<p>dove:\n- $\\mathbf{x}_l$ è l&rsquo;input al layer $l$\n- $\\mathcal{F}(\\cdot)$ è una trasformazione (es. convoluzione, attention, MLP)\n- $\\mathbf{W}_l$ sono i parametri apprendibili del layer $l$\n- $\\mathbf{x}_{l+1}$ è l&rsquo;output che viene passato al layer successivo</p>\n<h3 id=\"drop-path-applicato\">Drop Path Applicato</h3>\n<p>Con Drop Path, la formula diventa:</p>\n$$\n\\mathbf{x}_{l+1} = \\mathbf{x}_l + \\mathbf{b}_l \\cdot \\mathcal{F}(\\mathbf{x}_l, \\mathbf{W}_l)\n$$\n<p>dove $\\mathbf{b}_l$ è una <strong>variabile di Bernoulli</strong>:</p>\n$$\n\\mathbf{b}_l \\sim \\text{Bernoulli}(p_l)\n$$\n<p>con:\n- $p_l$: probabilità di <strong>mantenere</strong> (keep probability) il path al layer $l$\n- $1 - p_l$: probabilità di <strong>droppare</strong> il path (drop probability)</p>\n<p><strong>Comportamento:</strong>\n- Se $\\mathbf{b}_l = 1$: il path è attivo, la trasformazione viene applicata\n- Se $\\mathbf{b}_l = 0$: il path è droppato, passa solo l&rsquo;identità $\\mathbf{x}_l$</p>\n<h3 id=\"scaling-during-training\">Scaling During Training</h3>\n<p>Durante il training, per mantenere l&rsquo;aspettativa del valore costante, spesso si applica uno <strong>scaling factor</strong>:</p>\n$$\n\\mathbf{x}_{l+1} = \\mathbf{x}_l + \\frac{\\mathbf{b}_l}{p_l} \\cdot \\mathcal{F}(\\mathbf{x}_l, \\mathbf{W}_l)\n$$\n<p>Questo garantisce che:</p>\n$$\n\\mathbb{E}[\\mathbf{x}_{l+1}] = \\mathbf{x}_l + \\mathbb{E}\\left[\\frac{\\mathbf{b}_l}{p_l}\\right] \\cdot \\mathcal{F}(\\mathbf{x}_l, \\mathbf{W}_l) = \\mathbf{x}_l + \\mathcal{F}(\\mathbf{x}_l, \\mathbf{W}_l)\n$$\n<p>poiché:</p>\n$$\n\\mathbb{E}\\left[\\frac{\\mathbf{b}_l}{p_l}\\right] = \\frac{1}{p_l} \\cdot p_l = 1\n$$\n<h3 id=\"inference-test-time\">Inference (Test Time)</h3>\n<p>Durante l&rsquo;inferenza, <strong>non si applica drop path</strong>. La rete usa tutti i layer deterministicamente:</p>\n$$\n\\mathbf{x}_{l+1} = \\mathbf{x}_l + \\mathcal{F}(\\mathbf{x}_l, \\mathbf{W}_l)\n$$\n<p>Lo scaling applicato durante il training assicura che i valori attesi siano consistenti tra training e inference.</p>\n<h2 id=\"linear-decay-schedule\">Linear Decay Schedule</h2>\n<p>Una strategia comune è variare la drop probability in funzione della profondità del layer, usando un <strong>linear decay schedule</strong>:</p>\n$$\np_l = 1 - \\frac{l}{L} \\cdot \\text{drop_rate}\n$$\n<p>dove:\n- $l \\in \\{0, 1, \\ldots, L-1\\}$ è l&rsquo;indice del layer\n- $L$ è il numero totale di layer\n- $\\text{drop_rate} \\in [0, 1]$ è il tasso massimo di drop (per l&rsquo;ultimo layer)</p>\n<p><strong>Intuizione:</strong>\n- <strong>Layer iniziali</strong> (vicini all&rsquo;input): drop probability bassa → quasi sempre attivi\n- <strong>Layer finali</strong> (vicini all&rsquo;output): drop probability alta → più frequentemente droppati</p>\n<p>Questo riflette l&rsquo;idea che i layer iniziali estraggono feature fondamentali, mentre i layer finali affinano rappresentazioni più specifiche.</p>\n<h3 id=\"esempio-numerico\">Esempio Numerico</h3>\n<p>Supponiamo:\n- $L = 12$ layer totali\n- $\\text{drop_rate} = 0.3$</p>\n<p>Le probabilità di drop per ciascun layer saranno:</p>\n<table>\n<thead>\n<tr>\n<th>Layer $l$</th>\n<th>Drop Probability $1 - p_l$</th>\n<th>Keep Probability $p_l$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>$0.0 \\cdot 0.3 = 0.000$</td>\n<td>$1.000$</td>\n</tr>\n<tr>\n<td>1</td>\n<td>$\\frac{1}{12} \\cdot 0.3 = 0.025$</td>\n<td>$0.975$</td>\n</tr>\n<tr>\n<td>2</td>\n<td>$\\frac{2}{12} \\cdot 0.3 = 0.050$</td>\n<td>$0.950$</td>\n</tr>\n<tr>\n<td>&hellip;</td>\n<td>&hellip;</td>\n<td>&hellip;</td>\n</tr>\n<tr>\n<td>11</td>\n<td>$\\frac{11}{12} \\cdot 0.3 = 0.275$</td>\n<td>$0.725$</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"implementazione-in-pytorch\">Implementazione in PyTorch</h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.nn</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">nn</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">DropPath</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Drop Path (Stochastic Depth) per blocchi residuali.</span>\n\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        drop_prob (float): Probabilità di droppare il path (0.0 = no drop)</span>\n<span class=\"sd\">        scale_by_keep (bool): Se True, scala per 1/keep_prob durante training</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">drop_prob</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">scale_by_keep</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_prob</span> <span class=\"o\">=</span> <span class=\"n\">drop_prob</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">scale_by_keep</span> <span class=\"o\">=</span> <span class=\"n\">scale_by_keep</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Durante inference o se drop_prob = 0, ritorna input invariato</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_prob</span> <span class=\"o\">==</span> <span class=\"mf\">0.0</span> <span class=\"ow\">or</span> <span class=\"ow\">not</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"n\">x</span>\n\n        <span class=\"c1\"># Calcola keep probability</span>\n        <span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_prob</span>\n\n        <span class=\"c1\"># Crea maschera di Bernoulli con shape broadcast-compatibile</span>\n        <span class=\"c1\"># Shape: (batch_size, 1, 1, ..., 1) per broadcasting</span>\n        <span class=\"n\">shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],)</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">ndim</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">random_tensor</span> <span class=\"o\">=</span> <span class=\"n\">keep_prob</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">random_tensor</span><span class=\"o\">.</span><span class=\"n\">floor_</span><span class=\"p\">()</span>  <span class=\"c1\"># Bernoulli: 0 o 1</span>\n\n        <span class=\"c1\"># Applica drop path con scaling</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">scale_by_keep</span><span class=\"p\">:</span>\n            <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">div</span><span class=\"p\">(</span><span class=\"n\">keep_prob</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">random_tensor</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">*</span> <span class=\"n\">random_tensor</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">output</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">extra_repr</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s1\">&#39;drop_prob=</span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_prob</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">drop_path</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">drop_prob</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">scale_by_keep</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Versione funzionale di Drop Path.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"k\">if</span> <span class=\"n\">drop_prob</span> <span class=\"o\">==</span> <span class=\"mf\">0.0</span> <span class=\"ow\">or</span> <span class=\"ow\">not</span> <span class=\"n\">training</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span>\n\n    <span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">drop_prob</span>\n    <span class=\"n\">shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],)</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">ndim</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">random_tensor</span> <span class=\"o\">=</span> <span class=\"n\">keep_prob</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">random_tensor</span><span class=\"o\">.</span><span class=\"n\">floor_</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">scale_by_keep</span><span class=\"p\">:</span>\n        <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">div</span><span class=\"p\">(</span><span class=\"n\">keep_prob</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">random_tensor</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">*</span> <span class=\"n\">random_tensor</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">output</span>\n\n\n<span class=\"c1\"># Esempio di utilizzo in un blocco residuale</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ResidualBlock</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">drop_path_prob</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path</span> <span class=\"o\">=</span> <span class=\"n\">DropPath</span><span class=\"p\">(</span><span class=\"n\">drop_path_prob</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Connessione residuale con drop path</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n\n\n<span class=\"c1\"># Costruzione di una rete con linear decay schedule</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">DeepNetwork</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">depth</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"n\">drop_path_rate</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Linear decay: crescente dalla prima all&#39;ultima layer</span>\n        <span class=\"n\">dpr</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">drop_path_rate</span><span class=\"p\">,</span> <span class=\"n\">depth</span><span class=\"p\">)]</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">blocks</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ModuleList</span><span class=\"p\">([</span>\n            <span class=\"n\">ResidualBlock</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">drop_path_prob</span><span class=\"o\">=</span><span class=\"n\">dpr</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>\n            <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">depth</span><span class=\"p\">)</span>\n        <span class=\"p\">])</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">block</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">blocks</span><span class=\"p\">:</span>\n            <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">block</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span>\n</code></pre></div>\n</div>\n</details>\n\n<h2 id=\"differenze-con-dropout-classico\">Differenze con Dropout Classico</h2>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Dropout</th>\n<th>Drop Path</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Granularità</strong></td>\n<td>Singoli neuroni/canali</td>\n<td>Interi layer/blocchi</td>\n</tr>\n<tr>\n<td><strong>Applicazione</strong></td>\n<td>All&rsquo;interno di un layer</td>\n<td>Sulla residual connection</td>\n</tr>\n<tr>\n<td><strong>Posizione</strong></td>\n<td>Dopo attivazioni</td>\n<td>Sul path residuale</td>\n</tr>\n<tr>\n<td><strong>Effetto sulla profondità</strong></td>\n<td>Nessuno</td>\n<td>Riduce la profondità effettiva</td>\n</tr>\n<tr>\n<td><strong>Uso tipico</strong></td>\n<td>MLP, FC layers</td>\n<td>Architetture residuali profonde</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"vantaggi-di-drop-path\">Vantaggi di Drop Path</h2>\n<ol>\n<li><strong>Regolarizzazione efficace</strong>: Riduce l&rsquo;overfitting in reti molto profonde</li>\n<li><strong>Training più veloce</strong>: La rete si allena più velocemente poiché in media è meno profonda</li>\n<li><strong>Ensemble implicito</strong>: Durante il training si allenano esponenzialmente molte sotto-reti</li>\n<li><strong>Migliore generalizzazione</strong>: Forza ogni layer a contribuire in modo significativo</li>\n<li><strong>Riduzione della co-adaptation</strong>: I layer non possono dipendere rigidamente l&rsquo;uno dall&rsquo;altro</li>\n</ol>\n<h2 id=\"analisi-teorica\">Analisi Teorica</h2>\n<h3 id=\"numero-di-reti-possibili\">Numero di Reti Possibili</h3>\n<p>Con $L$ layer e drop path, il numero di configurazioni possibili durante il training è:</p>\n$$\n2^L\n$$\n<p>Ad esempio, con $L = 24$ layer (come in ViT-Base), otteniamo $2^{24} \\approx 16.7$ milioni di sotto-reti diverse!</p>\n<h3 id=\"profondita-attesa\">Profondità Attesa</h3>\n<p>La profondità attesa della rete durante il training è:</p>\n$$\n\\mathbb{E}[\\text{depth}] = \\sum_{l=1}^{L} p_l\n$$\n<p>Con linear decay schedule:</p>\n$$\n\\mathbb{E}[\\text{depth}] = \\sum_{l=1}^{L} \\left(1 - \\frac{l-1}{L-1} \\cdot \\text{drop_rate}\\right) = L \\cdot \\left(1 - \\frac{\\text{drop_rate}}{2}\\right)\n$$\n<p><strong>Esempio</strong>: Con $L = 24$ e $\\text{drop_rate} = 0.3$:</p>\n$$\n\\mathbb{E}[\\text{depth}] = 24 \\cdot (1 - 0.15) = 20.4 \\text{ layer}\n$$\n<p>In media, la rete si comporta come se avesse circa 20-21 layer invece di 24.</p>\n<h2 id=\"applicazioni-pratiche\">Applicazioni Pratiche</h2>\n<h3 id=\"vision-transformer-vit\">Vision Transformer (ViT)</h3>\n<p>Nei Vision Transformer, Drop Path viene applicato dopo il blocco di attention e dopo l&rsquo;MLP:</p>\n$$\n\\begin{align}\n\\mathbf{x}' &= \\mathbf{x} + \\text{DropPath}(\\text{Attention}(\\text{LN}(\\mathbf{x}))) \\\\\n\\mathbf{x}'' &= \\mathbf{x}' + \\text{DropPath}(\\text{MLP}(\\text{LN}(\\mathbf{x}')))\n\\end{align}\n$$\n<h3 id=\"swin-transformer\">Swin Transformer</h3>\n<p>Nel Swin Transformer, Drop Path viene applicato dopo ogni sotto-blocco:</p>\n$$\n\\begin{align}\n\\mathbf{x}_{l,1} &= \\mathbf{x}_l + \\text{DropPath}(\\text{W-MSA}(\\text{LN}(\\mathbf{x}_l))) \\\\\n\\mathbf{x}_{l,2} &= \\mathbf{x}_{l,1} + \\text{DropPath}(\\text{MLP}(\\text{LN}(\\mathbf{x}_{l,1}))) \\\\\n\\mathbf{x}_{l+1,1} &= \\mathbf{x}_{l,2} + \\text{DropPath}(\\text{SW-MSA}(\\text{LN}(\\mathbf{x}_{l,2}))) \\\\\n\\mathbf{x}_{l+1} &= \\mathbf{x}_{l+1,1} + \\text{DropPath}(\\text{MLP}(\\text{LN}(\\mathbf{x}_{l+1,1})))\n\\end{align}\n$$\n<h3 id=\"convnext-e-architetture-cnn\">ConvNeXt e Architetture CNN</h3>\n<p>Drop Path è altrettanto efficace nelle CNN moderne:</p>\n$$\n\\mathbf{x}_{l+1} = \\mathbf{x}_l + \\text{DropPath}(\\text{Conv}(\\text{x}_l))\n$$\n<h2 id=\"iperparametri-e-tuning\">Iperparametri e Tuning</h2>\n<h3 id=\"scelta-del-drop-rate\">Scelta del Drop Rate</h3>\n<p>Valori tipici per <code>drop_path_rate</code>:\n- <strong>Piccoli modelli</strong> (&lt; 12 layer): $0.1 - 0.2$\n- <strong>Modelli medi</strong> (12-24 layer): $0.2 - 0.3$\n- <strong>Modelli grandi</strong> (&gt; 24 layer): $0.3 - 0.5$</p>\n<h3 id=\"strategie-alternative\">Strategie Alternative</h3>\n<p>Oltre al linear decay, esistono altre strategie:</p>\n<p><strong>1. Uniform Drop Rate:</strong>\n$$\np_l = 1 - \\text{drop_rate} \\quad \\forall l\n$$</p>\n<p><strong>2. Quadratic Decay:</strong>\n$$\np_l = 1 - \\left(\\frac{l}{L}\\right)^2 \\cdot \\text{drop_rate}\n$$</p>\n<p><strong>3. Exponential Decay:</strong>\n$$\np_l = 1 - \\left(1 - e^{-\\lambda l}\\right) \\cdot \\text{drop_rate}\n$$</p>\n<h2 id=\"risultati-empirici\">Risultati Empirici</h2>\n<p>Studi hanno dimostrato che Drop Path:\n- <strong>Migliora l&rsquo;accuratezza</strong> del 1-2% su ImageNet per modelli profondi\n- <strong>Riduce l&rsquo;overfitting</strong> significativamente con dataset piccoli\n- <strong>Accelera la convergenza</strong> del 10-20% in termini di epoche necessarie\n- <strong>Scala bene</strong> con la profondità del modello</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>Drop Path è una tecnica di regolarizzazione fondamentale per architetture residuali profonde moderne. La sua capacità di:\n- Allenare ensemble impliciti di sotto-reti\n- Ridurre la profondità effettiva durante il training\n- Forzare ogni layer a contribuire significativamente</p>\n<p>&hellip;la rende uno strumento essenziale nel toolkit del deep learning moderno, specialmente per Vision Transformer e architetture CNN profonde.</p>\n<h2 id=\"riferimenti\">Riferimenti</h2>\n<ol>\n<li><strong>Huang et al. (2016)</strong>: &ldquo;Deep Networks with Stochastic Depth&rdquo; - Paper originale</li>\n<li><strong>Dosovitskiy et al. (2021)</strong>: &ldquo;An Image is Worth 16x16 Words&rdquo; - ViT paper</li>\n<li><strong>Liu et al. (2021)</strong>: &ldquo;Swin Transformer&rdquo; - Uso in Swin</li>\n<li><strong>Touvron et al. (2021)</strong>: &ldquo;Training data-efficient image transformers&rdquo; - DeiT paper</li>\n</ol>"
}