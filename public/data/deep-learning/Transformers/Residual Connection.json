{
  "title": "Residual Connections (Connessioni Residue)",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione\">Introduzione</h2>\n<p>Le <strong>residual connections</strong> (o <strong>skip connections</strong>) sono uno dei concetti più influenti del deep learning moderno. Introdotte nel paper seminale <strong>ResNet</strong> (He et al., 2015), hanno rivoluzionato la capacità di addestrare reti neurali estremamente profonde, risolvendo il problema della <strong>degradazione</strong> che affliggeva le architetture precedenti.</p>\n<h2 id=\"formulazione-base\">Formulazione Base</h2>\n<h3 id=\"schema-generale\">Schema Generale</h3>\n<p>In un blocco con residual connection, l&rsquo;output è definito come:</p>\n$$\n\\mathbf{y} = \\mathbf{x} + \\mathcal{F}(\\mathbf{x}, \\{\\mathbf{W}_i\\})\n$$\n<p>dove:\n- $\\mathbf{x}$ è l&rsquo;input al blocco\n- $\\mathcal{F}(\\mathbf{x}, \\{\\mathbf{W}_i\\})$ è una trasformazione parametrica (possibilmente complessa)\n- $\\{\\mathbf{W}_i\\}$ sono i parametri apprendibili\n- $\\mathbf{y}$ è l&rsquo;output del blocco</p>\n<p><strong>Componenti:</strong>\n1. <strong>Identity path</strong> (percorso identità): $\\mathbf{x}$ passa inalterato\n2. <strong>Residual path</strong> (percorso residuo): $\\mathcal{F}(\\mathbf{x})$ rappresenta la trasformazione appresa\n3. <strong>Addition</strong> (somma): combinazione elemento per elemento dei due percorsi</p>\n<h3 id=\"nel-contesto-dei-transformer\">Nel Contesto dei Transformer</h3>\n<p>Nei Transformer (ViT, Swin Transformer, BERT, ecc.), lo schema diventa:</p>\n$$\n\\mathbf{X}_{\\text{out}} = \\mathbf{X} + \\text{DropPath}(\\mathcal{F}(\\text{LayerNorm}(\\mathbf{X})))\n$$\n<p>dove tipicamente $\\mathcal{F}$ rappresenta:\n- <strong>Self-Attention</strong> (nel blocco di attenzione)\n- <strong>Feed-Forward Network/MLP</strong> (nel blocco MLP)</p>\n<h2 id=\"contesto-blocco-di-attenzione-nel-swin-transformer\">Contesto: Blocco di Attenzione nel Swin Transformer</h2>\n<h3 id=\"schema-completo-del-blocco\">Schema Completo del Blocco</h3>\n<p>Nel Swin Transformer, un blocco di attenzione completo segue questa struttura:</p>\n$$\n\\begin{align}\n\\mathbf{X}_{\\text{norm}} &= \\text{LayerNorm}(\\mathbf{X}) \\\\\n\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} &= \\mathbf{X}_{\\text{norm}} \\mathbf{W}_{qkv} \\\\\n\\mathbf{A} &= \\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} + \\mathbf{B}\\right) \\\\\n\\mathbf{O} &= \\mathbf{A}\\mathbf{V} \\\\\n\\mathbf{O}_{\\text{proj}} &= \\mathbf{O}\\mathbf{W}_{\\text{proj}} \\\\\n\\mathbf{X}_{\\text{attn}} &= \\mathbf{X} + \\text{DropPath}(\\mathbf{O}_{\\text{proj}})\n\\end{align}\n$$\n<p>L&rsquo;ultima equazione è la <strong>residual connection</strong>.</p>\n<h3 id=\"dimensionalita\">Dimensionalità</h3>\n<p>Per concretezza, consideriamo lo Stage 1 del Swin Transformer:</p>\n$$\n\\begin{align}\n\\mathbf{X} &\\in \\mathbb{R}^{B \\times 3136 \\times 96} \\quad \\text{(input)} \\\\\n\\mathbf{O}_{\\text{proj}} &\\in \\mathbb{R}^{B \\times 3136 \\times 96} \\quad \\text{(output attenzione)} \\\\\n\\mathbf{X}_{\\text{attn}} &\\in \\mathbb{R}^{B \\times 3136 \\times 96} \\quad \\text{(output blocco)}\n\\end{align}\n$$\n<p>dove:\n- $B$ = batch size\n- $3136 = 56 \\times 56$ = numero di patch\n- $96$ = dimensione dei canali</p>\n<p><strong>Requisito fondamentale:</strong> $\\mathbf{X}$ e $\\mathbf{O}_{\\text{proj}}$ devono avere <strong>esattamente le stesse dimensioni</strong> per poter essere sommati elemento per elemento.</p>\n<h2 id=\"significato-della-somma\">Significato della Somma</h2>\n<h3 id=\"interpretazione-algebrica\">Interpretazione Algebrica</h3>\n<p>La somma:</p>\n$$\n\\mathbf{X}_{\\text{attn}} = \\mathbf{X} + \\mathbf{O}_{\\text{proj}}\n$$\n<p>è una <strong>somma elemento per elemento</strong> (element-wise addition):</p>\n$$\n[\\mathbf{X}_{\\text{attn}}]_{b,i,c} = [\\mathbf{X}]_{b,i,c} + [\\mathbf{O}_{\\text{proj}}]_{b,i,c}\n$$\n<p>per ogni:\n- $b \\in \\{1, \\ldots, B\\}$ (esempio nel batch)\n- $i \\in \\{1, \\ldots, 3136\\}$ (patch)\n- $c \\in \\{1, \\ldots, 96\\}$ (canale)</p>\n<h3 id=\"interpretazione-concettuale\">Interpretazione Concettuale</h3>\n<p>Questa non è una somma casuale, ma ha un profondo significato:</p>\n$$\n\\underbrace{\\mathbf{X}_{\\text{attn}}}_{\\text{Nuova rappresentazione}} = \\underbrace{\\mathbf{X}}_{\\text{Informazione originale}} + \\underbrace{\\mathbf{O}_{\\text{proj}}}_{\\text{Informazione contestuale}}\n$$\n<p><strong>In parole:</strong>\n- $\\mathbf{X}$: ciò che il modello già conosce (feature originali dei patch)\n- $\\mathbf{O}_{\\text{proj}}$: ciò che il modello apprende attraverso l&rsquo;attenzione (interazioni tra patch)\n- $\\mathbf{X}_{\\text{attn}}$: feature arricchite che combinano conoscenza locale e contestuale</p>\n<h2 id=\"intuizione-visiva\">Intuizione Visiva</h2>\n<h3 id=\"esempio-concreto\">Esempio Concreto</h3>\n<p>Consideriamo un singolo patch $i$ in un&rsquo;immagine:</p>\n<p><strong>Prima dell&rsquo;attenzione:</strong>\n$$\n\\mathbf{x}_i \\in \\mathbb{R}^{96} \\quad \\text{(feature locali del patch)}\n$$</p>\n<p>Questo vettore contiene informazioni su:\n- Colore locale\n- Texture\n- Bordi\n- Pattern elementari</p>\n<p><strong>Dopo l&rsquo;attenzione:</strong>\n$$\n\\mathbf{o}_{i,\\text{proj}} \\in \\mathbb{R}^{96} \\quad \\text{(informazioni contestuali)}\n$$</p>\n<p>Questo vettore contiene informazioni su:\n- Come il patch $i$ si relaziona con altri patch\n- Quali patch sono semanticamente simili\n- Relazioni spaziali globali\n- Pattern di alto livello</p>\n<p><strong>Dopo la residual connection:</strong>\n$$\n\\mathbf{x}_{i,\\text{attn}} = \\mathbf{x}_i + \\mathbf{o}_{i,\\text{proj}}\n$$</p>\n<p>Il patch ora contiene:\n- <strong>Informazioni locali</strong> (da $\\mathbf{x}_i$): &ldquo;Io sono un pixel rosso con un bordo&rdquo;\n- <strong>Informazioni contestuali</strong> (da $\\mathbf{o}_{i,\\text{proj}}$): &ldquo;Faccio parte di un oggetto più grande, circondato da altri patch simili&rdquo;</p>\n<h3 id=\"analogia\">Analogia</h3>\n<p>Pensa alla residual connection come a:</p>\n<blockquote>\n<p><strong>Base knowledge + New insights = Enhanced understanding</strong></p>\n</blockquote>\n<p>È come leggere un libro:\n- $\\mathbf{X}$: ciò che già sapevi prima di leggere\n- $\\mathbf{O}_{\\text{proj}}$: nuove informazioni dal libro\n- $\\mathbf{X}_{\\text{attn}}$: la tua conoscenza arricchita dopo la lettura</p>\n<p><strong>Importante:</strong> Non stai <em>sostituendo</em> la vecchia conoscenza, ma la stai <em>arricchendo</em>.</p>\n<h2 id=\"motivazione-profonda-stabilita-del-training\">Motivazione Profonda: Stabilità del Training</h2>\n<h3 id=\"il-problema-delle-reti-profonde\">Il Problema delle Reti Profonde</h3>\n<p>Prima delle residual connections, addestrare reti molto profonde era problematico:</p>\n<p><strong>Degradation Problem:</strong>\n- Aggiungere più layer <strong>dovrebbe</strong> migliorare le prestazioni (più capacità di apprendimento)\n- Nella pratica, oltre una certa profondità, le prestazioni <strong>peggioravano</strong>\n- Questo non era dovuto a overfitting, ma a difficoltà di ottimizzazione</p>\n<p><strong>Vanishing/Exploding Gradients:</strong></p>\n<p>Consideriamo una rete profonda senza residual connections:</p>\n$$\n\\mathbf{y}_L = f_L(f_{L-1}(\\ldots f_2(f_1(\\mathbf{x})) \\ldots))\n$$\n<p>Il gradiente rispetto all&rsquo;input è:</p>\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_L} \\prod_{l=1}^{L} \\frac{\\partial f_l}{\\partial f_{l-1}}\n$$\n<p>Questa <strong>catena di prodotti</strong> può causare:\n- <strong>Vanishing gradients:</strong> se $\\|\\frac{\\partial f_l}{\\partial f_{l-1}}\\| < 1$ → i gradienti tendono a zero\n- <strong>Exploding gradients:</strong> se $\\|\\frac{\\partial f_l}{\\partial f_{l-1}}\\| > 1$ → i gradienti esplodono</p>\n<p><strong>Conseguenza:</strong> I layer iniziali non imparano, o imparano molto lentamente.</p>\n<h3 id=\"la-soluzione-residual-connections\">La Soluzione: Residual Connections</h3>\n<p>Con residual connections, la propagazione in avanti diventa:</p>\n$$\n\\mathbf{y}_l = \\mathbf{y}_{l-1} + \\mathcal{F}_l(\\mathbf{y}_{l-1})\n$$\n<p>Espandendo ricorsivamente:</p>\n$$\n\\mathbf{y}_L = \\mathbf{y}_0 + \\sum_{l=1}^{L} \\mathcal{F}_l(\\mathbf{y}_{l-1})\n$$\n<h3 id=\"analisi-del-gradiente\">Analisi del Gradiente</h3>\n<p>Calcoliamo il gradiente rispetto a un layer intermedio $\\mathbf{y}_l$:</p>\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_l} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_L} \\frac{\\partial \\mathbf{y}_L}{\\partial \\mathbf{y}_l}\n$$\n<p>Grazie alla struttura residuale:</p>\n$$\n\\mathbf{y}_L = \\mathbf{y}_l + \\sum_{i=l+1}^{L} \\mathcal{F}_i(\\mathbf{y}_{i-1})\n$$\n<p>Quindi:</p>\n$$\n\\frac{\\partial \\mathbf{y}_L}{\\partial \\mathbf{y}_l} = \\mathbf{I} + \\frac{\\partial}{\\partial \\mathbf{y}_l} \\left(\\sum_{i=l+1}^{L} \\mathcal{F}_i(\\mathbf{y}_{i-1})\\right)\n$$\n<p>dove $\\mathbf{I}$ è la <strong>matrice identità</strong>.</p>\n<p><strong>Risultato finale:</strong></p>\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_l} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_L} \\left(\\mathbf{I} + \\frac{\\partial}{\\partial \\mathbf{y}_l} \\left(\\sum_{i=l+1}^{L} \\mathcal{F}_i\\right)\\right)\n$$\n<h3 id=\"perche-questo-e-cruciale\">Perché Questo è Cruciale</h3>\n<p><strong>Il termine $\\mathbf{I}$ (identità) garantisce:</strong></p>\n<ol>\n<li><strong>Flusso diretto del gradiente:</strong> Anche se tutti i $\\mathcal{F}_i$ hanno gradienti che vaniscono, il gradiente fluisce comunque attraverso l&rsquo;identità</li>\n<li><strong>Niente vanishing gradients:</strong> Il gradiente non può mai andare completamente a zero</li>\n<li><strong>Shortcut verso layer profondi:</strong> L&rsquo;informazione può &ldquo;saltare&rdquo; layer intermedi problematici</li>\n</ol>\n<h3 id=\"confronto-matematico\">Confronto Matematico</h3>\n<p><strong>Senza residual connection:</strong>\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_0} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_L} \\prod_{l=1}^{L} \\frac{\\partial \\mathcal{F}_l}{\\partial \\mathbf{y}_{l-1}}\n$$</p>\n<p>Se un solo $\\frac{\\partial \\mathcal{F}_l}{\\partial \\mathbf{y}_{l-1}}$ è piccolo → il gradiente svanisce.</p>\n<p><strong>Con residual connection:</strong>\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_0} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}_L} \\left(\\mathbf{I} + \\text{termini aggiuntivi}\\right)\n$$</p>\n<p>Anche se i termini aggiuntivi vaniscono, $\\mathbf{I}$ garantisce un gradiente minimo.</p>\n<h2 id=\"apprendimento-di-funzioni-residue\">Apprendimento di Funzioni Residue</h2>\n<h3 id=\"intuizione-della-residual-function\">Intuizione della Residual Function</h3>\n<p>Le residual connections cambiano cosa il modello deve imparare:</p>\n<p><strong>Senza residual connection:</strong>\n$$\n\\mathcal{F}(\\mathbf{x}) \\approx \\mathbf{y}_{\\text{target}}\n$$</p>\n<p>Il modello deve imparare la <strong>trasformazione completa</strong> da input a output.</p>\n<p><strong>Con residual connection:</strong>\n$$\n\\mathbf{y} = \\mathbf{x} + \\mathcal{F}(\\mathbf{x})\n$$\n$$\n\\mathcal{F}(\\mathbf{x}) \\approx \\mathbf{y}_{\\text{target}} - \\mathbf{x}\n$$</p>\n<p>Il modello deve imparare solo la <strong>differenza</strong> (residuo) tra input e output.</p>\n<h3 id=\"perche-questo-e-piu-facile\">Perché Questo è Più Facile</h3>\n<p><strong>Scenario 1 - Mappatura Identità:</strong></p>\n<p>Se l&rsquo;output ideale è $\\mathbf{y} \\approx \\mathbf{x}$ (nessuna trasformazione necessaria):</p>\n<ul>\n<li><strong>Senza residual:</strong> $\\mathcal{F}$ deve imparare $\\mathcal{F}(\\mathbf{x}) = \\mathbf{x}$ (difficile!)</li>\n<li><strong>Con residual:</strong> $\\mathcal{F}$ deve solo imparare $\\mathcal{F}(\\mathbf{x}) = \\mathbf{0}$ (facile! Basta settare i pesi a zero)</li>\n</ul>\n<p><strong>Scenario 2 - Piccole Correzioni:</strong></p>\n<p>Se l&rsquo;output ideale è $\\mathbf{y} \\approx \\mathbf{x} + \\epsilon$ (piccola modifica):</p>\n<ul>\n<li><strong>Senza residual:</strong> $\\mathcal{F}$ deve rappresentare l&rsquo;intera funzione complessa</li>\n<li><strong>Con residual:</strong> $\\mathcal{F}$ deve solo imparare il piccolo residuo $\\epsilon$</li>\n</ul>\n<h3 id=\"esempio-numerico\">Esempio Numerico</h3>\n<p>Supponiamo che un layer debba trasformare:</p>\n$$\n\\mathbf{x} = [1.0, 2.0, 3.0] \\rightarrow \\mathbf{y}_{\\text{target}} = [1.1, 2.05, 3.2]\n$$\n<p><strong>Senza residual connection:</strong>\n$$\n\\mathcal{F}(\\mathbf{x}) = [1.1, 2.05, 3.2]\n$$</p>\n<p><strong>Con residual connection:</strong>\n$$\n\\mathcal{F}(\\mathbf{x}) = \\mathbf{y}_{\\text{target}} - \\mathbf{x} = [0.1, 0.05, 0.2]\n$$</p>\n<p>È molto più facile apprendere piccole correzioni che funzioni complete!</p>\n<h2 id=\"compatibilita-dimensionale\">Compatibilità Dimensionale</h2>\n<h3 id=\"requisito-fondamentale\">Requisito Fondamentale</h3>\n<p>Per poter sommare $\\mathbf{x}$ e $\\mathcal{F}(\\mathbf{x})$, devono avere <strong>esattamente le stesse dimensioni</strong>:</p>\n$$\n\\mathbf{x} \\in \\mathbb{R}^{d} \\quad \\Rightarrow \\quad \\mathcal{F}(\\mathbf{x}) \\in \\mathbb{R}^{d}\n$$\n<h3 id=\"proiezione-finale-nel-meccanismo-di-attenzione\">Proiezione Finale nel Meccanismo di Attenzione</h3>\n<p>Nel Swin Transformer, dopo la multi-head attention:</p>\n<ol>\n<li>\n<p><strong>Dopo concatenazione heads:</strong>\n   $$\n   \\mathbf{O} \\in \\mathbb{R}^{B \\times N \\times C}\n   $$</p>\n</li>\n<li>\n<p><strong>Proiezione finale:</strong>\n   $$\n   \\mathbf{O}_{\\text{proj}} = \\mathbf{O} \\mathbf{W}_{\\text{proj}}\n   $$\n   dove $\\mathbf{W}_{\\text{proj}} \\in \\mathbb{R}^{C \\times C}$</p>\n</li>\n<li>\n<p><strong>Risultato:</strong>\n   $$\n   \\mathbf{O}_{\\text{proj}} \\in \\mathbb{R}^{B \\times N \\times C}\n   $$</p>\n</li>\n</ol>\n<p><strong>Scopo di $\\mathbf{W}_{\\text{proj}}$:</strong>\n- Miscelare informazioni delle diverse head\n- <strong>Garantire compatibilità dimensionale</strong> con $\\mathbf{X}$ per la residual connection</p>\n<h3 id=\"quando-le-dimensioni-cambiano\">Quando le Dimensioni Cambiano</h3>\n<p>In alcuni casi (es. downsampling tra stage), le dimensioni cambiano:</p>\n$$\n\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C_{\\text{in}}} \\quad \\rightarrow \\quad \\mathbf{y} \\in \\mathbb{R}^{H' \\times W' \\times C_{\\text{out}}}\n$$\n<p>dove $H' < H$, $W' < W$, $C_{\\text{out}} > C_{\\text{in}}$.</p>\n<p><strong>Soluzione - Projection Shortcut:</strong></p>\n$$\n\\mathbf{y} = \\mathbf{W}_s \\mathbf{x} + \\mathcal{F}(\\mathbf{x})\n$$\n<p>dove $\\mathbf{W}_s$ è una proiezione lineare che adatta le dimensioni:</p>\n$$\n\\mathbf{W}_s: \\mathbb{R}^{H \\times W \\times C_{\\text{in}}} \\rightarrow \\mathbb{R}^{H' \\times W' \\times C_{\\text{out}}}\n$$\n<p>Tipicamente implementata con:\n- Convoluzione $1 \\times 1$ (per cambiare canali)\n- Strided convolution o pooling (per ridurre risoluzione spaziale)</p>\n<h2 id=\"vantaggi-delle-residual-connections\">Vantaggi delle Residual Connections</h2>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Descrizione</th>\n<th>Beneficio Matematico</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Stabilità del Training</strong></td>\n<td>Mantiene il flusso del gradiente</td>\n<td>$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}$ contiene sempre termine $\\mathbf{I}$</td>\n</tr>\n<tr>\n<td><strong>Riutilizzo delle Feature</strong></td>\n<td>Informazione originale preservata</td>\n<td>$\\mathbf{y} = \\mathbf{x} + \\Delta\\mathbf{x}$ invece di $\\mathbf{y} = f(\\mathbf{x})$</td>\n</tr>\n<tr>\n<td><strong>Facilita Apprendimento</strong></td>\n<td>Impara correzioni, non trasformazioni complete</td>\n<td>$\\mathcal{F}(\\mathbf{x}) \\approx \\mathbf{0}$ invece di $\\mathcal{F}(\\mathbf{x}) \\approx \\mathbf{x}$</td>\n</tr>\n<tr>\n<td><strong>Ensemble Implicito</strong></td>\n<td>Ogni percorso è una sotto-rete</td>\n<td>$2^L$ combinazioni di percorsi possibili</td>\n</tr>\n<tr>\n<td><strong>Profondità Arbitraria</strong></td>\n<td>Permette reti con centinaia di layer</td>\n<td>Gradiente non svanisce</td>\n</tr>\n<tr>\n<td><strong>Convergenza Più Veloce</strong></td>\n<td>Inizializzazione vicina all&rsquo;identità</td>\n<td>Meno epoche necessarie</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"implementazione-pratica\">Implementazione Pratica</h2>\n<h3 id=\"codice-base\">Codice Base</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.nn</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">nn</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ResidualBlock</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Blocco residuale generico&quot;&quot;&quot;</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Residual connection</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"con-drop-path\">Con Drop Path</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ResidualBlockWithDropPath</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Blocco residuale con Drop Path&quot;&quot;&quot;</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">drop_path</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path</span> <span class=\"o\">=</span> <span class=\"n\">DropPath</span><span class=\"p\">(</span><span class=\"n\">drop_path</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">drop_path</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Identity</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># x + DropPath(F(x))</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"attention-block-completo\">Attention Block Completo</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">SwinTransformerBlock</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Blocco Swin Transformer con residual connections&quot;&quot;&quot;</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"n\">drop_path</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">attn</span> <span class=\"o\">=</span> <span class=\"n\">WindowAttention</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"n\">window_size</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path1</span> <span class=\"o\">=</span> <span class=\"n\">DropPath</span><span class=\"p\">(</span><span class=\"n\">drop_path</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">drop_path</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Identity</span><span class=\"p\">()</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">mlp</span> <span class=\"o\">=</span> <span class=\"n\">MLP</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">mlp_ratio</span><span class=\"o\">=</span><span class=\"mf\">4.0</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path2</span> <span class=\"o\">=</span> <span class=\"n\">DropPath</span><span class=\"p\">(</span><span class=\"n\">drop_path</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">drop_path</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Identity</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Prima residual connection (attenzione)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path1</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">attn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n\n        <span class=\"c1\"># Seconda residual connection (MLP)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">drop_path2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">mlp</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">x</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"projection-shortcut-per-dimensioni-diverse\">Projection Shortcut per Dimensioni Diverse</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ResidualBlockWithProjection</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Blocco residuale con proiezione per dimensioni diverse&quot;&quot;&quot;</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dim_in</span><span class=\"p\">,</span> <span class=\"n\">dim_out</span><span class=\"p\">,</span> <span class=\"n\">downsample</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">dim_in</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">dim_in</span><span class=\"p\">,</span> <span class=\"n\">dim_out</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Projection shortcut se le dimensioni cambiano</span>\n        <span class=\"k\">if</span> <span class=\"n\">dim_in</span> <span class=\"o\">!=</span> <span class=\"n\">dim_out</span> <span class=\"ow\">or</span> <span class=\"n\">downsample</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">shortcut</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">dim_in</span><span class=\"p\">,</span> <span class=\"n\">dim_out</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">shortcut</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Identity</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Ws * x + F(x)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">shortcut</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n</code></pre></div>\n</div>\n</details>\n\n<h2 id=\"varianti-delle-residual-connections\">Varianti delle Residual Connections</h2>\n<h3 id=\"pre-activation-residual\">Pre-Activation Residual</h3>\n<p>Nel design originale di ResNet:</p>\n$$\n\\mathbf{y} = \\text{ReLU}(\\mathbf{x} + \\mathcal{F}(\\mathbf{x}))\n$$\n<p>Nel design &ldquo;pre-activation&rdquo; (He et al., 2016):</p>\n$$\n\\mathbf{y} = \\mathbf{x} + \\mathcal{F}(\\text{ReLU}(\\text{BN}(\\mathbf{x})))\n$$\n<p><strong>Vantaggio:</strong> Flusso del gradiente ancora più pulito.</p>\n<h3 id=\"post-layernorm-transformer-standard\">Post-LayerNorm (Transformer Standard)</h3>\n<p>Schema originale del Transformer (Vaswani et al., 2017):</p>\n$$\n\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathcal{F}(\\mathbf{x}))\n$$\n<h3 id=\"pre-layernorm-moderno\">Pre-LayerNorm (Moderno)</h3>\n<p>Schema usato in modelli moderni (GPT, Swin, ViT):</p>\n$$\n\\mathbf{y} = \\mathbf{x} + \\mathcal{F}(\\text{LayerNorm}(\\mathbf{x}))\n$$\n<p><strong>Vantaggi:</strong>\n- Training più stabile\n- Gradienti più puliti\n- No need for learning rate warmup</p>\n<h3 id=\"weighted-residual-connections\">Weighted Residual Connections</h3>\n<p>In alcuni modelli (es. ReZero, SkipInit):</p>\n$$\n\\mathbf{y} = \\mathbf{x} + \\alpha \\cdot \\mathcal{F}(\\mathbf{x})\n$$\n<p>dove $\\alpha$ è un parametro apprendibile inizializzato a 0 o vicino a 0.</p>\n<p><strong>Idea:</strong> Inizialmente la rete è quasi identità, poi gradualmente impara.</p>\n<h2 id=\"analisi-teorica\">Analisi Teorica</h2>\n<h3 id=\"numero-di-percorsi\">Numero di Percorsi</h3>\n<p>In una rete con $L$ blocchi residuali, ogni blocco offre due percorsi:\n- Identity path\n- Residual path</p>\n<p>Il numero totale di percorsi dall&rsquo;input all&rsquo;output è:</p>\n$$\n2^L\n$$\n<p><strong>Esempio:</strong> Con $L = 50$ (ResNet-50), abbiamo $2^{50} \\approx 10^{15}$ percorsi!</p>\n<h3 id=\"interpretazione-come-ensemble\">Interpretazione come Ensemble</h3>\n<p>La rete può essere vista come un <strong>ensemble implicito</strong> di $2^L$ reti più shallow che condividono parametri.</p>\n<p>Durante il training, diverse combinazioni di percorsi vengono attivate casualmente (specialmente con Drop Path), creando un effetto di ensemble.</p>\n<h3 id=\"lunghezza-effettiva-dei-percorsi\">Lunghezza Effettiva dei Percorsi</h3>\n<p>La lunghezza media dei percorsi in una rete con $L$ blocchi è:</p>\n$$\n\\mathbb{E}[\\text{path length}] = \\frac{L}{2}\n$$\n<p>Questo spiega perché le residual networks si comportano come reti più shallow.</p>\n<h2 id=\"applicazioni-e-risultati-empirici\">Applicazioni e Risultati Empirici</h2>\n<h3 id=\"imagenet-classification\">ImageNet Classification</h3>\n<p><strong>ResNet vs Plain Networks:</strong>\n- Plain-34: 28.54% top-1 error\n- ResNet-34: 25.03% top-1 error (↓3.5%)\n- ResNet-152: 21.43% top-1 error</p>\n<p><strong>Con più layer, performance migliora</strong> (contrariamente alle plain networks).</p>\n<h3 id=\"vision-transformer\">Vision Transformer</h3>\n<p>ViT-Large (24 layer) con residual connections:\n- Converge in ~300 epoche\n- Accuracy: 87.76% su ImageNet</p>\n<p>Senza residual connections:\n- Training instabile\n- Non converge</p>\n<h3 id=\"swin-transformer\">Swin Transformer</h3>\n<p>Swin-Base con residual connections + Drop Path:\n- 83.5% top-1 accuracy su ImageNet\n- Training stabile anche con 24 stage</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>Le <strong>residual connections</strong> sono una delle innovazioni più importanti del deep learning moderno. Permettono:</p>\n<ol>\n<li><strong>Training di reti arbitrariamente profonde</strong> risolvendo il vanishing gradient problem</li>\n<li><strong>Apprendimento più efficiente</strong> focalizzandosi su correzioni residue</li>\n<li><strong>Maggiore stabilità</strong> garantendo un flusso costante del gradiente</li>\n<li><strong>Ensemble implicito</strong> creando esponenzialmente molti percorsi</li>\n<li><strong>Riutilizzo delle feature</strong> preservando informazione originale</li>\n</ol>\n<p>Nella pratica, sono diventate uno <strong>standard de facto</strong> in quasi tutte le architetture moderne, dai CNN (ResNet, EfficientNet) ai Transformer (BERT, GPT, ViT, Swin).</p>\n<h2 id=\"riferimenti\">Riferimenti</h2>\n<ol>\n<li><strong>He et al. (2015)</strong>: &ldquo;Deep Residual Learning for Image Recognition&rdquo; - Paper originale ResNet</li>\n<li><strong>He et al. (2016)</strong>: &ldquo;Identity Mappings in Deep Residual Networks&rdquo; - Pre-activation design</li>\n<li><strong>Vaswani et al. (2017)</strong>: &ldquo;Attention Is All You Need&rdquo; - Residual in Transformer</li>\n<li><strong>Dosovitskiy et al. (2020)</strong>: &ldquo;An Image is Worth 16x16 Words&rdquo; - ViT</li>\n<li><strong>Liu et al. (2021)</strong>: &ldquo;Swin Transformer&rdquo; - Hierarchical vision transformer</li>\n<li><strong>Veit et al. (2016)</strong>: &ldquo;Residual Networks Behave Like Ensembles&rdquo; - Analisi teorica</li>\n</ol>"
}