{
  "title": "Cross-Attention: Ponte tra Sequenze Diverse",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione-e-motivazione\">Introduzione e Motivazione</h2>\n<p>Mentre la <strong>self-attention</strong> permette agli elementi di una sequenza di &ldquo;comunicare&rdquo; tra loro, la <strong>cross-attention</strong> estende questo concetto permettendo a elementi di sequenze diverse di interagire. È il meccanismo che consente a due flussi informativi indipendenti di influenzarsi reciprocamente in modo selettivo e intelligente.</p>\n<p>Immaginiamo di dover tradurre la frase inglese <em>&ldquo;The cat is sleeping on the couch&rdquo;</em> in italiano. Il nostro cervello non processa la traduzione parola per parola in modo meccanico. Invece, per generare &ldquo;Il gatto&rdquo;, guardiamo a &ldquo;The cat&rdquo;; per &ldquo;sta dormendo&rdquo;, consideriamo &ldquo;is sleeping&rdquo;; e così via. Questo processo di <strong>allineamento selettivo</strong> tra le parole della lingua sorgente e quelle della lingua target è esattamente ciò che la cross-attention modella matematicamente.</p>\n<h3 id=\"il-problema-dellallineamento-tra-sequenze\">Il Problema dell&rsquo;Allineamento tra Sequenze</h3>\n<p>Prima dell&rsquo;avvento dell&rsquo;attention, i modelli di traduzione automatica usavano architetture encoder-decoder con RNN, dove l&rsquo;encoder comprimeva l&rsquo;intera frase sorgente in un singolo vettore di stato. Il decoder doveva poi &ldquo;decomprimere&rdquo; questo vettore per generare la traduzione. Questo approccio soffriva di due problemi fondamentali:</p>\n<ol>\n<li>\n<p><strong>Bottleneck informativo</strong>: Tutto il contenuto della frase sorgente doveva essere compresso in un vettore di dimensione fissa, causando perdita di informazione per frasi lunghe.</p>\n</li>\n<li>\n<p><strong>Allineamento implicito</strong>: Il modello doveva imparare implicitamente le corrispondenze tra parole sorgenti e target senza un meccanismo esplicito per farlo.</p>\n</li>\n</ol>\n<p>La cross-attention risolve entrambi i problemi fornendo un meccanismo esplicito e differenziabile per l&rsquo;allineamento tra sequenze.</p>\n<h2 id=\"lintuizione-della-cross-attention\">L&rsquo;Intuizione della Cross-Attention</h2>\n<h3 id=\"lanalogia-del-traduttore-esperto\">L&rsquo;Analogia del Traduttore Esperto</h3>\n<p>Un traduttore esperto, quando traduce una frase, mantiene costantemente l&rsquo;attenzione sulla frase originale. Per ogni parola che genera nella lingua target, il traduttore:</p>\n<ol>\n<li><strong>Consulta</strong> l&rsquo;intera frase sorgente</li>\n<li><strong>Identifica</strong> quali parti sono più rilevanti per la parola corrente</li>\n<li><strong>Estrae</strong> le informazioni necessarie da quelle parti</li>\n<li><strong>Combina</strong> queste informazioni per generare la parola target</li>\n</ol>\n<p>La cross-attention replica esattamente questo processo:</p>\n<ul>\n<li>La <strong>query</strong> rappresenta &ldquo;cosa sto cercando&rdquo; (la parola target che stiamo generando)</li>\n<li>Le <strong>key</strong> rappresentano &ldquo;cosa può essere trovato&rdquo; (le parole nella frase sorgente)</li>\n<li>I <strong>value</strong> rappresentano &ldquo;il contenuto informativo&rdquo; di ciascuna parola sorgente</li>\n<li>I <strong>pesi di attention</strong> determinano quanto ogni parola sorgente è rilevante per la parola target corrente</li>\n</ul>\n<h3 id=\"un-esempio-dettagliato\">Un Esempio Dettagliato</h3>\n<p>Consideriamo la traduzione di <em>&ldquo;The black cat sleeps&rdquo;</em> → <em>&ldquo;Il gatto nero dorme&rdquo;</em>.</p>\n<p>Quando generiamo &ldquo;nero&rdquo;:\n- <strong>Query</strong>: rappresentazione di &ldquo;nero&rdquo; (informazione target)\n- <strong>Key</strong>: rappresentazioni di [&ldquo;The&rdquo;, &ldquo;black&rdquo;, &ldquo;cat&rdquo;, &ldquo;sleeps&rdquo;] \n- La cross-attention assegnerà peso alto a &ldquo;black&rdquo; e pesi bassi alle altre parole\n- <strong>Value</strong>: contenuti informativi delle parole sorgenti\n- <strong>Output</strong>: combinazione pesata che enfatizza l&rsquo;informazione da &ldquo;black&rdquo;</p>\n<p>Quando generiamo &ldquo;dorme&rdquo;:\n- La stessa query &ldquo;dorme&rdquo; guarderà alle stesse key sorgenti\n- Stavolta il peso maggiore andrà a &ldquo;sleeps&rdquo;\n- L&rsquo;output incorporerà principalmente l&rsquo;informazione da &ldquo;sleeps&rdquo;</p>\n<h2 id=\"formulazione-matematica-della-cross-attention\">Formulazione Matematica della Cross-Attention</h2>\n<h3 id=\"setup-delle-sequenze\">Setup delle Sequenze</h3>\n<p>Consideriamo due sequenze:\n- <strong>Sequenza sorgente</strong>: $\\mathbf{X}^{src} \\in \\mathbb{R}^{d \\times N_{src}}$ con $N_{src}$ elementi\n- <strong>Sequenza target</strong>: $\\mathbf{X}^{tgt} \\in \\mathbb{R}^{d \\times N_{tgt}}$ con $N_{tgt}$ elementi</p>\n<p>Nella cross-attention, le <strong>query</strong> provengono dalla sequenza target, mentre <strong>key</strong> e <strong>value</strong> provengono dalla sequenza sorgente:</p>\n$$\\mathbf{Q} = \\mathbf{W}_q \\mathbf{X}^{tgt} + \\mathbf{b}_q \\mathbf{1}^T \\in \\mathbb{R}^{d_k \\times N_{tgt}}$$\n$$\\mathbf{K} = \\mathbf{W}_k \\mathbf{X}^{src} + \\mathbf{b}_k \\mathbf{1}^T \\in \\mathbb{R}^{d_k \\times N_{src}}$$\n$$\\mathbf{V} = \\mathbf{W}_v \\mathbf{X}^{src} + \\mathbf{b}_v \\mathbf{1}^T \\in \\mathbb{R}^{d_v \\times N_{src}}$$\n<h3 id=\"matrice-dei-punteggi-asimmetrica\">Matrice dei Punteggi Asimmetrica</h3>\n<p>La matrice dei punteggi ha dimensioni $N_{src} \\times N_{tgt}$:</p>\n$$\\mathbf{S} = \\frac{\\mathbf{K}^T \\mathbf{Q}}{\\sqrt{d_k}} \\in \\mathbb{R}^{N_{src} \\times N_{tgt}}$$\n<p>Dove:\n- L&rsquo;elemento $S_{m,n} = \\frac{\\mathbf{k}_m^T \\mathbf{q}_n}{\\sqrt{d_k}}$ rappresenta la compatibilità tra il key $m$-esimo (sorgente) e la query $n$-esima (target)\n- La riga $m$ contiene i punteggi del key sorgente $m$ rispetto a tutte le query target\n- La colonna $n$ contiene i punteggi della query target $n$ rispetto a tutti i key sorgenti</p>\n<h3 id=\"normalizzazione-e-pesi-di-attention\">Normalizzazione e Pesi di Attention</h3>\n<p>La softmax viene applicata <strong>lungo ogni colonna</strong> (normalizzazione sulla dimensione sorgente per ogni query target):</p>\n$$a_{m,n} = \\frac{\\exp(S_{m,n})}{\\sum_{\\ell=1}^{N_{src}} \\exp(S_{\\ell,n})}$$\n<p>Questo garantisce che per ogni query target $n$:\n$$\\sum_{m=1}^{N_{src}} a_{m,n} = 1$$</p>\n<p>I pesi $a_{m,n}$ indicano quanto la posizione $m$ nella sequenza sorgente è rilevante per la posizione $n$ nella sequenza target.</p>\n<h3 id=\"calcolo-delloutput\">Calcolo dell&rsquo;Output</h3>\n<p>L&rsquo;output della cross-attention è:</p>\n$$\\mathbf{Y} = \\mathbf{V} \\cdot \\mathbf{A} \\in \\mathbb{R}^{d_v \\times N_{tgt}}$$\n<p>dove $\\mathbf{A} \\in \\mathbb{R}^{N_{src} \\times N_{tgt}}$ è la matrice dei pesi di attention.</p>\n<p>Esplicitamente, l&rsquo;output per la posizione target $n$ è:</p>\n$$\\mathbf{y}_n = \\sum_{m=1}^{N_{src}} a_{m,n} \\mathbf{v}_m$$\n<p>Questa è una <strong>combinazione pesata</strong> di tutti i value della sequenza sorgente, dove i pesi sono determinati dalla rilevanza di ciascun elemento sorgente per la query target corrente.</p>\n<h2 id=\"interpretazione-geometrica-e-semantica\">Interpretazione Geometrica e Semantica</h2>\n<h3 id=\"spazio-delle-query-vs-spazio-delle-key\">Spazio delle Query vs Spazio delle Key</h3>\n<p>La cross-attention opera in uno scenario dove:</p>\n<ul>\n<li>Le <strong>query</strong> vivono nello spazio semantico della sequenza target</li>\n<li>Le <strong>key</strong> e <strong>value</strong> vivono nello spazio semantico della sequenza sorgente</li>\n<li>Le trasformazioni lineari $\\mathbf{W}_q$ e $\\mathbf{W}_k$ proiettano questi spazi diversi in uno <strong>spazio comune di compatibilità</strong></li>\n</ul>\n<p>Questo spazio comune è dove avviene il &ldquo;matching&rdquo; tra informazioni provenienti da domini diversi.</p>\n<h3 id=\"funzione-di-allineamento\">Funzione di Allineamento</h3>\n<p>La cross-attention può essere vista come una <strong>funzione di allineamento soft</strong> $\\alpha: [1, N_{tgt}] \\times [1, N_{src}] \\rightarrow [0,1]$ dove:</p>\n$$\\alpha(n,m) = a_{m,n}$$\n<p>rappresenta quanto l&rsquo;elemento target $n$ è allineato con l&rsquo;elemento sorgente $m$.</p>\n<p>A differenza degli allineamenti hard tradizionali (una corrispondenza uno-a-uno), l&rsquo;allineamento soft permette:\n- <strong>Allineamenti molti-a-uno</strong>: una parola target può allinearsi con multiple parole sorgenti\n- <strong>Allineamenti uno-a-molti</strong>: una parola sorgente può influenzare multiple parole target\n- <strong>Allineamenti parziali</strong>: connessioni con pesi frazionari</p>\n<h2 id=\"cross-attention-vs-self-attention-differenze-fondamentali\">Cross-Attention vs Self-Attention: Differenze Fondamentali</h2>\n<h3 id=\"struttura-delle-matrici\">Struttura delle Matrici</h3>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Self-Attention</th>\n<th>Cross-Attention</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Dimensioni Query</strong></td>\n<td>$d_k \\times N$</td>\n<td>$d_k \\times N_{tgt}$</td>\n</tr>\n<tr>\n<td><strong>Dimensioni Key</strong></td>\n<td>$d_k \\times N$</td>\n<td>$d_k \\times N_{src}$</td>\n</tr>\n<tr>\n<td><strong>Dimensioni Value</strong></td>\n<td>$d_v \\times N$</td>\n<td>$d_v \\times N_{src}$</td>\n</tr>\n<tr>\n<td><strong>Matrice Punteggi</strong></td>\n<td>$N \\times N$ (quadrata)</td>\n<td>$N_{src} \\times N_{tgt}$ (rettangolare)</td>\n</tr>\n<tr>\n<td><strong>Simmetria</strong></td>\n<td>Simmetrica se $\\mathbf{W}_q = \\mathbf{W}_k$</td>\n<td>Asimmetrica per costruzione</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"semantica-delle-operazioni\">Semantica delle Operazioni</h3>\n<p><strong>Self-Attention</strong>: &ldquo;Come ogni elemento della sequenza dovrebbe guardare agli altri elementi della stessa sequenza?&rdquo;</p>\n<p><strong>Cross-Attention</strong>: &ldquo;Come ogni elemento della sequenza target dovrebbe guardare agli elementi della sequenza sorgente?&rdquo;</p>\n<h3 id=\"pattern-di-dipendenza\">Pattern di Dipendenza</h3>\n<p><strong>Self-Attention</strong>: Cattura dipendenze <strong>intra-sequenza</strong> (all&rsquo;interno della stessa sequenza).</p>\n<p><strong>Cross-Attention</strong>: Cattura dipendenze <strong>inter-sequenza</strong> (tra sequenze diverse).</p>\n<h2 id=\"multi-head-cross-attention\">Multi-Head Cross-Attention</h2>\n<h3 id=\"estensione-naturale\">Estensione Naturale</h3>\n<p>Come per la self-attention, la cross-attention beneficia dell&rsquo;uso di multiple teste per catturare diversi tipi di allineamenti:</p>\n$$\\text{MultiHeadCross}(\\mathbf{X}^{tgt}, \\mathbf{X}^{src}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n<p>dove:</p>\n$$\\text{head}_i = \\text{CrossAttention}(\\mathbf{X}^{tgt}\\mathbf{W}_i^Q, \\mathbf{X}^{src}\\mathbf{W}_i^K, \\mathbf{X}^{src}\\mathbf{W}_i^V)$$\n<h3 id=\"specializzazione-delle-teste\">Specializzazione delle Teste</h3>\n<p>Nella cross-attention multi-head, le diverse teste possono specializzarsi in:</p>\n<p><strong>Allineamenti lessicali</strong>: Corrispondenze dirette tra parole (es. &ldquo;cat&rdquo; → &ldquo;gatto&rdquo;)</p>\n<p><strong>Allineamenti sintattici</strong>: Strutture grammaticali equivalenti (es. soggetto con soggetto)</p>\n<p><strong>Allineamenti semantici</strong>: Concetti correlati (es. &ldquo;automobile&rdquo; → &ldquo;veicolo&rdquo;)</p>\n<p><strong>Allineamenti di ordine</strong>: Gestione delle differenze nell&rsquo;ordine delle parole tra lingue</p>\n<h2 id=\"applicazioni-della-cross-attention\">Applicazioni della Cross-Attention</h2>\n<h3 id=\"1-traduzione-automatica-neurale\">1. Traduzione Automatica Neurale</h3>\n<p><strong>Architettura</strong>: Transformer Encoder-Decoder\n- <strong>Encoder</strong>: Processa la sequenza sorgente con self-attention\n- <strong>Decoder</strong>: Usa self-attention per la sequenza target + cross-attention per accedere alla sorgente\n- <strong>Vantaggi</strong>: Allineamento esplicito, gestione di sequenze di lunghezza diversa</p>\n<p><strong>Formula nel Decoder</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code># Self-attention sul target\ntarget_self_attn = SelfAttention(target_sequence)\n\n# Cross-attention tra target e source  \ncross_attn = CrossAttention(target_self_attn, encoder_output)\n\n# Feed-forward\noutput = FeedForward(cross_attn)\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"2-question-answering\">2. Question Answering</h3>\n<p><strong>Setup</strong>: Data una domanda $Q$ e un contesto $C$, trova la risposta.\n- <strong>Query</strong>: Rappresentazione della domanda\n- <strong>Key/Value</strong>: Rappresentazioni del contesto\n- <strong>Output</strong>: Parti del contesto rilevanti per la domanda</p>\n<p><strong>Esempio</strong>:\n- Domanda: &ldquo;Quale animale dorme?&rdquo;\n- Contesto: &ldquo;Il gatto nero sta dormendo sul divano mentre il cane corre.&rdquo;\n- Cross-attention: Alto peso su &ldquo;gatto&rdquo; e &ldquo;dormendo&rdquo;</p>\n<h3 id=\"3-image-captioning\">3. Image Captioning</h3>\n<p><strong>Architettura</strong>: CNN + Transformer Decoder\n- <strong>CNN</strong>: Estrae features visive da regioni dell&rsquo;immagine\n- <strong>Cross-Attention</strong>: Allinea parole del caption con regioni visive\n- <strong>Self-Attention</strong>: Mantiene coerenza linguistica nel caption</p>\n<p><strong>Formula</strong>:\n$$\\text{word}_t = \\text{CrossAttention}(\\text{previous_words}, \\text{image_regions})$$</p>\n<h3 id=\"4-multimodal-understanding\">4. Multimodal Understanding</h3>\n<p><strong>Vision-Language Models</strong>:\n- <strong>Query</strong>: Rappresentazioni testuali\n- <strong>Key/Value</strong>: Rappresentazioni visuali\n- <strong>Applicazioni</strong>: VQA (Visual Question Answering), image retrieval</p>\n<p><strong>Speech-Text Alignment</strong>:\n- <strong>Query</strong>: Features audio\n- <strong>Key/Value</strong>: Rappresentazioni testuali\n- <strong>Applicazioni</strong>: Speech recognition, text-to-speech</p>\n<h2 id=\"architetture-che-utilizzano-cross-attention\">Architetture che Utilizzano Cross-Attention</h2>\n<h3 id=\"transformer-encoder-decoder\">Transformer Encoder-Decoder</h3>\n<p><strong>Struttura Completa</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Encoder:\n  - Multi-Head Self-Attention\n  - Position-wise Feed-Forward\n\nDecoder:\n  - Masked Multi-Head Self-Attention  # Evita lookahead\n  - Multi-Head Cross-Attention        # Con encoder output\n  - Position-wise Feed-Forward\n</code></pre></div>\n</div>\n</details>\n\n<p><strong>Flusso dell&rsquo;Informazione</strong>:\n1. Encoder processa la sequenza sorgente\n2. Decoder genera la sequenza target autoregressivamente\n3. Ad ogni step, il decoder usa cross-attention per &ldquo;consultare&rdquo; l&rsquo;encoder</p>\n<h3 id=\"bert-like-models-con-cross-attention\">BERT-like Models con Cross-Attention</h3>\n<p><strong>Modelli Multimodali</strong> come ViLBERT, LXMERT:\n- <strong>Stream Visivo</strong>: Processa features delle immagini\n- <strong>Stream Testuale</strong>: Processa token del testo<br />\n- <strong>Cross-Modal Layers</strong>: Cross-attention tra i due stream</p>\n<h3 id=\"retrieval-augmented-generation-rag\">Retrieval-Augmented Generation (RAG)</h3>\n<p><strong>Architettura</strong>:\n1. <strong>Retriever</strong>: Trova documenti rilevanti\n2. <strong>Cross-Attention</strong>: Allinea la query con i documenti retrieved\n3. <strong>Generator</strong>: Produce la risposta basandosi su query + documenti</p>\n<h2 id=\"analisi-della-complessita-computazionale\">Analisi della Complessità Computazionale</h2>\n<h3 id=\"complessita-temporale\">Complessità Temporale</h3>\n<p>Per sequenze di lunghezza $N_{src}$ e $N_{tgt}$:</p>\n<p><strong>Calcolo dei punteggi</strong>: $\\mathbf{K}^T \\mathbf{Q}$ richiede $O(N_{src} \\times N_{tgt} \\times d_k)$</p>\n<p><strong>Applicazione della softmax</strong>: $O(N_{src} \\times N_{tgt})$</p>\n<p><strong>Moltiplicazione finale</strong>: $\\mathbf{V} \\times \\mathbf{A}$ richiede $O(N_{src} \\times N_{tgt} \\times d_v)$</p>\n<p><strong>Complessità totale</strong>: $O(N_{src} \\times N_{tgt} \\times d)$ dove $d = \\max(d_k, d_v)$</p>\n<h3 id=\"confronto-con-self-attention\">Confronto con Self-Attention</h3>\n<table>\n<thead>\n<tr>\n<th>Tipo</th>\n<th>Complessità Temporale</th>\n<th>Complessità Spaziale</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Self-Attention</td>\n<td>$O(N^2 d)$</td>\n<td>$O(N^2)$</td>\n</tr>\n<tr>\n<td>Cross-Attention</td>\n<td>$O(N_{src} N_{tgt} d)$</td>\n<td>$O(N_{src} N_{tgt})$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"implicazioni-pratiche\">Implicazioni Pratiche</h3>\n<p><strong>Quando $N_{src} \\approx N_{tgt}$</strong>: La complessità è simile alla self-attention.</p>\n<p><strong>Quando $N_{src} \\gg N_{tgt}$</strong>: La cross-attention può essere più costosa (es. image captioning con molte regioni visive).</p>\n<p><strong>Quando $N_{src} \\ll N_{tgt}$</strong>: La cross-attention è più efficiente (es. conditioning su un piccolo prompt).</p>\n<h2 id=\"mascheramento-nella-cross-attention\">Mascheramento nella Cross-Attention</h2>\n<h3 id=\"causal-masking-nel-decoder\">Causal Masking nel Decoder</h3>\n<p>Nel decoder autoregressivo, si applica un mascheramento <strong>causale</strong> alla self-attention per prevenire che il modello &ldquo;veda il futuro&rdquo;:</p>\n$$\\text{mask}_{i,j} = \\begin{cases}\n-\\infty & \\text{se } j > i \\\\\n0 & \\text{se } j \\leq i\n\\end{cases}$$\n<p><strong>Importante</strong>: Questo mascheramento si applica solo alla <strong>self-attention</strong> nel decoder, NON alla cross-attention, perché:\n- La cross-attention accede all&rsquo;intera sequenza sorgente (già completamente osservata)\n- Non c&rsquo;è rischio di &ldquo;vedere il futuro&rdquo; nella sequenza sorgente</p>\n<h3 id=\"padding-masking\">Padding Masking</h3>\n<p>Per gestire sequenze di lunghezza variabile, si maschera l&rsquo;attention sui token di padding:</p>\n$$a_{m,n} = \\begin{cases}\n\\frac{\\exp(S_{m,n})}{\\sum_{\\ell \\neq \\text{pad}} \\exp(S_{\\ell,n})} & \\text{se key}_m \\neq \\text{PAD} \\\\\n0 & \\text{se key}_m = \\text{PAD}\n\\end{cases}$$\n<h3 id=\"content-based-masking\">Content-Based Masking</h3>\n<p>In alcune applicazioni, si possono applicare maschere basate sul contenuto:\n- <strong>Entity Masking</strong>: Nascondere entità specifiche\n- <strong>Domain Masking</strong>: Limitare l&rsquo;attention a parti specifiche della sorgente\n- <strong>Relevance Masking</strong>: Usare soglie per eliminare connessioni deboli</p>\n<h2 id=\"ottimizzazioni-e-implementazioni-efficienti\">Ottimizzazioni e Implementazioni Efficienti</h2>\n<h3 id=\"batching-efficiente\">Batching Efficiente</h3>\n<p>[Placeholder per implementazione ottimizzata del batching per cross-attention]</p>\n<h3 id=\"memory-efficient-cross-attention\">Memory-Efficient Cross-Attention</h3>\n<p>Per sequenze molto lunghe, si possono usare:</p>\n<p><strong>Checkpointing</strong>: Ricalcolare i gradienti invece di memorizzarli.</p>\n<p><strong>Chunked Cross-Attention</strong>: Processare la cross-attention in blocchi.</p>\n<p><strong>Sparse Cross-Attention</strong>: Limitare l&rsquo;attention a subset delle posizioni.</p>\n<h3 id=\"implementazione-hardware-aware\">Implementazione Hardware-Aware</h3>\n<p><strong>GPU Optimization</strong>: \n- Fusione delle operazioni matriciali\n- Utilizzazione della memoria shared\n- Ottimizzazione dei pattern di accesso alla memoria</p>\n<p><strong>TPU Optimization</strong>:\n- Batching delle operazioni per sfruttare le unità matriciali\n- Pipeline delle computazioni</p>\n<h2 id=\"interpretabilita-e-visualizzazione\">Interpretabilità e Visualizzazione</h2>\n<h3 id=\"visualizzazione-dei-pesi-di-attention\">Visualizzazione dei Pesi di Attention</h3>\n<p>I pesi di cross-attention possono essere visualizzati come <strong>heatmap</strong> $N_{src} \\times N_{tgt}$:</p>\n<p>[Placeholder per codice di visualizzazione delle attention weights]</p>\n<h3 id=\"analisi-dei-pattern-di-allineamento\">Analisi dei Pattern di Allineamento</h3>\n<p><strong>Analisi Quantitativa</strong>:\n- <strong>Entropia dei pesi</strong>: Misura la diffusione dell&rsquo;attention\n- <strong>Allineamento uno-a-uno</strong>: Percentuale di connessioni dominanti\n- <strong>Coverage</strong>: Quante posizioni sorgenti ricevono attention significativa</p>\n<p><strong>Analisi Qualitativa</strong>:\n- Pattern di allineamento linguisticamente plausibili\n- Gestione di fenomeni complessi (idiomi, riordino)\n- Robustezza a input rumorosi</p>\n<h3 id=\"probing-studies\">Probing Studies</h3>\n<p><strong>Esperimenti di Ablation</strong>:\n- Rimozione di specifiche teste di attention\n- Analisi dell&rsquo;importanza di diverse posizioni\n- Studio della degradazione delle prestazioni</p>\n<p><strong>Intervention Studies</strong>:\n- Modifica manuale dei pesi di attention\n- Controllo dell&rsquo;allineamento forzato\n- Analisi causale delle decisioni del modello</p>\n<h2 id=\"limitazioni-e-problemi-aperti\">Limitazioni e Problemi Aperti</h2>\n<h3 id=\"problemi-di-allineamento\">Problemi di Allineamento</h3>\n<p><strong>Many-to-Many Alignments</strong>: Difficoltà nel gestire corrispondenze complesse.</p>\n<p><strong>Null Alignments</strong>: Quando parole target non hanno corrispondenti sorgenti.</p>\n<p><strong>Spurious Alignments</strong>: Connessioni non linguisticamente motivate.</p>\n<h3 id=\"complessita-per-sequenze-lunghe\">Complessità per Sequenze Lunghe</h3>\n<p><strong>Quadratic Growth</strong>: La complessità cresce quadraticamente con la lunghezza.</p>\n<p><strong>Memory Bottleneck</strong>: Le matrici di attention diventano proibitive.</p>\n<p><strong>Attention Dilution</strong>: Su sequenze lunghe, l&rsquo;attention si &ldquo;diluisce&rdquo;.</p>\n<h3 id=\"bias-e-fairness\">Bias e Fairness</h3>\n<p><strong>Language Bias</strong>: Modelli addestrati su lingue specifiche possono avere bias.</p>\n<p><strong>Cultural Bias</strong>: Allineamenti che riflettono stereotipi culturali.</p>\n<p><strong>Domain Bias</strong>: Performance degradata su domini non visti durante il training.</p>\n<h2 id=\"metriche-di-valutazione\">Metriche di Valutazione</h2>\n<h3 id=\"metriche-di-allineamento\">Metriche di Allineamento</h3>\n<p><strong>Alignment Error Rate (AER)</strong>:\n$$\\text{AER} = 1 - \\frac{|A_{pred} \\cap A_{gold}| + |A_{pred} \\cap A_{probable}|}{|A_{pred}| + |A_{gold}|}$$</p>\n<p><strong>Precision/Recall dell&rsquo;Allineamento</strong>:\n- Precision: Frazione di allineamenti predetti che sono corretti\n- Recall: Frazione di allineamenti gold catturati</p>\n<h3 id=\"metriche-di-task-specific\">Metriche di Task-Specific</h3>\n<p><strong>Translation Quality</strong>: BLEU, METEOR, BERTScore per traduzione.</p>\n<p><strong>QA Performance</strong>: Exact Match, F1 per question answering.</p>\n<p><strong>Retrieval Metrics</strong>: MRR, NDCG per task di retrieval.</p>\n<h3 id=\"metriche-di-interpretabilita\">Metriche di Interpretabilità</h3>\n<p><strong>Attention Entropy</strong>: Misura la concentrazione dell&rsquo;attention.</p>\n<p><strong>Alignment Consistency</strong>: Consistenza degli allineamenti attraverso layer diversi.</p>\n<p><strong>Human Agreement</strong>: Accordo tra attention automatica e annotazioni umane.</p>\n<h2 id=\"implementazione-pratica\">Implementazione Pratica</h2>\n<p>[Placeholder per implementazione completa di Cross-Attention con PyTorch]</p>\n<h3 id=\"considerazioni-di-implementazione\">Considerazioni di Implementazione</h3>\n<p><strong>Numerical Stability</strong>: Gestione di overflow/underflow nella softmax.</p>\n<p><strong>Memory Management</strong>: Strategie per ridurre l&rsquo;utilizzo della memoria.</p>\n<p><strong>Gradient Flow</strong>: Assicurare flusso stabile dei gradienti.</p>\n<h3 id=\"testing-e-debugging\">Testing e Debugging</h3>\n<p><strong>Unit Tests</strong>: Test delle singole componenti.</p>\n<p><strong>Integration Tests</strong>: Test dell&rsquo;intera pipeline.</p>\n<p><strong>Attention Visualization</strong>: Debug attraverso visualizzazione.</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>La <strong>Cross-Attention</strong> rappresenta un meccanismo fondamentale per collegare informazioni provenienti da sequenze o modalità diverse. La sua capacità di creare allineamenti soft e differenziabili l&rsquo;ha resa indispensabile in una vasta gamma di applicazioni, dalla traduzione automatica ai modelli multimodali.</p>\n<h3 id=\"contributi-chiave\">Contributi Chiave</h3>\n<ol>\n<li><strong>Allineamento Esplicito</strong>: Meccanismo differenziabile per l&rsquo;allineamento tra sequenze</li>\n<li><strong>Flessibilità</strong>: Gestione naturale di sequenze di lunghezza diversa</li>\n<li><strong>Interpretabilità</strong>: Visualizzazione diretta delle corrispondenze apprese</li>\n<li><strong>Efficienza</strong>: Parallelizzazione completa del processo di allineamento</li>\n</ol>\n<h3 id=\"impatto-sul-deep-learning\">Impatto sul Deep Learning</h3>\n<p>La cross-attention ha:\n- Rivoluzionato la traduzione automatica neurale\n- Abilitato lo sviluppo di modelli multimodali sofisticati\n- Fornito le basi per architetture di retrieval-augmented generation\n- Ispirato nuovi metodi per il trasferimento di conoscenza cross-domain</p>\n<h3 id=\"considerazioni-future\">Considerazioni Future</h3>\n<p>Mentre la cross-attention continua a essere un componente centrale di molte architetture, la ricerca futura si concentrerà probabilmente su:\n- Riduzione della complessità computazionale\n- Miglioramento dell&rsquo;interpretabilità e controllo\n- Estensione a modalità e domini sempre più diversi\n- Integrazione con metodi di reasoning simbolico</p>\n<p>La comprensione profonda della cross-attention è essenziale per chiunque lavori con modelli che devono integrare informazioni da fonti multiple, rappresentando una competenza chiave nell&rsquo;era dei modelli multimodali e multi-task.</p>"
}