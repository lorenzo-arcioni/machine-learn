{
  "title": "Masked Self-Attention: Controllo del Flusso Informativo",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione-e-motivazione\">Introduzione e Motivazione</h2>\n<p>La <strong>Masked Self-Attention</strong> è una variante del meccanismo di self-attention in cui si <strong>controlla selettivamente</strong> quali posizioni della sequenza possono prestare attenzione ad altre posizioni. È come mettere dei &ldquo;paraocchi&rdquo; selettivi al meccanismo di attention, impedendo al modello di vedere certe informazioni in momenti specifici.</p>\n<p>Questo controllo è fondamentale in molti scenari pratici. Consideriamo un modello di linguaggio che deve predire la prossima parola nella frase <em>&ldquo;Il gatto nero sta ___&rdquo;</em>. Sarebbe &ldquo;imbrogliare&rdquo; se il modello potesse vedere che la parola mancante è &ldquo;dormendo&rdquo; mentre cerca di predirla. La masked self-attention risolve questo problema impedendo al modello di guardare &ldquo;nel futuro&rdquo; durante la generazione.</p>\n<h3 id=\"il-problema-del-lookahead\">Il Problema del Lookahead</h3>\n<p>Nella self-attention standard, ogni token può prestare attenzione a tutti gli altri token della sequenza, inclusi quelli che vengono &ldquo;dopo&rdquo; nella sequenza temporale o logica. Questo crea diversi problemi:</p>\n<ol>\n<li><strong>Data Leakage durante il Training</strong>: Il modello potrebbe imparare a &ldquo;copiare&rdquo; la risposta corretta invece di impararla davvero</li>\n<li><strong>Inconsistenza Train/Inference</strong>: Durante l&rsquo;inferenza, il modello genera token uno alla volta e non ha accesso ai token futuri</li>\n<li><strong>Violazione della Causalità</strong>: In molte applicazioni (linguaggio, time series), l&rsquo;ordine temporale è semanticamente importante</li>\n</ol>\n<h3 id=\"lanalogia-del-gioco-a-carte\">L&rsquo;Analogia del Gioco a Carte</h3>\n<p>Immaginiamo un gioco di carte dove devi predire la prossima carta basandoti su quelle già mostrate. Nella self-attention normale, è come se potessi vedere tutte le carte del mazzo contemporaneamente - non sarebbe un gioco equo. La masked self-attention è come imporre la regola che puoi vedere solo le carte già giocate, rendendo la predizione una vera sfida basata sulla storia osservata.</p>\n<h2 id=\"tipi-di-mascheramento\">Tipi di Mascheramento</h2>\n<h3 id=\"1-causal-masking-lower-triangular\">1. Causal Masking (Lower Triangular)</h3>\n<p>Il tipo più comune di mascheramento è il <strong>causal masking</strong>, dove ogni posizione può prestare attenzione solo alle posizioni precedenti (inclusa se stessa):</p>\n$$\\text{mask}_{i,j} = \\begin{cases}\n0 & \\text{se } j \\leq i \\text{ (permesso)} \\\\\n-\\infty & \\text{se } j > i \\text{ (mascherato)}\n\\end{cases}$$\n<p>Visualmente, per una sequenza di 5 token:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Position:   1  2  3  4  5\n    1    [  ✓  ✗  ✗  ✗  ✗ ]\n    2    [  ✓  ✓  ✗  ✗  ✗ ]\n    3    [  ✓  ✓  ✓  ✗  ✗ ]\n    4    [  ✓  ✓  ✓  ✓  ✗ ]\n    5    [  ✓  ✓  ✓  ✓  ✓ ]\n</code></pre></div>\n</div>\n</details>\n\n<p>Dove ✓ indica posizioni visibili e ✗ posizioni mascherate.</p>\n<h3 id=\"2-padding-masking\">2. Padding Masking</h3>\n<p>Per gestire sequenze di lunghezza variabile in un batch, si mascherano i token di padding:</p>\n$$\\text{mask}_{i,j} = \\begin{cases}\n0 & \\text{se token}_j \\neq \\text{PAD} \\\\\n-\\infty & \\text{se token}_j = \\text{PAD}\n\\end{cases}$$\n<h3 id=\"3-content-based-masking\">3. Content-Based Masking</h3>\n<p>Mascheramento basato sul contenuto specifico:\n- <strong>Entity Masking</strong>: Nascondere specifici tipi di entità\n- <strong>Random Masking</strong>: Come in BERT, mascherare token casuali\n- <strong>Structured Masking</strong>: Mascherare secondo pattern strutturali</p>\n<h3 id=\"4-attention-pattern-masking\">4. Attention Pattern Masking</h3>\n<p>Mascheramento per creare pattern di attention specifici:\n- <strong>Local Attention</strong>: Solo finestre locali\n- <strong>Strided Attention</strong>: Pattern con step fissi<br />\n- <strong>Dilated Attention</strong>: Pattern con dilatazioni</p>\n<h2 id=\"formulazione-matematica-del-causal-masking\">Formulazione Matematica del Causal Masking</h2>\n<h3 id=\"modificazione-della-matrice-dei-punteggi\">Modificazione della Matrice dei Punteggi</h3>\n<p>Nella self-attention standard, calcoliamo:</p>\n$$\\mathbf{S} = \\frac{\\mathbf{K}^T \\mathbf{Q}}{\\sqrt{d_k}} \\in \\mathbb{R}^{N \\times N}$$\n<p>Nel causal masking, modifichiamo la matrice aggiungendo la maschera:</p>\n$$\\mathbf{S}_{masked} = \\mathbf{S} + \\mathbf{M}$$\n<p>dove $\\mathbf{M}$ è la matrice di maschera:</p>\n$$\\mathbf{M}_{i,j} = \\begin{cases}\n0 & \\text{se } j \\leq i \\\\\n-\\infty & \\text{se } j > i\n\\end{cases}$$\n<h3 id=\"effetto-sulla-softmax\">Effetto sulla Softmax</h3>\n<p>La softmax applicata alla matrice mascherata diventa:</p>\n$$a_{i,j} = \\frac{\\exp(S_{i,j} + M_{i,j})}{\\sum_{k=1}^{N} \\exp(S_{i,k} + M_{i,k})}$$\n<p>Poiché $\\exp(-\\infty) = 0$, otteniamo:</p>\n$$a_{i,j} = \\begin{cases}\n\\frac{\\exp(S_{i,j})}{\\sum_{k=1}^{i} \\exp(S_{i,k})} & \\text{se } j \\leq i \\\\\n0 & \\text{se } j > i\n\\end{cases}$$\n<p>Questo garantisce che $\\sum_{j=1}^{N} a_{i,j} = 1$ con $a_{i,j} = 0$ per $j > i$.</p>\n<h3 id=\"matrice-di-attention-risultante\">Matrice di Attention Risultante</h3>\n<p>La matrice di attention mascherata ha struttura <strong>triangolare inferiore</strong>:</p>\n$$\\mathbf{A}_{masked} = \\begin{bmatrix}\na_{1,1} & 0 & 0 & \\cdots & 0 \\\\\na_{2,1} & a_{2,2} & 0 & \\cdots & 0 \\\\\na_{3,1} & a_{3,2} & a_{3,3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{N,1} & a_{N,2} & a_{N,3} & \\cdots & a_{N,N}\n\\end{bmatrix}$$\n<p>dove ogni riga $i$ ha pesi non-zero solo per le prime $i$ posizioni.</p>\n<h2 id=\"implementazione-efficiente-delle-maschere\">Implementazione Efficiente delle Maschere</h2>\n<h3 id=\"generazione-della-maschera-causale\">Generazione della Maschera Causale</h3>\n<p>[Placeholder per codice di generazione della maschera causale efficiente]</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">generate_causal_mask</span><span class=\"p\">(</span><span class=\"n\">seq_len</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Genera una maschera causale (triangolare inferiore)</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">triu</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">),</span> <span class=\"n\">diagonal</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">mask</span><span class=\"o\">.</span><span class=\"n\">masked_fill</span><span class=\"p\">(</span><span class=\"n\">mask</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">&#39;-inf&#39;</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">mask</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"ottimizzazioni-hardware\">Ottimizzazioni Hardware</h3>\n<p><strong>Fused Operations</strong>: Combinare l&rsquo;addizione della maschera con altri calcoli.</p>\n<p><strong>Memory Efficient</strong>: Evitare di materializzare esplicitamente matrici grandi.</p>\n<p><strong>Triangular Operations</strong>: Sfruttare la struttura triangolare per ottimizzazioni.</p>\n<h2 id=\"confronto-masked-vs-unmasked-self-attention\">Confronto: Masked vs Unmasked Self-Attention</h2>\n<h3 id=\"capacita-rappresentazionale\">Capacità Rappresentazionale</h3>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Unmasked</th>\n<th>Masked (Causal)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Contesto Disponibile</strong></td>\n<td>Intera sequenza</td>\n<td>Solo passato</td>\n</tr>\n<tr>\n<td><strong>Bidirezionalità</strong></td>\n<td>Bidirezionale</td>\n<td>Unidirezionale</td>\n</tr>\n<tr>\n<td><strong>Information Flow</strong></td>\n<td>Parallelo</td>\n<td>Sequenziale</td>\n</tr>\n<tr>\n<td><strong>Training Efficiency</strong></td>\n<td>Maggiore</td>\n<td>Minore</td>\n</tr>\n<tr>\n<td><strong>Inference Consistency</strong></td>\n<td>Inconsistente</td>\n<td>Consistente</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"esempio-concreto-predizione-di-testo\">Esempio Concreto: Predizione di Testo</h3>\n<p>Consideriamo la frase: <em>&ldquo;Il sole splende nel cielo azzurro&rdquo;</em></p>\n<p><strong>Unmasked Self-Attention</strong> (per predire &ldquo;azzurro&rdquo;):\n- Query &ldquo;azzurro&rdquo; può vedere: &ldquo;Il&rdquo;, &ldquo;sole&rdquo;, &ldquo;splende&rdquo;, &ldquo;nel&rdquo;, &ldquo;cielo&rdquo;, &ldquo;azzurro&rdquo;\n- Problema: Il modello vede la risposta che deve predire!</p>\n<p><strong>Masked Self-Attention</strong> (per predire &ldquo;azzurro&rdquo;):\n- Query &ldquo;azzurro&rdquo; può vedere solo: &ldquo;Il&rdquo;, &ldquo;sole&rdquo;, &ldquo;splende&rdquo;, &ldquo;nel&rdquo;, &ldquo;cielo&rdquo;\n- Corretto: Il modello deve predire basandosi solo sul contesto passato</p>\n<h2 id=\"applicazioni-della-masked-self-attention\">Applicazioni della Masked Self-Attention</h2>\n<h3 id=\"1-language-modeling-gpt\">1. Language Modeling (GPT)</h3>\n<p><strong>Architettura</strong>: Decoder-only Transformer con causal masking</p>\n<p><strong>Processo</strong>:\n1. Input: sequenza di token con causal mask\n2. Ogni posizione predice il token successivo\n3. Training: teacher forcing con mascheramento\n4. Inference: generazione autoregressiva</p>\n<p><strong>Vantaggi</strong>:\n- Consistenza tra training e inference\n- Scalabilità a sequenze lunghe\n- Generazione fluida e coerente</p>\n<h3 id=\"2-time-series-forecasting\">2. Time Series Forecasting</h3>\n<p><strong>Setup</strong>: Prevedere valori futuri basandosi solo sui valori passati</p>\n<p><strong>Formulazione</strong>:\n- Input: $[x_1, x_2, \\ldots, x_T]$\n- Target: $[x_2, x_3, \\ldots, x_{T+1}]$\n- Maschera: Causale per preservare l&rsquo;ordine temporale</p>\n<p><strong>Benefici</strong>:\n- Rispetta la natura temporale dei dati\n- Evita data leakage temporale\n- Permette previsioni multi-step</p>\n<h3 id=\"3-sequence-to-sequence-decoder\">3. Sequence-to-Sequence (Decoder)</h3>\n<p><strong>Architettura</strong>: Transformer Decoder in modelli seq2seq</p>\n<p><strong>Componenti</strong>:\n1. <strong>Masked Self-Attention</strong>: Sul target sequence\n2. <strong>Cross-Attention</strong>: Con encoder output\n3. <strong>Feed-Forward</strong>: Elaborazione finale</p>\n<p><strong>Processo di Training</strong>:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>Target: &lt;BOS&gt; Il gatto dorme &lt;EOS&gt;\nShift:       Il gatto dorme &lt;EOS&gt; &lt;PAD&gt;\nMask:   Causal + Padding mask\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"4-conversational-ai\">4. Conversational AI</h3>\n<p><strong>Contesto</strong>: Modelli di chat che devono generare risposte coerenti</p>\n<p><strong>Utilizzo</strong>:\n- History masking: Accesso solo al passato della conversazione\n- Turn-based masking: Separazione tra turni di conversazione\n- Context window: Limitazione della memoria conversazionale</p>\n<h2 id=\"varianti-avanzate-di-mascheramento\">Varianti Avanzate di Mascheramento</h2>\n<h3 id=\"1-sparse-attention-patterns\">1. Sparse Attention Patterns</h3>\n<p><strong>Local Attention</strong>: Ogni token vede solo una finestra locale</p>\n$$\\text{mask}_{i,j} = \\begin{cases}\n0 & \\text{se } |i-j| \\leq w \\text{ e } j \\leq i \\\\\n-\\infty & \\text{altrimenti}\n\\end{cases}$$\n<p>dove $w$ è la dimensione della finestra.</p>\n<p><strong>Strided Attention</strong>: Pattern con step fissi</p>\n$$\\text{mask}_{i,j} = \\begin{cases}\n0 & \\text{se } j \\leq i \\text{ e } j \\equiv i \\pmod{s} \\\\\n-\\infty & \\text{altrimenti}\n\\end{cases}$$\n<p>dove $s$ è lo stride.</p>\n<h3 id=\"2-hierarchical-masking\">2. Hierarchical Masking</h3>\n<p><strong>Block-wise Masking</strong>: Mascheramento a livello di blocchi</p>\n<p><strong>Multi-scale Masking</strong>: Diversi pattern per diverse scale temporali</p>\n<p><strong>Adaptive Masking</strong>: Pattern che si adattano al contenuto</p>\n<h3 id=\"3-learned-masking\">3. Learned Masking</h3>\n<p><strong>Trainable Masks</strong>: Pattern di mascheramento appresi durante il training</p>\n<p><strong>Content-Dependent</strong>: Maschere che dipendono dall&rsquo;input specifico</p>\n<p><strong>Task-Adaptive</strong>: Maschere specializzate per task diversi</p>\n<h2 id=\"analisi-della-complessita\">Analisi della Complessità</h2>\n<h3 id=\"complessita-computazionale\">Complessità Computazionale</h3>\n<p>La masked self-attention mantiene la stessa complessità asintotica della self-attention standard:</p>\n<p><strong>Tempo</strong>: $O(N^2 d)$ dove $N$ è la lunghezza della sequenza\n<strong>Spazio</strong>: $O(N^2)$ per memorizzare la matrice di attention</p>\n<h3 id=\"ottimizzazioni-specifiche\">Ottimizzazioni Specifiche</h3>\n<p><strong>Triangular Matrix Operations</strong>: Sfruttare la struttura per ridurre calcoli</p>\n<p><strong>Incremental Computation</strong>: Durante l&rsquo;inference, riutilizzare calcoli precedenti</p>\n<p><strong>Memory Efficient</strong>: Tecniche per ridurre l&rsquo;utilizzo della memoria</p>\n<h3 id=\"confronto-di-efficienza\">Confronto di Efficienza</h3>\n<table>\n<thead>\n<tr>\n<th>Operazione</th>\n<th>Standard</th>\n<th>Masked</th>\n<th>Ottimizzata</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Matrix Multiply</strong></td>\n<td>$O(N^2 d)$</td>\n<td>$O(N^2 d)$</td>\n<td>$O(N^2 d/2)$</td>\n</tr>\n<tr>\n<td><strong>Softmax</strong></td>\n<td>$O(N^2)$</td>\n<td>$O(N^2)$</td>\n<td>$O(N(N+1)/2)$</td>\n</tr>\n<tr>\n<td><strong>Memory</strong></td>\n<td>$N^2$</td>\n<td>$N^2$</td>\n<td>$N(N+1)/2$</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"problemi-e-limitazioni\">Problemi e Limitazioni</h2>\n<h3 id=\"1-riduzione-del-contesto\">1. Riduzione del Contesto</h3>\n<p><strong>Problema</strong>: Le posizioni iniziali hanno accesso a meno informazioni</p>\n<p><strong>Impatto</strong>: \n- Il token 1 vede solo se stesso\n- Il token 2 vede solo i token 1-2<br />\n- Il token $N$ vede tutti i token 1-N</p>\n<p><strong>Soluzioni</strong>:\n- Positional encoding più informativi\n- Pre-training su sequenze più lunghe\n- Tecniche di warm-up durante il training</p>\n<h3 id=\"2-training-inefficiency\">2. Training Inefficiency</h3>\n<p><strong>Problema</strong>: Meno parallelizzazione durante il training</p>\n<p><strong>Causa</strong>: Ogni posizione ha un contesto diverso</p>\n<p><strong>Mitigazioni</strong>:\n- Teacher forcing per accelerare il training\n- Curriculum learning con sequenze progressive\n- Tecniche di data augmentation</p>\n<h3 id=\"3-long-range-dependencies\">3. Long-Range Dependencies</h3>\n<p><strong>Problema</strong>: Difficoltà nel catturare dipendenze molto distanti</p>\n<p><strong>Esempio</strong>: In &ldquo;<em>All&rsquo;inizio del libro&hellip; [1000 parole] &hellip; e così la storia finisce</em>&rdquo;, la connessione &ldquo;inizio-fine&rdquo; è difficile da catturare.</p>\n<p><strong>Approcci</strong>:\n- Attention patterns gerarchici\n- Memory mechanisms esterni\n- Tecniche di compression del contesto</p>\n<h2 id=\"direzioni-di-ricerca-future\">Direzioni di Ricerca Future</h2>\n<h3 id=\"1-adaptive-masking\">1. Adaptive Masking</h3>\n<p><strong>Dynamic Masks</strong>: Maschere che si adattano dinamicamente al contenuto</p>\n<p><strong>Content-Aware Patterns</strong>: Pattern di attention basati sulla semantica</p>\n<p><strong>Task-Specific Optimization</strong>: Ottimizzazione delle maschere per task specifici</p>\n<h3 id=\"2-efficient-long-context\">2. Efficient Long-Context</h3>\n<p><strong>Hierarchical Attention</strong>: Attention a più livelli di granularità</p>\n<p><strong>Compress and Attend</strong>: Compressione del contesto prima dell&rsquo;attention</p>\n<p><strong>Memory-Augmented</strong>: Integrazione con memoria esterna</p>\n<h3 id=\"3-learnable-attention-patterns\">3. Learnable Attention Patterns</h3>\n<p><strong>Neural Architecture Search</strong>: Ricerca automatica di pattern ottimali</p>\n<p><strong>Meta-Learning</strong>: Apprendimento veloce di nuovi pattern</p>\n<p><strong>Multi-Task Patterns</strong>: Pattern condivisi tra task correlati</p>\n<h3 id=\"4-cross-modal-masking\">4. Cross-Modal Masking</h3>\n<p><strong>Vision-Language</strong>: Mascheramento coordinato tra modalità</p>\n<p><strong>Audio-Text</strong>: Allineamento temporale con mascheramento</p>\n<p><strong>Multimodal Generation</strong>: Generazione coordinata multi-modale</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>La <strong>Masked Self-Attention</strong> rappresenta un&rsquo;estensione fondamentale del meccanismo di attention che introduce il controllo esplicito del flusso informativo. La sua capacità di rispettare vincoli causali e temporali l&rsquo;ha resa indispensabile in una vasta gamma di applicazioni, dai language models ai sistemi di forecasting.</p>\n<h3 id=\"contributi-chiave\">Contributi Chiave</h3>\n<ol>\n<li><strong>Controllo Causale</strong>: Rispetto dei vincoli temporali e logici</li>\n<li><strong>Consistency</strong>: Allineamento tra training e inference  </li>\n<li><strong>Flessibilità</strong>: Supporto per diversi pattern di mascheramento</li>\n<li><strong>Interpretabilità</strong>: Controllo esplicito su cosa il modello può vedere</li>\n</ol>\n<h3 id=\"impatto-trasformativo\">Impatto Trasformativo</h3>\n<p>La masked self-attention ha:\n- Abilitato lo sviluppo di language models autoregressivi efficaci\n- Risolto problemi di data leakage in molti domini temporali\n- Fornito le basi per architetture decoder-only scalabili\n- Ispirato nuove tecniche di controllo dell&rsquo;information flow</p>\n<h3 id=\"sfide-aperte\">Sfide Aperte</h3>\n<p>Nonostante i successi, rimangono sfide significative:\n- Bilanciamento tra controllo causale e capacità rappresentazionale\n- Efficienza per sequenze molto lunghe\n- Gestione ottimale di pattern di attention complessi\n- Interpretabilità avanzata dei pattern appresi</p>\n<p>La comprensione approfondita della masked self-attention è cruciale per sviluppare modelli che rispettino vincoli causali pur mantenendo alta capacità predittiva, rappresentando una competenza essenziale nell&rsquo;era dei large language models e dei sistemi di AI generativa.</p>"
}