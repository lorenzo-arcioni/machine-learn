{
  "title": "Script Video: Implementazione di un Perceptron con PyTorch",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione\"><strong>Introduzione</strong></h2>\n<p><em>&ldquo;Allora, oggi voglio mostrarvi come implementare un perceptron da zero con PyTorch, e&hellip; diciamo che l&rsquo;obiettivo è veramente capire cosa succede sotto il cofano quando parliamo di reti neurali. Perché vedete, spesso si parla di deep learning, di neural network, però&hellip; beh, alla base di tutto c&rsquo;è questo concetto fondamentale che è il perceptron. Quindi partiamo dalle basi, implementiamo tutto passo passo, e&hellip; sì, alla fine dovreste avere una comprensione molto più solida di come funzionano questi sistemi.&rdquo;</em></p>\n<h2 id=\"cose-veramente-un-perceptron\"><strong>Cos&rsquo;è Veramente un Perceptron</strong></h2>\n<p><em>&ldquo;Allora, il perceptron&hellip; diciamo che è il modello più elementare di neurone artificiale, però non è che sia così banale come sembra.Fondamentalmente quello che fa è prendere degli input numerici, li moltiplica per dei pesi - che sono i parametri che il modello deve imparare - li somma, aggiunge un bias, e poi&hellip; ecco, passa tutto attraverso una funzione di attivazione. Nel nostro caso useremo la sigmoid.&rdquo;</em></p>\n<p><strong>Formula matematica:</strong></p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>y = σ(Wx + b)\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Okay, scomponiamo questa formula pezzo per pezzo, perché&hellip; diciamo che è importante capire cosa rappresenta ogni simbolo e soprattutto le dimensioni. Allora, y è il nostro output finale - quello che il perceptron ci restituisce, e nel nostro caso ha dimensione (200, 1) perché abbiamo 200 esempi e 1 output per esempio. Poi abbiamo σ, che è la sigma greca, e rappresenta la funzione di attivazione - nel nostro caso la sigmoid. Wx è&hellip; beh, il prodotto matriciale tra i pesi W e gli input x. E qui è dove le dimensioni diventano importanti: x ha shape (200, 2) perché abbiamo 200 punti bidimensionali, W ha shape (1, 2) perché dobbiamo andare da 2 input a 1 output, e&hellip; diciamo che il prodotto risulta in (200, 1). W sono i parametri che il modello deve imparare durante il training - praticamente i pesi delle connessioni sinaptiche, se vogliamo fare un&rsquo;analogia biologica. Infine b è il bias, che ha dimensione (1,) - praticamente un singolo scalare che viene aggiunto a ogni esempio. E&hellip; ecco, è un termine costante che permette di traslare la funzione. Senza il bias, la nostra retta passerebbe sempre per l&rsquo;origine, e&hellip; beh, questo limiterebbe molto la flessibilità del modello.&rdquo;</em></p>\n<p><em>&ldquo;Ora, geometricamente parlando&hellip; e questo è importante capirlo, eh&hellip; il perceptron cerca di trovare un iperpiano che separa le due classi. Se pensate a un problema bidimensionale, è come se stesse cercando una retta che divide i punti in due gruppi. Ecco, questo è quello che fa. Ovviamente funziona solo per problemi linearmente separabili, e&hellip; beh, questa era una delle limitazioni storiche che poi ha portato allo sviluppo delle reti multi-layer.&rdquo;</em></p>\n<p><strong>Diagramma dell&rsquo;architettura:</strong></p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code>     x₁ ────────┐\n                │  w₁\n                ├────► Σ ────► σ ────► y\n                │  w₂    +b\n     x₂ ────────┘\n\nInput Layer    Linear Transform    Activation    Output\n   (2,)            (1,2)×(2,)        σ()        (1,)\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Ecco, questo diagramma vi fa vedere esattamente la struttura del perceptron. Abbiamo due input - x₁ e x₂ - che&hellip; diciamo rappresentano le coordinate dei nostri punti. Ogni input ha il suo peso - w₁ e w₂ - e tutti convergono verso un singolo neurone che fa la somma pesata, aggiunge il bias b, e poi&hellip; applica la sigmoid. È proprio la rappresentazione visiva di quella formula che abbiamo visto prima.&rdquo;</em></p>\n<h2 id=\"setup-dellambiente\"><strong>Setup dell&rsquo;Ambiente</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Importiamo PyTorch e alcuni moduli:</span>\n<span class=\"c1\"># - torch: contiene le operazioni di base sui tensori</span>\n<span class=\"c1\"># - torch.nn: contiene i moduli per definire reti neurali</span>\n<span class=\"c1\"># - torch.optim: contiene gli algoritmi di ottimizzazione</span>\n<span class=\"c1\"># - matplotlib: ci serve per visualizzare i dati</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.nn</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">nn</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.optim</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">optim</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Okay, partiamo con gli import. Allora, torch ovviamente è il core di PyTorch, poi abbiamo nn che&hellip; beh, contiene tutti i building blocks per costruire le reti neurali, torch.optim per gli ottimizzatori - SGD, Adam e così via - e matplotlib per le visualizzazioni. Ah, una cosa importante: in PyTorch tutto ruota attorno ai tensori, che sono&hellip; diciamo come gli array di NumPy ma ottimizzati per GPU e per il calcolo automatico dei gradienti.&rdquo;</em></p>\n<h2 id=\"creazione-del-dataset\"><strong>Creazione del Dataset</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"s2\">&quot;In PyTorch tutto gira intorno ai tensori, che sono come array di NumPy ma ottimizzati per il calcolo su CPU e GPU.&quot;</span>\n<span class=\"c1\"># Creiamo un dataset fittizio</span>\n\n<span class=\"c1\"># Fissiamo un seme casuale (seed) per avere sempre gli stessi numeri casuali.</span>\n<span class=\"c1\"># In questo modo, se rieseguiamo il codice, otteniamo sempre lo stesso risultato.</span>\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Numero di punti per ciascuna classe</span>\n<span class=\"n\">n_points</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n\n<span class=\"c1\"># torch.randn genera numeri casuali da una distribuzione normale (media = 0, varianza = 1).</span>\n<span class=\"c1\"># Creiamo 100 punti 2D intorno a (2,2) per la classe 0.</span>\n<span class=\"n\">class0</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_points</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Creiamo altri 100 punti 2D intorno a (-2,-2) per la classe 1.</span>\n<span class=\"n\">class1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_points</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Uniamo i due gruppi di punti in un unico tensore X di input.</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cat</span><span class=\"p\">([</span><span class=\"n\">class0</span><span class=\"p\">,</span> <span class=\"n\">class1</span><span class=\"p\">],</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Creiamo i target (etichette): 0 per la prima classe, 1 per la seconda.</span>\n<span class=\"c1\"># unsqueeze(1) serve per trasformare il vettore in una colonna (necessario per PyTorch).</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cat</span><span class=\"p\">([</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_points</span><span class=\"p\">),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">n_points</span><span class=\"p\">)])</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Visualizziamo i punti per capire come sono distribuiti.</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">class0</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">class0</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s2\">&quot;blue&quot;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">&quot;Classe 0&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">class1</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">class1</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s2\">&quot;red&quot;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">&quot;Classe 1&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Bene, ora creiamo i dati per l&rsquo;esempio. Prima cosa: fisso il seed, perché&hellip; sennò ogni volta che eseguo il codice ottengo risultati diversi, e questo non va bene per la riproducibilità. Poi genero due cluster di punti: uno centrato su (2,2) per la classe 0, l&rsquo;altro su (-2,-2) per la classe 1. Uso torch.randn che&hellip; appunto, genera numeri da una distribuzione normale, poi aggiungo un offset per spostare i cluster. È importante notare che torch.cat mi permette di concatenare i tensori, e&hellip; ecco, unsqueeze(1) serve perché PyTorch si aspetta che i target abbiano una certa forma. La visualizzazione ci fa vedere subito che i dati sono linearmente separabili.&rdquo;</em></p>\n<h2 id=\"implementazione-del-modello\"><strong>Implementazione del Modello</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Definiamo il nostro perceptron come sottoclasse di nn.Module.</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Perceptron</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>  \n        <span class=\"c1\"># Definiamo uno strato lineare con 2 input (x e y) e 1 output.</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"c1\"># L&#39;output lineare viene trasformato con una funzione sigmoid,</span>\n        <span class=\"c1\"># che restituisce valori tra 0 e 1 (cioè probabilità).</span>\n        <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Creiamo un&#39;istanza del modello</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Perceptron</span><span class=\"p\">()</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Ecco, qui definiamo il nostro perceptron. Allora, in PyTorch ogni modello è una classe che eredita da nn.Module, e&hellip; questo è fondamentale perché ci dà accesso a tutto il sistema di automatic differentiation. Il layer lineare nn.Linear(2,1) implementa quella trasformazione Wx + b che abbiamo visto prima. I pesi vengono inizializzati automaticamente&hellip; PyTorch usa l&rsquo;inizializzazione di Kaiming di default, che è&hellip; diciamo una buona scelta per la maggior parte dei casi. Poi nella forward, applico prima la trasformazione lineare e poi la sigmoid, che mi mappa tutto tra 0 e 1, quindi posso interpretarlo come una probabilità.&rdquo;</em></p>\n<h2 id=\"loss-function-e-ottimizzatore\"><strong>Loss Function e Ottimizzatore</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Definiamo la funzione di costo (loss): Binary Cross Entropy,</span>\n<span class=\"c1\"># che misura l&#39;errore tra probabilità predetta e valore vero (0 o 1).</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">BCELoss</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Definiamo l&#39;ottimizzatore: Stochastic Gradient Descent (SGD).</span>\n<span class=\"c1\"># optimizer si occupa di aggiornare i pesi del modello durante il training.</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Per la loss function uso la Binary Cross Entropy, che&hellip; è praticamente lo standard per classificazione binaria. Perché? Beh, matematicamente deriva dal principio di maximum likelihood, e&hellip; ha questa bella proprietà che quando le predizioni sono sbagliate la loss cresce molto rapidamente, quindi spinge il modello a correggere gli errori. Per l&rsquo;optimizer, SGD con learning rate 0.1&hellip; è un valore che ho scelto empiricamente, diciamo che per questo tipo di problema funziona bene. Il learning rate controlla quanto aggressive sono gli aggiornamenti dei pesi, e&hellip; ecco, trovare il valore giusto è spesso una questione di trial and error.&rdquo;</em></p>\n<h2 id=\"training-loop\"><strong>Training Loop</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Numero di epoche di addestramento</span>\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>\n<span class=\"n\">losses</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"c1\"># Ciclo di training</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Forward pass: calcoliamo le predizioni del modello</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Calcoliamo la loss confrontando predizioni e target</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>  <span class=\"c1\"># .item() converte il tensore in numero Python</span>\n\n    <span class=\"c1\"># Backward pass:</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>  <span class=\"c1\"># azzera i gradienti accumulati</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>        <span class=\"c1\"># calcola i gradienti tramite backpropagation</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>       <span class=\"c1\"># aggiorna i pesi del modello</span>\n\n    <span class=\"c1\"># Ogni 10 epoche stampiamo la loss</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"mi\">10</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">epochs</span><span class=\"si\">}</span><span class=\"s2\">, Loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s2\">.4f</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Grafico dell&#39;andamento della loss</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">losses</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;Epoche&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;Loss&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Bene, qui abbiamo il cuore del training. Allora&hellip; ogni epoca ha tre fasi principali. Prima il forward pass: passo i dati al modello e ottengo le predizioni. Poi calcolo la loss confrontando quello che ha predetto il modello con i target veri. E qui&hellip; attenzione, una cosa importante: zero_grad() prima del backward pass. Perché PyTorch accumula i gradienti, e se non li azzero a ogni iterazione&hellip; beh, ottengo risultati completamente sbagliati. loss.backward() è dove avviene la magia della backpropagation - PyTorch calcola automaticamente tutti i gradienti attraverso la chain rule. E infine optimizer.step() applica gli aggiornamenti ai parametri. Vedrete che la loss dovrebbe diminuire progressivamente&hellip; se tutto va bene, naturalmente.&rdquo;</em></p>\n<h2 id=\"valutazione-e-analisi\"><strong>Valutazione e Analisi</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Facciamo le predizioni finali senza calcolare i gradienti</span>\n<span class=\"c1\"># torch.no_grad() disattiva il tracciamento dei gradienti</span>\n<span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n    <span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">round</span><span class=\"p\">()</span>  <span class=\"c1\"># round arrotonda le probabilità a 0 o 1</span>\n\n<span class=\"c1\"># Calcoliamo l&#39;accuracy: numero di predizioni corrette / totale</span>\n<span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"o\">.</span><span class=\"n\">eq</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Accuracy: </span><span class=\"si\">{</span><span class=\"n\">accuracy</span><span class=\"o\">*</span><span class=\"mi\">100</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">%&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Per la valutazione&hellip; ecco, uso torch.no_grad() che disabilita il tracking dei gradienti. Questo è importante perché durante l&rsquo;inference non mi servono i gradienti, e&hellip; beh, risparmio memoria e velocità. Poi arrotondo le probabilità a 0 o 1 con una soglia a 0.5 - che è la scelta standard - e calcolo l&rsquo;accuracy. Ovviamente l&rsquo;accuracy è una metrica semplificata, in scenari reali dovreste guardare anche precision, recall, F1-score&hellip; però per questo esempio va bene così.&rdquo;</em></p>\n<h2 id=\"visualizzazione-della-decision-boundary\"><strong>Visualizzazione della Decision Boundary</strong></h2>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n\n<span class=\"c1\"># Definiamo una griglia di punti 2D che copra l&#39;intero spazio dei dati</span>\n<span class=\"n\">x_min</span><span class=\"p\">,</span> <span class=\"n\">x_max</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"p\">()</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">()</span><span class=\"o\">+</span><span class=\"mi\">1</span>\n<span class=\"n\">y_min</span><span class=\"p\">,</span> <span class=\"n\">y_max</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"p\">()</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">()</span><span class=\"o\">+</span><span class=\"mi\">1</span>\n<span class=\"n\">xx</span><span class=\"p\">,</span> <span class=\"n\">yy</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">x_min</span><span class=\"p\">,</span> <span class=\"n\">x_max</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span>\n                     <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">y_min</span><span class=\"p\">,</span> <span class=\"n\">y_max</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Trasformiamo la griglia in tensore PyTorch</span>\n<span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">c_</span><span class=\"p\">[</span><span class=\"n\">xx</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">(),</span> <span class=\"n\">yy</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">()],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Facciamo predizioni sulla griglia</span>\n<span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">xx</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Disegniamo la decision boundary</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">contourf</span><span class=\"p\">(</span><span class=\"n\">xx</span><span class=\"p\">,</span> <span class=\"n\">yy</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mf\">0.5</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s2\">&quot;RdBu&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">class0</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">class0</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s2\">&quot;blue&quot;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">&quot;Classe 0&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">class1</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">class1</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s2\">&quot;red&quot;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">&quot;Classe 1&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><em>&ldquo;Questa parte è&hellip; secondo me una delle più interessanti, perché ci fa vedere geometricamente cosa ha imparato il modello. Quello che faccio è creare una griglia fitta di punti che copre tutto lo spazio dei dati, poi&hellip; eh, faccio predizioni su ogni punto di questa griglia. Il risultato è questa visualizzazione dove si vede chiaramente la decision boundary - cioè la linea che separa le due classi. E vedrete che è una linea retta, perché&hellip; appunto, il perceptron implementa un classificatore lineare. Se avessimo dati non linearmente separabili, beh&hellip; dovremmo usare architetture più complesse.&rdquo;</em></p>\n<h2 id=\"conclusioni-e-prospettive\"><strong>Conclusioni e Prospettive</strong></h2>\n<p><em>&ldquo;Ecco, questo è il nostro perceptron implementato da zero. Abbiamo visto&hellip; diciamo tutti i pezzi fondamentali: la teoria matematica, l&rsquo;implementazione pratica, il training loop con backpropagation, e anche come visualizzare quello che ha imparato il modello. Ora, ovviamente questo è solo l&rsquo;inizio, perché&hellip; beh, i modelli moderni sono molto più complessi. Però questi concetti - il forward pass, la loss function, l&rsquo;ottimizzazione basata su gradiente - sono sempre gli stessi, anche nelle architetture più avanzate. Quindi&hellip; sì, padroneggiare il perceptron è fondamentale per capire tutto il resto del deep learning.&rdquo;</em></p>"
}