{
  "title": "Layer Normalization",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione\">Introduzione</h2>\n<p>La <strong>Layer Normalization</strong> è una tecnica di normalizzazione introdotta da Jimmy Ba, Jamie Ryan Kiros, e Geoffrey Hinton nel 2016 come alternativa alla Batch Normalization. A differenza della Batch Normalization che normalizza attraverso il batch dimension, la Layer Normalization normalizza attraverso le features di ogni singolo esempio, rendendola indipendente dalla dimensione del batch e particolarmente efficace per architetture sequenziali come <span class=\"text-gray-600\">RNN</span> e <span class=\"text-gray-600\">Transformers</span>.</p>\n<h2 id=\"motivazione-e-differenze-rispetto-alla-batch-normalization\">Motivazione e Differenze rispetto alla Batch Normalization</h2>\n<h3 id=\"limitazioni-della-batch-normalization\">Limitazioni della Batch Normalization</h3>\n<p>La Batch Normalization presenta diverse limitazioni che la Layer Normalization affronta:</p>\n<ol>\n<li><strong>Dipendenza dalla dimensione del batch</strong>: Le statistiche diventano rumorose con batch piccoli</li>\n<li><strong>Difficoltà con sequenze di lunghezza variabile</strong>: Problematica per <span class=\"text-gray-600\">RNN</span> e applicazioni sequenziali</li>\n<li><strong>Discrepanza train-test</strong>: Comportamento diverso tra training (statistiche del batch) e inferenza (statistiche di popolazione)</li>\n<li><strong>Problemi con batch distribuito</strong>: Sincronizzazione delle statistiche tra dispositivi</li>\n</ol>\n<h3 id=\"vantaggi-della-layer-normalization\">Vantaggi della Layer Normalization</h3>\n<p>La Layer Normalization risolve questi problemi:</p>\n<ol>\n<li><strong>Indipendenza dal batch size</strong>: Ogni esempio viene normalizzato individualmente</li>\n<li><strong>Consistenza train-test</strong>: Stesso comportamento in training e inferenza</li>\n<li><strong>Efficacia con sequenze</strong>: Funziona naturalmente con <span class=\"text-gray-600\">RNN</span> e architetture sequenziali</li>\n<li><strong>Semplicità computazionale</strong>: Non richiede sincronizzazione tra esempi</li>\n</ol>\n<h2 id=\"formulazione-matematica\">Formulazione Matematica</h2>\n<h3 id=\"definizione-base\">Definizione Base</h3>\n<p>Per un input $\\mathbf{x} \\in \\mathbb{R}^H$ dove $H$ è il numero di features, la Layer Normalization calcola:</p>\n<h4 id=\"1-media-per-ogni-esempio\">1. Media per ogni esempio</h4>\n$$\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i$$\n<h4 id=\"2-varianza-per-ogni-esempio\">2. Varianza per ogni esempio</h4>\n$$\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2$$\n<h4 id=\"3-normalizzazione\">3. Normalizzazione</h4>\n$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n<h4 id=\"4-scaling-e-shifting\">4. Scaling e Shifting</h4>\n$$y_i = \\gamma \\hat{x}_i + \\beta$$\n<p>dove $\\gamma$ e $\\beta$ sono parametri appresi di dimensione $H$.</p>\n<h3 id=\"notazione-vettoriale\">Notazione Vettoriale</h3>\n<p>Per un singolo esempio $\\mathbf{x} \\in \\mathbb{R}^H$:</p>\n$$\n\\mu = \\frac{1}{H} \\mathbf{1}^\\top \\mathbf{x} = \\frac{1}{H} \\sum_{i=1}^H x_i\n$$\n$$\n\\sigma^2 = \\frac{1}{H} \\|\\mathbf{x} - \\mu \\mathbf{1}\\|_2^2 = \\frac{1}{H} \\sum_{i=1}^H (x_i - \\mu)^2\n$$\n$$\n\\hat{\\mathbf{x}} = \\frac{\\mathbf{x} - \\mu \\mathbf{1}}{\\sqrt{\\sigma^2 + \\epsilon}}\n$$\n$$\n\\mathbf{y} = \\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{x}} + \\boldsymbol{\\beta}\n$$\n<h3 id=\"estensione-ai-batch\">Estensione ai Batch</h3>\n<p>Per un batch di esempi $X \\in \\mathbb{R}^{N \\times H}$ dove $N$ è la dimensione del batch:</p>\n$$\n\\mu_i = \\frac{1}{H} \\sum_{j=1}^{H} X_{i,j} \\quad \\forall i = 1, \\ldots, N\n$$\n$$\n\\sigma_i^2 = \\frac{1}{H} \\sum_{j=1}^{H} (X_{i,j} - \\mu_i)^2 \\quad \\forall i = 1, \\ldots, N\n$$\n$$\n\\hat{X}_{i,j} = \\frac{X_{i,j} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}\n$$\n$$\nY_{i,j} = \\gamma_j \\hat{X}_{i,j} + \\beta_j\n$$\n<p>Ogni esempio nel batch viene normalizzato <strong>indipendentemente</strong> utilizzando le proprie statistiche.</p>\n<h2 id=\"confronto-visuale-batch-vs-layer-normalization\">Confronto Visuale: Batch vs Layer Normalization</h2>\n<p><img src=\"/images/tikz/1b82ff847b4ac0d1c2a05d0da3c6e80c.svg\" style=\"display: block; width: 100%; height: auto; max-height: 600px;\" class=\"tikz-svg\" /></p>\n<h2 id=\"proprieta-matematiche\">Proprietà Matematiche</h2>\n<h3 id=\"invarianza-per-trasformazioni-affini\">Invarianza per Trasformazioni Affini</h3>\n<p>Come la Batch Normalization, anche la Layer Normalization gode di proprietà di invarianza. Per una trasformazione affine scalare:</p>\n$$x'_i = ax_i + b \\quad \\forall i$$\n<h4 id=\"dimostrazione\">Dimostrazione</h4>\n<p><strong>Passo 1: Media trasformata</strong>\n$$\\mu' = \\frac{1}{H}\\sum_{i=1}^{H} (ax_i + b) = a\\mu + b$$</p>\n<p><strong>Passo 2: Varianza trasformata</strong>\n$$\\sigma'^2 = \\frac{1}{H}\\sum_{i=1}^{H} (ax_i + b - a\\mu - b)^2 = a^2\\sigma^2$$</p>\n<p><strong>Passo 3: Normalizzazione</strong>\n$$\\hat{x}'_i = \\frac{ax_i + b - (a\\mu + b)}{\\sqrt{a^2\\sigma^2 + \\epsilon}} = \\frac{a(x_i - \\mu)}{|a|\\sqrt{\\sigma^2 + \\epsilon/a^2}}$$</p>\n<p><strong>Risultato:</strong>\n- Se $a > 0$: $\\hat{x}'_i = \\hat{x}_i$ (per $|a| \\gg \\sqrt{\\epsilon}$)\n- Se $a < 0$: $\\hat{x}'_i = -\\hat{x}_i$</p>\n$$\\boxed{\\text{LN}(ax + b) = \\text{sign}(a) \\cdot \\text{LN}(x)}$$\n<h3 id=\"caso-vettoriale\">Caso Vettoriale</h3>\n<p>Per trasformazioni diagonali $\\mathbf{x}' = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$ con $\\mathbf{A} = \\text{diag}(a_1, \\ldots, a_H)$:</p>\n$$\\boxed{\\text{LN}(\\mathbf{A}\\mathbf{x} + \\mathbf{b}) = \\text{sign}(\\mathbf{A}) \\odot \\text{LN}(\\mathbf{x})}$$\n<p>dove $\\text{sign}(\\mathbf{A}) = \\text{diag}(\\text{sign}(a_1), \\ldots, \\text{sign}(a_H))$.</p>\n<h2 id=\"analisi-dei-gradienti\">Analisi dei Gradienti</h2>\n<h3 id=\"derivata-rispetto-allinput\">Derivata rispetto all&rsquo;input</h3>\n<p>Per calcolare la derivata $\\frac{\\partial L}{\\partial x_i}$, seguiamo un approccio simile alla Batch Normalization ma con una differenza fondamentale: in Layer Normalization tutti gli $x_j$ dello stesso esempio contribuiscono alla normalizzazione di $x_i$.</p>\n<p>Partendo da:\n$$\\hat{x}_j = \\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$</p>\n<p>dove:\n$$\\mu = \\frac{1}{H}\\sum_{k=1}^H x_k, \\quad \\sigma^2 = \\frac{1}{H}\\sum_{k=1}^H (x_k - \\mu)^2$$</p>\n<p>Vogliamo calcolare:\n$$\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^H \\frac{\\partial L}{\\partial \\hat{x}_j} \\frac{\\partial \\hat{x}_j}{\\partial x_i}$$</p>\n<h4 id=\"calcolo-di-math_inline_59\">Calcolo di $\\frac{\\partial \\hat{x}_j}{\\partial x_i}$</h4>\n<p>Seguendo un procedimento analogo alla Batch Normalization, definiamo $s = \\sqrt{\\sigma^2 + \\epsilon}$:</p>\n$$\\frac{\\partial \\hat{x}_j}{\\partial x_i} = \\frac{1}{s}\\left(\\delta_{ij} - \\frac{1}{H} - \\frac{\\hat{x}_j\\hat{x}_i}{H}\\right)$$\n<h4 id=\"gradiente-finale\">Gradiente finale</h4>\n$$\\boxed{\\frac{\\partial L}{\\partial x_i} = \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\left[\\frac{\\partial L}{\\partial \\hat{x}_i} - \\frac{1}{H}\\sum_{j=1}^H\\frac{\\partial L}{\\partial \\hat{x}_j} - \\frac{\\hat{x}_i}{H}\\sum_{j=1}^H\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j\\right]}$$\n<h3 id=\"interpretazione-dei-termini\">Interpretazione dei termini</h3>\n<ol>\n<li><strong>Termine diretto</strong>: $\\frac{\\partial L}{\\partial \\hat{x}_i}$ - gradiente locale</li>\n<li><strong>Termine di ricentraggio</strong>: $-\\frac{1}{H}\\sum_{j=1}^H\\frac{\\partial L}{\\partial \\hat{x}_j}$ - mantiene media zero</li>\n<li><strong>Termine di decorrelazione</strong>: $-\\frac{\\hat{x}_i}{H}\\sum_{j=1}^H\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j$ - riduce correlazioni</li>\n</ol>\n<h3 id=\"derivate-dei-parametri\">Derivate dei parametri</h3>\n$$\\frac{\\partial L}{\\partial \\gamma_j} = \\sum_{\\text{batch}} \\frac{\\partial L}{\\partial y_j} \\hat{x}_j$$\n$$\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{\\text{batch}} \\frac{\\partial L}{\\partial y_j}$$\n<h2 id=\"implementazione-computazionale\">Implementazione Computazionale</h2>\n<h3 id=\"algoritmo-forward\">Algoritmo Forward</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">layer_norm_forward</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    x: (N, H) - batch di N esempi con H features</span>\n<span class=\"sd\">    gamma, beta: (H,) - parametri appresi</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"c1\"># Calcolo statistiche per ogni esempio</span>\n    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># (N, 1)</span>\n    <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">var</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>    <span class=\"c1\"># (N, 1)</span>\n\n    <span class=\"c1\"># Normalizzazione</span>\n    <span class=\"n\">x_norm</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span>  <span class=\"c1\"># (N, H)</span>\n\n    <span class=\"c1\"># Scaling e shifting</span>\n    <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">gamma</span> <span class=\"o\">*</span> <span class=\"n\">x_norm</span> <span class=\"o\">+</span> <span class=\"n\">beta</span>  <span class=\"c1\"># (N, H)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">out</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x_norm</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">var</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"algoritmo-backward\">Algoritmo Backward</h3>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">layer_norm_backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    dout: (N, H) - gradiente dall&#39;output</span>\n<span class=\"sd\">    cache: tuple con valori dal forward pass</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">x_norm</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">var</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">H</span> <span class=\"o\">=</span> <span class=\"n\">dout</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n\n    <span class=\"c1\"># Gradienti dei parametri</span>\n    <span class=\"n\">dgamma</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dout</span> <span class=\"o\">*</span> <span class=\"n\">x_norm</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># (H,)</span>\n    <span class=\"n\">dbeta</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>            <span class=\"c1\"># (H,)</span>\n\n    <span class=\"c1\"># Gradiente rispetto a x_norm</span>\n    <span class=\"n\">dx_norm</span> <span class=\"o\">=</span> <span class=\"n\">dout</span> <span class=\"o\">*</span> <span class=\"n\">gamma</span>  <span class=\"c1\"># (N, H)</span>\n\n    <span class=\"c1\"># Gradienti intermedi</span>\n    <span class=\"n\">dvar</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dx_norm</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"o\">-</span><span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">1.5</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">dmean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dx_norm</span> <span class=\"o\">*</span> <span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"o\">+</span> \\\n            <span class=\"n\">dvar</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">H</span>\n\n    <span class=\"c1\"># Gradiente finale rispetto all&#39;input</span>\n    <span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"n\">dx_norm</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span> <span class=\"o\">+</span> \\\n         <span class=\"n\">dvar</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">H</span> <span class=\"o\">+</span> \\\n         <span class=\"n\">dmean</span> <span class=\"o\">/</span> <span class=\"n\">H</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"n\">dgamma</span><span class=\"p\">,</span> <span class=\"n\">dbeta</span>\n</code></pre></div>\n</div>\n</details>\n\n<h2 id=\"layer-normalization-in-architetture-specifiche\">Layer Normalization in Architetture Specifiche</h2>\n<h3 id=\"reti-neurali-feedforward\">Reti Neurali Feedforward</h3>\n<p>In una rete feedforward standard:</p>\n$$\n\\begin{aligned}\n\\mathbf{z}^{(l)} &= W^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} \\\\\n\\mathbf{a}^{(l)} &= \\phi(\\text{LN}(\\mathbf{z}^{(l)}))\n\\end{aligned}\n$$\n<h3 id=\"reti-neurali-ricorrenti-recurrent-neural-networksrnn\">Reti Neurali Ricorrenti (<span class=\"text-gray-600\">RNN</span>)</h3>\n<p>Per una <span class=\"text-gray-600\">RNN</span> con Layer Normalization:</p>\n$$\n\\begin{aligned}\n\\mathbf{h}_t &= W_h \\mathbf{h}_{t-1} + W_x \\mathbf{x}_t + \\mathbf{b} \\\\\n\\tilde{\\mathbf{h}}_t &= \\text{LN}(\\mathbf{h}_t) \\\\\n\\mathbf{h}_t &= \\tanh(\\tilde{\\mathbf{h}}_t)\n\\end{aligned}\n$$\n<p>La Layer Normalization stabilizza il training delle <span class=\"text-gray-600\">RNN</span> riducendo il problema dei gradienti che esplodono o svaniscono.</p>\n<h3 id=\"transformer-architecture\">Transformer Architecture</h3>\n<p>Nel Transformer, la Layer Normalization viene tipicamente applicata in configurazione &ldquo;Pre-LN&rdquo;:</p>\n<p><img src=\"/images/tikz/81943d42b693210bfbd7f3f9ba2c935f.svg\" style=\"display: block; width: 100%; height: auto; max-height: 600px;\" class=\"tikz-svg\" /></p>\n<p>Matematicamente:</p>\n$$\n\\begin{aligned}\n\\mathbf{y}_1 &= \\mathbf{x} + \\text{MultiHeadAttention}(\\text{LN}(\\mathbf{x})) \\\\\n\\mathbf{y}_2 &= \\mathbf{y}_1 + \\text{FeedForward}(\\text{LN}(\\mathbf{y}_1))\n\\end{aligned}\n$$\n<h2 id=\"rmsnorm-una-semplificazione-della-layer-normalization\">RMSNorm: Una Semplificazione della Layer Normalization</h2>\n<h3 id=\"motivazione\">Motivazione</h3>\n<p>La <strong>RMSNorm</strong> (Root Mean Square Layer Normalization) è una variante semplificata proposta per ridurre i costi computazionali mantenendo i benefici della normalizzazione.</p>\n<h3 id=\"formulazione\">Formulazione</h3>\n<p>Invece di calcolare media e varianza, RMSNorm usa solo la Root Mean Square:</p>\n$$\\text{RMS}(\\mathbf{x}) = \\sqrt{\\frac{1}{H} \\sum_{i=1}^H x_i^2}$$\n<p>La normalizzazione diventa:</p>\n$$\\hat{x}_i = \\frac{x_i}{\\text{RMS}(\\mathbf{x})} \\cdot \\sqrt{H}$$\n<p>Con scaling:</p>\n$$y_i = \\gamma_i \\hat{x}_i$$\n<p>Notare che RMSNorm <strong>non ha il parametro di bias</strong> $\\beta$ e <strong>non sottrae la media</strong>.</p>\n<h3 id=\"vantaggi-di-rmsnorm\">Vantaggi di RMSNorm</h3>\n<ol>\n<li><strong>Computazionalmente più efficiente</strong>: Solo una statistica da calcolare</li>\n<li><strong>Stabilità numerica</strong>: Evita la sottrazione della media</li>\n<li><strong>Prestazioni competitive</strong>: Risultati simili alla Layer Normalization in molte applicazioni</li>\n</ol>\n<h3 id=\"confronto-matematico\">Confronto Matematico</h3>\n<table>\n<thead>\n<tr>\n<th>Aspetto</th>\n<th>Layer Normalization</th>\n<th>RMSNorm</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Statistica</strong></td>\n<td>$\\mu, \\sigma^2$</td>\n<td>Solo RMS</td>\n</tr>\n<tr>\n<td><strong>Normalizzazione</strong></td>\n<td>$\\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$</td>\n<td>$\\frac{x_i \\sqrt{H}}{\\sqrt{\\sum_j x_j^2 + \\epsilon}}$</td>\n</tr>\n<tr>\n<td><strong>Parametri</strong></td>\n<td>$\\gamma, \\beta$</td>\n<td>Solo $\\gamma$</td>\n</tr>\n<tr>\n<td><strong>Complessità</strong></td>\n<td>$O(2H)$</td>\n<td>$O(H)$</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"analisi-teorica-approfondita\">Analisi Teorica Approfondita</h2>\n<h3 id=\"stabilita-dei-gradienti\">Stabilità dei Gradienti</h3>\n<p>La Layer Normalization migliora la stabilità dei gradienti attraverso diversi meccanismi:</p>\n<h4 id=\"1-controllo-della-magnitudine\">1. Controllo della Magnitudine</h4>\n<p>Il gradiente rispetto all&rsquo;input ha magnitudine limitata:</p>\n$$\\left\\|\\frac{\\partial L}{\\partial \\mathbf{x}}\\right\\|_2 \\leq \\frac{\\|\\boldsymbol{\\gamma}\\|_2}{\\sqrt{\\sigma^2 + \\epsilon}} \\left\\|\\frac{\\partial L}{\\partial \\hat{\\mathbf{x}}}\\right\\|_2$$\n<h4 id=\"2-ricentraggio-automatico\">2. Ricentraggio Automatico</h4>\n<p>La componente di ricentraggio nel gradiente:</p>\n$$-\\frac{1}{H}\\sum_{j=1}^H\\frac{\\partial L}{\\partial \\hat{x}_j}$$\n<p>mantiene i gradienti centrati, riducendo il bias nella direzione di ottimizzazione.</p>\n<h4 id=\"3-decorrelazione\">3. Decorrelazione</h4>\n<p>Il termine di decorrelazione:</p>\n$$-\\frac{\\hat{x}_i}{H}\\sum_{j=1}^H\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j$$\n<p>riduce le correlazioni spurie tra gradienti di features diverse.</p>\n<h3 id=\"conditioning-del-problema-di-ottimizzazione\">Conditioning del Problema di Ottimizzazione</h3>\n<p>La Layer Normalization migliora il <strong>condition number</strong> della matrice Hessiana. Per una loss quadratica semplificata:</p>\n$$L = \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{t}\\|_2^2$$\n<p>dove $\\mathbf{y} = \\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{x}} + \\boldsymbol{\\beta}$, la Hessiana rispetto ai parametri $\\boldsymbol{\\gamma}$ è:</p>\n$$H_{\\boldsymbol{\\gamma}} = \\text{diag}(\\hat{\\mathbf{x}} \\odot \\hat{\\mathbf{x}})$$\n<p>Poiché $\\|\\hat{\\mathbf{x}}\\|_2^2 = H$ (per costruzione della normalizzazione), gli autovalori della Hessiana sono più uniformemente distribuiti, migliorando il conditioning.</p>\n<h2 id=\"effetti-di-regolarizzazione\">Effetti di Regolarizzazione</h2>\n<h3 id=\"regolarizzazione-implicita\">Regolarizzazione Implicita</h3>\n<p>La Layer Normalization introduce una regolarizzazione implicita attraverso:</p>\n<ol>\n<li><strong>Constraining della norma</strong>: Gli input normalizzati hanno norma fissata</li>\n<li><strong>Riduzione dell&rsquo;overfitting</strong>: Limita la dipendenza da valori specifici delle features</li>\n<li><strong>Smoothing del landscape</strong>: Rende la superficie di ottimizzazione più liscia</li>\n</ol>\n<h3 id=\"analisi-della-varianza\">Analisi della Varianza</h3>\n<p>Per un input con componenti i.i.d. $x_i \\sim \\mathcal{N}(0, \\sigma_x^2)$, dopo Layer Normalization:</p>\n$$\\mathbb{E}[\\hat{x}_i] = 0, \\quad \\text{Var}[\\hat{x}_i] = \\frac{H-1}{H} \\approx 1$$\n<p>Questo garantisce che le features normalizzate abbiano varianza unitaria, indipendentemente dalla distribuzione originale.</p>\n<h2 id=\"complessita-computazionale-e-ottimizzazioni\">Complessità Computazionale e Ottimizzazioni</h2>\n<h3 id=\"complessita-temporale\">Complessità Temporale</h3>\n<ul>\n<li><strong>Forward pass</strong>: $O(H)$ per ogni esempio</li>\n<li><strong>Backward pass</strong>: $O(H)$ per ogni esempio</li>\n<li><strong>Totale per batch</strong>: $O(NH)$ dove $N$ è la dimensione del batch</li>\n</ul>\n<h3 id=\"complessita-spaziale\">Complessità Spaziale</h3>\n<ul>\n<li><strong>Parametri</strong>: $O(H)$ per $\\boldsymbol{\\gamma}$ e $\\boldsymbol{\\beta}$</li>\n<li><strong>Cache per backward</strong>: $O(NH)$ per memorizzare input normalizzati e statistiche</li>\n</ul>\n<h3 id=\"ottimizzazioni-hardware\">Ottimizzazioni Hardware</h3>\n<h4 id=\"vectorizzazione\">Vectorizzazione</h4>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Operazione vettorizzata efficiente</span>\n<span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"p\">((</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">x_norm</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<h4 id=\"fusione-di-kernel\">Fusione di Kernel</h4>\n<p>Su GPU/TPU, le operazioni di Layer Normalization possono essere fuse per ridurre il memory bandwidth:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\">// Kernel CUDA fuso per Layer Normalization</span>\n<span class=\"kr\">__global__</span><span class=\"w\"> </span><span class=\"kt\">void</span><span class=\"w\"> </span><span class=\"n\">layernorm_kernel</span><span class=\"p\">(</span><span class=\"kt\">float</span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">input</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"kt\">float</span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">output</span><span class=\"p\">,</span><span class=\"w\"> </span>\n<span class=\"w\">                                </span><span class=\"kt\">float</span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">gamma</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"kt\">float</span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">beta</span><span class=\"p\">,</span><span class=\"w\"> </span>\n<span class=\"w\">                                </span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">eps</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nb\">blockIdx</span><span class=\"p\">.</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"nb\">blockDim</span><span class=\"p\">.</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"nb\">threadIdx</span><span class=\"p\">.</span><span class=\"n\">x</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"c1\">// Calcolo statistiche locali</span>\n<span class=\"w\">        </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">mean</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mf\">0.0f</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">var</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mf\">0.0f</span><span class=\"p\">;</span>\n<span class=\"w\">        </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"n\">mean</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">input</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">];</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"n\">mean</span><span class=\"w\"> </span><span class=\"o\">/=</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"p\">;</span>\n\n<span class=\"w\">        </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">diff</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">input</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">mean</span><span class=\"p\">;</span>\n<span class=\"w\">            </span><span class=\"n\">var</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">diff</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">diff</span><span class=\"p\">;</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"n\">var</span><span class=\"w\"> </span><span class=\"o\">/=</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"p\">;</span>\n\n<span class=\"w\">        </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">inv_std</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">rsqrtf</span><span class=\"p\">(</span><span class=\"n\">var</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">eps</span><span class=\"p\">);</span>\n\n<span class=\"w\">        </span><span class=\"c1\">// Normalizzazione e scaling</span>\n<span class=\"w\">        </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">norm</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">mean</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">inv_std</span><span class=\"p\">;</span>\n<span class=\"w\">            </span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">H</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">norm</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">gamma</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">beta</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">];</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n</div>\n</details>\n\n<h2 id=\"varianti-avanzate\">Varianti Avanzate</h2>\n<h3 id=\"adaptive-layer-normalization-adaln\">Adaptive Layer Normalization (AdaLN)</h3>\n<p>Utilizzata in applicazioni come la generazione condizionata:</p>\n$$y_i = \\gamma_{\\text{cond}} \\hat{x}_i + \\beta_{\\text{cond}}$$\n<p>dove $\\gamma_{\\text{cond}}$ e $\\beta_{\\text{cond}}$ dipendono da informazioni condizionali esterne.</p>\n<h3 id=\"weight-standardization-layer-normalization\">Weight Standardization + Layer Normalization</h3>\n<p>Combinazione che normalizza sia i pesi che le attivazioni:</p>\n$$\n\\begin{aligned}\n\\tilde{W}_{i,j} &= \\frac{W_{i,j} - \\mu_W}{\\sqrt{\\sigma_W^2 + \\epsilon}} \\\\\n\\mathbf{z} &= \\tilde{W} \\mathbf{x} + \\mathbf{b} \\\\\n\\mathbf{y} &= \\text{LN}(\\mathbf{z})\n\\end{aligned}\n$$\n<h3 id=\"learnable-layer-normalization\">Learnable Layer Normalization</h3>\n<p>Parametrizzazione più ricca dei parametri di scaling:</p>\n$$y_i = f_\\theta(\\hat{x}_i)$$\n<p>dove $f_\\theta$ è una piccola rete neurale invece di una semplice trasformazione affine.</p>\n<h2 id=\"analisi-empirica-e-proprieta-emergenti\">Analisi Empirica e Proprietà Emergenti</h2>\n<h3 id=\"convergenza-piu-veloce\">Convergenza più Veloce</h3>\n<p>Empiricamente, la Layer Normalization accelera la convergenza riducendo il numero di epoche necessarie. Questo è dovuto a:</p>\n<ol>\n<li><strong>Gradienti più stabili</strong>: Meno oscillazioni durante l&rsquo;ottimizzazione</li>\n<li><strong>Learning rate più alti</strong>: Possibilità di usare step size maggiori</li>\n<li><strong>Ridotta sensibilità all&rsquo;inizializzazione</strong>: Meno dipendenza dai valori iniziali dei parametri</li>\n</ol>\n<h3 id=\"generalizzazione\">Generalizzazione</h3>\n<p>Studi empirici mostrano che la Layer Normalization migliora la generalizzazione attraverso:</p>\n<ol>\n<li><strong>Riduzione del gap train-test</strong>: Comportamento identico in training e inferenza</li>\n<li><strong>Robustezza ai cambiamenti di distribuzione</strong>: Meno sensibile a shift negli input</li>\n<li><strong>Prevenzione dell&rsquo;overfitting</strong>: Regolarizzazione implicita delle rappresentazioni</li>\n</ol>\n<h2 id=\"confronti-sperimentali-e-applicazioni-specifiche\">Confronti Sperimentali e Applicazioni Specifiche</h2>\n<h3 id=\"performance-su-diverse-architetture\">Performance su Diverse Architetture</h3>\n<h4 id=\"transformers\">Transformers</h4>\n<p>La Layer Normalization è diventata standard nei Transformers moderni (GPT, BERT, T5) grazie a:</p>\n<ul>\n<li><strong>Stabilità con sequenze lunghe</strong>: Non dipende da statistiche del batch</li>\n<li><strong>Parallelizzazione efficiente</strong>: Ogni posizione può essere normalizzata indipendentemente</li>\n<li><strong>Miglior handling dell&rsquo;attenzione</strong>: Stabilizza i pattern di attenzione multi-head</li>\n</ul>\n<h4 id=\"reti-neurali-ricorrenti\">Reti Neurali Ricorrenti</h4>\n<p>Per <span class=\"text-gray-600\">RNN</span>/LSTM, la Layer Normalization offre vantaggi unici:</p>\n<p>$\n\\begin{aligned}\n\\mathbf{f}_t &= \\sigma(W_f \\cdot \\text{LN}([\\mathbf{h}_{t-1}, \\mathbf{x}_t]) + \\mathbf{b}_f) \\\\\n\\mathbf{i}_t &= \\sigma(W_i \\cdot \\text{LN}([\\mathbf{h}_{t-1}, \\mathbf{x}_t]) + \\mathbf{b}_i) \\\\\n\\tilde{\\mathbf{C}}_t &= \\tanh(W_C \\cdot \\text{LN}([\\mathbf{h}_{t-1}, \\mathbf{x}_t]) + \\mathbf{b}_C) \\\\\n\\mathbf{o}_t &= \\sigma(W_o \\cdot \\text{LN}([\\mathbf{h}_{t-1}, \\mathbf{x}_t]) + \\mathbf{b}_o)\n\\end{aligned}\n$</p>\n<h3 id=\"confronto-quantitativo-delle-tecniche-di-normalizzazione\">Confronto Quantitativo delle Tecniche di Normalizzazione</h3>\n<table>\n<thead>\n<tr>\n<th>Metrica</th>\n<th>Batch Norm</th>\n<th>Layer Norm</th>\n<th>Instance Norm</th>\n<th>Group Norm</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Dipendenza batch size</strong></td>\n<td>Alta</td>\n<td>Nulla</td>\n<td>Nulla</td>\n<td>Bassa</td>\n</tr>\n<tr>\n<td><strong>Train-test consistency</strong></td>\n<td>Bassa</td>\n<td>Alta</td>\n<td>Alta</td>\n<td>Alta</td>\n</tr>\n<tr>\n<td><strong>Costo computazionale</strong></td>\n<td>Medio</td>\n<td>Basso</td>\n<td>Basso</td>\n<td>Medio</td>\n</tr>\n<tr>\n<td><strong>Memory overhead</strong></td>\n<td>Alto</td>\n<td>Basso</td>\n<td>Basso</td>\n<td>Medio</td>\n</tr>\n<tr>\n<td><strong>Efficacia con CNN</strong></td>\n<td>Alta</td>\n<td>Bassa</td>\n<td>Media</td>\n<td>Alta</td>\n</tr>\n<tr>\n<td><strong>Efficacia con RNN</strong></td>\n<td>Bassa</td>\n<td>Alta</td>\n<td>Media</td>\n<td>Media</td>\n</tr>\n<tr>\n<td><strong>Efficacia con Transformer</strong></td>\n<td>Media</td>\n<td>Alta</td>\n<td>Bassa</td>\n<td>Media</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"aspetti-teorici-avanzati-specifici-della-layer-normalization\">Aspetti Teorici Avanzati Specifici della Layer Normalization</h2>\n<h3 id=\"dinamiche-di-training-uniche\">Dinamiche di Training Uniche</h3>\n<p>A differenza della Batch Normalization, la Layer Normalization introduce dinamiche di training specifiche:</p>\n<h4 id=\"auto-stabilizzazione\">Auto-Stabilizzazione</h4>\n<p>Ogni esempio si auto-stabilizza durante il training:</p>\n<p>$\\frac{d}{dt}\\|\\mathbf{x}(t)\\|_2^2 = 2\\mathbf{x}(t)^\\top\\frac{d\\mathbf{x}(t)}{dt}$</p>\n<p>Dopo Layer Normalization, questa dinamica viene controllata dai parametri $\\boldsymbol{\\gamma}$:</p>\n<p>$\\frac{d}{dt}\\|\\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{x}}(t)\\|_2^2 = 2(\\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{x}}(t))^\\top \\boldsymbol{\\gamma} \\odot \\frac{d\\hat{\\mathbf{x}}(t)}{dt}$</p>\n<h4 id=\"convergenza-locale\">Convergenza Locale</h4>\n<p>La Layer Normalization garantisce convergenza locale più robusta perché le statistiche sono determinate singolarmente per ogni esempio, eliminando l&rsquo;interdipendenza tra esempi nel batch.</p>\n<h3 id=\"analisi-spettrale-della-hessiana\">Analisi Spettrale della Hessiana</h3>\n<p>Per la Layer Normalization, la matrice Hessiana presenta proprietà spettrali interessanti:</p>\n<p>$H_{LN} = \\frac{\\partial^2 L}{\\partial \\mathbf{x}^2} \\bigg|_{\\text{after LN}}$</p>\n<p>Gli autovalori tendono ad essere più uniformemente distribuiti rispetto al caso non normalizzato, con:</p>\n<p>$\\lambda_{\\max}(H_{LN}) / \\lambda_{\\min}(H_{LN}) \\ll \\lambda_{\\max}(H) / \\lambda_{\\min}(H)$</p>\n<p>Questo spiega matematicamente perché la Layer Normalization permette learning rate più alti.</p>\n<h2 id=\"limitazioni-specifiche-della-layer-normalization\">Limitazioni Specifiche della Layer Normalization</h2>\n<h3 id=\"problemi-con-features-eterogenee\">Problemi con Features Eterogenee</h3>\n<p>Quando le features hanno significati semantici molto diversi, la normalizzazione attraverso tutte le features può essere problematica:</p>\n<p><strong>Esempio</strong>: In un embedding che concatena features di testo e immagini:\n$\\mathbf{x} = [\\mathbf{x}_{\\text{text}}, \\mathbf{x}_{\\text{image}}] \\in \\mathbb{R}^{H_{\\text{text}} + H_{\\text{image}}}$</p>\n<p>La Layer Normalization calcola:\n$\\mu = \\frac{1}{H_{\\text{text}} + H_{\\text{image}}} \\left(\\sum_{i=1}^{H_{\\text{text}}} x_{\\text{text},i} + \\sum_{j=1}^{H_{\\text{image}}} x_{\\text{image},j}\\right)$</p>\n<p>Questo può causare mixing indesiderato tra modalità diverse.</p>\n<h3 id=\"limitazioni-con-attivazioni-sparse\">Limitazioni con Attivazioni Sparse</h3>\n<p>Per attivazioni molto sparse (molti zeri), la Layer Normalization può amplificare il rumore:</p>\n<p>Se $|\\{i: x_i \\neq 0\\}| \\ll H$, allora:\n$\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^H (x_i - \\mu)^2 \\approx \\frac{1}{H} \\sum_{i: x_i \\neq 0} x_i^2$</p>\n<p>La normalizzazione può rendere i valori non-zero artificialmente grandi.</p>\n<h2 id=\"estensioni-e-varianti-specifiche\">Estensioni e Varianti Specifiche</h2>\n<h3 id=\"conditional-layer-normalization\">Conditional Layer Normalization</h3>\n<p>Per tasks condizionali (es. style transfer), i parametri dipendono dal contesto:</p>\n<p>$\n\\begin{aligned}\n\\boldsymbol{\\gamma}_c &= f_\\gamma(\\mathbf{c}) \\\\\n\\boldsymbol{\\beta}_c &= f_\\beta(\\mathbf{c}) \\\\\n\\mathbf{y} &= \\boldsymbol{\\gamma}_c \\odot \\hat{\\mathbf{x}} + \\boldsymbol{\\beta}_c\n\\end{aligned}\n$</p>\n<p>dove $\\mathbf{c}$ è l&rsquo;informazione condizionale e $f_\\gamma, f_\\beta$ sono reti neurali.</p>\n<h3 id=\"switchable-layer-normalization\">Switchable Layer Normalization</h3>\n<p>Combina vantaggi di diverse normalizzazioni:</p>\n<p>$\\mathbf{y} = \\lambda \\cdot \\text{LN}(\\mathbf{x}) + (1-\\lambda) \\cdot \\text{BN}(\\mathbf{x})$</p>\n<p>dove $\\lambda \\in [0,1]$ è appreso durante il training.</p>\n<h3 id=\"feature-wise-layer-normalization\">Feature-wise Layer Normalization</h3>\n<p>Normalizza solo sottogruppi di features:</p>\n<p>$\\hat{x}_i = \\frac{x_i - \\mu_{\\mathcal{G}(i)}}{\\sqrt{\\sigma_{\\mathcal{G}(i)}^2 + \\epsilon}}$</p>\n<p>dove $\\mathcal{G}(i)$ indica il gruppo di features contenente l&rsquo;$i$-esima feature.</p>\n<h2 id=\"implementazioni-ottimizzate-e-considerazioni-pratiche\">Implementazioni Ottimizzate e Considerazioni Pratiche</h2>\n<h3 id=\"memory-efficient-layer-normalization\">Memory-Efficient Layer Normalization</h3>\n<p>Per sequenze molto lunghe, è possibile implementare versioni memory-efficient:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">memory_efficient_layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Versione memory-efficient per sequenze lunghe</span>\n<span class=\"sd\">    x: (batch_size, seq_len, hidden_size)</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"c1\"># Streaming computation of statistics</span>\n    <span class=\"n\">chunk_size</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>\n    <span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">hidden_size</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n\n    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">chunk_size</span><span class=\"p\">):</span>\n        <span class=\"n\">end_idx</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"n\">chunk_size</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">)</span>\n        <span class=\"n\">chunk</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">end_idx</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n\n        <span class=\"c1\"># Standard layer norm on chunk</span>\n        <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">chunk</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdim</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">chunk</span><span class=\"o\">.</span><span class=\"n\">var</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdim</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">chunk_norm</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">chunk</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n        <span class=\"n\">output</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">:</span><span class=\"n\">end_idx</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">gamma</span> <span class=\"o\">*</span> <span class=\"n\">chunk_norm</span> <span class=\"o\">+</span> <span class=\"n\">beta</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">output</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"gradient-checkpointing-per-layer-normalization\">Gradient Checkpointing per Layer Normalization</h3>\n<p>Per modelli molto grandi, il gradient checkpointing può ridurre la memoria:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">checkpointed_layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">create_forward_fn</span><span class=\"p\">():</span>\n        <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward_fn</span><span class=\"p\">(</span><span class=\"n\">x_inner</span><span class=\"p\">):</span>\n            <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x_inner</span><span class=\"p\">,</span> <span class=\"n\">x_inner</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">:],</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">forward_fn</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">checkpoint</span><span class=\"o\">.</span><span class=\"n\">checkpoint</span><span class=\"p\">(</span><span class=\"n\">create_forward_fn</span><span class=\"p\">(),</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<h2 id=\"debugging-e-analisi-delle-performance\">Debugging e Analisi delle Performance</h2>\n<h3 id=\"monitoring-delle-statistiche\">Monitoring delle Statistiche</h3>\n<p>È importante monitorare le statistiche della Layer Normalization durante il training:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">LayerNormMonitor</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">mean_history</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">var_history</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">grad_norm_history</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">):</span>\n        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n            <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">var</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">mean_history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">.</span><span class=\"n\">cpu</span><span class=\"p\">())</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">var_history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">var</span><span class=\"o\">.</span><span class=\"n\">cpu</span><span class=\"p\">())</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">gamma</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">grad_norm_history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">gamma</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">cpu</span><span class=\"p\">())</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">:],</span> <span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n</details>\n\n<h3 id=\"problemi-comuni-e-soluzioni\">Problemi Comuni e Soluzioni</h3>\n<h4 id=\"1-instabilita-numerica-con-varianza-piccola\">1. Instabilità Numerica con Varianza Piccola</h4>\n<p><strong>Problema</strong>: $\\sigma^2 \\approx 0$ causa divisione per zero\n<strong>Soluzione</strong>: Aumentare $\\epsilon$ o usare precision più alta</p>\n<h4 id=\"2-gradienti-che-esplodono-con-learning-rate-alto\">2. Gradienti che Esplodono con Learning Rate Alto</h4>\n<p><strong>Problema</strong>: Il termine $\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}$ amplifica i gradienti\n<strong>Soluzione</strong>: Gradient clipping o learning rate scheduling</p>\n<h4 id=\"3-performance-degradation-con-batch-size-variabile\">3. Performance Degradation con Batch Size Variabile</h4>\n<p><strong>Problema</strong>: A differenza di Batch Norm, Layer Norm è robusta a batch size variabile\n<strong>Vantaggio</strong>: Può essere usata efficacemente con batch size = 1</p>\n<h2 id=\"ricerca-attuale-e-direzioni-future\">Ricerca Attuale e Direzioni Future</h2>\n<h3 id=\"theoretical-understanding\">Theoretical Understanding</h3>\n<p>Ricerca recente si foca su:</p>\n<ol>\n<li><strong>Connessioni con l&rsquo;ottimizzazione</strong>: Relazione tra Layer Normalization e preconditioner naturali</li>\n<li><strong>Generalizzazione</strong>: Perché Layer Normalization migliora la generalizzazione più di altre tecniche</li>\n<li><strong>Expressivity</strong>: Come Layer Normalization influenza la capacità espressiva delle reti</li>\n</ol>\n<h3 id=\"nuove-varianti-emergenti\">Nuove Varianti Emergenti</h3>\n<h4 id=\"1-adaptive-computation-layer-norm\">1. Adaptive Computation Layer Norm</h4>\n<p>Adatta la normalizzazione basandosi sulla difficulty degli esempi</p>\n<h4 id=\"2-learnable-activation-layer-norm\">2. Learnable Activation Layer Norm</h4>\n<p>Integra la normalizzazione direttamente nelle funzioni di attivazione</p>\n<h4 id=\"3-attention-guided-layer-norm\">3. Attention-guided Layer Norm</h4>\n<p>Usa meccanismi di attenzione per decidere quali features normalizzare</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>La Layer Normalization rappresenta un&rsquo;evoluzione significativa rispetto alla Batch Normalization, offrendo:</p>\n<h3 id=\"vantaggi-chiave\">Vantaggi Chiave</h3>\n<ol>\n<li><strong>Indipendenza dal batch</strong>: Funziona con qualsiasi dimensione di batch, incluso batch size = 1</li>\n<li><strong>Consistenza train-test</strong>: Identico comportamento in training e inferenza</li>\n<li><strong>Efficacia con sequenze</strong>: Naturalmente adatta per RNN e Transformer</li>\n<li><strong>Semplicità implementativa</strong>: Meno overhead computazionale e di memoria</li>\n<li><strong>Stabilità numerica</strong>: Meno problemi di sincronizzazione in ambienti distribuiti</li>\n</ol>\n<h3 id=\"svantaggi-da-considerare\">Svantaggi da Considerare</h3>\n<ol>\n<li><strong>Performance su CNN</strong>: Generalmente inferiore alla Batch Normalization per visione computazionale</li>\n<li><strong>Features eterogenee</strong>: Può causare problemi con features di natura molto diversa</li>\n<li><strong>Attivazioni sparse</strong>: Potenziali problemi con rappresentazioni molto sparse</li>\n</ol>\n<h3 id=\"raccomandazioni-duso\">Raccomandazioni d&rsquo;Uso</h3>\n<ul>\n<li><strong>Preferire per</strong>: Transformer, RNN, applicazioni con batch size variabile, inferenza single-sample</li>\n<li><strong>Evitare per</strong>: CNN profonde per visione computazionale (preferire Batch Norm o Group Norm)</li>\n<li><strong>Ibridi</strong>: Considerare varianti che combinano diversi approcci per applicazioni specifiche</li>\n</ul>\n<p>La Layer Normalization ha dimostrato di essere uno strumento fondamentale per l&rsquo;architettura dei modelli moderni, in particolare nel natural language processing e nei large language models, dove la sua indipendenza dal batch size e la consistenza train-test sono requisiti essenziali per scalabilità e performance.</p>"
}