{
  "title": "Batch Normalization",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione\">Introduzione</h2>\n<p>La <strong>Batch Normalization</strong> √® una tecnica di normalizzazione introdotta da Sergey Ioffe e Christian Szegedy nel 2015 che ha rivoluzionato l&rsquo;addestramento delle reti neurali profonde. Questa tecnica affronta il problema del <strong>Internal Covariate Shift</strong>, stabilizzando la distribuzione degli input ad ogni layer durante l&rsquo;addestramento e permettendo l&rsquo;uso di learning rate pi√π elevati, una convergenza pi√π rapida e una maggiore robustezza nell&rsquo;inizializzazione dei pesi.</p>\n<h2 id=\"il-problema-del-covariate-shift\">Il Problema del Covariate Shift</h2>\n<h3 id=\"definizione-formale-del-covariate-shift\">Definizione Formale del Covariate Shift</h3>\n<p>Il <strong>Covariate Shift</strong> si verifica quando la distribuzione degli input cambia tra il training e il test set. Qui, ogni singolo input √® rappresentato come un <strong>vettore di feature</strong> </p>\n$$\nX =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_d\n\\end{bmatrix}\n\\in \\mathbb{R}^d\n$$\n<p>dove $d$ √® il numero di feature per ciascun campione. Allo stesso modo, la variabile target associata a quell&rsquo;input pu√≤ essere rappresentata come <strong>vettore colonna</strong>:</p>\n$$\nY =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_c\n\\end{bmatrix}\n\\in \\mathbb{R}^c\n$$\n<p>dove $c$ rappresenta il numero di componenti del target (ad esempio il numero di classi in una codifica one-hot).</p>\n<p>Formalmente, si parla di covariate shift quando la <strong>distribuzione marginale dei vettori di input</strong> differisce tra training e test set:</p>\n$$\nP_{train}(X) \\neq P_{test}(X)\n$$\n<p>Questo significa che i singoli vettori di input osservati durante l&rsquo;addestramento provengono da una distribuzione diversa rispetto a quelli osservati in fase di test. Non stiamo confrontando matrici di dataset interi, ma la distribuzione dei <strong>singoli vettori di input</strong>.</p>\n<p>Nonostante ci√≤, la relazione condizionale tra input e output rimane invariata:</p>\n$$\nP_{train}(Y|X) = P_{test}(Y|X)\n$$\n<p>In altre parole, il modo in cui i vettori target $Y$ dipendono dai vettori di input $X$ non cambia tra training e test. Questo implica che la ‚Äúregola‚Äù che il modello deve apprendere resta valida, ma il modello potrebbe comunque avere difficolt√† a generalizzare se i vettori di input osservati durante il test sono distribuiti in maniera diversa rispetto a quelli visti in addestramento.</p>\n<p><strong>Esempio concreto:</strong></p>\n<p>Immaginiamo di classificare il rischio di malattia in base a due variabili: et√† e pressione sanguigna.</p>\n<ul>\n<li>Nel <strong>training set</strong>, molte persone hanno et√† tra 20 e 40 anni.  </li>\n<li>Nel <strong>test set</strong>, molte persone hanno et√† tra 40 e 60 anni.  </li>\n</ul>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">pandas</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">pd</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">seaborn</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">sns</span>\n\n<span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span><span class=\"n\">style</span><span class=\"o\">=</span><span class=\"s2\">&quot;whitegrid&quot;</span><span class=\"p\">,</span> <span class=\"n\">font_scale</span><span class=\"o\">=</span><span class=\"mf\">1.2</span><span class=\"p\">)</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n<span class=\"n\">palette</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;blue&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;#406c80&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;orange&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;#cf8532&quot;</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># Dati train</span>\n<span class=\"n\">train_age</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n<span class=\"n\">train_bp</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n<span class=\"n\">train_risk</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">train_age</span> <span class=\"o\">+</span> <span class=\"n\">train_bp</span><span class=\"o\">/</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">200</span><span class=\"p\">))</span> <span class=\"o\">&gt;</span> <span class=\"mi\">100</span>\n\n<span class=\"c1\"># Dati test</span>\n<span class=\"n\">test_age</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n<span class=\"n\">test_bp</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">125</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n<span class=\"n\">test_risk</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">test_age</span> <span class=\"o\">+</span> <span class=\"n\">test_bp</span><span class=\"o\">/</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">200</span><span class=\"p\">))</span> <span class=\"o\">&gt;</span> <span class=\"mi\">100</span>\n\n<span class=\"n\">df_train</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Age&#39;</span><span class=\"p\">:</span> <span class=\"n\">train_age</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BP&#39;</span><span class=\"p\">:</span> <span class=\"n\">train_bp</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Risk&#39;</span><span class=\"p\">:</span> <span class=\"n\">train_risk</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Set&#39;</span><span class=\"p\">:</span><span class=\"s1\">&#39;Train&#39;</span><span class=\"p\">})</span>\n<span class=\"n\">df_test</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Age&#39;</span><span class=\"p\">:</span> <span class=\"n\">test_age</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BP&#39;</span><span class=\"p\">:</span> <span class=\"n\">test_bp</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Risk&#39;</span><span class=\"p\">:</span> <span class=\"n\">test_risk</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Set&#39;</span><span class=\"p\">:</span><span class=\"s1\">&#39;Test&#39;</span><span class=\"p\">})</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">concat</span><span class=\"p\">([</span><span class=\"n\">df_train</span><span class=\"p\">,</span> <span class=\"n\">df_test</span><span class=\"p\">])</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">14</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">))</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">kdeplot</span><span class=\"p\">(</span><span class=\"n\">df_train</span><span class=\"p\">[</span><span class=\"s1\">&#39;Age&#39;</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Train&#39;</span><span class=\"p\">,</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">palette</span><span class=\"p\">[</span><span class=\"s2\">&quot;blue&quot;</span><span class=\"p\">])</span>\n<span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">kdeplot</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">[</span><span class=\"s1\">&#39;Age&#39;</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Test&#39;</span><span class=\"p\">,</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">palette</span><span class=\"p\">[</span><span class=\"s2\">&quot;orange&quot;</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&quot;Distribuzione Et√†&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;Et√†&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;Densit√†&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">()</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">kdeplot</span><span class=\"p\">(</span><span class=\"n\">df_train</span><span class=\"p\">[</span><span class=\"s1\">&#39;BP&#39;</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Train&#39;</span><span class=\"p\">,</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">palette</span><span class=\"p\">[</span><span class=\"s2\">&quot;blue&quot;</span><span class=\"p\">])</span>\n<span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">kdeplot</span><span class=\"p\">(</span><span class=\"n\">df_test</span><span class=\"p\">[</span><span class=\"s1\">&#39;BP&#39;</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Test&#39;</span><span class=\"p\">,</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">palette</span><span class=\"p\">[</span><span class=\"s2\">&quot;orange&quot;</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&quot;Distribuzione Pressione Sanguigna&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;Pressione Sanguigna&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s2\">&quot;Densit√†&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Salvo il grafico su file</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s2\">&quot;covariate_shift_distributions.png&quot;</span><span class=\"p\">,</span> <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span> <span class=\"n\">bbox_inches</span><span class=\"o\">=</span><span class=\"s1\">&#39;tight&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Mostro il grafico</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><img src=\"/images/posts/covariate_shift_distributions.png\" alt=\"Distribuzione Et√† e Pressione Sanguigna\" style=\"display: block; margin-left: auto; margin-right: auto;\"></p>\n<p><br></p>\n<p>In questo caso:</p>\n$$\nP_{train}(X) \\neq P_{test}(X)\n$$\n<p>Tuttavia, se la probabilit√† di avere la malattia dato et√† e pressione √® la stessa nei due set, cio√®:</p>\n$$\nP(Y = \\text{malattia} \\mid X = (\\text{et√†, pressione})) \\text{ √® identica in train e test,}\n$$\n<p>allora:</p>\n$$\nP_{train}(Y|X) = P_{test}(Y|X)\n$$\n<p>Il modello ha imparato la ‚Äúregola‚Äù corretta, anche se i dati osservati nel test set sono distribuiti in modo diverso rispetto a quelli del training set.</p>\n<h3 id=\"internal-covariate-shift\">Internal Covariate Shift</h3>\n<p>L&rsquo;<strong>Internal Covariate Shift</strong> √® un fenomeno analogo che si verifica all&rsquo;interno della rete neurale. Durante l&rsquo;addestramento, i parametri di ogni layer cambiano, causando una variazione continua nella distribuzione degli input ai layer successivi.</p>\n<p>Consideriamo un layer $l$ con input $x^{(l)}$ e parametri $\\theta^{(l)}$. L&rsquo;output del layer √®:</p>\n$$z^{(l)} = f^{(l)}(x^{(l)}; \\theta^{(l)})$$\n<p>Durante l&rsquo;addestramento, quando i parametri $\\theta^{(l-1)}$ del layer precedente vengono aggiornati, la distribuzione di $x^{(l)}$ cambia, anche se i parametri $\\theta^{(l)}$ rimangono fissi temporaneamente.</p>\n<h4 id=\"conseguenze\">Conseguenze</h4>\n<p>Questo fenomeno causa diversi problemi:</p>\n<ol>\n<li>\n<p><strong>Vanishing/Exploding Gradients</strong>: Se gli input ad un layer hanno varianza molto piccola o molto grande, i gradienti possono diventare troppo piccoli o troppo grandi.</p>\n<p>Consideriamo un singolo layer feedforward lineare con input $x \\in \\mathbb{R}^d$, pesi $W \\in \\mathbb{R}^{d \\times n}$, bias $b \\in \\mathbb{R}^n$ e output:</p>\n$$\n    z = W^\\top x + b\n    $$\n<p>Supponiamo di applicare una funzione di perdita $\\mathcal{L}(z)$. Il gradiente rispetto ai pesi √®:</p>\n$$\n    \\frac{\\partial \\mathcal{L}}{\\partial W} = x \\cdot \\left(\\frac{\\partial \\mathcal{L}}{\\partial z}\\right)^\\top\n    $$\n<p>Quello che ci interessa √® la norma attesa del gradiente. Se assumiamo che $x$ e $\\frac{\\partial \\mathcal{L}}{\\partial z}$ siano <strong>indipendenti</strong> e con <strong>media zero</strong>, allora:</p>\n$$\n    \\mathbb{E}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial W}\\Big] = \\mathbb{E}[x] \\cdot \\mathbb{E}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial z}\\Big]^\\top = 0\n    $$\n<p>La media dice solo dove &ldquo;centra&rdquo; i gradienti, ma non quanto fluttuano da campione a campione. In media, quindi, il gradiente non ha bias. Ci√≤ che conta per la stabilit√† √® invece la <strong>varianza</strong>, che dice quanto fluttuano i gradienti da campione a campione.</p>\n<p>La varianza del gradiente di ciascun peso √®:</p>\n$$\n    \\mathrm{Var}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial W}\\Big] = \\mathbb{E}\\Big[\\Big(\\frac{\\partial \\mathcal{L}}{\\partial W}\\Big)^2\\Big] - \\Big(\\mathbb{E}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial W}\\Big]\\Big)^2\n    $$\n<p>Poich√© l‚Äôaspettativa $\\mathbb{E}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial W}\\Big]$ √® zero:</p>\n$$\n    \\mathrm{Var}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial W}\\Big] = \\mathbb{E}[x^2] \\cdot \\mathbb{E}\\Big[\\Big(\\frac{\\partial \\mathcal{L}}{\\partial z}\\Big)^2\\Big] = \\mathrm{Var}[x] \\cdot \\mathrm{Var}\\Big[\\frac{\\partial \\mathcal{L}}{\\partial z}\\Big]\n    $$\n<p>Quindi</p>\n<ul>\n<li>Se $\\mathrm{Var}[x] \\gg 1$, alcuni gradienti $g$ diventano molto grandi.  </li>\n<li>Con gradient descent: $W \\gets W - \\eta g$  </li>\n<li>\n<p>Piccoli $\\eta$ possono mitigare, ma gradienti troppo grandi causano <strong>exploding gradients</strong> (i pesi saltano troppo in un solo passo). </p>\n</li>\n<li>\n<p>Se $\\mathrm{Var}[x] \\ll 1$, i gradienti diventano molto piccoli ‚Üí <strong>vanishing gradients</strong>, aggiornamenti dei pesi quasi nulli.</p>\n</li>\n</ul>\n<p>La stabilit√† dei gradienti quindi dipende direttamente dalla varianza degli input. Controllare o normalizzare la varianza degli input √® essenziale per evitare gradienti esplosivi o nulli.  </p>\n</li>\n<li>\n<p><strong>Saturazione delle funzioni di attivazione</strong>: Funzioni come la sigmoide o la tanh possono saturare se gli input sono troppo grandi in valore assoluto.</p>\n<p>Consideriamo una funzione di attivazione sigmoide:</p>\n$$\n    \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\sigma'(z) = \\sigma(z)(1-\\sigma(z))\n    $$\n<p>Se l‚Äôinput $z$ ha distribuzione con media $\\mu_z$ e varianza $\\sigma_z^2$ molto grande:</p>\n<ul>\n<li>Per $|z| \\gg 1$, $\\sigma(z) \\approx 0$ o $1$  </li>\n<li>Quindi:<br />\n$$\n    \\sigma'(z) = \\sigma(z)(1-\\sigma(z)) \\approx 0\n    $$</li>\n</ul>\n$$\n    \\lim_{|z| \\to \\infty} \\sigma'(z) = 0\n    $$\n<p>La derivata della sigmoide tende a zero quando la varianza degli input √® troppo grande, causando saturazione e vanishing gradient.</p>\n</li>\n<li>\n<p><strong>Instabilit√† nell&rsquo;addestramento</strong>: La continua variazione delle distribuzioni rende difficile l&rsquo;ottimizzazione.</p>\n<p>Sia un layer $l$ con output:</p>\n$$\n    z^{(l)} = f^{(l)}(x^{(l)}; \\theta^{(l)})\n    $$\n<p>e un layer successivo $l+1$ con input:</p>\n$$\n    x^{(l+1)} = z^{(l)}\n    $$\n<p>Supponiamo che la distribuzione di $x^{(l+1)}$ abbia media $\\mu^{(l+1)}$ e varianza $\\sigma^{2(l+1)}$. Durante l‚Äôaddestramento, quando aggiorniamo $\\theta^{(l)}$:</p>\n$$\n    x_{\\text{new}}^{(l+1)} = f^{(l)}(x^{(l)}; \\theta_{\\text{new}}^{(l)})\n    $$\n<p>La media e la varianza cambiano continuamente:</p>\n$$\n    \\mu_{\\text{new}}^{(l+1)} \\neq \\mu^{(l+1)}, \\quad\n    \\sigma_{\\text{new}}^{2(l+1)} \\neq \\sigma^{2(l+1)}\n    $$\n<p>Di conseguenza, il layer $l+1$ deve adattarsi a input distribuiti in modo diverso ad ogni passo.</p>\n<p>Formalmente, il gradiente rispetto a $\\theta^{(l+1)}$ dipende da $x^{(l+1)}$:</p>\n$$\n    \\frac{\\partial \\mathcal{L}}{\\partial \\theta^{(l+1)}} = \n    \\frac{\\partial \\mathcal{L}}{\\partial z^{(l+1)}} \n    \\frac{\\partial z^{(l+1)}}{\\partial \\theta^{(l+1)}}\n    $$\n<p>Se la distribuzione di $x^{(l+1)}$ cambia ad ogni passo, la distribuzione del gradiente cambia anch‚Äôessa, rendendo l‚Äôottimizzazione instabile.</p>\n<p>L‚Äôaspettativa e la varianza del gradiente dipendono dalla distribuzione degli input del layer successivo. Una distribuzione instabile causa gradienti instabili, quindi l‚Äôaddestramento √® meno stabile.</p>\n</li>\n</ol>\n<h2 id=\"formulazione-matematica-della-batch-normalization\">Formulazione Matematica della Batch Normalization</h2>\n<h3 id=\"algoritmo-base\">Algoritmo Base</h3>\n<p>Sia $B = \\{x_1, x_2, \\ldots, x_m\\}$ un mini-batch di $m$ esempi. Per ogni feature $i$, la batch normalization esegue i seguenti passi:</p>\n<h4 id=\"1-calcolo-della-media-del-batch\">1. Calcolo della Media del Batch</h4>\n$$\\mu_B^{(i)} = \\frac{1}{m} \\sum_{j=1}^{m} x_j^{(i)}$$\n<p>dove $x_j^{(i)}$ √® l&rsquo;$i$-esima feature del $j$-esimo esempio nel batch.</p>\n<h4 id=\"2-calcolo-della-varianza-del-batch\">2. Calcolo della Varianza del Batch</h4>\n$$\\sigma_B^{2(i)} = \\frac{1}{m} \\sum_{j=1}^{m} (x_j^{(i)} - \\mu_B^{(i)})^2$$\n<h4 id=\"3-normalizzazione\">3. Normalizzazione</h4>\n$$\\hat{x}_j^{(i)} = \\frac{x_j^{(i)} - \\mu_B^{(i)}}{\\sqrt{\\sigma_B^{2(i)} + \\epsilon}}$$\n<p>dove $\\epsilon$ √® una piccola costante (tipicamente $10^{-8}$) aggiunta per stabilit√† numerica per evitare divisioni per zero.</p>\n<h4 id=\"4-scaling-e-shifting\">4. Scaling e Shifting</h4>\n$$y_j^{(i)} = \\gamma^{(i)} \\hat{x}_j^{(i)} + \\beta^{(i)}$$\n<p>dove $\\gamma^{(i)}$ e $\\beta^{(i)}$ sono parametri appresi durante l&rsquo;addestramento.</p>\n<h3 id=\"notazione-vettoriale-e-matriciale\">Notazione Vettoriale e Matriciale</h3>\n<p>Per un batch di input </p>\n$$\nX = \n\\begin{bmatrix}\n\\mathbf{x}_1^\\top \\\\\n\\mathbf{x}_2^\\top \\\\\n\\vdots \\\\\n\\mathbf{x}_m^\\top\n\\end{bmatrix} \n\\in \\mathbb{R}^{m \\times d},\n$$\n<p>dove $m$ √® la dimensione del batch e $d$ il numero di feature, la <strong>Batch Normalization</strong> normalizza ciascuna feature separatamente.</p>\n<ol>\n<li><strong>Media del batch per ogni feature</strong>:</li>\n</ol>\n$$\n\\boldsymbol{\\mu}_B = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{x}_i \\in \\mathbb{R}^d\n$$\nche in notazione matriciale diventa:\n$$\n\\boldsymbol{\\mu}_B = \\frac{1}{m} \\mathbf{1}_m^\\top X \\in \\mathbb{R}^{1 \\times d},\n$$\n<ol>\n<li><strong>Varianza del batch per ogni feature</strong>:</li>\n</ol>\n$$\n\\boldsymbol{\\sigma}_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (\\mathbf{x}_i - \\boldsymbol{\\mu}_B) \\odot (\\mathbf{x}_i - \\boldsymbol{\\mu}_B) \\in \\mathbb{R}^d\n$$\nche in notazione matriciale diventa:\n$$\n\\boldsymbol{\\sigma}_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (\\mathbf{x}_i - \\boldsymbol{\\mu}_B) \\odot (\\mathbf{x}_i - \\boldsymbol{\\mu}_B) \\in \\mathbb{R}^{1 \\times d}\n$$\n<ol>\n<li><strong>Normalizzazione batch (per feature)</strong>:</li>\n</ol>\n$$\n\\hat{\\mathbf{x}}_i = \\frac{\\mathbf{x}_i - \\boldsymbol{\\mu}_B}{\\sqrt{\\boldsymbol{\\sigma}_B^2 + \\epsilon}} \\in \\mathbb{R}^d\n$$\nche in notazione matriciale diventa:\n$$\n\\hat{X} = (X - \\mathbf{1}_m \\boldsymbol{\\mu}_B) \\underbrace{\\oslash}_\\text{Divisione element-wise} \\sqrt{\\mathbf{1}_m \\boldsymbol{\\sigma}_B^2 + \\epsilon} \\in \\mathbb{R}^{m \\times d}\n$$\n<ol>\n<li><strong>Scaling e shifting con parametri apprendibili</strong>:</li>\n</ol>\n$$\n\\mathbf{y}_i = \\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{x}}_i + \\boldsymbol{\\beta} \\in \\mathbb{R}^d\n$$\n$$\nY = \\hat{X} \\odot \\boldsymbol{\\gamma} + \\boldsymbol{\\beta} \\in \\mathbb{R}^{m \\times d}\n$$\n<p>dove $\\odot$ indica il prodotto elemento per elemento, e $\\boldsymbol{\\gamma}$ e $\\boldsymbol{\\beta}$ sono vettori di dimensione $d$ che consentono al modello di riadattare scala e media di ogni feature.</p>\n<h2 id=\"rete-neurale-semplice-con-e-senza-batch-normalization\">Rete Neurale Semplice: Con e Senza Batch Normalization</h2>\n<p>Sia un input $\\mathbf{x} \\in \\mathbb{R}^d$ e un hidden layer con $h$ unit√†, funzione di attivazione $\\phi(\\cdot)$ e output finale $\\hat{\\mathbf{y}} \\in \\mathbb{R}^c$.</p>\n<h3 id=\"1-senza-batch-normalization\">1. Senza Batch Normalization</h3>\n$$\n\\begin{aligned}\n\\mathbf{z}^{(1)} &= W^{(1)\\top} \\mathbf{x} + \\mathbf{b}^{(1)} \\in \\mathbb{R}^h \\\\\n\\mathbf{a}^{(1)} &= \\phi\\big(\\mathbf{z}^{(1)}\\big) \\in \\mathbb{R}^h \\\\\n\\mathbf{z}^{(2)} &= W^{(2)\\top} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} \\in \\mathbb{R}^c \\\\\n\\hat{\\mathbf{y}} &= \\psi(\\mathbf{z}^{(2)}) \\in \\mathbb{R}^c\n\\end{aligned}\n$$\n<ul>\n<li>$W^{(1)} \\in \\mathbb{R}^{d \\times h}, W^{(2)} \\in \\mathbb{R}^{h \\times c}$  </li>\n<li>$\\phi$ = funzione di attivazione (ReLU, sigmoide, ecc.)  </li>\n<li>$\\psi$ = funzione di output (softmax, identit√†, ecc.)</li>\n</ul>\n<h3 id=\"2-con-batch-normalization\">2. Con Batch Normalization</h3>\n<p>Aggiungiamo BN <strong>prima dell‚Äôattivazione</strong> nel hidden layer:</p>\n$$\n\\begin{aligned}\n\\mathbf{z}^{(1)} &= W^{(1)\\top} \\mathbf{x} + \\mathbf{b}^{(1)} \\in \\mathbb{R}^h \\\\\n\\hat{\\mathbf{z}}^{(1)} &= \\frac{\\mathbf{z}^{(1)} - \\boldsymbol{\\mu}_B}{\\sqrt{\\boldsymbol{\\sigma}_B^2 + \\epsilon}} \\\\\n\\mathbf{y}^{(1)} &= \\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{z}}^{(1)} + \\boldsymbol{\\beta} \\\\\n\\mathbf{a}^{(1)} &= \\phi\\big(\\mathbf{y}^{(1)}\\big) \\in \\mathbb{R}^h \\\\\n\\mathbf{z}^{(2)} &= W^{(2)\\top} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} \\in \\mathbb{R}^c \\\\\n\\hat{\\mathbf{y}} &= \\psi(\\mathbf{z}^{(2)}) \\in \\mathbb{R}^c\n\\end{aligned}\n$$\n<p><img src=\"/images/tikz/407828ea15ae1302b02047e7638249ea.svg\" style=\"display: block; width: 100%; height: auto; max-height: 600px;\" class=\"tikz-svg\" /></p>\n<ul>\n<li>$\\boldsymbol{\\mu}_B, \\boldsymbol{\\sigma}_B^2 \\in \\mathbb{R}^h$ = media e varianza sul batch  </li>\n<li>$\\boldsymbol{\\gamma}, \\boldsymbol{\\beta} \\in \\mathbb{R}^h$ = parametri apprendibili di scaling e shifting  </li>\n<li>La BN stabilizza la distribuzione dei valori prima dell‚Äôattivazione, riducendo <strong>Internal Covariate Shift</strong></li>\n</ul>\n<h3 id=\"differenze-chiave\">Differenze chiave</h3>\n<table>\n<thead>\n<tr>\n<th>Versione</th>\n<th>Note</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Senza BN</td>\n<td>$\\mathbf{a}^{(1)} = \\phi(W^\\top \\mathbf{x} + b)$ ‚Üí distribuzione dei valori cambia ad ogni aggiornamento dei pesi</td>\n</tr>\n<tr>\n<td>Con BN</td>\n<td>$\\mathbf{a}^{(1)} = \\phi(\\boldsymbol{\\gamma} \\odot \\hat{\\mathbf{z}} + \\boldsymbol{\\beta})$ ‚Üí distribuzione pi√π stabile, gradienti pi√π controllati</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"proprieta-teoriche\">Propriet√† Teoriche</h2>\n<h3 id=\"invarianza-per-trasformazioni-affini\">Invarianza per Trasformazioni Affini</h3>\n<p>La batch normalization √® invariante rispetto a trasformazioni affini scalari, nel senso che:\n$\\text{BN}(ax + b) = \\pm \\text{BN}(x)$</p>\n<p>dove il segno dipende dal segno di $a$.</p>\n<p><strong>Interpretazione:</strong> L&rsquo;output normalizzato √® lo stesso (a meno del segno) indipendentemente da come gli input vengono scalati o traslati.</p>\n<h4 id=\"dimostrazione-dellinvarianza\">Dimostrazione dell&rsquo;Invarianza</h4>\n<p>Consideriamo un batch $\\{x_1, x_2, \\ldots, x_m\\}$ e la sua trasformazione affine:\n$$x'_i = ax_i + b \\quad \\forall i = 1, \\ldots, m$$</p>\n<p><strong>Passo 1: Calcolo della media trasformata</strong>\n$$\\mu' = \\frac{1}{m}\\sum_{i=1}^{m} x'_i = \\frac{1}{m}\\sum_{i=1}^{m} (ax_i + b) = a\\frac{1}{m}\\sum_{i=1}^{m} x_i + b = a\\mu + b$$</p>\n<p><strong>Passo 2: Calcolo della varianza trasformata</strong>\n$$\\sigma'^2 = \\frac{1}{m}\\sum_{i=1}^{m} (x'_i - \\mu')^2$$</p>\n<p>Sostituendo:\n$$\\sigma'^2 = \\frac{1}{m}\\sum_{i=1}^{m} (ax_i + b - a\\mu - b)^2 = \\frac{1}{m}\\sum_{i=1}^{m} a^2(x_i - \\mu)^2 = a^2\\sigma^2$$</p>\n<p><strong>Passo 3: Normalizzazione dei dati trasformati</strong>\n$$\\hat{x}'_i = \\frac{x'_i - \\mu'}{\\sqrt{\\sigma'^2 + \\epsilon}}$$</p>\n<p>Sostituendo le espressioni trovate:\n$$\\hat{x}'_i = \\frac{ax_i + b - (a\\mu + b)}{\\sqrt{a^2\\sigma^2 + \\epsilon}} = \\frac{a(x_i - \\mu)}{\\sqrt{a^2\\sigma^2 + \\epsilon}}$$</p>\n<p><strong>Caso $a > 0$:</strong>\n$$\\hat{x}'_i = \\frac{a(x_i - \\mu)}{|a|\\sqrt{\\sigma^2 + \\epsilon/a^2}} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon/a^2}}$$</p>\n<p><strong>Caso limite:</strong> Quando $|a| \\gg \\sqrt{\\epsilon}$, allora $\\epsilon/a^2 \\to 0$:\n$$\\hat{x}'_i \\to \\frac{x_i - \\mu}{\\sqrt{\\sigma^2}} = \\hat{x}_i$$</p>\n<p><strong>Caso $a < 0$:</strong>\n$$\\hat{x}'_i = \\frac{a(x_i - \\mu)}{|a|\\sqrt{\\sigma^2 + \\epsilon/a^2}} = -\\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon/a^2}} = -\\hat{x}_i$$</p>\n<h4 id=\"conseguenze-pratiche\">Conseguenze Pratiche</h4>\n<ol>\n<li><strong>Robustezza rispetto al preprocessing:</strong> La rete √® insensibile a normalizzazioni diverse dei dati</li>\n<li><strong>Invarianza rispetto ai pesi:</strong> Scaling dei pesi di un layer non influenza l&rsquo;output normalizzato</li>\n<li><strong>Accelerazione del training:</strong> Riduce la dipendenza dall&rsquo;inizializzazione dei parametri</li>\n</ol>\n<hr />\n<h4 id=\"caso-vettoriale\">Caso vettoriale</h4>\n<h5 id=\"setup-e-notazione\">Setup e Notazione</h5>\n<p>Batch: $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_m\\}$ con $\\mathbf{x}_i \\in \\mathbb{R}^d$, trasformazione $\\mathbf{x}'_i = \\mathbf{A}\\mathbf{x}_i + \\mathbf{b}$, $\\mathbf{A} = \\text{diag}(a_1, \\ldots, a_d)$</p>\n$$\\boldsymbol{\\mu} = \\frac{1}{m}\\sum_{i=1}^{m} \\mathbf{x}_i, \\quad \\boldsymbol{\\sigma}^2 = \\frac{1}{m}\\sum_{i=1}^{m} (\\mathbf{x}_i - \\boldsymbol{\\mu}) \\odot (\\mathbf{x}_i - \\boldsymbol{\\mu}), \\quad \\hat{\\mathbf{x}}_i = \\frac{\\mathbf{x}_i - \\boldsymbol{\\mu}}{\\sqrt{\\boldsymbol{\\sigma}^2 + \\epsilon \\mathbf{1}}}$$\n<h5 id=\"trasformazione\">Trasformazione</h5>\n<p><strong>Media trasformata:</strong>\n$$\\boldsymbol{\\mu}' = \\frac{1}{m}\\sum_{i=1}^{m} (\\mathbf{A}\\mathbf{x}_i + \\mathbf{b}) = \\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{b}$$</p>\n<p><strong>Varianza trasformata:</strong>\n$$\\boldsymbol{\\sigma}'^2 = \\frac{1}{m}\\sum_{i=1}^{m} (\\mathbf{A}\\mathbf{x}_i + \\mathbf{b} - \\mathbf{A}\\boldsymbol{\\mu} - \\mathbf{b}) \\odot (\\mathbf{A}\\mathbf{x}_i + \\mathbf{b} - \\mathbf{A}\\boldsymbol{\\mu} - \\mathbf{b})$$\n$$= \\frac{1}{m}\\sum_{i=1}^{m} (\\mathbf{A}(\\mathbf{x}_i - \\boldsymbol{\\mu})) \\odot (\\mathbf{A}(\\mathbf{x}_i - \\boldsymbol{\\mu})) = (\\mathbf{A} \\odot \\mathbf{A}) \\odot \\boldsymbol{\\sigma}^2 = \\mathbf{A}^2 \\boldsymbol{\\sigma}^2$$</p>\n<p><strong>Normalizzazione trasformata:</strong>\n$$\\hat{\\mathbf{x}}'_i = \\frac{\\mathbf{A}\\mathbf{x}_i + \\mathbf{b} - \\mathbf{A}\\boldsymbol{\\mu} - \\mathbf{b}}{\\sqrt{\\mathbf{A}^2 \\boldsymbol{\\sigma}^2 + \\epsilon \\mathbf{1}}} = \\frac{\\mathbf{A}(\\mathbf{x}_i - \\boldsymbol{\\mu})}{\\sqrt{\\mathbf{A}^2 \\boldsymbol{\\sigma}^2 + \\epsilon \\mathbf{1}}}$$</p>\n$$= \\frac{\\mathbf{A}(\\mathbf{x}_i - \\boldsymbol{\\mu})}{\\sqrt{\\mathbf{A}^2(\\boldsymbol{\\sigma}^2 + \\epsilon \\mathbf{A}^{-2})}} = \\frac{\\mathbf{A}(\\mathbf{x}_i - \\boldsymbol{\\mu})}{|\\mathbf{A}|\\sqrt{\\boldsymbol{\\sigma}^2 + \\epsilon \\mathbf{A}^{-2}}}$$\n$$= \\text{sign}(\\mathbf{A}) \\odot \\frac{\\mathbf{x}_i - \\boldsymbol{\\mu}}{\\sqrt{\\boldsymbol{\\sigma}^2 + \\epsilon \\mathbf{A}^{-2}}}$$\n<h4 id=\"risultato\">Risultato</h4>\n<p>Per $|\\mathbf{A}| \\gg \\sqrt{\\epsilon} \\mathbf{1}$: $\\epsilon \\mathbf{A}^{-2} \\to \\mathbf{0}$</p>\n$$\\boxed{\\text{BN}(\\mathbf{A}\\mathbf{x} + \\mathbf{b}) = \\text{sign}(\\mathbf{A}) \\odot \\text{BN}(\\mathbf{x})}$$\n<p>dove $\\text{sign}(\\mathbf{A}) = \\text{diag}(\\text{sign}(a_1), \\ldots, \\text{sign}(a_d))$</p>\n<hr />\n<h3 id=\"effetto-sulla-distribuzione-dei-gradienti\">Effetto sulla Distribuzione dei Gradienti</h3>\n<p>La <strong>Batch Normalization (BN)</strong> non agisce solo sulla distribuzione degli attivazioni forward, ma ha anche un impatto fondamentale sulla <strong>propagazione dei gradienti</strong> durante il backpropagation. Analizziamo nel dettaglio.</p>\n<h4 id=\"1-derivata-rispetto-allinput-normalizzato\">1. Derivata rispetto all‚Äôinput normalizzato</h4>\n<p>Dato un input $x_i$ che viene normalizzato in:</p>\n$$\n\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n$$\n<p>e successivamente scalato e traslato tramite i parametri appresi $\\gamma, \\beta$:</p>\n$$\ny_i = \\gamma \\hat{x}_i + \\beta\n$$\n<p>la derivata della loss $L$ rispetto all‚Äôinput normalizzato $\\hat{x}_i$ √®:</p>\n$$\n\\frac{\\partial L}{\\partial \\hat{x}_i} = \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial \\hat{x}_i} = \\frac{\\partial L}{\\partial y_i} \\cdot \\gamma\n$$\n<p>üëâ Questo significa che il gradiente verso $\\hat{x}_i$ viene semplicemente <strong>scalato da $\\gamma$</strong>, mantenendo un controllo esplicito sulla sua ampiezza.</p>\n<h4 id=\"2-derivata-rispetto-allinput-originale\">2. Derivata rispetto all‚Äôinput originale</h4>\n<p>Il passo cruciale √® calcolare la derivata rispetto all‚Äôinput non normalizzato $x_i$. La formula completa √®:</p>\n<p>Partiamo da\n$$\n\\hat{x}_j=\\frac{x_j-\\mu}{\\sqrt{\\sigma^2+\\epsilon}},\\qquad \n\\mu=\\frac{1}{m}\\sum_{k=1}^m x_k,\\qquad\n\\sigma^2=\\frac{1}{m}\\sum_{k=1}^m (x_k-\\mu)^2.\n$$</p>\n<p>Vogliamo calcolare\n$$\n\\frac{\\partial L}{\\partial x_i}=\\sum_{j=1}^m\\frac{\\partial L}{\\partial \\hat{x}_j}\\frac{\\partial \\hat{x}_j}{\\partial x_i}.\n$$</p>\n<p>Calcoliamo $\\dfrac{\\partial \\hat{x}_j}{\\partial x_i}$. Definiamo $s=\\sqrt{\\sigma^2+\\epsilon}$. Allora\n$$\n\\hat{x}_j=(x_j-\\mu)s^{-1}.\n$$\nPer la regola della catena:\n$$\n\\frac{\\partial \\hat{x}_j}{\\partial x_i}\n= s^{-1}\\frac{\\partial (x_j-\\mu)}{\\partial x_i} + (x_j-\\mu)\\frac{\\partial (s^{-1})}{\\partial \\sigma^2}\\frac{\\partial \\sigma^2}{\\partial x_i}.\n$$</p>\n<p>Calcoliamo i termini necessari.</p>\n<ol>\n<li>\n<p>$\\displaystyle\\frac{\\partial (x_j-\\mu)}{\\partial x_i}=\\delta_{ij}-\\frac{\\partial\\mu}{\\partial x_i}=\\delta_{ij}-\\frac{1}{m}.$</p>\n</li>\n<li>\n<p>$\\displaystyle\\frac{\\partial (s^{-1})}{\\partial \\sigma^2} = \\frac{d}{d\\sigma^2}(\\sigma^2+\\epsilon)^{-1/2} = -\\tfrac{1}{2}(\\sigma^2+\\epsilon)^{-3/2} = -\\tfrac{1}{2}s^{-3}.$</p>\n</li>\n<li>\n<p>Usando $\\sigma^2=\\tfrac{1}{m}\\sum_k x_k^2-\\mu^2$ (o derivando direttamente), si ottiene\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i}=\\frac{2}{m}(x_i-\\mu).\n$$</p>\n</li>\n</ol>\n<p>Inserendo (2) e (3):\n$$\n\\frac{\\partial (s^{-1})}{\\partial \\sigma^2}\\frac{\\partial \\sigma^2}{\\partial x_i}\n= -\\tfrac{1}{2}s^{-3}\\cdot \\frac{2}{m}(x_i-\\mu) = -\\frac{1}{m}s^{-3}(x_i-\\mu).\n$$</p>\n<p>Quindi\n$$\n\\frac{\\partial \\hat{x}_j}{\\partial x_i}\n= s^{-1}\\!\\left(\\delta_{ij}-\\frac{1}{m}\\right) + (x_j-\\mu)\\left(-\\frac{1}{m}s^{-3}(x_i-\\mu)\\right).\n$$</p>\n<p>Raccogliendo $s^{-1}$:\n$$\n\\frac{\\partial \\hat{x}_j}{\\partial x_i}\n= \\frac{1}{s}\\left(\\delta_{ij}-\\frac{1}{m}\\right) - \\frac{1}{m}\\frac{(x_j-\\mu)(x_i-\\mu)}{s^{3}}\n= \\frac{1}{s}\\left(\\delta_{ij}-\\frac{1}{m} - \\frac{(x_j-\\mu)(x_i-\\mu)}{m(\\sigma^2+\\epsilon)}\\right).\n$$</p>\n<p>Usando $\\hat{x}_k=\\dfrac{x_k-\\mu}{s}$ si riscrive l&rsquo;ultimo termine:\n$$\n\\frac{(x_j-\\mu)(x_i-\\mu)}{s^{2}}=\\hat{x}_j\\hat{x}_i\n$$\nquindi\n$$\n\\frac{\\partial \\hat{x}_j}{\\partial x_i}\n= \\frac{1}{s}\\left(\\delta_{ij}-\\frac{1}{m} - \\frac{\\hat{x}_j\\hat{x}_i}{m}\\right).\n$$</p>\n<p>Ora calcoliamo $\\dfrac{\\partial L}{\\partial x_i}$:\n$$\n\\frac{\\partial L}{\\partial x_i}\n= \\sum_{j=1}^m \\frac{\\partial L}{\\partial \\hat{x}_j}\\frac{\\partial \\hat{x}_j}{\\partial x_i}\n= \\sum_{j=1}^m \\frac{\\partial L}{\\partial \\hat{x}_j}\\cdot \\frac{1}{s}\\left(\\delta_{ij}-\\frac{1}{m} - \\frac{\\hat{x}_j\\hat{x}_i}{m}\\right).\n$$</p>\n<p>Svolgendo la somma:\n$$\n\\frac{\\partial L}{\\partial x_i}\n= \\frac{1}{s}\\left(\\frac{\\partial L}{\\partial \\hat{x}_i} - \\frac{1}{m}\\sum_{j=1}^m\\frac{\\partial L}{\\partial \\hat{x}_j} - \\frac{\\hat{x}_i}{m}\\sum_{j=1}^m\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j\\right).\n$$</p>\n<p>Sostituendo $s=\\sqrt{\\sigma^2+\\epsilon}$ otteniamo la formula finale:\n$$\n\\boxed{\\displaystyle\n\\frac{\\partial L}{\\partial x_i} = \n\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\left[ \n\\frac{\\partial L}{\\partial \\hat{x}_i} \n- \\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} \n- \\frac{\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j \n\\right].}\n$$</p>\n<p>dove $m$ √® la dimensione del batch.</p>\n<p>Analizziamone i termini:</p>\n<ol>\n<li>\n<p><strong>Termine diretto</strong>:<br />\n   $$\n   \\frac{\\partial L}{\\partial \\hat{x}_i}\n   $$\n   contribuisce con il gradiente locale di ogni esempio.</p>\n</li>\n<li>\n<p><strong>Termine di ricentraggio</strong>:<br />\n   $$\n   - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\n   $$\n   questo assicura che la somma dei gradienti sul batch sia <strong>zero</strong>, mantenendo i gradienti ricentrati come lo erano gli input.</p>\n</li>\n<li>\n<p><strong>Termine di decorrelazione</strong>:<br />\n   $$\n   - \\frac{\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j\n   $$\n   qui il gradiente viene corretto in base alla correlazione con l‚Äôinput normalizzato $\\hat{x}_j$.<br />\n   üëâ Questo riduce la <strong>correlazione tra gradienti diversi nel batch</strong>, migliorando la stabilit√† dell‚Äôottimizzazione.</p>\n</li>\n</ol>\n<p>Infine, il tutto √® riscalato dal fattore:</p>\n$$\n\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\n$$\n<p>che garantisce che i gradienti abbiano <strong>varianza controllata</strong>, impedendo esplosioni o scomparse del gradiente.</p>\n<h4 id=\"3-interpretazione-complessiva\">3. Interpretazione complessiva</h4>\n<ul>\n<li>La BN <strong>ricentra i gradienti</strong> ‚Üí niente drift verso direzioni comuni del batch.  </li>\n<li>La BN <strong>riscalda i gradienti</strong> ‚Üí controlla la scala, riducendo vanishing/exploding gradients.  </li>\n<li>La BN <strong>riduce la correlazione</strong> ‚Üí ogni esempio nel batch contribuisce in maniera pi√π indipendente.  </li>\n</ul>\n<p>üëâ In sintesi, la Batch Normalization agisce come una <strong>regolarizzazione implicita</strong> anche nel backward pass, rendendo la superficie di ottimizzazione pi√π liscia e favorendo una convergenza pi√π stabile e veloce.</p>\n<h2 id=\"backpropagation-attraverso-la-batch-normalization\">Backpropagation attraverso la Batch Normalization</h2>\n<h3 id=\"derivate-parziali\">Derivate Parziali</h3>\n<p>Per implementare correttamente la batch normalization, √® necessario calcolare le derivate parziali per tutti i parametri coinvolti.</p>\n<h4 id=\"derivata-rispetto-a-math_inline_191-e-math_inline_192\">Derivata rispetto a $\\gamma$ e $\\beta$</h4>\n$$\\frac{\\partial L}{\\partial \\gamma} = \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\hat{x}_i$$\n$$\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i}$$\n<h4 id=\"derivata-rispetto-allinput-normalizzato\">Derivata rispetto all&rsquo;input normalizzato</h4>\n$$\\frac{\\partial L}{\\partial \\hat{x}_i} = \\frac{\\partial L}{\\partial y_i} \\gamma$$\n<h4 id=\"derivata-rispetto-alla-varianza\">Derivata rispetto alla varianza</h4>\n$$\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\sigma_B^2} \n&= \\sum_{i=1}^m \\frac{\\partial L}{\\partial \\hat{x}_i}\\cdot \\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2} \\\\[0.75em]\n&= \\sum_{i=1}^m \\frac{\\partial L}{\\partial \\hat{x}_i} (x_i - \\mu_B)\\cdot \\frac{\\partial}{\\partial \\sigma_B^2}(\\sigma_B^2 + \\epsilon)^{-1/2} \\\\[0.75em]\n&= \\sum_{i=1}^m \\frac{\\partial L}{\\partial \\hat{x}_i} (x_i - \\mu_B)\\left(-\\tfrac{1}{2}\\right)(\\sigma_B^2 + \\epsilon)^{-3/2} \\\\[0.75em]\n&= \\sum_{i=1}^m \\frac{\\partial L}{\\partial \\hat{x}_i} (x_i - \\mu_B)\\,\\frac{-1}{2}(\\sigma_B^2 + \\epsilon)^{-3/2}\n\\end{align*}\n$$\n<h4 id=\"derivata-rispetto-alla-media\">Derivata rispetto alla media</h4>\n$$\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\mu_B} \n&= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot \\frac{\\partial \\hat{x}_i}{\\partial \\mu_B} \n    + \\frac{\\partial L}{\\partial \\sigma_B^2} \\cdot \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B} \\\\[0.75em]\n&= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot \\left(-\\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\right) \n    + \\frac{\\partial L}{\\partial \\sigma_B^2} \\cdot \\left(\\frac{-2}{m}\\sum_{i=1}^m (x_i - \\mu_B)\\right) \\\\[0.75em]\n&= \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{x}_i}\\,\\frac{-1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \n    + \\frac{\\partial L}{\\partial \\sigma_B^2}\\,\\frac{-2}{m}\\sum_{i=1}^m (x_i - \\mu_B)\n\\end{align*}\n$$\n<h4 id=\"derivata-rispetto-allinput-originale\">Derivata rispetto all&rsquo;input originale</h4>\n$$\n\\begin{align*}\n\\frac{\\partial L}{\\partial x_i} \n&= \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\left[\n    \\frac{\\partial L}{\\partial \\hat{x}_i} \n    - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} \n    - \\frac{\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j\n\\right] \\\\[0.75em]\n&= \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\frac{\\partial L}{\\partial \\hat{x}_i} \n   - \\frac{1}{m\\sqrt{\\sigma_B^2 + \\epsilon}} \\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} \n   - \\frac{\\hat{x}_i}{m\\sqrt{\\sigma_B^2 + \\epsilon}} \\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j \\\\[0.75em]\n&= \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\frac{\\partial L}{\\partial \\hat{x}_i} \n   + \\left(-\\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\right)\\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} \n   + \\left(-\\tfrac{1}{2}(\\sigma_B^2 + \\epsilon)^{-\\tfrac{3}{2}}\\right)\\frac{2\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j \\\\[0.75em]\n&= \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\frac{\\partial L}{\\partial \\hat{x}_i} \n   + \\frac{1}{m}\\frac{\\partial L}{\\partial \\mu_B} \n   + \\frac{2(x_i - \\mu_B)}{m}\\frac{\\partial L}{\\partial \\sigma_B^2}.\n\\end{align*}\n$$\n<h2 id=\"batch-normalization-durante-linferenza\">Batch Normalization durante l&rsquo;Inferenza</h2>\n<p>Durante la fase di test o inferenza, non abbiamo accesso a un batch di esempi, quindi non possiamo calcolare statistiche del batch. Invece, utilizziamo le <strong>statistiche della popolazione</strong> stimate durante l&rsquo;addestramento.</p>\n<h3 id=\"calcolo-delle-statistiche-di-popolazione\">Calcolo delle Statistiche di Popolazione</h3>\n<p>Durante l&rsquo;addestramento, manteniamo una media mobile delle statistiche del batch:</p>\n$$\\mu_{pop} = \\alpha \\mu_{pop} + (1-\\alpha) \\mu_B$$\n$$\\sigma_{pop}^2 = \\alpha \\sigma_{pop}^2 + (1-\\alpha) \\sigma_B^2$$\n<p>dove $\\alpha$ √® tipicamente 0.9 o 0.99.</p>\n<h3 id=\"normalizzazione-durante-linferenza\">Normalizzazione durante l&rsquo;Inferenza</h3>\n$$\\hat{x} = \\frac{x - \\mu_{pop}}{\\sqrt{\\sigma_{pop}^2 + \\epsilon}}$$\n$$y = \\gamma \\hat{x} + \\beta$$\n<h3 id=\"perche-e-importante\">Perch√© √® importante</h3>\n<p>Usare le statistiche di popolazione durante l&rsquo;inferenza √® cruciale perch√©:\n- <strong>Stabilizza le attivazioni</strong>: evita che la normalizzazione dipenda da un batch di test troppo piccolo o non rappresentativo.<br />\n- <strong>Garantisce coerenza</strong>: i dati vengono trasformati nello stesso modo indipendentemente dalla dimensione del batch o dal singolo esempio.<br />\n- <strong>Preserva le prestazioni</strong>: senza questo accorgimento, la rete si troverebbe a elaborare input con distribuzioni diverse rispetto a quelle viste in addestramento, causando un forte degrado della qualit√† delle predizioni.  </p>\n<h2 id=\"effetti-della-batch-normalization\">Effetti della Batch Normalization</h2>\n<h3 id=\"stabilizzazione-del-training\">Stabilizzazione del Training</h3>\n<p>La batch normalization riduce la sensibilit√† all&rsquo;inizializzazione dei pesi. Matematicamente, questo pu√≤ essere compreso osservando che la normalizzazione limita la magnitudine degli input a ogni layer, indipendentemente dall&rsquo;inizializzazione precedente.</p>\n<h3 id=\"regolarizzazione-implicita\">Regolarizzazione Implicita</h3>\n<p>La batch normalization ha un effetto regolarizzante implicito. Questo avviene perch√©:</p>\n<ol>\n<li>\n<p><strong>Rumore del Batch</strong>: Le statistiche calcolate su mini-batch introducono rumore stocastico che agisce come regolarizzazione.</p>\n</li>\n<li>\n<p><strong>Normalizzazione</strong>: La normalizzazione riduce l&rsquo;overfitting forzando la rete a essere meno dipendente da valori specifici degli input.</p>\n</li>\n</ol>\n<h3 id=\"learning-rates-piu-elevati\">Learning Rates pi√π Elevati</h3>\n<p>La batch normalization permette l&rsquo;uso di learning rate pi√π elevati attraverso un meccanismo molto efficace: il <strong>ricentramento automatico dei gradienti</strong>.</p>\n<h4 id=\"ricentramento-automatico-dei-gradienti\">Ricentramento Automatico dei Gradienti</h4>\n<p>Dalla formula del gradiente della batch normalization:</p>\n$$\\frac{\\partial L}{\\partial x_i} = \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\left[\\frac{\\partial L}{\\partial \\hat{x}_i} - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} - \\frac{\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j\\right]$$\n<p><strong>Teorema (Ricentramento)</strong>: La somma dei gradienti su un batch √® sempre zero:</p>\n$$\\boxed{\\sum_{i=1}^m \\frac{\\partial L}{\\partial x_i} = 0}$$\n<p><strong>Dimostrazione</strong>: \n$$\\begin{align*}\n\\sum_{i=1}^m \\frac{\\partial L}{\\partial x_i} &= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\sum_{i=1}^m \\left[\\frac{\\partial L}{\\partial \\hat{x}_i} - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} - \\frac{\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j\\right]\\\\\n&= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\left[\\sum_{i=1}^m\\frac{\\partial L}{\\partial \\hat{x}_i} - \\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j} - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j \\sum_{i=1}^m\\hat{x}_i\\right]\n\\end{align*}$$</p>\n<p>Poich√© per costruzione della batch normalization: $\\frac{1}{m}\\sum_{i=1}^m\\hat{x}_i = 0 \\implies \\sum_{i=1}^m\\hat{x}_i = 0$, otteniamo:</p>\n$$\\sum_{i=1}^m \\frac{\\partial L}{\\partial x_i} = \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\left[\\sum_{i=1}^m\\frac{\\partial L}{\\partial \\hat{x}_i} - \\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\right] = 0$$\n<h4 id=\"proprieta-di-stabilita\">Propriet√† di Stabilit√†</h4>\n<p><strong>Teorema (Decorrelazione)</strong>: Il termine correttivo $-\\frac{\\hat{x}_i}{m}\\sum_{j=1}^{m}\\frac{\\partial L}{\\partial \\hat{x}_j}\\hat{x}_j$ rimuove automaticamente la componente del gradiente correlata con l&rsquo;input normalizzato.</p>\n<p><strong>Conseguenza per l&rsquo;Ottimizzazione</strong>: Questo ricentramento garantisce che i gradienti non abbiano un bias sistematico in una direzione specifica, riducendo le oscillazioni durante l&rsquo;ottimizzazione e permettendo l&rsquo;uso di learning rate pi√π elevati senza instabilit√†.</p>\n<h4 id=\"implicazioni-pratiche\">Implicazioni Pratiche</h4>\n<p>Il ricentramento automatico fornisce una garanzia algebrica che:</p>\n<ol>\n<li><strong>Elimina bias direzionali</strong>: $\\sum_{i=1}^m \\frac{\\partial L}{\\partial x_i} = 0$ sempre</li>\n<li><strong>Riduce correlazioni</strong>: I gradienti sono decorrelati rispetto agli input normalizzati</li>\n<li><strong>Stabilizza l&rsquo;aggiornamento</strong>: Le oscillazioni sono naturalmente attenuate</li>\n</ol>\n<p>Questa propriet√† matematica rigorosa √® l&rsquo;unico meccanismo con dimostrazione completa che spiega perch√© la batch normalization permette learning rate pi√π elevati in modo affidabile e prevedibile.</p>\n<h2 id=\"varianti-della-batch-normalization\">Varianti della Batch Normalization</h2>\n<h3 id=\"layer-normalization\"><a href=\"/theory/deep-learning/Neural Networks/Layer Normalization\" class=\"text-blue-600 hover:underline\">Layer Normalization</a></h3>\n<p>Invece di normalizzare across il batch, la layer normalization normalizza across le features:</p>\n$$\\mu_i = \\frac{1}{H} \\sum_{j=1}^{H} x_{i,j}$$\n$$\\sigma_i^2 = \\frac{1}{H} \\sum_{j=1}^{H} (x_{i,j} - \\mu_i)^2$$\n<p>dove $H$ √® il numero di features per ogni esempio.</p>\n<h3 id=\"instance-normalization\"><span class=\"text-gray-600\">Instance Normalization</span></h3>\n<p>Normalizza ogni feature map indipendentemente:</p>\n$$\\mu_{i,j} = \\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{i,j,h,w}$$\n<h3 id=\"group-normalization\"><span class=\"text-gray-600\">Group Normalization</span></h3>\n<p>Divide le features in gruppi e normalizza all&rsquo;interno di ogni gruppo:</p>\n$$\\mu_{i,g} = \\frac{1}{C_g HW} \\sum_{c \\in \\mathcal{G}_g} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{i,c,h,w}$$\n<p>dove $\\mathcal{G}_g$ √® il set di canali nel gruppo $g$.</p>\n<h2 id=\"analisi-della-complessita-computazionale\">Analisi della Complessit√† Computazionale</h2>\n<h3 id=\"complessita-temporale\">Complessit√† Temporale</h3>\n<p>Per un layer con $d$ features e batch size $m$:</p>\n<ul>\n<li><strong>Forward pass</strong>: $O(md)$ per calcolare media, varianza, e normalizzazione</li>\n<li><strong>Backward pass</strong>: $O(md)$ per calcolare tutti i gradienti</li>\n</ul>\n<h3 id=\"complessita-spaziale\">Complessit√† Spaziale</h3>\n<ul>\n<li><strong>Memoria aggiuntiva</strong>: $O(d)$ per memorizzare $\\gamma$, $\\beta$, statistiche di popolazione</li>\n<li><strong>Memoria temporanea</strong>: $O(md)$ per memorizzare input normalizzati durante il forward pass</li>\n</ul>\n<h2 id=\"limitazioni-teoriche\">Limitazioni Teoriche</h2>\n<h3 id=\"dipendenza-dalla-dimensione-del-batch\">Dipendenza dalla Dimensione del Batch</h3>\n<p>La batch normalization √® sensibile alla dimensione del batch. Per batch molto piccoli, le statistiche del batch diventano rumorose e possono degradare le performance. Questo √® particolarmente problematico quando:</p>\n$$\\text{Var}[\\mu_B] = \\frac{\\sigma^2}{m}$$\n<p>dove la varianza della media del batch √® inversamente proporzionale alla dimensione del batch.</p>\n<h3 id=\"discrepanza-train-test\">Discrepanza Train-Test</h3>\n<p>Esiste una discrepanza fondamentale tra il comportamento durante training (usando statistiche del batch) e test (usando statistiche di popolazione). Questa discrepanza pu√≤ causare:</p>\n<ol>\n<li><strong>Shift di distribuzione</strong> tra training e test</li>\n<li><strong>Performance degradation</strong> se le statistiche di popolazione non sono ben stimate</li>\n</ol>\n<h2 id=\"connessioni-con-teoria-dellottimizzazione\">Connessioni con Teoria dell&rsquo;Ottimizzazione</h2>\n<h3 id=\"landscape-dellottimizzazione\">Landscape dell&rsquo;Ottimizzazione</h3>\n<p>La batch normalization modifica il landscape di ottimizzazione rendendo la loss function pi√π smooth. Questo pu√≤ essere compreso attraverso l&rsquo;analisi delle derivate seconde (matrice Hessiana).</p>\n<p>Per una loss function $L(W)$, la batch normalization tende a ridurre il condition number della Hessiana:</p>\n$$\\kappa(H) = \\frac{\\lambda_{\\max}(H)}{\\lambda_{\\min}(H)}$$\n<p>dove $\\lambda_{\\max}$ e $\\lambda_{\\min}$ sono il massimo e minimo autovalore della Hessiana.</p>\n<h3 id=\"invarianza-dei-gradienti\">Invarianza dei Gradienti</h3>\n<p>Una propriet√† importante √® che la batch normalization introduce un tipo di invarianza dei gradienti. Se riscaliamo i pesi di un layer per una costante $\\alpha$:</p>\n$$W' = \\alpha W$$\n<p>l&rsquo;output dopo batch normalization rimane invariato (up to the learned parameters $\\gamma$ and $\\beta$), rendendo l&rsquo;ottimizzazione pi√π stabile.</p>\n<h2 id=\"applicazioni-pratiche-e-considerazioni\">Applicazioni Pratiche e Considerazioni</h2>\n<h3 id=\"placement-nella-rete\">Placement nella Rete</h3>\n<p>La posizione della batch normalization √® critica:</p>\n<ol>\n<li><strong>Prima dell&rsquo;attivazione</strong>: $BN(Wx + b) \\rightarrow \\text{activation}$</li>\n<li><strong>Dopo l&rsquo;attivazione</strong>: $BN(\\text{activation}(Wx + b))$</li>\n</ol>\n<p>La scelta influenza le propriet√† della normalizzazione e le performance del modello.</p>\n<h3 id=\"interazione-con-dropout\">Interazione con Dropout</h3>\n<p>La batch normalization e il dropout possono interagire in modi complessi. √à generalmente raccomandato applicare dropout dopo la batch normalization per evitare conflitti nella normalizzazione delle statistiche.</p>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>La batch normalization rappresenta un breakthrough teorico e pratico nel deep learning. La sua efficacia deriva da una combinazione di:</p>\n<ol>\n<li><strong>Stabilizzazione delle distribuzioni interne</strong></li>\n<li><strong>Effetto regolarizzante implicito</strong>  </li>\n<li><strong>Miglioramento del conditioning del problema di ottimizzazione</strong></li>\n<li><strong>Robustezza nell&rsquo;inizializzazione</strong></li>\n</ol>\n<p>Dal punto di vista matematico, la batch normalization trasforma il problema di ottimizzazione in uno pi√π trattabile, permettendo l&rsquo;addestramento efficiente di reti molto profonde. La sua formulazione elegante e le propriet√† teoriche ben definite la rendono uno strumento fondamentale nell&rsquo;arsenale del deep learning moderno.</p>\n<p>La comprensione profonda della matematica sottostante √® essenziale per utilizzare efficacemente questa tecnica e per sviluppare ulteriori miglioramenti nelle architetture neurali future.</p>"
}