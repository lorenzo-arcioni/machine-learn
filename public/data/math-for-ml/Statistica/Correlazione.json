{
  "title": "Correlazione",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>La <strong>Correlazione</strong> è una misura statistica che quantifica la forza e la direzione della relazione lineare tra due variabili. È uno strumento fondamentale nell&rsquo;analisi dei dati per comprendere come le variabili si comportano congiuntamente e per identificare potenziali relazioni predittive.</p>\n<h2 id=\"1-definizione-e-concetti-fondamentali\"><strong>1. Definizione e Concetti Fondamentali</strong></h2>\n<p>La correlazione misura il grado in cui due variabili tendono a variare insieme. È importante distinguere tra:</p>\n<ul>\n<li><strong>Correlazione</strong>: Misura l&rsquo;associazione tra variabili, ma non implica causalità.</li>\n<li><strong>Causalità</strong>: Indica che una variabile influenza direttamente l&rsquo;altra.</li>\n</ul>\n<blockquote>\n<p><strong>Nota importante</strong>: &ldquo;Correlazione non implica causalità&rdquo; - due variabili possono essere correlate senza che una causi l&rsquo;altra, ad esempio a causa di una terza variabile confondente.</p>\n</blockquote>\n<h3 id=\"11-covarianza\"><strong>1.1. Covarianza</strong></h3>\n<p>Prima di definire la correlazione, introduciamo la <strong>covarianza</strong>, che misura come due variabili variano congiuntamente rispetto alle loro medie.</p>\n<p>Per due variabili aleatorie $X$ e $Y$, la covarianza è definita come:</p>\n$$\n\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\n$$\n<p>Per un campione di $n$ osservazioni $\\{(x_i, y_i)\\}_{i=1}^n$, la covarianza campionaria è:</p>\n$$\n\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n$$\n<p>Dove:\n- $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ è la media campionaria di $X$\n- $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ è la media campionaria di $Y$</p>\n<p><strong>Interpretazione:</strong>\n- Se $\\text{Cov}(X, Y) > 0$: le variabili tendono a crescere insieme (relazione positiva)\n- Se $\\text{Cov}(X, Y) < 0$: quando una cresce, l&rsquo;altra tende a decrescere (relazione negativa)\n- Se $\\text{Cov}(X, Y) = 0$: le variabili non hanno una relazione lineare</p>\n<p><strong>Limite della covarianza:</strong> Il valore della covarianza dipende dalle unità di misura delle variabili, rendendo difficile interpretarne la forza della relazione.</p>\n<h2 id=\"2-coefficiente-di-correlazione-di-pearson\"><strong>2. Coefficiente di Correlazione di Pearson</strong></h2>\n<p>Il <strong>Coefficiente di Correlazione di Pearson</strong> (o correlazione lineare) risolve il problema della dipendenza dalle unità di misura normalizzando la covarianza.</p>\n<h3 id=\"21-definizione\"><strong>2.1. Definizione</strong></h3>\n<p>Per due variabili aleatorie $X$ e $Y$, il coefficiente di correlazione di Pearson è:</p>\n$$\n\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n$$\n<p>Dove:\n- $\\sigma_X = \\sqrt{\\mathbb{V}[X]}$ è la deviazione standard di $X$\n- $\\sigma_Y = \\sqrt{\\mathbb{V}[Y]}$ è la deviazione standard di $Y$</p>\n<p>Per un campione, il coefficiente di correlazione campionario è:</p>\n$$\nr_{X,Y} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\n<h3 id=\"22-proprieta\"><strong>2.2. Proprietà</strong></h3>\n<ol>\n<li><strong>Valori limitati</strong>: $-1 \\leq r_{X,Y} \\leq 1$</li>\n<li><strong>Simmetria</strong>: $r_{X,Y} = r_{Y,X}$</li>\n<li><strong>Invarianza per trasformazioni lineari</strong>: Se $Y' = aY + b$ con $a > 0$, allora $r_{X,Y'} = r_{X,Y}$</li>\n<li><strong>Indipendenza</strong>: Se $X$ e $Y$ sono indipendenti, allora $r_{X,Y} = 0$ (ma non viceversa)</li>\n</ol>\n<h3 id=\"23-interpretazione\"><strong>2.3. Interpretazione</strong></h3>\n<ul>\n<li>$r = 1$: <strong>correlazione positiva perfetta</strong> (relazione lineare crescente)</li>\n<li>$r = -1$: <strong>correlazione negativa perfetta</strong> (relazione lineare decrescente)</li>\n<li>$r = 0$: <strong>assenza di correlazione lineare</strong> (non implica assenza di relazione non lineare)</li>\n<li>$|r| \\in [0.7, 1]$: correlazione forte</li>\n<li>$|r| \\in [0.4, 0.7]$: correlazione moderata</li>\n<li>$|r| \\in [0, 0.4]$: correlazione debole</li>\n</ul>\n<h3 id=\"24-dimostrazione-math_inline_56\"><strong>2.4. Dimostrazione: $-1 \\leq r \\leq 1$</strong></h3>\n<p><strong>Teorema:</strong> Per ogni coppia di variabili $X$ e $Y$, vale $-1 \\leq r_{X,Y} \\leq 1$.</p>\n<p><strong>Dimostrazione:</strong></p>\n<p>Consideriamo le variabili standardizzate:</p>\n$$\nZ_X = \\frac{X - \\mathbb{E}[X]}{\\sigma_X}, \\quad Z_Y = \\frac{Y - \\mathbb{E}[Y]}{\\sigma_Y}\n$$\n<p>Allora $\\mathbb{E}[Z_X] = \\mathbb{E}[Z_Y] = 0$ e $\\mathbb{V}[Z_X] = \\mathbb{V}[Z_Y] = 1$.</p>\n<p>Per ogni $t \\in \\mathbb{R}$, consideriamo la variabile aleatoria $W = Z_X + tZ_Y$. Poiché la varianza è sempre non negativa:</p>\n$$\n\\mathbb{V}[W] = \\mathbb{V}[Z_X + tZ_Y] \\geq 0\n$$\n<p>Sviluppando:</p>\n$$\n\\mathbb{V}[Z_X + tZ_Y] = \\mathbb{V}[Z_X] + t^2\\mathbb{V}[Z_Y] + 2t\\text{Cov}(Z_X, Z_Y)\n$$\n$$\n= 1 + t^2 + 2t\\rho_{X,Y} \\geq 0\n$$\n<p>Questa è una disuguaglianza valida per ogni $t \\in \\mathbb{R}$. Il discriminante della parabola deve essere non positivo:</p>\n$$\n\\Delta = 4\\rho_{X,Y}^2 - 4 \\leq 0\n$$\n$$\n\\rho_{X,Y}^2 \\leq 1\n$$\n<p>Quindi $-1 \\leq \\rho_{X,Y} \\leq 1$. $\\square$</p>\n<h2 id=\"3-forme-alternative-e-matriciali\"><strong>3. Forme Alternative e Matriciali</strong></h2>\n<h3 id=\"31-forma-matriciale\"><strong>3.1. Forma Matriciale</strong></h3>\n<p>Data una matrice di dati $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ con $n$ osservazioni e $p$ variabili, la <strong>matrice di correlazione</strong> $\\mathbf{R} \\in \\mathbb{R}^{p \\times p}$ ha elementi:</p>\n$$\nR_{ij} = r_{X_i, X_j}\n$$\n<p>Dove $X_i$ e $X_j$ sono le colonne $i$-esima e $j$-esima di $\\mathbf{X}$.</p>\n<p>La matrice di correlazione può essere calcolata come:</p>\n$$\n\\mathbf{R} = \\mathbf{D}^{-1/2} \\mathbf{S} \\mathbf{D}^{-1/2}\n$$\n<p>Dove:\n- $\\mathbf{S}$ è la matrice di covarianza\n- $\\mathbf{D} = \\text{diag}(S_{11}, S_{22}, \\ldots, S_{pp})$ è la matrice diagonale delle varianze</p>\n<h3 id=\"32-formula-computazionale\"><strong>3.2. Formula Computazionale</strong></h3>\n<p>Una forma alternativa utile per il calcolo è:</p>\n$$\nr_{X,Y} = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2 - (\\sum x_i)^2} \\sqrt{n\\sum y_i^2 - (\\sum y_i)^2}}\n$$\n<p>Questa formula evita il calcolo esplicito delle medie ed è più efficiente computazionalmente.</p>\n<h2 id=\"4-correlazione-di-rango\"><strong>4. Correlazione di Rango</strong></h2>\n<p>Quando i dati non seguono una distribuzione normale o contengono outlier, è preferibile utilizzare misure di correlazione non parametriche basate sui ranghi.</p>\n<h3 id=\"41-correlazione-di-spearman\"><strong>4.1. Correlazione di Spearman</strong></h3>\n<p>Il <strong>coefficiente di correlazione di Spearman</strong> $\\rho_s$ è il coefficiente di Pearson applicato ai ranghi delle osservazioni:</p>\n$$\n\\rho_s = r_{\\text{rank}(X), \\text{rank}(Y)}\n$$\n<p>Se non ci sono ranghi ripetuti, esiste una formula semplificata:</p>\n$$\n\\rho_s = 1 - \\frac{6\\sum_{i=1}^{n} d_i^2}{n(n^2-1)}\n$$\n<p>Dove $d_i$ è la differenza tra i ranghi di $x_i$ e $y_i$.</p>\n<p><strong>Caratteristiche:</strong>\n- Misura la correlazione monotona (non necessariamente lineare)\n- Robusto agli outlier\n- Appropriato per dati ordinali</p>\n<h3 id=\"42-correlazione-di-kendall\"><strong>4.2. Correlazione di Kendall</strong></h3>\n<p>Il <strong>coefficiente tau di Kendall</strong> ($\\tau$) misura la concordanza tra le coppie di osservazioni:</p>\n$$\n\\tau = \\frac{n_c - n_d}{\\frac{1}{2}n(n-1)}\n$$\n<p>Dove:\n- $n_c$ è il numero di coppie concordanti\n- $n_d$ è il numero di coppie discordanti</p>\n<p>Una coppia $(x_i, y_i)$ e $(x_j, y_j)$ è:\n- <strong>Concordante</strong> se $(x_i - x_j)(y_i - y_j) > 0$\n- <strong>Discordante</strong> se $(x_i - x_j)(y_i - y_j) < 0$</p>\n<h2 id=\"5-test-di-significativita\"><strong>5. Test di Significatività</strong></h2>\n<h3 id=\"51-test-per-la-correlazione-di-pearson\"><strong>5.1. Test per la Correlazione di Pearson</strong></h3>\n<p>Per testare l&rsquo;ipotesi nulla $H_0: \\rho = 0$ contro $H_1: \\rho \\neq 0$, si utilizza la statistica:</p>\n$$\nt = r\\sqrt{\\frac{n-2}{1-r^2}}\n$$\n<p>Che segue una distribuzione $t$ di Student con $n-2$ gradi di libertà.</p>\n<p><strong>Procedura:</strong>\n1. Calcolare la statistica $t$\n2. Confrontare con il valore critico $t_{\\alpha/2, n-2}$\n3. Se $|t| > t_{\\alpha/2, n-2}$, rifiutare $H_0$</p>\n<h3 id=\"52-intervallo-di-confidenza\"><strong>5.2. Intervallo di Confidenza</strong></h3>\n<p>Per costruire un intervallo di confidenza per $\\rho$, si utilizza la <strong>trasformazione di Fisher</strong>:</p>\n$$\nz = \\frac{1}{2}\\ln\\left(\\frac{1+r}{1-r}\\right) = \\text{arctanh}(r)\n$$\n<p>La statistica $z$ è approssimativamente normale con:</p>\n$$\nz \\sim \\mathcal{N}\\left(\\frac{1}{2}\\ln\\left(\\frac{1+\\rho}{1-\\rho}\\right), \\frac{1}{n-3}\\right)\n$$\n<p>L&rsquo;intervallo di confidenza al $(1-\\alpha)\\%$ per $\\rho$ è:</p>\n$$\n\\left[\\tanh\\left(z - z_{\\alpha/2}\\sqrt{\\frac{1}{n-3}}\\right), \\tanh\\left(z + z_{\\alpha/2}\\sqrt{\\frac{1}{n-3}}\\right)\\right]\n$$\n<h2 id=\"6-correlazione-parziale\"><strong>6. Correlazione Parziale</strong></h2>\n<p>La <strong>correlazione parziale</strong> misura la correlazione tra due variabili $X$ e $Y$ controllando l&rsquo;effetto di una o più variabili addizionali $Z$.</p>\n<h3 id=\"61-definizione\"><strong>6.1. Definizione</strong></h3>\n<p>La correlazione parziale tra $X$ e $Y$ dato $Z$ è:</p>\n$$\nr_{XY \\cdot Z} = \\frac{r_{XY} - r_{XZ}r_{YZ}}{\\sqrt{1-r_{XZ}^2}\\sqrt{1-r_{YZ}^2}}\n$$\n<p><strong>Interpretazione:</strong>\n- Misura la correlazione &ldquo;pura&rdquo; tra $X$ e $Y$ rimuovendo l&rsquo;effetto lineare di $Z$\n- Utile per identificare correlazioni spurie dovute a variabili confondenti</p>\n<h3 id=\"62-esempio\"><strong>6.2. Esempio</strong></h3>\n<p>Consideriamo:\n- $X$: consumo di gelato\n- $Y$: numero di annegamenti\n- $Z$: temperatura</p>\n<p>Potremmo osservare una forte correlazione tra $X$ e $Y$ ($r_{XY} > 0$), ma questa potrebbe essere spurie causata da $Z$. La correlazione parziale $r_{XY \\cdot Z}$ potrebbe essere vicina a zero, indicando che la temperatura è la vera causa comune.</p>\n<h2 id=\"7-correlazione-e-regressione\"><strong>7. Correlazione e Regressione</strong></h2>\n<p>Esiste una stretta relazione tra correlazione e regressione lineare semplice.</p>\n<h3 id=\"71-relazione-con-il-coefficiente-di-determinazione\"><strong>7.1. Relazione con il Coefficiente di Determinazione</strong></h3>\n<p>Nella regressione lineare semplice $y = w_0 + w_1 x + \\epsilon$, il coefficiente di determinazione $R^2$ è uguale al quadrato del coefficiente di correlazione:</p>\n$$\nR^2 = r_{X,Y}^2\n$$\n<p><strong>Dimostrazione:</strong></p>\n<p>Il coefficiente di determinazione è definito come:</p>\n$$\nR^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\n$$\n<p>Nella regressione semplice, può essere dimostrato che:</p>\n$$\nR^2 = \\frac{[\\sum(x_i - \\bar{x})(y_i - \\bar{y})]^2}{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2} = r_{X,Y}^2\n$$\n<p>Questo segue direttamente dalla definizione di $r_{X,Y}$. $\\square$</p>\n<h3 id=\"72-relazione-con-il-coefficiente-angolare\"><strong>7.2. Relazione con il Coefficiente Angolare</strong></h3>\n<p>Il coefficiente angolare della regressione lineare può essere espresso in termini di correlazione:</p>\n$$\nw_1 = r_{X,Y} \\frac{\\sigma_Y}{\\sigma_X}\n$$\n<p>Questo mostra che:\n- Il segno di $w_1$ è determinato dal segno di $r_{X,Y}$\n- La pendenza dipende sia dalla correlazione che dal rapporto delle deviazioni standard</p>\n<h2 id=\"8-limitazioni-e-precauzioni\"><strong>8. Limitazioni e Precauzioni</strong></h2>\n<h3 id=\"81-correlazione-misura-solo-relazioni-lineari\"><strong>8.1. Correlazione misura solo relazioni lineari</strong></h3>\n<p>Il coefficiente di Pearson cattura solo relazioni lineari. Relazioni non lineari (quadratiche, esponenziali, ecc.) possono avere $r \\approx 0$ anche se esiste una forte dipendenza.</p>\n<p><strong>Esempio:</strong> Per la relazione $y = x^2$ con $x \\in [-1, 1]$, si ha $r_{X,Y} = 0$ ma esiste una dipendenza funzionale perfetta.</p>\n<h3 id=\"82-outlier-e-valori-influenti\"><strong>8.2. Outlier e Valori Influenti</strong></h3>\n<p>La correlazione di Pearson è sensibile agli outlier, che possono:\n- Aumentare artificialmente la correlazione\n- Mascherare correlazioni reali\n- Invertire il segno della correlazione</p>\n<p><strong>Soluzione:</strong> Utilizzare correlazioni di rango (Spearman, Kendall) o identificare e trattare gli outlier.</p>\n<h3 id=\"83-correlazione-spuria\"><strong>8.3. Correlazione Spuria</strong></h3>\n<p>Due variabili possono essere correlate senza alcuna relazione causale diretta, a causa di:\n- <strong>Variabile confondente</strong>: Una terza variabile influenza entrambe\n- <strong>Causalità inversa</strong>: La direzione causale è opposta a quella ipotizzata\n- <strong>Coincidenza</strong>: Correlazione puramente casuale</p>\n<h3 id=\"84-dimensione-del-campione\"><strong>8.4. Dimensione del Campione</strong></h3>\n<p>Con campioni piccoli:\n- La correlazione può essere significativa anche se debole\n- La stima è meno affidabile\n- È necessario verificare la significatività statistica</p>\n<h2 id=\"9-applicazioni-pratiche\"><strong>9. Applicazioni Pratiche</strong></h2>\n<h3 id=\"91-analisi-esplorativa-dei-dati\"><strong>9.1. Analisi Esplorativa dei Dati</strong></h3>\n<ul>\n<li>Identificazione di variabili correlate prima della modellazione</li>\n<li>Costruzione di matrici di correlazione per dataset multivariati</li>\n<li>Visualizzazione tramite heatmap o scatter plot</li>\n</ul>\n<h3 id=\"92-feature-selection\"><strong>9.2. Feature Selection</strong></h3>\n<ul>\n<li>Rimozione di variabili altamente correlate (multicollinearità)</li>\n<li>Selezione delle feature più correlate con la variabile target</li>\n<li>Identificazione di gruppi di variabili ridondanti</li>\n</ul>\n<h3 id=\"93-validazione-di-ipotesi\"><strong>9.3. Validazione di Ipotesi</strong></h3>\n<ul>\n<li>Verifica di relazioni teoriche tra variabili</li>\n<li>Test di ipotesi scientifiche</li>\n<li>Analisi di serie temporali (autocorrelazione)</li>\n</ul>\n<h3 id=\"94-diagnostica-di-modelli\"><strong>9.4. Diagnostica di Modelli</strong></h3>\n<ul>\n<li>Analisi dei residui nella regressione</li>\n<li>Verifica dell&rsquo;indipendenza degli errori</li>\n<li>Identificazione di pattern non catturati dal modello</li>\n</ul>\n<h2 id=\"10-conclusioni\"><strong>10. Conclusioni</strong></h2>\n<p>La correlazione è uno strumento fondamentale per:\n- Quantificare relazioni lineari tra variabili\n- Guidare l&rsquo;analisi esplorativa e la modellazione\n- Identificare potenziali relazioni causali (che richiedono ulteriori indagini)</p>\n<p><strong>Punti chiave da ricordare:</strong>\n- Correlazione ≠ Causalità\n- Il coefficiente di Pearson misura solo relazioni lineari\n- Utilizzare misure alternative (Spearman, Kendall) per dati non parametrici\n- Verificare sempre la significatività statistica\n- Considerare la correlazione parziale per controllare variabili confondenti\n- Esplorare visivamente i dati prima di interpretare la correlazione</p>\n<p><strong>Risorse aggiuntive:</strong>\n- <em>Statistical Methods</em> - Snedecor &amp; Cochran\n- <em>The Elements of Statistical Learning</em> - Hastie, Tibshirani, Friedman\n- <em>Introduction to Statistical Learning</em> - James, Witten, Hastie, Tibshirani</p>"
}