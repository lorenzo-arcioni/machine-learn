{
  "title": "📚 Singular Value Decomposition (SVD)",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"introduzione\">📌 Introduzione</h2>\n<p>La <strong>Singular Value Decomposition (SVD)</strong> è una delle tecniche più potenti e versatili dell&rsquo;algebra lineare. È un metodo che ci permette di &ldquo;guardare dentro&rdquo; una matrice e capire il comportamento profondo della trasformazione lineare che rappresenta, rivelandone le direzioni principali di azione e le dimensioni lungo cui opera.</p>\n<p>Immagina una trasformazione come qualcosa che prende un insieme di punti e li sposta, allunga, schiaccia o ruota nello spazio. La SVD ci permette di scomporre questa trasformazione complessa in tre passaggi semplici e interpretabili:</p>\n<ul>\n<li>una <strong>rotazione iniziale</strong>, che riallinea il sistema di riferimento;</li>\n<li>una <strong>scalatura</strong>, che modifica le lunghezze lungo gli assi principali;</li>\n<li>una <strong>rotazione finale</strong>, che orienta il risultato nello spazio d&rsquo;uscita.</li>\n</ul>\n<p>Questa capacità di scomporre e reinterpretare trasformazioni la rende una tecnica centrale in molti campi: compressione delle immagini, riconoscimento facciale, sistemi di raccomandazione, ricerca semantica nei testi, e tanto altro.</p>\n<h2 id=\"intuizione-caso-math_inline_26\">🔍 Intuizione (caso $m > n$)</h2>\n<p>Diamo ora un&rsquo;intuizione dietro alla SVD per $m > n$ (in modo analogo si può definire anche per $m < n$).</p>\n<p>Sia $\\{\\mathbf v_1, \\cdots, \\mathbf v_n\\}$ una base dello spazio di partenza, con $\\mathbf v_i \\in \\mathbb{R}^n$, e sia $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ una matrice qualsiasi. Applichiamo $\\mathbf{A}$ ai vettori della base:</p>\n$$\n\\mathbf A \\mathbf v_1 = \\sigma_1 \\mathbf u_1,\\quad \\cdots,\\quad \\mathbf A \\mathbf v_n = \\sigma_n \\mathbf u_n\n$$\n<p>dove:</p>\n<ul>\n<li>$\\{\\mathbf u_i\\}_{i=1}^n$ sono versori (tali che $||\\mathbf u_i|| = 1$).</li>\n<li>$\\{\\sigma_i\\}_{i=1}^n$ i fattori di scala dei $\\mathbf u_i$.</li>\n</ul>\n<p>Quindi, dato che abbiamo una base, ogni vettore $\\mathbf x$ può essere scritto come:</p>\n$$\n\\mathbf x = z_1 \\mathbf v_1 + \\cdots + z_n \\mathbf v_n\n$$\n<p>dove $z_i$ è la coordinata $i$ del vettore $\\mathbf x$ nella base $\\{\\mathbf v_1, \\cdots, \\mathbf v_n\\}$.</p>\n<p>Applicando $\\mathbf A$:</p>\n$$\n\\mathbf A \\mathbf x = \\sigma_1 z_1 \\mathbf u_1 + \\cdots + \\sigma_n z_n \\mathbf u_n\n$$\n<p>Sostituendo termini, si ottiene la forma canonica:</p>\n$$\n\\mathbf{A} \\mathbf{x} = \\sum_{i=1}^{n} \\sigma_i z_i \\mathbf{u}_i\n$$\n<p>Cerchiamo ora di scrivere questa formula in forma matriciale:</p>\n<h3 id=\"1-raggruppiamo-le-proiezioni\">1. <strong>Raggruppiamo le proiezioni</strong></h3>\n<p>Definiamo il vettore completo delle proiezioni:\n$$\n  \\mathbf{z} =\n  \\begin{bmatrix}\n  z_1 \\\\\n  \\vdots \\\\\n  z_n\n  \\end{bmatrix}\n  = V^{-1} \\mathbf{x}\n$$\ndove $V^{-1}$ è la matrice del cambiamento di base, che mappa da $\\{\\mathbf{e}_1, \\cdots, \\mathbf{e}_n\\}$ a $\\{\\mathbf{v}_1, \\cdots, \\mathbf{v}_n\\}$.</p>\n<h3 id=\"2-applichiamo-la-scalatura\">2. <strong>Applichiamo la scalatura</strong></h3>\n<p>Ora moltiplichiamo ogni componente $z_i$ per $\\sigma_i$, cioè applichiamo la matrice diagonale dei valori singolari:</p>\n$$\n\\Sigma \\mathbf z\n=\n\\begin{bmatrix}\n\\sigma_1 &        &        \\\\\n        & \\ddots &        \\\\\n        &        & \\sigma_n \\\\\\\\\n\\hline\\\\\n0       & \\cdots & 0       \\\\\n\\vdots  &        & \\vdots  \\\\\n0       & \\cdots & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nz_1 \\\\\n\\vdots \\\\\nz_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma_1 z_1 \\\\\n\\vdots \\\\\n\\sigma_n z_n \\\\\\\\\n\\hline\\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n$$\n<p>dove $\\Sigma\\in\\mathbb R^{m\\times n}$ è rettangolare (solo $n$ valori singolari).</p>\n<h3 id=\"3-sommiamo-lungo-le-direzioni-math_inline_50\">3. <strong>Sommiamo lungo le direzioni $\\mathbf{u}_i$</strong></h3>\n<p>Costruiamo il vettore somma come combinazione lineare pesata dei vettori $\\mathbf{u}_i$:</p>\n$$\n\\sum_{i=1}^n (\\sigma_i z_i) \\mathbf{u}_i = U_n (\\Sigma \\mathbf{z}) = U_n \\Sigma V^T \\mathbf{x}\n$$\n<p>dove </p>\n$$\nU_n =\n\\begin{bmatrix}\n\\vert & & \\vert \\\\\n\\mathbf{u}_1 & \\cdots & \\mathbf{u}_n \\\\\n\\vert & & \\vert\n\\end{bmatrix}\n$$\n<p>è la matrice che contiene i vettori $\\mathbf{u}_i$ come colonne.</p>\n<h3 id=\"4-completamento-di-math_inline_53-a-matrice-math_inline_54\">4. Completamento di $U$ a matrice $m\\times m$</h3>\n<p>Poiché $m>n$, servono ancora $m-n$ colonne $\\mathbf u_{n+1},\\dots,\\mathbf u_m$ per ottenere\n$$\nU =\n\\begin{bmatrix}\n\\vert & & \\vert & \\vert & & \\vert \\\\\n\\mathbf{u}_1 & \\cdots & \\mathbf{u}_n & \\mathbf{u}_{n+1} & \\cdots & \\mathbf{u}_m\\\\\n\\vert & & \\vert & \\vert & & \\vert \\\\\n\\end{bmatrix}\n\\in\\mathbb R^{m\\times m},\n$$\nuna matrice completa. Si sceglie $\\{\\mathbf u_{n+1},\\dots,\\mathbf u_m\\}$ in modo che la matrice $U$ contenga tutti vettori indipendenti.</p>\n<h3 id=\"5-forma-finale-della-trasformazione\">5. <strong>Forma finale della trasformazione</strong></h3>\n<p>Poiché il risultato vale per ogni $\\mathbf{x}$, allora:</p>\n$$\n\\mathbf{A} \\mathbf{x} = U \\Sigma V^T \\mathbf{x} \\quad \\Rightarrow \\quad \\boxed{\\mathbf{A} = U \\Sigma V^T}\n$$\n<p>Questa è la <strong>forma matriciale canonica della trasformazione lineare $\\mathbf{A}$</strong>.</p>\n<h3 id=\"ma-di-preciso-a-cosa-e-servito\">Ma di preciso a cosa è servito?</h3>\n<p>Senza ulteriori <strong>restrizioni sulla base</strong> $\\{\\mathbf v_i\\}$, la scomposizione ottenuta non ci fornisce reali vantaggi. In particolare, se i vettori $\\mathbf v_i$ non sono scelti in modo ortonormale, allora:</p>\n<ul>\n<li>la matrice $V$ <strong>non è ortogonale</strong> (cioè $V^T \\ne V^{-1}$, il che è molto svantaggioso computazionalmente),</li>\n<li>la decomposizione $A = U \\Sigma V^T$ <strong>non ha una struttura particolarmente utile o interpretabile</strong>, ed è solo una riscrittura di $A = U \\Sigma V^{-1}$,</li>\n<li>le coordinate $z_i$ devono essere ottenute tramite inversione $V^{-1} \\mathbf{x}$ e <strong>non come semplici proiezioni scalari</strong>, come nel caso ortonormale.</li>\n</ul>\n<p>Inoltre, se la base $\\{\\mathbf v_i\\}$ <strong>non contiene gli autovettori (normalizzati) di $A^T A$</strong>, la decomposizione non evidenzia il comportamento geometrico fondamentale di $A$, ovvero:</p>\n<ul>\n<li>\n<p>$A^T A$ è <strong>simmetrica e semi-definita positiva</strong>, e rappresenta l&rsquo;operazione:  </p>\n<blockquote>\n<p><em>applica $A$, poi proietta indietro con $A^T$</em>.<br />\n  La sua geometria descrive <strong>come $A$ deforma gli input</strong>.</p>\n</blockquote>\n</li>\n<li>\n<p>I suoi autovettori (che diventano i $\\mathbf v_i$ in SVD) <strong>danno le direzioni principali</strong> lungo cui $A$ agisce per stretching o compressione.</p>\n</li>\n</ul>\n<p>Quindi, se imponiamo che $V$ contenga gli <strong>autovettori ortonormali di $A^T A$</strong> (i quali formano sempre una base per lo spazio $\\mathbb R^n$), otteniamo:</p>\n<p>✅ una base che diagonalizza $A^T A$,<br />\n✅ una decomposizione dove $V^T = V^{-1}$,<br />\n✅ una decomposizione dove $U \\in \\mathbb R^m$ contiene gli autovettori di $AA^T$ (ortonormali),<br />\n✅ valori singolari $\\sigma_i = \\sqrt{\\lambda_i}$ (radici degli autovalori),<br />\n✅ una visione chiara: $A$ <strong>ruota</strong>, <strong>scala</strong>, e poi <strong>ruota di nuovo</strong>.</p>\n<p>👉 In sintesi: <strong>la SVD è potente solo se $V$ (e quindi $U$) sono ortogonali</strong> e legati allo spettro di $A^T A$ e $A A^T$, rispettivamente. Altrimenti, la decomposizione perde la sua forza geometrica e computazionale.</p>\n<p>Quindi, scegliendo la base $V$ che ha per vettori gli autovettori della matrice $A^TA$, abbiamo la garanzia che i vettori $\\{\\mathbf v_i\\}_{i=1}^n$ siano ortogonali tra loro e, inoltre, che puntino verso le direzioni dove avviene il massimo stretch della trasformazione $A$; i cosidetti <strong>Right Singular Vectors</strong>. In questo modo, otteniamo un&rsquo;altra importante proprietà: <strong>la matrice $U$ contiene gli autovettori di $AA^T$</strong>, i cosidetti <strong>Left Singular Vectors</strong>.</p>\n<p>Questa proprietà è fondamentale perché:</p>\n<ul>\n<li>\n<p><strong>Scomposizione geometrica</strong>:<br />\n  $$\n  A = U \\, \\Sigma \\, V^T\n  $$\n  descrive $A$ come rotazione → stretching → rotazione.</p>\n</li>\n<li>\n<p><strong>Riduzione del problema</strong>:<br />\n  Permette di lavorare in basi dove $A$ agisce diagonalmente, semplificando l&rsquo;analisi e la risoluzione dei sistemi.</p>\n</li>\n<li>\n<p><strong>Stabilità numerica</strong>:<br />\n  Le matrici ortogonali $U$ e $V$ non amplificano errori, rendendo la SVD ideale per il calcolo numerico.</p>\n</li>\n<li>\n<p><strong>Compressione e filtraggio</strong>:<br />\n  I valori singolari ordinano l&rsquo;importanza delle direzioni → utile per approssimazione a rango ridotto e PCA.</p>\n</li>\n<li>\n<p><strong>Pseudoinversa e minimi quadrati</strong>:<br />\n  La SVD fornisce una soluzione ottima anche in presenza di sistemi non invertibili o sovradeterminati.</p>\n</li>\n</ul>\n<h2 id=\"proprieta-spettrali-di-math_inline_102-e-math_inline_103\">👻 Proprietà spettrali di $A^T A$ e $A A^T$</h2>\n<p><strong>Ipotesi</strong><br />\nSia $\\{\\mathbf v_i\\}_{i=1}^n$ un insieme di vettori ortonormali tali che<br />\n$$\nA^T A\\,\\mathbf v_i = \\sigma_i^2\\,\\mathbf v_i,\n\\qquad i=1,\\dots,n\n$$<br />\ncioè i $\\mathbf v_i$ sono già autovettori di $A^T A$ (e quindi ortogonali) con autovalori $\\sigma_i^2$.</p>\n<p><strong>Obiettivo</strong><br />\nDimostrare che<br />\n1. $\\{\\mathbf u_i\\}$, definiti da $\\mathbf u_i = \\tfrac1{\\sigma_i}A\\,\\mathbf v_i$, sono autovettori di $A A^T$.<br />\n2. Gli autovalori corrispondenti sono anch&rsquo;essi $\\sigma_i^2$.</p>\n<ol>\n<li>\n<p><strong>Definizione di $\\mathbf u_i$</strong><br />\n    Poiché $\\mathbf v_i$ è autovettore di $A^T A$ con autovalore $\\sigma_i^2$, poniamo<br />\n    $$\n      \\mathbf u_i \\;:=\\;\\frac{A\\,\\mathbf v_i}{\\|A\\,\\mathbf v_i\\|}\n      = \\frac{A\\,\\mathbf v_i}{\\sigma_i}.\n    $$<br />\n    Perché $\\sigma_i > 0$ (valore singolare), questa definizione è ben posta e $\\|\\mathbf u_i\\|=1$.</p>\n</li>\n<li>\n<p><strong>Calcolo di $A A^T\\,\\mathbf u_i$</strong><br />\n    Partiamo da<br />\n    $$\n      \\mathbf u_i = \\frac1{\\sigma_i}A\\,\\mathbf v_i\n      \\;\\Longrightarrow\\;\n      A^T\\,\\mathbf u_i = \\frac1{\\sigma_i}A^T A\\,\\mathbf v_i\n      = \\frac1{\\sigma_i}\\,\\sigma_i^2\\,\\mathbf v_i\n      = \\sigma_i\\,\\mathbf v_i.\n    $$<br />\n    Ora applichiamo $A$ a questa relazione:\n    $$\n      A A^T\\,\\mathbf u_i\n      = A\\bigl(\\sigma_i\\,\\mathbf v_i\\bigr)\n      = \\sigma_i\\,A\\,\\mathbf v_i\n      = \\sigma_i\\bigl(\\sigma_i\\,\\mathbf u_i\\bigr)\n      = \\sigma_i^2\\,\\mathbf u_i.\n    $$</p>\n</li>\n<li>\n<p><strong>Conclusione sugli autovettori di $A A^T$</strong><br />\n    Abbiamo mostrato che\n    $$\n      (A A^T)\\,\\mathbf u_i = \\sigma_i^2\\,\\mathbf u_i,\n    $$\n    dunque ciascuno $\\mathbf u_i$ è autovettore di $A A^T$ con autovalore $\\sigma_i^2$.</p>\n</li>\n<li>\n<p><strong>Ortonormalità</strong>  </p>\n<ul>\n<li>Gli $\\mathbf v_i$ erano ortonormali per ipotesi.  </li>\n<li>Gli $\\mathbf u_i$, essendo autovettori di una matrice simmetrica, sono anch&rsquo;essi ortonormali (si verifica $u_i^T u_j=0$ per $i\\neq j$ e $=1$ per $i=j$ in modo analogo al caso di $v_i$).</li>\n</ul>\n</li>\n</ol>\n<p><strong>Risultato finale</strong><br />\n- $A^T A$ ha autovettori $\\{\\mathbf v_i\\}$ con autovalori $\\{\\sigma_i^2\\}$.<br />\n- $A A^T$ ha autovettori $\\{\\mathbf u_i\\}$ con gli stessi autovalori $\\{\\sigma_i^2\\}$.<br />\n- Entrambe le famiglie di autovettori sono ortonormali.</p>\n<h2 id=\"interpretazione-geometrica\">📐 Interpretazione Geometrica</h2>\n<p>Questa formula mostra come la SVD scompone ogni trasformazione lineare in una <strong>sequenza ordinata</strong> di operazioni:</p>\n<ol>\n<li><strong>Rotazione/Riflessione</strong> (o cambio di base) del vettore di input nello spazio delle $\\mathbf{v}_i$, tramite $\\mathbf{V}^T = V^{-1}$.</li>\n<li><strong>Scalatura anisotropa</strong> lungo queste direzioni, con coefficienti $\\sigma_i$.</li>\n<li><strong>Rotazione/Riflessione</strong> finale nello spazio delle $\\mathbf{u}_i$, tramite $\\mathbf{U}$.</li>\n</ol>\n<p>Questa decomposizione è non solo utile dal punto di vista computazionale, ma rivela anche la <strong>struttura interna</strong> della trasformazione stessa.</p>\n<p>Quindi sia $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ una matrice qualsiasi. La <strong>SVD</strong> è una fattorizzazione della matrice nella forma:</p>\n$$\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}\n$$\n<p>dove:</p>\n<ul>\n<li>$\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$: matrice ortogonale delle <strong>left singular vectors</strong></li>\n<li>$\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}$: matrice diagonale con valori $\\sigma_i$ detti <strong>singular values</strong> in ordine decrescente</li>\n<li>$\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$: matrice ortogonale delle <strong>right singular vectors</strong></li>\n</ul>\n<h2 id=\"approfondimento\">🧠 Approfondimento</h2>\n<p>Ogni trasformazione lineare $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, per quanto complessa, può essere sempre <strong>scomposta in tre fasi geometriche</strong>:</p>\n$$\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n$$\n<p>Questa decomposizione corrisponde alla seguente <strong>pipeline geometrica</strong>:</p>\n<h3 id=\"1-rotazione-iniziale-dello-spazio-math_inline_148\">🔹 1. Rotazione iniziale dello spazio ($\\mathbf{V}^T$)</h3>\n<ul>\n<li>\n<p>Ruota (o riflette) lo spazio originale per allinearlo alle direzioni principali della trasformazione.</p>\n</li>\n<li>\n<p>Trasforma ogni vettore $\\mathbf{A}$ nel nuovo sistema di riferimento ortonormale: $\\mathbf z= \\mathbf V^⊤ \\mathbf x$</p>\n</li>\n<li>\n<p>Intuitivamente, è come esprimere $\\mathbf{A}$ in una nuova base ortogonale costruita sui concetti principali della trasformazione.</p>\n</li>\n</ul>\n<h3 id=\"2-scalatura-assiale-math_inline_152\">🔹 2. Scalatura assiale ($\\mathbf{\\Sigma}$)</h3>\n<ul>\n<li>$\\mathbf{\\Sigma}$ è una matrice <strong>diagonale</strong> che <strong>scala</strong> ogni coordinata <strong>indipendentemente</strong> lungo un asse ortogonale.</li>\n<li>I valori diagonali $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq 0$ sono i <strong>valori singolari</strong> e rappresentano <strong>quanto</strong> viene deformato lo spazio in ciascuna direzione.</li>\n<li>Nessuna rotazione o shearing: solo <strong>dilatazione o contrazione</strong>.</li>\n<li>In questo passaggio avviene il &ldquo;cuore&rdquo; della trasformazione: le direzioni principali vengono <strong>ingrandite o compresse</strong> in base alla loro <strong>importanza informativa</strong>.</li>\n</ul>\n<h3 id=\"3-rotazione-finale-math_inline_155\">🔹 3. Rotazione finale ($\\mathbf{U}$)</h3>\n<ul>\n<li>\n<p>Dopo che il vettore è stato proiettato e scalato lungo le direzioni principali, $\\mathbf{U}$ applica una rotazione (o riflessione) per posizionare il risultato nello spazio d&rsquo;uscita: quello di $\\mathbb{R}^m$ se $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$.</p>\n</li>\n<li>\n<p>La trasformazione $\\mathbf{U}$ agisce come un cambio di base nello spazio del codominio:\nessa assegna un significato geometrico e direzionale al risultato, stabilendo in quale direzione finale andrà ogni componente scalata.</p>\n</li>\n<li>\n<p>Geometricamente, $\\mathbf{U}$ determina l&rsquo;orientamento dell&rsquo;ellisse risultante: mentre $\\mathbf{V}^T$ allinea l&rsquo;ingresso alle direzioni principali e $\\Sigma$ deforma (scala) secondo quelle direzioni, $\\mathbf{U}$ decide come disporre quella deformazione nello spazio originale d&rsquo;uscita.</p>\n</li>\n</ul>\n<h3 id=\"esempio-visivo\">🌌 Esempio Visivo</h3>\n<p>Immagina un <strong>cerchio unitario</strong> nello spazio 2D. Applichiamo $\\mathbf{A}$ tramite la sua SVD:</p>\n<p><img src=\"/images/posts/svd_pipeline.png\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n<br></p>\n<p>L&rsquo;immagine illustra geometricamente la decomposizione a valori singolari (SVD) di una matrice $\\mathbf A$, mostrando come può essere interpretata come una sequenza di trasformazioni.</p>\n<ul>\n<li><strong>$\\mathbf{V}^T$</strong> ruota il cerchio nella direzione delle <strong>direzioni principali</strong> (quelle dove deve avvenire lo stretching).</li>\n<li><strong>$\\mathbf{\\Sigma}$</strong> schiaccia o dilata il cerchio lungo i suoi assi principali, trasformandolo in un ellisse.</li>\n<li><strong>$\\mathbf{U}$</strong> riallinea (ruota o riflette) l&rsquo;ellisse nell&rsquo;output space, secondo le direzioni principali dell&rsquo;immagine di $\\mathbf{A}$, cioè gli autovettori di $\\mathbf{A} \\mathbf{A}^T$.</li>\n</ul>\n<p>Risultato: da una figura simmetrica e isotropa (cerchio), otteniamo un oggetto deformato ma <strong>con significato direzionale</strong>.</p>\n<h3 id=\"forma-matriciale-compatta\">🧮 Forma matriciale compatta</h3>\n<p>Tutti i risultati precedenti possono essere espressi elegantemente in forma matriciale. A partire dalla <strong>SVD</strong>:</p>\n$$\nA = U \\Sigma V^T,\n$$\n<p>possiamo scrivere:</p>\n<ul>\n<li>Per il prodotto $A^T A$:</li>\n</ul>\n$$\nA^T A = (U \\Sigma V^T)^T (U \\Sigma V^T)\n= V \\Sigma^T U^T U \\Sigma V^T\n= V \\Sigma^T \\Sigma V^T.\n$$\n<p>Poiché $U^T U = I$, otteniamo:</p>\n$$\nA^T A = V (\\Sigma^T \\Sigma) V^T,\n$$\n<p>che mostra che $A^T A$ è <strong>diagonalizzabile</strong> tramite $V$, e ha <strong>autovalori</strong> dati da $\\sigma_i^2$.</p>\n<ul>\n<li>Analogamente, per $A A^T$:</li>\n</ul>\n$$\nA A^T = (U \\Sigma V^T)(U \\Sigma V^T)^T\n= U \\Sigma V^T V \\Sigma^T U^T\n= U \\Sigma \\Sigma^T U^T.\n$$\n<p>Quindi:</p>\n$$\nA A^T = U (\\Sigma \\Sigma^T) U^T,\n$$\n<p>che mostra che $A A^T$ è <strong>diagonalizzabile</strong> tramite $U$, e ha gli <strong>stessi autovalori</strong> $\\sigma_i^2$ (tranne eventuali zeri in più se $m \\ne n$).</p>\n<p>Queste espressioni confermano in forma compatta che:\n- Le colonne di $V$ sono autovettori di $A^T A$,\n- Le colonne di $U$ sono autovettori di $A A^T$,\n- Gli autovalori in entrambi i casi sono i quadrati dei valori singolari contenuti in $\\Sigma$.</p>\n<ul>\n<li>\n<p><strong>Caso $m > n$:</strong><br />\n  $U \\in \\mathbb{R}^{m \\times m}$ è ortogonale. I primi $n$ autovettori corrispondono agli autovalori $\\sigma_i^2 > 0$ di $A A^\\top$; i restanti $m - n$ sono autovettori associati all&rsquo;autovalore $0$.  </p>\n</li>\n<li>\n<p><strong>Caso $n > m$:</strong><br />\n  $V \\in \\mathbb{R}^{n \\times n}$ è ortogonale. I primi $m$ autovettori corrispondono agli autovalori $\\sigma_i^2 > 0$ di $A^\\top A$; i restanti $n - m$ sono autovettori associati all&rsquo;autovalore $0$.  </p>\n</li>\n</ul>\n<h2 id=\"proprieta\">📏 Proprietà</h2>\n<ul>\n<li>I vettori di $\\mathbf{U}$ e $\\mathbf{V}$ formano <strong>basi ortonormali</strong> per lo spazio delle righe e delle colonne.</li>\n<li>I <strong>valori singolari</strong> $\\sigma_i$ rappresentano la <strong>quantità di informazione</strong> trasportata lungo ciascuna direzione.</li>\n<li>$\\text{rank}(\\mathbf{A}) =$ numero di valori singolari non nulli.</li>\n<li>Può essere vista come una generalizzazione dell&rsquo;autodecomposizione (eigendecomposition) per matrici rettangolari.</li>\n</ul>\n<h2 id=\"riduzione-dimensionale-tramite-truncated-svd\">🔧 Riduzione Dimensionale tramite Truncated SVD</h2>\n<p>Spesso, molte direzioni in cui la matrice $\\mathbf{A}$ proietta i dati risultano <strong>trascurabili o rumorose</strong>. La <strong>Truncated SVD</strong> consiste nel conservare <strong>solo i primi $k \\ll \\min(m, n)$ valori singolari</strong> più grandi:</p>\n$$\n\\mathbf{A} \\approx \\mathbf{A}_k = \\mathbf{U}_k \\mathbf{\\Sigma}_k \\mathbf{V}_k^T\n$$\n<ul>\n<li>$\\mathbf{U}_k \\in \\mathbb{R}^{m \\times k}$ contiene i primi $k$ vettori singolari sinistri.</li>\n<li>$\\mathbf{\\Sigma}_k \\in \\mathbb{R}^{k \\times k}$ contiene i primi $k$ valori singolari (i più grandi).</li>\n<li>$\\mathbf{V}_k^T \\in \\mathbb{R}^{k \\times n}$ contiene i primi $k$ vettori singolari destri.</li>\n</ul>\n<p>🔍 <strong>Perché funziona?</strong></p>\n<ol>\n<li>\n<p>I valori singolari $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r$ sono ordinati in modo decrescente:<br />\n<strong>i primi rappresentano le direzioni in cui $\\mathbf{A}$ ha la massima &ldquo;energia&rdquo;</strong> (varianza proiettata).</p>\n</li>\n<li>\n<p>Geometricamente:  </p>\n</li>\n<li>Ogni direzione $\\mathbf{v}_i$ corrisponde a un asse principale su cui $\\mathbf{A}$ proietta i dati.</li>\n<li>Il valore $\\sigma_i$ misura <strong>quanto è importante quella direzione</strong>.</li>\n<li>\n<p>Troncando dopo $k$, scartiamo le direzioni meno influenti.</p>\n</li>\n<li>\n<p>Matematicamente:<br />\n   $$ \n   \\mathbf{A}_k = \\arg\\min_{\\text{rank-}k\\text{ matrices } \\mathbf{A}} \\|\\mathbf{A} - \\mathbf{A}\\|_F \n   $$\n   cioè $\\mathbf{A}_k$ è la <strong>migliore approssimazione di rango $k$</strong> di $\\mathbf{A}$ in norma di Frobenius (somma dei quadrati degli scarti).</p>\n</li>\n</ol>\n<p>🚀 <strong>Utilità</strong>:\n- <strong>Compressione dei dati</strong>: conserviamo solo l&rsquo;informazione essenziale.\n- <strong>Riduzione del rumore</strong>: eliminiamo direzioni deboli o casuali.\n- <strong>Estrazione di concetti latenti</strong>: fondamentale in NLP, raccomandazione, clustering.</p>\n<h2 id=\"differenze-tra-svd-ed-eigendecomposition\">🧾 Differenze tra SVD ed Eigendecomposition</h2>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Tipo matrice</th>\n<th>Fattorizzazione</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SVD</td>\n<td>qualsiasi</td>\n<td>$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$</td>\n</tr>\n<tr>\n<td>Eigendecomp</td>\n<td>solo quadrate</td>\n<td>$\\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}$</td>\n</tr>\n</tbody>\n</table>\n<p>Nota: SVD è più generale e robusta.</p>\n<h2 id=\"algoritmo-per-calcolare-la-svd\">⚙️ Algoritmo per Calcolare la SVD</h2>\n<p>Sebbene abbiamo discusso a fondo il significato geometrico e le proprietà della decomposizione SVD, <strong>non abbiamo ancora affrontato il modo in cui essa viene effettivamente calcolata</strong>.</p>\n<p>In breve, calcolare la SVD di una matrice $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ significa trovare tre matrici $\\mathbf{U}, \\mathbf{\\Sigma}, \\mathbf{V}$ tali che:</p>\n$$\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n$$\n<p>L&rsquo;algoritmo per trovare queste matrici <strong>non si basa semplicemente su manipolazioni algebriche dirette</strong>, ma coinvolge:</p>\n<ul>\n<li>la <strong>diagonalizzazione</strong> delle matrici simmetriche $A^T A$ e $A A^T$,</li>\n<li>il <strong>calcolo degli autovalori e autovettori</strong> di queste matrici,</li>\n<li>e la costruzione dei vettori singolari normalizzati a partire da queste informazioni.</li>\n</ul>\n<p>In pratica, si ricorre a <strong>tecniche numeriche stabili</strong> come:\n- il metodo delle <strong>rotazioni di Jacobi</strong>,\n- la <strong>bidiagonalizzazione</strong> di $\\mathbf{A}$ tramite trasformazioni di Householder,\n- e successivi algoritmi iterativi per l&rsquo;estrazione dei valori singolari.</p>\n<p>👉 Per i dettagli sul <strong>procedimento numerico e algoritmico</strong> per ottenere la SVD, rimandiamo alla seguente nota dedicata:</p>\n<p>📎 <span class=\"text-gray-600\">Algoritmo per la SVD</span></p>\n<h2 id=\"limiti-della-svd\">⚠️ Limiti della SVD</h2>\n<ul>\n<li>Complessità computazionale elevata: $\\mathcal{O}(mn\\min(m,n))$</li>\n<li>Poco scalabile su matrici <strong>molto grandi</strong> (es. $10^6 \\times 10^6$)</li>\n<li>Non si adatta bene a <strong>matrici dinamiche</strong> o sparse (come nel linguaggio naturale)</li>\n<li>Richiede <strong>riaddestramento completo</strong> per ogni nuovo documento/termine</li>\n</ul>\n<h2 id=\"vantaggi\">✅ Vantaggi</h2>\n<ul>\n<li>Estrae automaticamente <strong>relazioni latenti</strong></li>\n<li>Riduce il rumore e le ridondanze</li>\n<li>Ottimo per compattare l&rsquo;informazione</li>\n<li>Facilita la <strong>similarità semantica</strong> tra oggetti (es. documenti, parole)</li>\n</ul>\n<h2 id=\"applicazioni\">🧪 Applicazioni</h2>\n<ul>\n<li><strong>NLP</strong>: <a href=\"/theory/nlp/Semantica/Vector Semantics/Latent Semantic Analysis\" class=\"text-blue-600 hover:underline\">Latent Semantic Analysis</a> (LSA)</li>\n<li><strong>Motori di raccomandazione</strong>: filtraggio collaborativo</li>\n<li><strong>Visione artificiale</strong>: compressione di immagini</li>\n<li><strong>Machine Learning</strong>: preprocessing per PCA e clustering</li>\n</ul>\n<h2 id=\"conclusione\">🧭 Conclusione</h2>\n<p>La SVD non è solo una tecnica matematica ma un <strong>principio guida</strong> per strutturare, comprimere e interpretare dati complessi. In ambito linguistico, è lo strumento matematico fondante di molte tecniche semantiche moderne.</p>"
}