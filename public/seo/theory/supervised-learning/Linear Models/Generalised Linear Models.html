<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generalized Linear Models (GLMs) | Supervised Learning | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="supervised learning, labeled data, classification, regression, model, data, learning">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Generalized Linear Models (GLMs)">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/supervised-learning/Linear Models/Generalised Linear Models">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Generalized Linear Models (GLMs)">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/supervised-learning/Linear Models/Generalised Linear Models">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Generalized Linear Models (GLMs)",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/supervised-learning/Linear Models/Generalised Linear Models",
      "datePublished": "2025-08-07T01:41:11.012Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/supervised-learning/Linear Models/Generalised Linear Models" class="react-redirect">üöÄ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>Generalized Linear Models (GLMs)</h1>
                <div class="meta">
                    <strong>Topic:</strong> Supervised Learning | 
                    <strong>Updated:</strong> 07/08/2025
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<p>I <strong>Generalized Linear Models (GLMs)</strong> sono una classe flessibile di modelli statistici che generalizzano la regressione lineare per target che seguono distribuzioni appartenenti alla famiglia esponenziale. Forniscono un framework unificato per trattare problemi con target continui, binari o discreti. I GLMs sono ampiamente utilizzati in vari campi, come l&rsquo;econometria, la biostatistica e il machine learning, grazie alla loro capacit√† di adattarsi a diversi tipi di dati e di modellare relazioni complesse tra variabili.</p>
<h2 id="componenti-principali-dei-glms">Componenti principali dei GLMs</h2>
<p>Un GLM √® definito da tre componenti principali: la <strong>distribuzione dei target</strong>, la <strong>funzione di link</strong> e la <strong>funzione di varianza</strong>. Questi componenti lavorano insieme per modellare la relazione tra le variabili indipendenti (input) e la variabile dipendente (target).</p>
<h3 id="1-distribuzione-dei-target">1. Distribuzione dei target</h3>
<p>I target $y$ appartengono alla <strong>famiglia esponenziale delle distribuzioni</strong>, che √® una classe di distribuzioni che pu√≤ essere espressa nella forma:</p>
$$
p(y|\theta, \phi) = \exp\left(\frac{y\theta - b(\theta)}{\phi} + c(y, \phi)\right)
$$
<p>Dove:
- <strong>$\theta$</strong>: √® il <strong>parametro naturale</strong>, una trasformazione del parametro della distribuzione che facilita la rappresentazione nella forma esponenziale. Questo parametro √® legato alla media della distribuzione.
- <strong>$b(\theta)$</strong>: √® la <strong>funzione cumulativa logaritmica</strong>, che determina la normalizzazione della distribuzione e la relazione tra $\theta$ e la media della distribuzione.
- <strong>$\phi$</strong>: √® il <strong>parametro di dispersione</strong>, che controlla la variabilit√† dei dati intorno alla media. Questo parametro √® rilevante per distribuzioni come la gamma e la normale, ma √® fissato per distribuzioni come Bernoulli e Poisson.
- <strong>$c(y, \phi)$</strong>: √® la <strong>funzione di normalizzazione</strong>, che garantisce che la distribuzione si integri a 1 e dipende dalla specifica distribuzione utilizzata.</p>
<p><strong>Esempi comuni di distribuzioni nella famiglia esponenziale</strong>:
- <strong>Normale</strong>: $\theta = \mu$, $\phi = \sigma^2$.
- <strong>Bernoulliana</strong>: $\theta = \log\frac{\pi}{1-\pi}$.
- <strong>Poissoniana</strong>: $\theta = \log(\lambda)$.</p>
<h3 id="2-funzione-di-link">2. Funzione di link</h3>
<p>La <strong>funzione di link</strong> collega la media condizionale $\mu = \mathbb{E}[y|x]$ al <strong>predictor lineare</strong> $\eta$, che √® una combinazione lineare delle variabili indipendenti:</p>
$$
g(\mu) = \eta = \mathbf{w}^\top \mathbf{x} + b
$$
<p>Dove:
- <strong>$g(\cdot)$</strong>: √® la funzione di link, che trasforma la media $\mu$ in una scala lineare.
- <strong>$\eta$</strong>: √® il <strong>predictor lineare</strong>, una combinazione lineare degli input $\mathbf{x}$ con pesi $\mathbf{w}$ e un termine di bias $b$.</p>
<p><strong>Esempi comuni di funzioni di link</strong>:
- <strong>Identit√†</strong>: $g(\mu) = \mu$ (usata nella regressione lineare).
- <strong>Logit</strong>: $g(\mu) = \log\frac{\mu}{1-\mu}$ (usata nella regressione logistica).
- <strong>Logaritmica</strong>: $g(\mu) = \log(\mu)$ (usata nella regressione di Poisson).</p>
<h3 id="3-funzione-di-varianza">3. Funzione di varianza</h3>
<p>Nei GLMs, la varianza di $y$ √® una funzione della media $\mu$:</p>
$$
\text{Var}(y) = \phi v(\mu)
$$
<p>Dove:
- <strong>$v(\mu)$</strong>: √® la <strong>funzione di varianza</strong>, che dipende dalla specifica distribuzione utilizzata. Ad esempio, per la distribuzione normale, $v(\mu) = 1$, mentre per la distribuzione di Poisson, $v(\mu) = \mu$.</p>
<h2 id="formulazione-matematica">Formulazione matematica</h2>
<h3 id="4-probabilita-condizionale">4. Probabilit√† condizionale</h3>
<p>La probabilit√† condizionale di $y$ dato $\mathbf{x}$ e i parametri $\mathbf{w}$ √® data da:</p>
$$
p(y | \mathbf{x}, \mathbf{w}) = \exp\left(\frac{y\theta - b(\theta)}{\phi} + c(y, \phi)\right)
$$
<p>Dove:
- <strong>$\theta = g^{-1}(\eta)$</strong>: √® il parametro naturale, ottenuto applicando l&rsquo;inversa della funzione di link al predictor lineare $\eta$.
- <strong>$\eta = \mathbf{w}^\top \mathbf{x}$</strong>: √® il predictor lineare.</p>
<h3 id="5-stima-dei-parametri">5. Stima dei parametri</h3>
<p>La stima dei parametri $\mathbf{w}$ avviene massimizzando la <strong>log-verosimiglianza</strong>:</p>
$$
\mathcal{L}(\mathbf{w}) = \sum_{i=1}^N \left( \frac{y_i \theta_i - b(\theta_i)}{\phi} + c(y_i, \phi) \right)
$$
<p>Dove:
- <strong>$\theta_i = g^{-1}(\mathbf{w}^\top \mathbf{x}_i)$</strong>: √® il parametro naturale per l&rsquo;osservazione $i$-esima.
- <strong>Metodi iterativi</strong>: come il <strong>reweighted least squares</strong> o il <strong>metodo di Newton-Raphson</strong> sono utilizzati per massimizzare la log-verosimiglianza.</p>
<h3 id="6-predizione">6. Predizione</h3>
<p>Per un nuovo input $\mathbf{x}^*$, la predizione avviene in due passaggi:
1. Calcolo del predictor lineare:
   $$
   \eta^* = \mathbf{w}^\top \mathbf{x}^*
   $$
2. Applicazione della funzione di link inversa per ottenere la media predetta:
   $$
   \mu^* = g^{-1}(\eta^*)
   $$</p>
<h2 id="esempi-comuni-di-glms">Esempi comuni di GLMs</h2>
<h3 id="7-regressione-lineare">7. <a href="/theory/supervised-learning/Linear Models/Regressione Lineare" class="text-blue-600 hover:underline">Regressione Lineare</a></h3>
<ul>
<li><strong>Distribuzione</strong>: Normale.</li>
<li><strong>Funzione di link</strong>: Identit√† $g(\mu) = \mu$.</li>
<li><strong>Modello</strong>: $y = \mathbf{w}^\top \mathbf{x} + \epsilon$, con $\epsilon \sim \mathcal{N}(0, \sigma^2)$.</li>
</ul>
<h3 id="8-regressione-logistica">8. <a href="/theory/supervised-learning/Linear Models/Regressione Logistica" class="text-blue-600 hover:underline">Regressione Logistica</a></h3>
<ul>
<li><strong>Distribuzione</strong>: Bernoulliana.</li>
<li><strong>Funzione di link</strong>: Logit $g(\mu) = \log\frac{\mu}{1-\mu}$.</li>
<li><strong>Modello</strong>: $\pi = \sigma(\mathbf{w}^\top \mathbf{x})$, dove $\sigma(\cdot)$ √® la funzione sigmoide.</li>
</ul>
<h3 id="9-regressione-di-poisson">9. Regressione di Poisson</h3>
<ul>
<li><strong>Distribuzione</strong>: Poissoniana.</li>
<li><strong>Funzione di link</strong>: Logaritmica $g(\mu) = \log(\mu)$.</li>
<li><strong>Modello</strong>: $\lambda = \exp(\mathbf{w}^\top \mathbf{x})$.</li>
</ul>
<h2 id="proprieta-dei-glms">Propriet√† dei GLMs</h2>
<h3 id="10-flessibilita">10. Flessibilit√†</h3>
<p>I GLMs sono estremamente flessibili e possono modellare diversi tipi di dati scegliendo la distribuzione e una funzione di link appropriata. Ad esempio, possono essere utilizzati per dati continui, binari o di conteggio.</p>
<h3 id="11-proprieta-analitiche">11. Propriet√† analitiche</h3>
<p>La linearit√† nei parametri consente una stima efficiente tramite metodi iterativi standard, come il metodo di Newton-Raphson o il reweighted least squares.</p>
<h3 id="12-limitazioni">12. Limitazioni</h3>
<ul>
<li><strong>Assunzione di indipendenza</strong>: I GLMs assumono che i target siano indipendenti e distribuiti in modo identico (i.i.d.).</li>
<li><strong>Scelta della distribuzione e della funzione di link</strong>: Scelte errate possono compromettere le prestazioni del modello.</li>
</ul>
<h2 id="espansioni-e-variazioni">Espansioni e variazioni</h2>
<h3 id="1-glmms-generalized-linear-mixed-models">1. GLMMs (Generalized Linear Mixed Models)</h3>
<p>I <strong>GLMMs</strong> sono un&rsquo;estensione dei GLMs che include effetti randomici per modellare dati gerarchici o correlati. Sono particolarmente utili in contesti come studi longitudinali o dati con strutture gerarchiche.</p>
<h3 id="2-regolarizzazione">2. Regolarizzazione</h3>
<p>Tecniche di regolarizzazione come <strong>Lasso</strong>, <strong>Ridge</strong> o <strong>Elastic Net</strong> possono essere applicate ai GLMs per evitare overfitting e migliorare la generalizzazione del modello.</p>
<h3 id="3-modelli-non-lineari">3. Modelli non lineari</h3>
<p>I GLMs possono essere combinati con funzioni di base (ad esempio, polinomi o spline) per modellare relazioni non lineari tra le variabili indipendenti e il target.</p>
<h2 id="conclusione">Conclusione</h2>
<p>I <strong>Generalized Linear Models (GLMs)</strong> sono uno strumento potente e versatile per l&rsquo;analisi statistica e il machine learning. Grazie alla loro flessibilit√† e alla capacit√† di adattarsi a diversi tipi di dati, i GLMs sono ampiamente utilizzati in molti campi applicativi. Tuttavia, √® importante comprendere le loro assunzioni e limitazioni per utilizzarli in modo efficace.</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> supervised learning, labeled data, classification, regression, model, data, learning</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/supervised-learning/Linear Models/Generalised Linear Models">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'Generalized Linear Models (GLMs)',
          page_location: 'http://localhost:3000/theory/supervised-learning/Linear Models/Generalised Linear Models'
        });
      }
    </script>
</body>
</html>