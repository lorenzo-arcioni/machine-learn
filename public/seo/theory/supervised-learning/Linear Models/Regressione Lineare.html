<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regressione Lineare | Supervised Learning | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="supervised learning, labeled data, classification, regression, model, data, training, learning">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Regressione Lineare">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/supervised-learning/Linear Models/Regressione Lineare">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Regressione Lineare">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/supervised-learning/Linear Models/Regressione Lineare">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Regressione Lineare",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/supervised-learning/Linear Models/Regressione Lineare",
      "datePublished": "2025-08-23T17:28:54.567Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/supervised-learning/Linear Models/Regressione Lineare" class="react-redirect">üöÄ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>Regressione Lineare</h1>
                <div class="meta">
                    <strong>Topic:</strong> Supervised Learning | 
                    <strong>Updated:</strong> 23/08/2025
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<p>La <strong>Regressione Lineare</strong> √® un modello statistico utilizzato per descrivere la relazione tra una variabile dipendente (target) e una o pi√π variabili indipendenti (predittori). Assume una relazione lineare tra le variabili e minimizza l&rsquo;errore quadratico medio.</p>
<h2 id="1-formulazione-generale"><strong>1. Formulazione Generale</strong></h2>
<p>Assumiamo di avere un dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$, dove $x_i \in \mathbb R$ sono le variabili indipendenti e $y_i \in \mathbb R$ la variabile dipendente.
Un modello di regressione lineare ha la forma:</p>
$$
y_i = f(\mathbf{x}_i) + \epsilon_i = \mathbf{x}_i^T \mathbf{w} + \epsilon_i = (1)w_0 + x_iw_1 + \epsilon_i
$$
<p>Dove:
- $y_i \in \mathbb R$ √® la variabile target,
- $\mathbf{x}_i = \begin{bmatrix} 1 \\ x_i \end{bmatrix}$, dove gli $x_i$ sono le variabili indipendenti (features),
- $\mathbf{w} = \begin{bmatrix} w_0 \\ w_1 \end{bmatrix} \in \mathbb R^2$ sono i coefficienti del modello (parametri da stimare),
- $1$ √® il cosi detto bias,
- $\epsilon_i$ √® l&rsquo;errore (rumore) che segue una <a href="/theory/math-for-ml/Probabilit√†/Distribuzioni/Distribuzione Normale" class="text-blue-600 hover:underline">distribuzione normale</a> $\mathcal{N}(0, \sigma^2)$.</p>
<p>Quindi possiamo riscrivere il modello in forma matriciale come:</p>
$$
\mathbf{y} = \mathbf{X} \mathbf{w} + \mathbf{\epsilon}
$$
<p>Dove:
- $\mathbf{y}$√® il vettore delle variabili dipendenti $n \times 1$,
- $\mathbf{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix}$√® la matrice delle variabili indipendenti $n \times 2$,
- $\mathbf{w}$√® il vettore dei parametri $2 \times 1$,
- $\mathbf{\epsilon}$ √® il vettore degli errori $n \times 1$.</p>
<p>Nel caso pi√π generale (<strong>regressione multipla</strong>), considerando il dataset diventa $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$. In questo caso, la formula vettoriale diventa:</p>
$$
y_i = \mathbf{x}_i^T \mathbf{w} + \mathbf{\epsilon}_i
$$
<p>In forma matriciale:</p>
$$
\mathbf{y} = \mathbf{X} \mathbf{w} + \mathbf{\epsilon}
$$
<p>Dove:
- $\mathbf{y}$√® il vettore delle variabili dipendenti $n \times 1$,
$$
\mathbf{X} = \begin{bmatrix}
1 & \text{------} \mathbf{x}_1^T \text{------} \\
1 & \text{------} \mathbf{x}_2^T \text{------} \\
\vdots & \vdots \\
1 & \text{------} \mathbf{x}_n^T \text{------}
\end{bmatrix}
$$ 
√® la matrice dei dati con dimensioni $n \times (m+1)$, dove $m$ sono le variabili indipendenti.
- $\mathbf{w} = \begin{bmatrix} w_0 \\ w_1 \\ \vdots \\ w_m \end{bmatrix} \in \mathbb R^{m+1}$ sono i coefficienti del modello (parametri da stimare),
- $\boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix} \in \mathbb R^n$ sono i rumori (errore) che seguono una distribuzione normale $\mathcal{N}(0, \sigma^2)$.</p>
<p>Nel caso <strong>multivariato</strong>, il dataset diventa $\mathcal{D} = \{(\mathbf{x}_i, \mathbf y_i)\}_{i=1}^n$, dove $\mathbf x_i \in \mathbb R^m$ sono i vettori di variabili indipendenti e $\mathbf y_i \in \mathbb R^p$ i vettori di variabili dipendenti. In questo caso, la formula vettoriale diventa:</p>
$$
\mathbf{y}_i = \mathbf{x}_i^T \mathbf{W} + \mathbf{\large \epsilon}_i
$$
<p>Volendo quindi rappresentare il nostro modello di regressione lineare in forma matriciale (considerando l&rsquo;intero dataset), possiamo definire:</p>
$$
\underbrace{\begin{bmatrix}
\text{------} \mathbf{y}_1^\top \text{------} \\
\text{------} \mathbf{y}_2^\top \text{------} \\
\vdots \\
\text{------} \mathbf{y}_n^\top \text{------}
\end{bmatrix}}_{\large \mathbf{Y}}
=
\underbrace{\begin{bmatrix}
1 \ |\text{------} \mathbf{x}_1^\top \text{------}\\
1 \ |\text{------} \mathbf{x}_2^\top \text{------}\\
\vdots \\
1 \ |\text{------} \mathbf{x}_n^\top \text{------}
\end{bmatrix}}_{\large \mathbf{X}}
\underbrace{\begin{bmatrix}
| & | &  & | \\
\mathbf{w}_1 & \mathbf{w}_2 & \dots & \mathbf{w}_p \\
| & | &  & | 
\end{bmatrix}}_{\large \mathbf{W}}
+ 
\underbrace{\begin{bmatrix}
\text{------} \boldsymbol{\epsilon}_1^\top \text{------} \\
\text{------} \boldsymbol{\epsilon}_2^\top \text{------} \\
\vdots \\
\text{------} \boldsymbol{\epsilon}_n^\top \text{------}
\end{bmatrix}}_{\large{\boldsymbol \epsilon}}
$$
<p>Dove:
- $\mathbf{Y}$ √® la matrice delle variabili dipendenti $n \times p$,
- $\mathbf{X}$ √® la matrice delle variabili indipendenti $n \times (m+1)$,
- $\mathbf{W}$ √® la matrice dei coefficienti $(m+1) \times p$, e $\mathbf w_i = \begin{bmatrix} w_{0,i} \\ w_{1,i} \\ \vdots \\ w_{m,i} \end{bmatrix}$ con $i \in [p]$,
- $\large{\boldsymbol \epsilon}$ √® la matrice degli errori $n \times p$, e $\boldsymbol{\epsilon}_i \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$ con $i \in [p]$.</p>
<h2 id="2-assunzioni-della-regressione-lineare"><strong>2. Assunzioni della Regressione Lineare</strong></h2>
<ol>
<li><strong>Linearit√†</strong>: La relazione tra le variabili indipendenti e la variabile dipendente √® lineare. Quindi $\mathbf y$ si puoÃÄ scrivere come una combinazione lineare delle variabili indipendenti $\mathbf x$ e degli errori: $\mathbf y = \mathbf x^T \mathbf w + \large{\mathbf \epsilon}$.</li>
<li><strong>Indipendenza e Normalit√†</strong>: Gli errori $\epsilon_i$ sono indipendenti tra loro e seguono una distribuzione normale $\epsilon_i \sim\mathcal{N}(0, \sigma^2)$. Questo implica che le variabili dipendenti sono distribuite normalmente con:<ul>
<li>Media: $\mathbb E[\mathbf y | \mathbf x] = f(\mathbf x) = \mathbf x^T \mathbf w$.</li>
<li><strong>Proof:</strong>
    $\mathbb E[\mathbf y | \mathbf  x] = \mathbb E[f(\mathbf x) + \epsilon | \mathbf x] = \underbrace{\mathbb E[f(\mathbf x)|\mathbf x]}_\text{Una volta fissato x, f(x) √® deterministico, quindi = f(x)} + \underbrace{\mathbb E[\epsilon |\mathbf x]}_\text{epsilon non dipende da x, quindi = 0} = f(\mathbf x) + 0 = f(\mathbf x). \ \square$</li>
<li>Varianza: $\mathbb V[\mathbf y | \mathbf x] = \sigma^2$</li>
<li><strong>Proof:</strong> $\mathbb V[\mathbf y | \mathbf x] = \mathbb V[f(\mathbf x) + \epsilon | \mathbf x] = \underbrace{\mathbb V[f(\mathbf x)|\mathbf x]}_\text{f(x) √® una costante, quindi = 0} + \mathbb V[\epsilon |\mathbf x] + 2 \cdot \underbrace{\mathbb Cov[\underbrace{f(\mathbf x)}_\text{√® costante dato x}, \epsilon | \mathbf x]}_\text{=0} = 0 + \sigma^2 + 0 = \sigma^2. \ \square$</li>
</ul>
</li>
</ol>
<p>Da questo si ottiene che:
   $$
   y_i | \mathbf x_i, \mathbf w \sim \mathcal{N}(f(\mathbf x_i), \sigma^2).
   $$
3. <strong>Assenza di multicollinearit√†</strong>: Le variabili indipendenti non devono essere linearmente dipendenti tra loro.<br />
   - Se esiste una relazione lineare tra alcune colonne della matrice <strong>$\mathbf{X}$</strong>, allora la matrice $\mathbf{X}^T \mathbf{X}$ diventa <strong>singolare</strong> (cio√® non invertibile). Questo √® problematico perch√© nella stima dei parametri con il metodo dei minimi quadrati ordinari (OLS), l&rsquo;espressione<br />
     $$
     \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
     $$
     richiede che $(\mathbf{X}^T \mathbf{X})^{-1}$ esista, il che non √® possibile se $\mathbf{X}^T \mathbf{X}$ √® singolare.
  In <a href="/theory/supervised-learning/Linear Models/Multicollinearit√†" class="text-blue-600 hover:underline">questa</a> nota viene approfondito il problema della multicollinearit√†.
   - Per risolvere la multicollinearit√† si possono adottare strategie come:
     - Rimuovere una delle variabili altamente correlate.
     - Utilizzare metodi di regressione penalizzata come <strong>Ridge Regression</strong> o <strong>Lasso</strong>.
     - Applicare una <strong>PCA (Principal Component Analysis)</strong> per trasformare le variabili indipendenti in nuove variabili non correlate.</p>
<p>Le assunzioni della regressione lineare sono importanti per garantire la robustezza del modello e la sua applicabilita in situazioni reali.</p>
<h2 id="3-stima-dei-parametri"><strong>3. Stima dei Parametri</strong></h2>
<p>La stima dei coefficienti $\mathbf{w}$ nella regressione lineare √® basata sulla minimizzazione dell&rsquo;errore quadratico medio (<strong>MSE - Mean Squared Error</strong>), che deriva direttamente dal principio della <strong>massima verosimiglianza</strong> sotto l&rsquo;assunzione di rumore gaussiano.</p>
<h3 id="31-assunzione-di-rumore-gaussiano"><strong>3.1. Assunzione di Rumore Gaussiano</strong></h3>
<p>Si assume che il rumore $\epsilon_i$ in ogni osservazione sia distribuito secondo una <strong>normale con media zero e varianza costante</strong> $\sigma^2$:</p>
$$
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$
<p>Quindi </p>
$$
p(\epsilon_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{\epsilon_i^2}{2\sigma^2} \right)
$$
<p>e dato che $\epsilon = y_i - \mathbf{x}_i^\top \mathbf{w}$, sostituendo abbiamo:</p>
$$
p(y_i \mid \mathbf{x}_i, \mathbf{w}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_i - \mathbf{x}_i^\top \mathbf{w})^2}{2\sigma^2} \right)
$$
<p>Quindi, la distribuzione condizionale della variabile dipendente $y_i$, dato l&rsquo;input $x_i$, √® anch&rsquo;essa gaussiana:</p>
$$
y_i \mid \mathbf x_i, \mathbf{w} \sim \mathcal{N}(\mathbf{x}_i^\top \mathbf{w}, \sigma^2).
$$
<h3 id="32-costruzione-della-funzione-di-verosimiglianza"><strong>3.2. Costruzione della Funzione di Verosimiglianza</strong></h3>
<p>Dati $N$ esempi indipendenti $\{(\mathbf x_i, y_i)\}_{i=1}^{N}$, la <strong>funzione di verosimiglianza</strong> √® il prodotto delle probabilit√† condizionali di tutte le osservazioni:</p>
$$
L(\mathbf{w}) = \prod_{i=1}^{N} p(y_i \mid \mathbf x_i, \mathbf{w})
$$
<p>Sostituendo la distribuzione gaussiana:</p>
$$
L(\mathbf{w}) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_i - \mathbf{x}_i^\top \mathbf{w})^2}{2\sigma^2} \right)
$$
<h3 id="33-log-verosimiglianza"><strong>3.3. Log-Verosimiglianza</strong></h3>
<p>Per facilitare il calcolo, prendiamo il logaritmo della funzione di verosimiglianza (<strong>log-likelihood</strong>):</p>
$$
\log L(\mathbf{w}) = \sum_{i=1}^{N} \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_i - \mathbf{x}_i^\top \mathbf{w})^2}{2\sigma^2} \right) \right)
$$
<p>Separando i termini:</p>
$$
\log L(\mathbf{w}) = \sum_{i=1}^{N} \left[ -\frac{1}{2} \log (2\pi\sigma^2) - \frac{(y_i - \mathbf{x}_i^\top \mathbf{w})^2}{2\sigma^2} \right] = - \frac{N}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^\top \mathbf{w})^2.
$$
<p>Prendere il logaritmo della funzione di verosimiglianza non modifica il problema di ottimizzazione perch√© il logaritmo √® una <strong>funzione monotona crescente</strong>. Questo significa che <strong>massimizzare la verosimiglianza √® equivalente a massimizzare la log-verosimiglianza</strong>:</p>
$$
\arg\max_{\mathbf{w}} L(\mathbf{w}) = \arg\max_{\mathbf{w}} \log L(\mathbf{w})
$$
<p>I principali vantaggi del logaritmo sono:
1. <strong>Trasforma il prodotto in somma</strong>, semplificando i calcoli:
   $$
   \log L(\mathbf{w}) = \sum_{i=1}^{N} \log p(y_i \mid x_i, \mathbf{w})
   $$
2. <strong>Evita problemi di precisione numerica</strong>, riducendo il rischio di underflow quando $N$ √® grande.
3. <strong>Preserva la convessit√† della funzione obiettivo</strong>, facilitando l&rsquo;ottimizzazione.</p>
<p>In sintesi, la log-verosimiglianza √® un&rsquo;utile trasformazione che rende il problema di stima pi√π semplice e numericamente stabile senza alterarne la soluzione</p>
<h3 id="34-stima-dei-parametri-con-massima-verosimiglianza"><strong>3.4. Stima dei Parametri con Massima Verosimiglianza</strong></h3>
<p>Seguiamo ora questi passaggi: 
1. <strong>Identificazione dei termini dipendenti da $\mathbf{w}$</strong> 
    - Il primo termine, $-\frac{N}{2} \log (2\pi\sigma^2)$, <strong>non dipende</strong> da $\mathbf{w}$, quindi √® una costante e pu√≤ essere ignorato nell&rsquo;ottimizzazione. 
2. <strong>Massimizzazione della log-verosimiglianza equivale a minimizzare la penalit√† quadratica</strong> 
    - Il secondo termine, $- \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^\top \mathbf{w})^2$, <strong>dipende da $\mathbf{w}$</strong> e deve essere massimizzato. 
    - Poich√© questo termine √® <strong>negativo</strong>, massimizzarlo significa <strong>minimizzare</strong> la somma dei quadrati degli errori: 
   $$ \min_{\mathbf{w}} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^\top \mathbf{w})^2 $$</p>
<p>Che in notazione matriciale generale diventa:
   $$
   \min_{W} \|Y -XW\|^2_F
   $$
   Dove:
   - $Y$ √® la matrice delle osservazioni $n \times p$
   - $X$ √® la matrice dei regressori $n \times (m+1)$
   - $W$√® la matrice dei coefficienti $(m+1) \times p$, e $w_i = \begin{bmatrix} w_{0,i} \\ w_{1,i} \\ \vdots \\ w_{m,i} \end{bmatrix}$ con $i \in [p]$</p>
<ol>
<li><strong>Il fattore $2\sigma^2$ non influisce sull&rsquo;argomento del minimo</strong> <ul>
<li>Il termine √® diviso per $2\sigma^2$, ma poich√© la varianza $\sigma^2$ √® una costante positiva, rimuoverlo <strong>non cambia la posizione del minimo</strong>. Quindi, il problema di <strong>massima verosimiglianza</strong> si riduce esattamente alla minimizzazione della somma dei quadrati degli errori, che √® la funzione di costo dell&rsquo;<strong>errore quadratico medio (MSE)</strong> nella regressione lineare.</li>
</ul>
</li>
</ol>
<p>Dividendo per $N$ otteniamo la funzione di costo <strong>MSE</strong> (<strong>Mean Squared Error</strong>):</p>
$$
MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$
<p>dove:</p>
$$
\hat{y}_i = \mathbf{x}_i^\top \mathbf{w}
$$
<p>√® la previsione del modello. O equivalentemente:</p>
$$
\hat{Y} = \mathbf{X} \mathbf{W}.
$$
<p>Che in notazione matriciale generale diventa:</p>
$$
MSE = \frac{1}{N} \|Y - \hat{Y}\|_F^2
$$
<p>Quindi, <strong>minimizzare il MSE √® equivalente alla stima di massima verosimiglianza quando si assume rumore gaussiano con varianza costante</strong>.</p>
<h3 id="35-metodi-per-minimizzare-la-funzione-di-costo-mse"><strong>3.5. Metodi per Minimizzare la Funzione di Costo (MSE)</strong></h3>
<p>Poich√© l‚ÄôMSE deriva dalla <strong>Massima Verosimiglianza (MLE)</strong> sotto l‚Äôassunzione di rumore gaussiano, possiamo stimare i parametri della regressione lineare con diversi approcci:  </p>
<h4 id="1-soluzione-analitica-minimi-quadrati-ordinari-ols-ordinary-least-squares"><strong>1Ô∏è‚É£ Soluzione Analitica: Minimi Quadrati Ordinari (OLS - Ordinary Least Squares)</strong></h4>
<ul>
<li>Trova direttamente i coefficienti che minimizzano l‚ÄôMSE.  </li>
<li>La soluzione chiusa √®:<br />
  $$
  \mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
  $$</li>
<li><strong>Limiti:</strong>  </li>
<li>Richiede l‚Äôinversione della matrice $(\mathbf{X}^\top \mathbf{X})$, che pu√≤ essere numericamente instabile in presenza di <strong>multicollinearit√†</strong>.  </li>
<li>Non √® scalabile per dataset molto grandi.  </li>
</ul>
<h4 id="2-approccio-bayesiano-massima-a-posteriori-map-maximum-a-posteriori"><strong>2Ô∏è‚É£ Approccio Bayesiano: Massima A Posteriori (MAP - Maximum A Posteriori)</strong></h4>
<ul>
<li>Estende MLE introducendo una <strong>distribuzione a priori</strong> sui parametri $\mathbf{w}$.  </li>
<li>Se il prior √® <strong>gaussiano</strong> $\mathcal{N}(0, \lambda I)$, si ottiene la <strong>Regressione Ridge</strong>.  </li>
</ul>
<h4 id="3-minimi-quadrati-con-regolarizzazione-ridge-lasso-elastic-net"><strong>3Ô∏è‚É£ Minimi Quadrati con <a href="/theory/introduction/Regolarizzazione" class="text-blue-600 hover:underline">Regolarizzazione</a> (Ridge, Lasso, Elastic Net)</strong></h4>
<p>Aggiungono una penalizzazione ai coefficienti per migliorare la <strong>stabilit√†</strong> e il <strong>controllo della complessit√†</strong> del modello:  </p>
<p>‚úÖ <strong>Ridge Regression</strong> (<em>L2-regularization</em>)<br />
  $$
  \min_{\mathbf{w}} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^\top \mathbf{w})^2 + \lambda ||\mathbf{w}||_2^2
  $$
  - Penalizza i coefficienti grandi, ma non li azzera.<br />
  - <strong>Equivalente alla MAP con prior gaussiano.</strong>  </p>
<p>‚úÖ <strong>Lasso Regression</strong> (<em>L1-regularization</em>)<br />
  $$
  \min_{\mathbf{w}} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^\top \mathbf{w})^2 + \lambda ||\mathbf{w}||_1
  $$
  - Impone <strong>sparsit√†</strong>, azzerando alcuni coefficienti.<br />
  - Seleziona automaticamente le feature pi√π importanti.  </p>
<p>‚úÖ <strong>Elastic Net</strong> (<em>combinazione di Ridge e Lasso</em>)<br />
  $$
  \min_{\mathbf{w}} \sum_{i=1}^{N} (y_i - \mathbf{x}_i^\top \mathbf{w})^2 + \lambda_1 ||\mathbf{w}||_1 + \lambda_2 ||\mathbf{w}||_2^2
  $$
  - Unisce i vantaggi di Ridge e Lasso.<br />
  - Utile quando le feature sono <strong>correlate tra loro</strong>.  </p>
<h4 id="4-soluzioni-iterative-discesa-del-gradiente-gradient-descent"><strong>4Ô∏è‚É£ Soluzioni Iterative: Discesa del Gradiente (Gradient Descent)</strong></h4>
<p>Metodo iterativo che aggiorna i coefficienti nella direzione del gradiente negativo:<br />
  $$
  \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \alpha \nabla MSE
  $$
- <strong>Vantaggi:</strong><br />
  ‚úÖ Scalabile su dataset di grandi dimensioni.<br />
  ‚úÖ Utile quando $(\mathbf{X}^\top \mathbf{X})^{-1}$ √® difficile da calcolare.</p>
<ul>
<li><strong>Varianti:</strong><br />
  üîπ <em>Batch Gradient Descent</em> (usa tutto il dataset a ogni iterazione).<br />
  üîπ <em>Stochastic Gradient Descent (SGD)</em> (aggiorna i pesi a ogni singolo campione).<br />
  üîπ <em>Mini-batch Gradient Descent</em> (compromesso tra batch e SGD).  </li>
</ul>
<h2 id="4-interpretazione-dei-coefficienti"><strong>4. Interpretazione dei Coefficienti</strong></h2>
<p>Nella regressione lineare, i coefficienti $\mathbf{w}$ rappresentano l&rsquo;effetto che una variazione unitaria di una variabile indipendente ha sulla variabile dipendente, mantenendo le altre variabili costanti.</p>
<h3 id="41-significato-dei-coefficienti"><strong>4.1. Significato dei Coefficienti</strong></h3>
<p>Consideriamo il modello di regressione lineare semplice con una sola variabile indipendente:</p>
$$
y = w_0 + w_1 x + \epsilon
$$
<p>Dove:
- $w_0$ √® <strong>l&rsquo;intercetta</strong> (bias), che rappresenta il valore previsto di $y$ quando $x = 0$.
- $w_1$ √® <strong>il coefficiente angolare</strong>, che misura la variazione di $y$ per una variazione unitaria di $x$.</p>
<p>Se $w_1 > 0$, significa che all&rsquo;aumentare di $x$, anche $y$ tende ad aumentare (relazione positiva).<br />
Se $w_1 < 0$, significa che all&rsquo;aumentare di $x$, $y$ tende a diminuire (relazione negativa).  </p>
<h3 id="42-interpretazione-nella-regressione-multipla"><strong>4.2. Interpretazione nella Regressione Multipla</strong></h3>
<p>Nel caso della regressione multipla:</p>
$$
y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_m x_m + \epsilon
$$
<p>Ogni coefficiente $w_j$ indica l&rsquo;effetto di una variazione unitaria di $x_j$ su $y$, <strong>tenendo tutte le altre variabili costanti</strong>.  </p>
<p>Esempio:
- Se $w_2 = 3$, significa che, <strong>a parit√† di tutte le altre variabili</strong>, un aumento di 1 unit√† in $x_2$ comporta un aumento medio di 3 unit√† in $y$.</p>
<h3 id="43-punteggio-standardizzato-beta-coefficients"><strong>4.3. Punteggio Standardizzato (Beta Coefficients)</strong></h3>
<p>Nella regressione lineare multipla, i coefficienti standardizzati (noti anche come <strong>Beta Coefficients</strong>) permettono di confrontare l&rsquo;importanza relativa delle variabili indipendenti eliminando l&rsquo;effetto delle diverse unit√† di misura.  </p>
<p>Quando una regressione utilizza coefficienti <strong>non standardizzati</strong>, i valori ottenuti dipendono dalle unit√† di misura delle variabili. Ci√≤ rende difficile confrontare l&rsquo;influenza relativa di ciascuna variabile indipendente sulla variabile dipendente.  </p>
<p>Per risolvere questo problema, utilizziamo i <strong>coefficienti standardizzati</strong>, definiti dalla formula:  </p>
$$
\beta_j = w_j \cdot \frac{\sigma_{x_j}}{\sigma_y}
$$
<p>Dove:<br />
- $\beta_j$ √® il coefficiente standardizzato della variabile $x_j$.<br />
- $w_j$ √® il coefficiente non standardizzato ottenuto dalla regressione.<br />
- $\sigma_{x_j}$ √® la deviazione standard della variabile indipendente $x_j$.<br />
- $\sigma_y$ √® la deviazione standard della variabile dipendente $y$.  </p>
<h4 id="interpretazione"><strong>Interpretazione</strong></h4>
<p>I coefficienti standardizzati indicano <strong>quanto cambia la variabile dipendente $y$, espressa in deviazioni standard</strong>, a seguito di una variazione di <strong>una deviazione standard</strong> nella variabile indipendente $x_j$.  </p>
<p>In altre parole:<br />
- Se $\beta_j = 0.5$, significa che un aumento di una deviazione standard in $x_j$ comporta un aumento di <strong>0.5 deviazioni standard</strong> in $y$.<br />
- Se $\beta_j = -0.3$, significa che un aumento di una deviazione standard in $x_j$ comporta una <strong>diminuzione di 0.3 deviazioni standard</strong> in $y$.  </p>
<h4 id="vantaggi-delluso-dei-coefficienti-standardizzati"><strong>Vantaggi dell&rsquo;uso dei Coefficienti Standardizzati</strong></h4>
<ul>
<li><strong>Permettono di confrontare direttamente l&rsquo;impatto delle variabili</strong>: il valore assoluto di $\beta_j$ indica l&rsquo;importanza relativa di $x_j$ rispetto alle altre variabili nel modello.  </li>
<li><strong>Eliminano il problema delle diverse unit√† di misura</strong>, rendendo il confronto pi√π intuitivo.  </li>
<li><strong>Facilitano l&rsquo;interpretazione pratica</strong> nei modelli con variabili di diversa scala (ad esempio, reddito in euro e et√† in anni).  </li>
</ul>
<p>In sintesi, l&rsquo;uso dei coefficienti standardizzati √® utile per comprendere <strong>quali variabili hanno un impatto maggiore sulla variabile dipendente</strong> e per confrontare la loro influenza in maniera oggettiva. </p>
<h3 id="44-intervalli-di-confidenza"><strong>4.4. Intervalli di Confidenza</strong></h3>
<p>Poich√© i coefficienti sono stimati da un campione, √® utile calcolare il loro intervallo di confidenza per capire la loro precisione.</p>
<p>L&rsquo;intervallo di confidenza al <strong>95%</strong> per un coefficiente $w_j$ √®:</p>
$$
[w_j - t_{\alpha/2} \cdot SE(w_j), \quad w_j + t_{\alpha/2} \cdot SE(w_j)]
$$
<p>Dove:
- $SE(w_j)$ √® l&rsquo;errore standard del coefficiente.
- $t_{\alpha/2}$ √® il valore della distribuzione $t$ di Student con $(n - m - 1)$ gradi di libert√†.</p>
<p>Se l&rsquo;intervallo include <strong>zero</strong>, il coefficiente potrebbe non essere significativo.</p>
<h3 id="45-p-value-e-significativita-statistica"><strong>4.5. p-value e Significativit√† Statistica</strong></h3>
<p>Per valutare se un coefficiente √® statisticamente significativo, si utilizza il <strong>test t</strong>:</p>
$$
t_j = \frac{w_j}{SE(w_j)}
$$
<p>Il <strong>p-value</strong> associato a $t_j$ indica la probabilit√† di osservare un valore cos√¨ estremo sotto l&rsquo;ipotesi nulla $H_0: w_j = 0$.</p>
<ul>
<li>Se $p < 0.05$, il coefficiente √® <strong>statisticamente significativo</strong> al livello del 5%.</li>
<li>Se $p > 0.05$, non abbiamo prove sufficienti per affermare che il coefficiente sia diverso da zero.</li>
</ul>
<h3 id="46-multicollinearita-e-interpretazione"><strong>4.6. Multicollinearit√† e Interpretazione</strong></h3>
<p>Se due o pi√π variabili indipendenti sono fortemente correlate, si verifica <strong>multicollinearit√†</strong>, che rende difficile interpretare i coefficienti.<br />
Un alto <strong>Variance Inflation Factor (VIF)</strong> indica multicollinearit√†:</p>
$$
VIF_j = \frac{1}{1 - R_j^2}
$$
<p>Dove $R_j^2$ √® il coefficiente di determinazione della regressione di $x_j$ sulle altre variabili indipendenti.</p>
<ul>
<li>Se $VIF > 10$, indica un problema di <strong>multicollinearit√† elevata</strong>.</li>
</ul>
<p>Per mitigare la multicollinearit√†, si possono usare:
1. <strong>Ridge Regression</strong> o <strong>Lasso Regression</strong> (penalizzazione).
2. <strong>Rimuovere variabili ridondanti</strong>.
3. <strong>Utilizzare la PCA (Principal Component Analysis)</strong>.</p>
<h3 id="conclusione"><strong>Conclusione</strong></h3>
<p>L&rsquo;interpretazione corretta dei coefficienti √® fondamentale per comprendere l&rsquo;effetto delle variabili indipendenti su $y$. √à importante considerare intervalli di confidenza, p-value e multicollinearit√† per trarre conclusioni valide dal modello.</p>
<h2 id="5-valutazione-del-modello"><strong>5. Valutazione del Modello</strong></h2>
<p>Per determinare la qualit√† di un modello di regressione, utilizziamo diverse metriche e test statistici. Questi strumenti permettono di valutare <strong>quanto bene il modello spiega la variabilit√† dei dati</strong> e <strong>se i coefficienti stimati sono statisticamente significativi</strong>.  </p>
<h3 id="51-errore-quadratico-medio-mse"><strong>5.1. Errore Quadratico Medio (MSE)</strong></h3>
<p>L&rsquo;<strong>Errore Quadratico Medio</strong> (<strong>Mean Squared Error</strong>, MSE) misura l&rsquo;accuratezza del modello calcolando la media dei quadrati degli errori di previsione.</p>
<ul>
<li>Un <strong>MSE pi√π basso</strong> indica un modello con <strong>migliore capacit√† predittiva</strong>.  </li>
<li>Poich√© l&rsquo;MSE √® espresso nelle unit√† al quadrato di $y$, non √® sempre intuitivo da interpretare. Per questo motivo, spesso si usa la <strong>Radice dell&rsquo;Errore Quadratico Medio</strong> (<strong>RMSE</strong>):  </li>
</ul>
$$
RMSE = \sqrt{MSE}
$$
<hr />
<h3 id="52-coefficiente-di-determinazione-math_inline_186"><strong>5.2. Coefficiente di Determinazione ($R^2$)</strong></h3>
<p>Il coefficiente di determinazione $R^2$ misura <strong>la proporzione della varianza della variabile dipendente spiegata dal modello</strong>:  </p>
$$
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
$$
<p>Dove:<br />
- $\sum (y_i - \hat{y}_i)^2$ rappresenta la <strong>devianza residua</strong> (errore del modello).<br />
- $\sum (y_i - \bar{y})^2$ rappresenta la <strong>devianza totale</strong> (variabilit√† totale dei dati rispetto alla loro media).<br />
- $\bar{y}$ √® la media dei valori osservati di $y$.  </p>
<p><strong>Interpretazione:</strong><br />
- $R^2$ varia tra <strong>0 e 1</strong>:
  - Un valore vicino a <strong>1</strong> indica che il modello spiega quasi tutta la variabilit√† dei dati.<br />
  - Un valore vicino a <strong>0</strong> indica che il modello ha <strong>scarsa capacit√† esplicativa</strong>.<br />
- Un $R^2$ elevato non implica necessariamente che il modello sia valido: pu√≤ essere influenzato dalla presenza di variabili irrilevanti.  </p>
<p>Per modelli con molte variabili, si preferisce il <strong>$R^2$ aggiustato</strong>, che penalizza l&rsquo;aggiunta di variabili non significative:</p>
$$
R^2_{\text{adj}} = 1 - \left( \frac{(1 - R^2)(N - 1)}{N - p - 1} \right)
$$
<p>Dove $p$ √® il numero di variabili indipendenti nel modello.  </p>
<hr />
<h3 id="53-test-f-significativita-globale-del-modello"><strong>5.3. Test F: Significativit√† Globale del Modello</strong></h3>
<p>Il <strong>Test F</strong> valuta se il modello, nel suo complesso, √® statisticamente significativo, ovvero se almeno una delle variabili indipendenti ha un effetto su $y$. L&rsquo;ipotesi nulla ($H_0$) del test √®:  </p>
$$
H_0: \quad w_1 = w_2 = ... = w_p = 0
$$
<p>Se il test F risulta significativo (p-value &lt; soglia, es. 0.05), possiamo rifiutare $H_0$, indicando che <strong>almeno una variabile indipendente ha un effetto significativo su $y$</strong>.  </p>
<p>Il valore della statistica F √® calcolato come:  </p>
$$
F = \frac{\left( \frac{R^2}{p} \right)}{\left( \frac{1 - R^2}{N - p - 1} \right)}
$$
<p>Dove:<br />
- $p$ √® il numero di variabili indipendenti.<br />
- $N$ √® il numero di osservazioni.  </p>
<p>Un valore di <strong>F alto</strong> e un <strong>p-value basso</strong> indicano un modello globalmente significativo.</p>
<hr />
<h3 id="54-p-value-dei-coefficienti"><strong>5.4. p-value dei Coefficienti</strong></h3>
<p>Ogni coefficiente $w_j$ della regressione ha un <strong>p-value</strong>, che misura la probabilit√† di ottenere un effetto uguale o maggiore <strong>se l&rsquo;effetto reale fosse nullo</strong>.  </p>
<p>L&rsquo;ipotesi nulla per ciascun coefficiente √®:  </p>
$$
H_0: \quad w_j = 0
$$
<p>Se il <strong>p-value √® inferiore</strong> a una soglia prestabilita (tipicamente 0.05 o 0.01), possiamo rifiutare $H_0$, indicando che la variabile $x_j$ ha un effetto significativo su $y$.  </p>
<p><strong>Interpretazione:</strong><br />
- Un <strong>p-value &lt; 0.05</strong> suggerisce che la variabile $x_j$ √® statisticamente significativa.<br />
- Un <strong>p-value alto</strong> indica che l&rsquo;effetto della variabile potrebbe essere dovuto al caso e che la variabile potrebbe non essere utile nel modello.  </p>
<p>Se pi√π variabili hanno p-value alti, potrebbe essere necessario <strong>semplificare il modello</strong> eliminando quelle non significative.</p>
<hr />
<h3 id="55-considerazioni-finali-sulla-valutazione-del-modello"><strong>5.5. Considerazioni Finali sulla Valutazione del Modello</strong></h3>
<p>Un modello di regressione ideale dovrebbe:<br />
‚úÖ Avere un <strong>MSE basso</strong> e, preferibilmente, un RMSE interpretabile.<br />
‚úÖ Presentare un <strong>$R^2$ elevato</strong>, ma non eccessivamente vicino a 1 per evitare overfitting.<br />
‚úÖ Superare il <strong>test F</strong>, indicando che almeno una variabile ha un effetto su $y$.<br />
‚úÖ Avere <strong>coefficienti con p-value bassi</strong>, per garantire che le variabili siano significative.  </p>
<p>L&rsquo;analisi dei residui (differenze tra $y_i$ e $\hat{y}_i$) √® un altro strumento fondamentale per verificare la bont√† del modello e individuare eventuali problemi di eteroschedasticit√† o non linearit√†.</p>
<h2 id="6-estensione-del-modello">6. Estensione del Modello</h2>
<p>La regressione lineare √® un modello potente e versatile, ma in alcuni casi la relazione tra le variabili indipendenti e la variabile dipendente non √® lineare. In queste situazioni, √® possibile estendere il modello di regressione lineare per catturare relazioni pi√π complesse. Di seguito esploriamo alcune delle principali estensioni del modello di regressione lineare.</p>
<h3 id="61-regressione-polinomiale">6.1. Regressione Polinomiale</h3>
<p>La regressione polinomiale √® un&rsquo;estensione della regressione lineare che permette di modellare relazioni non lineari tra le variabili indipendenti e la variabile dipendente. Questo viene fatto introducendo termini polinomiali delle variabili indipendenti nel modello.</p>
<h4 id="611-formulazione-del-modello">6.1.1. Formulazione del Modello</h4>
<p>Consideriamo il caso di una singola variabile indipendente $x$. Il modello di regressione polinomiale di grado $d$ √® dato da:</p>
$$
y = w_0 + w_1 x + w_2 x^2 + \dots + w_d x^d + \epsilon
$$
<p>Dove:</p>
<ul>
<li>$w_0, w_1, \dots, w_d$ sono i coefficienti del modello.</li>
<li>$x^2, x^3, \dots, x^d$ sono i termini polinomiali della variabile indipendente $x$.</li>
<li>$\epsilon$ √® il termine di errore.</li>
</ul>
<p>In forma matriciale, il modello pu√≤ essere scritto come:</p>
$$
y = Xw + \epsilon
$$
<p>Dove:</p>
$$
X = \begin{bmatrix} 
1 & x_1 & x_1^2 & \dots & x_1^d \\
1 & x_2 & x_2^2 & \dots & x_2^d \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \dots & x_n^d
\end{bmatrix}
$$
<p>√® la matrice delle variabili indipendenti con i termini polinomiali.</p>
$$
w = \begin{bmatrix} 
w_0 \\
w_1 \\
\vdots \\
w_d
\end{bmatrix}
$$
<p>√® il vettore dei coefficienti.</p>
<h4 id="612-scelta-del-grado-del-polinomio">6.1.2. Scelta del Grado del Polinomio</h4>
<p>La scelta del grado $d$ del polinomio √® cruciale:</p>
<ul>
<li>Un grado troppo basso pu√≤ portare a <strong>underfitting</strong>, ovvero il modello non cattura la complessit√† dei dati.</li>
<li>Un grado troppo alto pu√≤ portare a <strong>overfitting</strong>, ovvero il modello si adatta troppo ai dati di training e generalizza male su nuovi dati.</li>
</ul>
<p>Per scegliere il grado ottimale, si possono utilizzare tecniche come la cross-validation o l&rsquo;analisi dell&rsquo;errore di validazione.</p>
<h4 id="613-esempio-di-regressione-polinomiale">6.1.3. Esempio di Regressione Polinomiale</h4>
<p>Supponiamo di avere un dataset con una relazione non lineare tra $x$ e $y$. Un modello di regressione polinomiale di grado 2 potrebbe essere:</p>
$$
y = w_0 + w_1 x + w_2 x^2 + \epsilon
$$
<p>Questo modello pu√≤ catturare relazioni quadratiche tra $x$ e $y$, come ad esempio una parabola.</p>
<h3 id="62-regressione-con-interazioni">6.2. Regressione con Interazioni</h3>
<p>La regressione con interazioni permette di modellare l&rsquo;effetto combinato di due o pi√π variabili indipendenti. Questo √® utile quando l&rsquo;effetto di una variabile dipendente su $y$ dipende dal valore di un&rsquo;altra variabile.</p>
<h4 id="621-formulazione-del-modello">6.2.1. Formulazione del Modello</h4>
<p>Consideriamo due variabili indipendenti $x_1$ e $x_2$. Il modello di regressione con interazione √® dato da:</p>
$$
y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + \epsilon
$$
<p>Dove:</p>
<ul>
<li>$w_0$ √® l&rsquo;intercetta.</li>
<li>$w_1$ e $w_2$ sono i coefficienti delle variabili $x_1$ e $x_2$.</li>
<li>$w_3$ √® il coefficiente del termine di interazione $x_1 x_2$.</li>
<li>$\epsilon$ √® il termine di errore.</li>
</ul>
<h4 id="622-interpretazione-dei-coefficienti">6.2.2. Interpretazione dei Coefficienti</h4>
<ul>
<li>Coefficiente di interazione ($w_3$): Misura l&rsquo;effetto combinato di $x_1$ e $x_2$ su $y$. Se $w_3$ √® positivo, l&rsquo;effetto di $x_1$ su $y$ aumenta all&rsquo;aumentare di $x_2$, e viceversa.</li>
</ul>
<h4 id="623-esempio-di-regressione-con-interazioni">6.2.3. Esempio di Regressione con Interazioni</h4>
<p>Supponiamo di voler modellare l&rsquo;effetto del prezzo ($x_1$) e della pubblicit√† ($x_2$) sulle vendite ($y$). Un modello con interazione potrebbe essere:</p>
$$
y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + \epsilon
$$
<p>Questo modello cattura l&rsquo;effetto combinato del prezzo e della pubblicit√† sulle vendite.</p>
<h3 id="63-regressione-con-funzioni-di-base">6.3. Regressione con Funzioni di Base</h3>
<p>La regressione con funzioni di base √® un&rsquo;estensione della regressione lineare che permette di modellare relazioni non lineari utilizzando funzioni di base (basis functions) delle variabili indipendenti.</p>
<h4 id="631-formulazione-del-modello">6.3.1. Formulazione del Modello</h4>
<p>Consideriamo una variabile indipendente $x$. Il modello di regressione con funzioni di base √® dato da:</p>
$$
y = w_0 + w_1 \phi_1(x) + w_2 \phi_2(x) + \dots + w_d \phi_d(x) + \epsilon
$$
<p>Dove:</p>
<ul>
<li>$\phi_1(x), \phi_2(x), \dots, \phi_d(x)$ sono le funzioni di base.</li>
<li>$w_0, w_1, \dots, w_d$ sono i coefficienti del modello.</li>
<li>$\epsilon$ √® il termine di errore.</li>
</ul>
<h4 id="632-scelta-delle-funzioni-di-base">6.3.2. Scelta delle Funzioni di Base</h4>
<p>Le funzioni di base possono essere scelte in base alla natura dei dati e alla relazione attesa tra le variabili. Alcune scelte comuni includono:</p>
<ul>
<li>Funzioni polinomiali: $\phi_j(x) = x^j$</li>
<li>Funzioni trigonometriche: $\phi_j(x) = \sin(jx), \phi_j(x) = \cos(jx)$</li>
<li>Funzioni radiali: $\phi_j(x) = \exp\left(\frac{-(x - \mu_j)^2}{2\sigma^2}\right)$</li>
</ul>
<h4 id="633-esempio-di-regressione-con-funzioni-di-base">6.3.3. Esempio di Regressione con Funzioni di Base</h4>
<p>Supponiamo di voler modellare una relazione periodica tra $x$ e $y$. Un modello con funzioni trigonometriche potrebbe essere:</p>
$$
y = w_0 + w_1 \sin(x) + w_2 \cos(x) + \epsilon
$$
<p>Questo modello pu√≤ catturare relazioni periodiche come quelle presenti in dati stagionali.</p>
<h3 id="64-regressione-non-parametrica">6.4. Regressione Non Parametrica</h3>
<p>La regressione non parametrica √® un approccio che non assume una forma specifica per la relazione tra le variabili indipendenti e la variabile dipendente. Invece, il modello si adatta ai dati in modo flessibile.</p>
<h4 id="641-metodi-comuni">6.4.1. Metodi Comuni</h4>
<p>Alcuni metodi comuni di regressione non parametrica includono:</p>
<ul>
<li><strong>Kernel Regression</strong>: Stima la relazione tra $x$ e $y$ utilizzando una funzione di kernel per pesare i dati vicini.</li>
<li><strong>Spline Regression</strong>: Utilizza funzioni spline per modellare la relazione tra $x$ e $y$.</li>
<li><strong>Local Regression (LOESS)</strong>: Adatta un modello di regressione lineare localmente ai dati.</li>
</ul>
<h4 id="642-vantaggi-e-svantaggi">6.4.2. Vantaggi e Svantaggi</h4>
<ul>
<li><strong>Vantaggi</strong>:</li>
<li>Flessibilit√† nel modellare relazioni complesse.</li>
<li>
<p>Non richiede assunzioni sulla forma della relazione.</p>
</li>
<li>
<p><strong>Svantaggi</strong>:</p>
</li>
<li>Maggiore complessit√† computazionale.</li>
<li>Rischio di overfitting se non si controlla la complessit√† del modello.</li>
</ul>
<h3 id="65-regressione-ponderata-weighted-regression">6.5. Regressione Ponderata (Weighted Regression)</h3>
<p>La regressione ponderata √® una variante della regressione lineare in cui ogni osservazione ha un peso specifico. Questo √® utile quando alcune osservazioni sono pi√π importanti o affidabili di altre.</p>
<h4 id="651-formulazione-del-modello">6.5.1. Formulazione del Modello</h4>
<p>Nella regressione ponderata, l&rsquo;obiettivo √® minimizzare la somma degli errori quadratici ponderati:</p>
$$
\min_{\mathbf{v}} \sum_{i=1}^{N} v_i \left( y_i - \mathbf{w}^T \mathbf{x}_i \right)^2
$$
<p>Dove:</p>
<ul>
<li>$v_i$ √® il peso associato all&rsquo;osservazione $i$.</li>
<li>$\mathbf{w}$ √® il vettore dei coefficienti.</li>
</ul>
<h4 id="652-esempio-di-regressione-ponderata">6.5.2. Esempio di Regressione Ponderata</h4>
<p>Supponiamo di avere un dataset in cui alcune osservazioni sono pi√π affidabili di altre. Possiamo assegnare pesi maggiori a queste osservazioni per migliorare la stima del modello.</p>
<h3 id="66-conclusione">6.6. Conclusione</h3>
<p>Le estensioni del modello di regressione lineare permettono di catturare relazioni pi√π complesse tra le variabili indipendenti e la variabile dipendente. La scelta del modello dipende dalla natura dei dati e dalla relazione attesa. √à importante bilanciare la flessibilit√† del modello con il rischio di overfitting, utilizzando tecniche come la cross-validation e la regolarizzazione.</p>
<h2 id="7-conclusioni"><strong>7. Conclusioni</strong></h2>
<ul>
<li>La <strong>Regressione Lineare</strong> √® un modello semplice ma potente per analizzare relazioni tra variabili.</li>
<li>Richiede l&rsquo;analisi delle <strong>assunzioni</strong> per evitare problemi di interpretabilit√†.</li>
<li>Pu√≤ essere estesa con <strong>regolarizzazione</strong> e <strong>modelli polinomiali</strong> per migliorare le prestazioni</li>
</ul>
<p><strong>Risorse aggiuntive:</strong>
- <em>The Elements of Statistical Learning</em> - Hastie, Tibshirani, Friedman.
- <em>Introduction to Statistical Learning</em> - James, Witten, Hastie, Tibshirani.</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> supervised learning, labeled data, classification, regression, model, data, training, learning</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/supervised-learning/Linear Models/Regressione Lineare">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'Regressione Lineare',
          page_location: 'http://localhost:3000/theory/supervised-learning/Linear Models/Regressione Lineare'
        });
      }
    </script>
</body>
</html>