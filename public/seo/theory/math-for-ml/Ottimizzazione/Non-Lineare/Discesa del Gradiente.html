<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Discesa del Gradiente | Mathematics for Machine Learning | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="mathematics, machine learning, linear algebra, calculus, statistics, model, data, neural">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Discesa del Gradiente">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Discesa del Gradiente">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Discesa del Gradiente",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente",
      "datePublished": "2025-08-06T20:12:10.725Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente" class="react-redirect">üöÄ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>Discesa del Gradiente</h1>
                <div class="meta">
                    <strong>Topic:</strong> Mathematics for Machine Learning | 
                    <strong>Updated:</strong> 06/08/2025
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<p>La discesa del gradiente (<em>Gradient Descent</em>, GD) √® un algoritmo iterativo di minimizzazione del primo ordine. Viene definito <strong>iterativo</strong> poich√© esegue una sequenza di aggiornamenti successivi per determinare un minimo locale della funzione obiettivo, a partire da una condizione iniziale.</p>
<p>Un aspetto cruciale della discesa del gradiente √® che, nel caso di funzioni <strong>non convesse</strong>, non possiamo garantire che l&rsquo;algoritmo trovi il minimo globale. Infatti, tali funzioni possono presentare <strong>molteplici minimi locali</strong>, e il punto di convergenza dipender√† dalle condizioni iniziali del modello.</p>
<p>√à possibile che invece di un minimo locale (o globale), l&rsquo;algoritmo si interrompa su un punto di sella. Durante la discesa del gradiente, l‚Äôalgoritmo cerca punti dove il valore della funzione diminuisce. Se si avvicina a un punto di sella, il gradiente (cio√® l&rsquo;indicazione della direzione in cui scendere) pu√≤ diventare molto piccolo, e questo pu√≤ <strong>rallentare o bloccare</strong> temporaneamente l‚Äôottimizzazione. Anche se i punti di sella esistono, √® <strong>molto improbabile</strong> che la discesa del gradiente si fermi esattamente su uno di essi, per due motivi principali:</p>
<ol>
<li>
<p><strong>Instabilit√† numerica</strong>: i punti di sella sono instabili ‚Äî basta una piccola variazione (come un errore di arrotondamento o un passo leggermente diverso) per spingere l‚Äôalgoritmo lontano dal punto di sella.</p>
</li>
<li>
<p><strong>Alta dimensionalit√†</strong>: negli spazi ad alta dimensione, i punti di sella sono molto pi√π frequenti dei minimi, ma anche molto pi√π &ldquo;facili da evitare&rdquo;. √à molto raro &ldquo;cadere&rdquo; perfettamente in un punto di sella, e ancor pi√π raro restarci a lungo.</p>
</li>
</ol>
<p>In pratica, anche se ci si pu√≤ avvicinare a un punto di sella, la discesa del gradiente tende naturalmente a superarlo e continuare verso un minimo.</p>
<p>L&rsquo;intuizione alla base della discesa del gradiente √® piuttosto semplice:</p>
<ol>
<li>Si parte da un punto iniziale nello spazio dei parametri.  </li>
<li>Si calcola il gradiente della funzione obiettivo in quel punto, il quale indica la direzione di massima crescita.  </li>
<li>Per minimizzare la funzione, ci si sposta nella direzione opposta a quella del gradiente, effettuando un &ldquo;passo&rdquo; in quella direzione.  </li>
<li>Questo processo viene ripetuto fino al raggiungimento di un criterio di arresto (convergenza).</li>
</ol>
<p><img src="/images/posts/gradient-descent.jpg" alt="Gradient Descent"></p>
<p><em>Figura 1.0: Discesa del Gradiente su una funzione loss non convessa</em></p>
<p>Formalmente, il processo di aggiornamento iterativo pu√≤ essere espresso come:</p>
$$
\Theta^{(t+1)} \leftarrow \Theta^{(t)} - \alpha \nabla \ell_{\Theta^{(t)}}
$$
<p>dove:</p>
<ul>
<li>$\Theta^{(t)}$ rappresenta i parametri del modello all&rsquo;iterazione $t$,</li>
<li>$\alpha$ √® il <strong>tasso di apprendimento</strong> (<em>learning rate</em>), un iperparametro che determina l&rsquo;ampiezza del passo nella direzione del gradiente,</li>
<li>$\nabla \ell_{\Theta^{(t)}}$ √® il gradiente della funzione di perdita $\ell$ rispetto ai parametri $\Theta$.</li>
</ul>
<p>Ovviamente, per poter calcolare correttamente il gradiente di una funzione, e quindi eseguire correttamente la discesa del gradiente, abbiamo bisogno che la funzione $\ell$ sia differenziabile in ogni suo punto.</p>
<p>Infatti, non basta che sia definita la derivata parziale di $\ell$ rispetto a ogni singola variabile $\theta_i$, ma √® necessario che $\ell$ abbia un gradiente continuo.</p>
<h2 id="differenziabilita">Differenziabilit√†</h2>
<p>Come abbiamo visto, il gradiente √® l‚Äôelemento chiave nel funzionamento della discesa del gradiente. Ma ci si potrebbe chiedere: <strong>tutte le funzioni di perdita permettono il calcolo del gradiente?</strong></p>
<p>La risposta √®: <strong>non sempre</strong>.</p>
<p>Non tutte le funzioni sono <strong>differenziabili</strong>, cio√® non tutte ammettono un gradiente ben definito in ogni punto del dominio. Questo √® un problema rilevante, perch√© <strong>la discesa del gradiente richiede che la funzione sia differenziabile</strong>, altrimenti il gradiente potrebbe non esistere in certi punti e l‚Äôalgoritmo potrebbe bloccarsi o dare risultati errati.</p>
<h3 id="derivate-parziali-definite-math_inline_42-differenziabilita">Derivate parziali definite $‚â†$ Differenziabilit√†</h3>
<p>In una funzione di pi√π variabili, avere <strong>tutte le derivate parziali definite</strong> non √® sufficiente per garantire la differenziabilit√†. Infatti, pu√≤ succedere che tutte le derivate esistano, ma non siano continue ‚Äî e questo √® un segnale che la funzione <strong>non √® veramente differenziabile</strong>.</p>
<p>Un esempio classico √® la seguente funzione:</p>
<ul>
<li>$f(x, y) = 0$ se $(x, y) = (0, 0)$</li>
<li>$f(x, y) = \frac{x^2 y}{x^2 + y^2}$ altrimenti</li>
</ul>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                    const code = this.parentElement.querySelector('pre');
                    if (code) {
                        navigator.clipboard.writeText(code.innerText);
                        this.textContent = 'Copied!';
                        setTimeout(() => this.textContent = 'Copy', 2000);
                    }
                ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># Funzione definita a tratti</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="c1"># Griglia</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Figura Matplotlib</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>  <span class="c1"># circa 1920x1080</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Superficie</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Punto non differenziabile all&#39;origine</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;(0, 0)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="c1"># Etichette</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Funzione non differenziabile in (0, 0)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;f(x, y)&#39;</span><span class="p">)</span>

<span class="c1"># Vista iniziale</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>

<span class="c1"># Salvataggio in HD</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;gradient-non-differentiable.png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/gradient-non-differentiable.png" alt="Gradient Descent 3" width="1000" style="display: block; margin: 0 auto;"></p>
<p><em>Figura 1.3: Funzione non differenziabile</em></p>
<p>Questa funzione ha derivate parziali definite ovunque, ma sono <strong>discontinue nell‚Äôorigine</strong>, e questo significa che $f$ <strong>non √® differenziabile</strong> in $(0, 0)$.</p>
<h4 id="1-derivate-parziali-in-math_inline_48">1. Derivate parziali in $(0,0)$</h4>
<p>Per definizione,
$$
f_x(0,0)
=\lim_{h\to0}\frac{f(h,0)-f(0,0)}{h}
=\lim_{h\to0}\frac{0-0}{h}=0,
\qquad
f_y(0,0)
=\lim_{k\to0}\frac{f(0,k)-f(0,0)}{k}
=\lim_{k\to0}\frac{0-0}{k}=0.
$$</p>
<h4 id="2-espressioni-di-math_inline_49-e-math_inline_50-per-math_inline_51">2. Espressioni di $f_x$ e $f_y$ per $(x,y)\neq(0,0)$</h4>
<p>Usando la derivazione di un quoziente:
$$
f(x,y)=\frac{u(x,y)}{v(x,y)},
\quad
u=x^2y,\quad v=x^2+y^2,
$$
$$
f_x
=\frac{u_x\,v - u\,v_x}{v^2}
=\frac{(2x y)(x^2+y^2) - (x^2y)(2x)}{(x^2+y^2)^2}
=\frac{2x y^3}{(x^2+y^2)^2},
$$
$$
f_y
=\frac{u_y\,v - u\,v_y}{v^2}
=\frac{(x^2)(x^2+y^2) - (x^2y)(2y)}{(x^2+y^2)^2}
=\frac{x^4 - x^2y^2}{(x^2+y^2)^2}.
$$</p>
<h4 id="3-limiti-lungo-la-retta-math_inline_52">3. Limiti lungo la retta $y = m x$</h4>
<p>Sostituiamo $y=mx$ con $x\to0$:
$$
f_x(x,mx)
=\frac{2x\,(mx)^3}{\bigl(x^2+(mx)^2\bigr)^2}
=\frac{2m^3\,x^4}{x^4\,(1+m^2)^2}
=\frac{2m^3}{(1+m^2)^2},
$$
$$
f_y(x,mx)
=\frac{x^4 - x^2\,(mx)^2}{\bigl(x^2+(mx)^2\bigr)^2}
=\frac{x^4(1-m^2)}{x^4\,(1+m^2)^2}
=\frac{1-m^2}{(1+m^2)^2}.
$$
Questi valori dipendono dal parametro $m$. In particolare:
- Se $m=0$, $f_x\to0$ e $f_y\to1$.<br />
- Se $m=1$, $f_x\to\dfrac{2}{4}=\tfrac12$ e $f_y\to0$.</p>
<h4 id="4-conclusione-sulla-discontinuita">4. Conclusione sulla discontinuit√†</h4>
<p>Poich√©
$$
\lim_{(x,y)\to(0,0)}\nabla f(x,y)
$$
assume valori diversi a seconda della retta di avvicinamento ($m$ diverso), il gradiente <strong>non √® continuo</strong> in $(0,0)$, pur avendo entrambe le derivate parziali esistenti e finite.</p>
<h3 id="implicazioni-pratiche">Implicazioni pratiche</h3>
<p>Fortunatamente, nella pratica si usano spesso funzioni di perdita ben progettate, che sono <strong>lisce e differenziabili</strong> quasi ovunque. Tuttavia, <strong>non √® raro incontrare funzioni di perdita non differenziabili</strong>, ad esempio con funzioni <em>piecewise</em> o attivazioni come la <em>ReLU</em>.</p>
<p>In questi casi, si adottano diverse strategie per rendere il problema trattabile:</p>
<ul>
<li><strong>Modifica o sostituzione della funzione</strong> con una variante liscia (es. ReLU ‚Üí Softplus)</li>
<li><strong>Tecniche come il &ldquo;reparametrization trick&rdquo;</strong> nei modelli generativi come le VAE, che permettono il passaggio del gradiente anche quando la funzione non √® differenziabile nel senso classico</li>
</ul>
<p>In conclusione, <strong>la differenziabilit√† √® un requisito fondamentale per l‚Äôapplicazione diretta della discesa del gradiente</strong>, ma esistono metodi e tecniche per aggirare o gestire in modo efficace i casi in cui essa venga meno.</p>
<h2 id="interpretazione-geometrica">Interpretazione Geometrica</h2>
<p>Dal punto di vista geometrico, la discesa del gradiente segue una traiettoria nello spazio dei parametri, cercando il punto in cui la funzione di perdita assume un valore minimo. Se la funzione √® convessa, l&rsquo;algoritmo converger√† al minimo globale; altrimenti, si fermer√† in un minimo locale. </p>
<p>√à importante notare che, a causa della precisione finita delle macchine, difficilmente si raggiunger√† un punto esattamente stazionario, ma ci si fermer√† quando la variazione della funzione di perdita diventa trascurabile.</p>
<p>Volendo possiamo anche, tramite l&rsquo;unrolling ricorsivo, riscrivere esplicitamente $\Theta^{(t+1)}$ come:</p>
$$
\begin{align*}
\Theta^{(1)}   &= \Theta^{(0)} - \alpha \nabla \ell_{\Theta^{(0)}}\\
\Theta^{(2)}   &= \Theta^{(1)} - \alpha \nabla \ell_{\Theta^{(1)}}\\
               &= \Theta^{(0)} - \alpha \nabla \ell_{\Theta^{(0)}} - \alpha \nabla \ell_{\Theta^{(1)}}\\
\vdots\\
\Theta^{(t+1)} &= \Theta^{(0)} - \alpha \sum_{i=0}^{t} \nabla \ell_{\Theta^{(i)}}.
\end{align*}
$$
<p>Il criterio di arresto pi√π comune √® la verifica della norma del gradiente:</p>
$$
\|\nabla \ell_{\Theta^{(t)}}\| \leq \epsilon
$$
<p>dove $\epsilon$ √® una soglia positiva molto piccola che determina il livello di precisione desiderato.</p>
<p>Ma ce ne sono anche altri come:</p>
<ul>
<li><strong>Nessuna modifica dei parametri</strong>: se $\Theta^{(t+1)} = \Theta^{(t)}$, il criterio di arresto viene raggiunto.</li>
<li><strong>Loss non migliorata</strong>: se $|\ell_{\Theta^{(t)}} - \ell_{\Theta^{(t+1)}}| \leq \epsilon$, il criterio di arresto viene raggiunto.</li>
</ul>
<h2 id="proprieta-del-gradiente">Propriet√† del Gradiente</h2>
<h3 id="ortogonalita-e-massima-crescita">Ortogonalit√† e Massima Crescita</h3>
<p>Non abbiamo ancora fornito una giustificazione formale dell&rsquo;affermazione secondo cui il gradiente di una funzione in un dato punto rappresenta la direzione di massima crescita della funzione stessa.</p>
<p>Per comprendere meglio questo concetto, introduciamo la <strong>derivata direzionale</strong>, una generalizzazione della derivata tradizionale nel dominio unidimensionale $\mathbb{R}$. Mentre nella retta reale esiste un&rsquo;unica direzione lungo cui calcolare la derivata, in $\mathbb{R}^n$ (per $n \geq 2$) non esiste una direzione privilegiata per valutare la variazione di una funzione.</p>
<p>La derivata direzionale di una funzione differenziabile $f: \mathbb{R}^n \to \mathbb{R}$ lungo una direzione unitaria $\mathbf{v}$ √® definita come:</p>
$$
D_{\mathbf{v}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h \mathbf{v}) - f(\mathbf{x})}{h}.
$$
<p>Questa definizione generalizza la derivata parziale $\frac{\partial f}{\partial x}$, che assume che la direzione considerata sia allineata con uno degli assi canonici (ovvero, solo una variabile cambia alla volta mentre le altre restano fisse). Al contrario, la derivata direzionale consente una variazione simultanea di tutte le variabili lungo una direzione arbitraria $\mathbf{v}$.</p>
<h3 id="relazione-tra-curve-di-livello-derivata-direzionale-e-gradiente">Relazione tra Curve di Livello, Derivata Direzionale e Gradiente</h3>
<p>Le <strong>curve di livello</strong> di una funzione sono le curve (o ipersuperfici) lungo le quali la funzione assume lo stesso valore. Ci√≤ implica che la derivata direzionale della funzione in un punto appartenente alla curva, lungo una direzione tangente alla curva stessa, sia nulla, poich√© la funzione non varia localmente in quella direzione.</p>
<p>Formalmente, se $\mathbf{v}$ √® un vettore tangente alla curva di livello di $f$, allora</p>
$$
\langle \nabla f, \mathbf{v} \rangle = 0.
$$
<p>Questo risultato implica che il <strong>gradiente √® ortogonale alla curva di livello</strong> e punta nella direzione di massima crescita della funzione. Inoltre, si pu√≤ dimostrare che esso √® orientato verso curve di livello con valori maggiori della funzione (ovvero, verso l&rsquo;incremento della funzione stessa).</p>
<p>Questo concetto √® riassunto nella seguente rappresentazione grafica:</p>
<ul>
<li>Le curve di livello rappresentano i punti di uguale valore della funzione.</li>
<li>Il gradiente √® sempre perpendicolare a tali curve.</li>
<li>La discesa del gradiente segue la direzione opposta al gradiente stesso per minimizzare la funzione.</li>
</ul>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                    const code = this.parentElement.querySelector('pre');
                    if (code) {
                        navigator.clipboard.writeText(code.innerText);
                        this.textContent = 'Copied!';
                        setTimeout(() => this.textContent = 'Copy', 2000);
                    }
                ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>  <span class="c1"># Necessario per i plot 3D</span>

<span class="c1"># Definizione della funzione f(x, y)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Esempio di funzione con pi√π minimi locali.</span>
<span class="sd">    Puoi modificarla come preferisci.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Definizione del gradiente di f(x, y)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calcola il gradiente di f(x, y): restituisce (df/dx, df/dy).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dfdx</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">dfdy</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dfdx</span><span class="p">,</span> <span class="n">dfdy</span>

<span class="c1"># Creazione della griglia di punti per i plot</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># Numero di punti per dimensione</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Calcolo del gradiente su una griglia pi√π rada per la visualizzazione del campo vettoriale</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># Passo per la selezione dei punti per il campo vettoriale</span>
<span class="n">x_quiver</span> <span class="o">=</span> <span class="n">X</span><span class="p">[::</span><span class="n">step</span><span class="p">,</span> <span class="p">::</span><span class="n">step</span><span class="p">]</span>
<span class="n">y_quiver</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[::</span><span class="n">step</span><span class="p">,</span> <span class="p">::</span><span class="n">step</span><span class="p">]</span>
<span class="n">dfdx</span><span class="p">,</span> <span class="n">dfdy</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x_quiver</span><span class="p">,</span> <span class="n">y_quiver</span><span class="p">)</span>

<span class="c1"># Creazione della figura con 2 righe e 2 colonne, usando width_ratios per modificare le dimensioni dei subplot in alto</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_gridspec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># -------------------------------------------------------------------------</span>
<span class="c1"># Pannello 1 (in alto a sinistra): Plot 3D della superficie (pi√π grande)</span>
<span class="c1"># -------------------------------------------------------------------------</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Superficie 3D&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;f(x, y)&#39;</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------------------</span>
<span class="c1"># Pannello 2 (in alto a destra): Mappa di colore e curve di livello (pi√π quadrato)</span>
<span class="c1"># -------------------------------------------------------------------------</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">cont</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Mappa di colore e curve di livello&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">adjustable</span><span class="o">=</span><span class="s1">&#39;box&#39;</span><span class="p">)</span>  <span class="c1"># Forza un aspetto quadrato</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cont</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;Valore di f(x, y)&#39;</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------------------</span>
<span class="c1"># Pannello 3 (in basso a sinistra): Campo vettoriale del gradiente positivo</span>
<span class="c1"># -------------------------------------------------------------------------</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">x_quiver</span><span class="p">,</span> <span class="n">y_quiver</span><span class="p">,</span> <span class="n">dfdx</span><span class="p">,</span> <span class="n">dfdy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Campo vettoriale del gradiente&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------------------</span>
<span class="c1"># Pannello 4 (in basso a destra): Campo vettoriale del gradiente negativo</span>
<span class="c1"># -------------------------------------------------------------------------</span>
<span class="n">ax4</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">Q2</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">x_quiver</span><span class="p">,</span> <span class="n">y_quiver</span><span class="p">,</span> <span class="o">-</span><span class="n">dfdx</span><span class="p">,</span> <span class="o">-</span><span class="n">dfdy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Campo vettoriale del gradiente negativo&#39;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;./images/gradient.jpg&#39;</span><span class="p">,</span> 
           <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> 
           <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">,</span>
           <span class="n">pad_inches</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>  <span class="c1"># Aggiungere questo parametro</span>
           <span class="c1">#facecolor=fig.get_facecolor(),  # Mantenere il colore di sfondo</span>
           <span class="n">transparent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Disabilitare la trasparenza</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/gradient.jpg" alt="Gradient Descent"></p>
<p><em>Figura 1.1: Visualizzazione della funzione $f(x,y)$ con la sua superficie 3D, mappa di livello e campi vettoriali del gradiente positivo e negativo, evidenziando le direzioni di massima variazione.</em></p>
<p>Questa propriet√† √® fondamentale nelle tecniche di ottimizzazione basate sul gradiente, in quanto garantisce che muovendosi nella direzione opposta al gradiente si riduce il valore della funzione obiettivo.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                    const code = this.parentElement.querySelector('pre');
                    if (code) {
                        navigator.clipboard.writeText(code.innerText);
                        this.textContent = 'Copied!';
                        setTimeout(() => this.textContent = 'Copy', 2000);
                    }
                ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Definizione della funzione e del suo gradiente</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y</span>

<span class="c1"># Creazione della griglia di punti</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Punto di interesse P0</span>
<span class="n">P0</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">gx</span><span class="p">,</span> <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">P0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">P0</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># gradiente in P0</span>

<span class="c1"># Calcolo della direzione tangente (v) a partire dalla condizione: &lt;grad, v&gt; = 0</span>
<span class="c1"># Scegliamo v = (2, -1) che √® ortogonale a (1,2) (poich√® 1*2 + 2*(-1) = 0)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># normalizzo</span>

<span class="c1"># Calcolo del vettore -gradiente in P0 (direzione di discesa)</span>
<span class="n">neg_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">])</span>
<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">neg_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-10</span><span class="p">:</span>
    <span class="n">neg_grad_unit</span> <span class="o">=</span> <span class="n">neg_grad</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">neg_grad</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">neg_grad_unit</span> <span class="o">=</span> <span class="n">neg_grad</span>

<span class="c1"># Lunghezze per le frecce</span>
<span class="n">arrow_len</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Creazione della figura con 3 pannelli affiancati</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># Pannello 1: Curva di livello + vettore tangente (df/dv = 0)</span>
<span class="c1"># ---------------------------</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cont1</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">P0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">P0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$P_0$&#39;</span><span class="p">)</span>
<span class="c1"># Calcolo del punto finale per la freccia tangente</span>
<span class="n">P1</span> <span class="o">=</span> <span class="p">(</span><span class="n">P0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">arrow_len</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">P0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">arrow_len</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">P1</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">P0</span><span class="p">,</span> 
             <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Vettore tangente: $\frac{df}{dv}(P_0)=0$&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># Pannello 2: Curva di livello + vettore -gradiente</span>
<span class="c1"># ---------------------------</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">cont2</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">P0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">P0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$P_0$&#39;</span><span class="p">)</span>
<span class="c1"># Punto finale per la freccia -gradiente</span>
<span class="n">P2</span> <span class="o">=</span> <span class="p">(</span><span class="n">P0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">arrow_len</span> <span class="o">*</span> <span class="n">neg_grad_unit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">P0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">arrow_len</span> <span class="o">*</span> <span class="n">neg_grad_unit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">P2</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">P0</span><span class="p">,</span>
             <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Vettore -$\nabla f$ in $P_0$&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># Pannello 3: Campo vettoriale dell&#39;intero gradiente</span>
<span class="c1"># ---------------------------</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">cont3</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># Campo vettoriale (quiver) per il gradiente</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">xq</span> <span class="o">=</span> <span class="n">X</span><span class="p">[::</span><span class="n">step</span><span class="p">,</span> <span class="p">::</span><span class="n">step</span><span class="p">]</span>
<span class="n">yq</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[::</span><span class="n">step</span><span class="p">,</span> <span class="p">::</span><span class="n">step</span><span class="p">]</span>
<span class="n">gxq</span><span class="p">,</span> <span class="n">gyq</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">yq</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">yq</span><span class="p">,</span> <span class="n">gxq</span><span class="p">,</span> <span class="n">gyq</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Campo vettoriale: $\nabla f$&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;./images/gradient.jpg&#39;</span><span class="p">,</span> 
           <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> 
           <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">,</span>
           <span class="n">pad_inches</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>  <span class="c1"># Aggiungere questo parametro</span>
           <span class="c1">#facecolor=fig.get_facecolor(),  # Mantenere il colore di sfondo</span>
           <span class="n">transparent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Disabilitare la trasparenza</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/gradient-orthogonal.jpg" alt="Gradient Descent 2"></p>
<p><em>Figura 1.2: Ortogonalit√† tra il vettore tangente alla curva di livello e il vettore -gradiente</em></p>
<h2 id="learning-rate">Learning Rate</h2>
<p>Nella legge di aggiornamento della discesa del gradiente:</p>
$$
\Theta^{(t+1)} = \Theta^{(t)} - \alpha \nabla \ell_{\Theta^{(t)}},
$$
<p>il parametro $\alpha$ gioca un ruolo fondamentale. Questo parametro si chiama <strong>learning rate</strong> (tasso di apprendimento) ed √® un <strong>iperparametro</strong>, cio√® non viene appreso durante l‚Äôottimizzazione, ma deve essere scelto manualmente (o tramite ricerca automatica).</p>
<p>Il learning rate √® <strong>sempre positivo</strong>: se fosse negativo, infatti, ci si muoverebbe nella direzione opposta a quella desiderata, <strong>massimizzando</strong> invece che minimizzando la funzione di perdita.</p>
<h3 id="effetti-del-learning-rate">Effetti del learning rate</h3>
<p>Il valore di $\alpha$ determina <strong>quanto grande √® ogni passo</strong> che l‚Äôalgoritmo compie nella direzione opposta al gradiente. Non coincide esattamente con la lunghezza del passo (che dipende anche dalla norma del gradiente), ma √® <strong>proporzionale ad essa</strong>.</p>
<p>A seconda della sua scelta, il comportamento dell‚Äôalgoritmo pu√≤ variare notevolmente:</p>
<ul>
<li>Se <strong>$\alpha$ √® troppo piccolo</strong>, l‚Äôalgoritmo avanza molto lentamente e richiede molte iterazioni per convergere.</li>
<li>Se <strong>$\alpha$ √® troppo grande</strong>, si rischia di <strong>superare il minimo</strong>, causando <strong>oscillazioni</strong> o addirittura <strong>divergenza</strong>.</li>
<li>Esiste un valore &ldquo;ottimale&rdquo; $\alpha^*$ per ogni punto, che minimizzerebbe la funzione lungo la direzione di discesa. Tuttavia, trovare questo valore √® difficile perch√© richiederebbe una soluzione chiusa del problema, che <strong>non √® disponibile in generale</strong> per funzioni non lineari.</li>
</ul>
<p>Questa situazione √® illustrata nella seguente figura:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                    const code = this.parentElement.querySelector('pre');
                    if (code) {
                        navigator.clipboard.writeText(code.innerText);
                        this.textContent = 'Copied!';
                        setTimeout(() => this.textContent = 'Copy', 2000);
                    }
                ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Dati sintetici</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Funzione di perdita</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Gradiente</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gradients</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="n">db</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>

<span class="c1"># Allenamento</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">trajectory</span>

<span class="c1"># Parametri per i plot</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Small Œ±&#39;</span><span class="p">,</span> <span class="s1">&#39;Optimal Œ±&#39;</span><span class="p">,</span> <span class="s1">&#39;Large Œ±&#39;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ca02c&#39;</span><span class="p">,</span> <span class="s1">&#39;#d62728&#39;</span><span class="p">]</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="p">[</span><span class="n">train</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">]</span>

<span class="c1"># Curve di livello</span>
<span class="n">w_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">b_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">W</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="n">b_range</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">w_range</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">b_range</span><span class="p">])</span>

<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">trajectories</span><span class="p">,</span> <span class="n">titles</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">contours</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">)</span>
    <span class="n">w_vals</span><span class="p">,</span> <span class="n">b_vals</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">traj</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_vals</span><span class="p">,</span> <span class="n">b_vals</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Minimo&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Confronto tra traiettorie di gradient descent con diversi learning rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.85</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p align="center">
  <img src="/images/posts/learning-rate-comparison-sgd.png" alt="Confronto tra diversi learning rate">
</p>

<p><em>Figura 2.0: Confronto tra learning rate troppo piccolo, troppo grande e ottimale</em></p>
<h3 id="line-search">Line Search</h3>
<p>Una strategia per scegliere dinamicamente il valore di $\alpha$ √® il <strong>line search</strong>: una procedura che, una volta nota la direzione di discesa $-\nabla \ell_{\Theta^{(t)}}$, cerca il valore di $\alpha$ che <strong>massimizza la diminuzione</strong> della funzione di perdita lungo quella direzione. In pratica, si risolve un piccolo problema di ottimizzazione interno a ogni passo.</p>
<p>Questa tecnica √® pi√π costosa, ma pu√≤ migliorare la stabilit√† e l&rsquo;efficacia dell‚Äôottimizzazione.</p>
<h3 id="decadimento-del-learning-rate">Decadimento del learning rate</h3>
<p>In alternativa al line search, √® comune utilizzare <strong>strategie di decadimento</strong> del learning rate, cio√® farlo <strong>diminuire nel tempo</strong> secondo una certa regola:</p>
<ul>
<li><strong>Decadimento lineare</strong>:<br />
  $$ \alpha^{(t+1)} = \alpha^{(0)} - \rho t $$</li>
<li><strong>Decadimento razionale</strong>:<br />
  $$ \alpha^{(t+1)} = \frac{\alpha^{(0)}}{1 + \rho t} $$</li>
<li><strong>Decadimento esponenziale</strong>:<br />
  $$ \alpha^{(t+1)} = \alpha^{(0)} e^{-\rho t} $$</li>
</ul>
<p>dove $\rho$ √® un parametro di decadimento.</p>
<p>L‚Äôidea alla base √® che all‚Äôinizio si vogliono fare <strong>passi ampi</strong> per esplorare rapidamente lo spazio dei parametri, mentre verso la fine servono <strong>passi piccoli</strong> per affinare la soluzione e garantire la convergenza ottimale.</p>
<h3 id="considerazioni-pratiche">Considerazioni pratiche</h3>
<p>Non esiste una &ldquo;ricetta perfetta&rdquo; per scegliere il learning rate o la sua strategia di aggiornamento. Molto spesso, la scelta viene fatta tramite:</p>
<ul>
<li><strong>esperienza pratica</strong></li>
<li><strong>grid search o random search</strong></li>
<li><strong>ottimizzazione bayesiana o altri metodi automatici</strong></li>
</ul>
<p>Alcuni algoritmi, come <strong>Adam</strong>, includono meccanismi per <strong>adattare automaticamente il learning rate</strong> per ogni parametro, rendendo l&rsquo;ottimizzazione pi√π robusta e spesso pi√π veloce.</p>
<h2 id="batch-mini-batch-e-stochastic-gradient-descent">Batch, Mini-Batch e Stochastic Gradient Descent</h2>
<p>La discesa del gradiente nella sua forma classica (chiamata <strong>Batch Gradient Descent</strong>) utilizza l&rsquo;intero dataset per calcolare il gradiente della funzione di perdita. Questo approccio fornisce una direzione precisa, ma pu√≤ essere computazionalmente costoso, specialmente su dataset di grandi dimensioni.</p>
<p>Per ovviare a questo problema, sono state sviluppate varianti pi√π efficienti:</p>
<h3 id="1-batch-gradient-descent">1. <strong>Batch Gradient Descent</strong></h3>
<p>In questo approccio, ad ogni iterazione viene utilizzato <strong>l&rsquo;intero dataset</strong> per calcolare il gradiente:</p>
$$
\Theta^{(t+1)} \leftarrow \Theta^{(t)} - \alpha \cdot \frac{1}{n} \sum_{i=1}^{n} \nabla \ell^{(i)}(\Theta^{(t)}).
$$
<p>Si calcola quindi il gradiente della funzione di perdita per ogni esempio del dataset, e poi si effettua il passo di discesa con la media dei gradiente. Quindi un epoca in questo caso sonsiste in un solo passo di discesa.</p>
<ul>
<li>Vantaggi: direzione precisa della discesa.</li>
<li>Svantaggi: lento per dataset molto grandi, non aggiornabile in tempo reale.</li>
</ul>
<h3 id="2-stochastic-gradient-descent-sgd">2. <strong>Stochastic Gradient Descent (SGD)</strong></h3>
<p>In questo caso, l&rsquo;aggiornamento dei parametri viene effettuato <strong>per ogni singolo esempio</strong> del dataset:</p>
$$
\Theta^{(t+1)} \leftarrow \Theta^{(t)} - \alpha \cdot \nabla \ell^{(i)}(\Theta^{(t)}).
$$
<p>Quindi un epoca in questo caso consiste in $n$ passi di discesa (iterazioni). Questo perch√© si calcola il gradiente per ogni esempio del dataset, quindi si effettua $n$ passi di discesa.</p>
<ul>
<li>Vantaggi: aggiornamenti molto rapidi, buona approssimazione della direzione di discesa.</li>
<li>Svantaggi: il rumore introdotto da ogni esempio pu√≤ causare oscillazioni e rendere difficile la convergenza stabile.</li>
</ul>
<h3 id="3-mini-batch-gradient-descent">3. <strong>Mini-Batch Gradient Descent</strong></h3>
<p>Rappresenta un compromesso tra le due precedenti. Si utilizza un <strong>sottoinsieme (mini-batch)</strong> di $m$ campioni (con $m \ll n$) per calcolare il gradiente:</p>
$$
\Theta^{(t+1)} \leftarrow \Theta^{(t)} - \alpha \cdot \frac{1}{m} \sum_{j=1}^{m} \nabla \ell^{(j)}(\Theta^{(t)}).
$$
<p>Qui calcoliamo ogni volta il gradiente su $m$ esempi, quindi un epoca in questo caso consiste in $\frac{n}{m}$ passi di discesa (iterazioni).</p>
<ul>
<li>Vantaggi: bilancia precisione e velocit√†, sfrutta l&rsquo;efficienza computazionale del calcolo vettoriale su GPU.</li>
<li>√à la scelta pi√π comune nelle reti neurali moderne.</li>
</ul>
<h4 id="considerazioni-sulluso-dei-mini-batch">Considerazioni sull&rsquo;uso dei Mini-Batch</h4>
<ul>
<li>
<p>Ogni mini-batch pu√≤ essere elaborato in <strong>parallelo</strong>, caratteristica che si sposa bene con l&rsquo;aumento di disponibilit√† e potenza delle <strong>architetture parallele</strong> come le <strong>GPGPU</strong> (General Purpose Graphic Processing Unit), sempre pi√π usate nei compiti di deep learning. In questo caso, la dimensione massima del batch √® limitata dall‚Äôhardware e dalla rappresentazione in memoria dei dati.</p>
</li>
<li>
<p>Mini-batch di <strong>piccole dimensioni</strong> possono avere un <strong>effetto regolarizzante</strong>, introducendo <strong>varianza nella stima del gradiente</strong>. Questo pu√≤ impedire all‚Äôalgoritmo di raggiungere il minimo esatto, contribuendo cos√¨ a <strong>ridurre l‚Äôoverfitting</strong>. Tuttavia, batch troppo piccoli (nel limite, apprendimento online con un solo dato per volta) introducono <strong>una varianza troppo elevata</strong>, richiedendo l‚Äôuso di un <strong>learning rate piccolo</strong> (meglio se <strong>decrescente</strong>) per mantenere la stabilit√† dell‚Äôalgoritmo.</p>
</li>
</ul>
<h3 id="confronto-grafico">Confronto Grafico</h3>
<p>Il seguente esempio Python illustra la differenza tra Batch, Mini-Batch e Stochastic Gradient Descent, evidenziando le traiettorie nel piano dei parametri:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                    const code = this.parentElement.querySelector('pre');
                    if (code) {
                        navigator.clipboard.writeText(code.innerText);
                        this.textContent = 'Copied!';
                        setTimeout(() => this.textContent = 'Copy', 2000);
                    }
                ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Dati sintetici</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Funzione di perdita</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Gradiente</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gradients</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="n">db</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>

<span class="c1"># Addestramento con step uniformi</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;batch&#39;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">w</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="n">b</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span>
            <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span>
            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">w</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="n">b</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span>
            <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;minibatch&#39;</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="n">steps</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
            <span class="n">w</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw</span>
            <span class="n">b</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span>
            <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">trajectory</span>

<span class="c1"># Tracciamento traiettorie (30 step)</span>
<span class="n">traj_batch</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">traj_sgd</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">traj_minibatch</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;minibatch&#39;</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Curve di livello</span>
<span class="n">w_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">b_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">W</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="n">b_range</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">w_range</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">b_range</span><span class="p">])</span>

<span class="c1"># Livelli coerenti e ordinati</span>
<span class="n">min_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">lower_limit</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_loss</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">all_levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lower_limit</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Z</span><span class="p">),</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">all_levels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contours</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%.2f</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Colori desaturati</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#3e8250&#39;</span><span class="p">,</span> <span class="s1">&#39;#567991&#39;</span><span class="p">,</span> <span class="s1">&#39;#b05541&#39;</span><span class="p">]</span>

<span class="c1"># Traiettorie</span>
<span class="k">for</span> <span class="n">traj</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">traj_batch</span><span class="p">,</span> <span class="n">traj_sgd</span><span class="p">,</span> <span class="n">traj_minibatch</span><span class="p">],</span>
                              <span class="p">[</span><span class="s1">&#39;Batch GD&#39;</span><span class="p">,</span> <span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="s1">&#39;Mini-Batch GD&#39;</span><span class="p">],</span>
                              <span class="n">colors</span><span class="p">):</span>
    <span class="n">w_vals</span><span class="p">,</span> <span class="n">b_vals</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">traj</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_vals</span><span class="p">,</span> <span class="n">b_vals</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

<span class="c1"># Minimo globale (approssimato analiticamente: w=3, b=2)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Minimo&#39;</span><span class="p">)</span>

<span class="c1"># Zoom centrato ma visibile anche l&#39;origine</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>

<span class="c1"># Stile</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Curve di livello della funzione di perdita con traiettorie&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/gradient-methods.png" alt="Gradient Descent Methods"></p>
<p><em>Figura 1.3: Confronto visivo tra le traiettorie di Batch Gradient Descent, SGD e Mini-Batch Gradient Descent</em></p>
<h3 id="conclusione">Conclusione</h3>
<p>Le varianti del Gradient Descent offrono una gamma di compromessi tra accuratezza, velocit√† e stabilit√†. In pratica:</p>
<ul>
<li><strong>Batch GD</strong> √® utile per modelli piccoli e dataset contenuti.</li>
<li><strong>SGD</strong> √® adatto a scenari online o dataset giganteschi.</li>
<li><strong>Mini-Batch GD</strong> √® lo standard nell&rsquo;apprendimento profondo per la sua efficienza.</li>
</ul>
<p>Le tecniche moderne includono anche ottimizzatori avanzati (come Adam, RMSProp, Adagrad), che combinano il gradiente con meccanismi adattivi per un miglior controllo della discesa, che tratteremo proprio nella sezione successiva.</p>
<h2 id="discesa-del-gradiente-con-momentum">Discesa del Gradiente con Momentum</h2>
<p>Uno dei principali limiti della discesa del gradiente standard √® la sua <strong>lentezza di convergenza</strong> in presenza di <strong>vallate strette e profonde</strong> nella funzione di perdita, oppure in direzioni con <strong>curvature molto diverse</strong> (ad esempio funzioni ‚Äúa sella‚Äù o ‚Äúa banana‚Äù). In questi casi, l‚Äôalgoritmo pu√≤ oscillare lungo le direzioni di maggiore curvatura, rallentando notevolmente il percorso verso il minimo.</p>
<p>Per mitigare questo problema, viene introdotto il concetto di <strong>momentum</strong>, ispirato alla fisica newtoniana: invece di aggiornare i parametri unicamente in base al gradiente attuale e al learning rate, si tiene conto anche della <strong>direzione e velocit√† del movimento passato</strong>, accumulando ‚Äúinerzia‚Äù lungo le direzioni coerenti.</p>
<h3 id="formula-dellaggiornamento-con-momentum">Formula dell&rsquo;Aggiornamento con Momentum</h3>
<p>L‚Äôalgoritmo introduce una variabile ausiliaria $\mathbf{v}^{(t)}$ che rappresenta la ‚Äúvelocit√†‚Äù del sistema, aggiornata iterativamente secondo:</p>
$$
\begin{aligned}
\mathbf{v}^{(t+1)} &= \lambda \cdot \mathbf{v}^{(t)} - \alpha \cdot \nabla \ell_{\Theta^{(t)}}, \\
\Theta^{(t+1)} &= \Theta^{(t)} + \mathbf{v}^{(t+1)}.
\end{aligned}
$$
<p>dove:</p>
<ul>
<li>$\alpha$ √® il <strong>learning rate</strong>,</li>
<li>$\lambda \in [0,1)$ √® il <strong>coefficiente di momentum</strong>, che controlla il peso del termine di velocit√† accumulato (valori tipici: $\lambda = 0.9$),</li>
<li>$\nabla \ell_{\Theta^{(t)}}$ √® il gradiente della funzione di perdita all‚Äôiterazione $t$,</li>
<li>$\mathbf{v}^{(t)}$ √® la velocit√† accumulata al passo precedente. Al tempo $t=0$, $\mathbf{v}^{(0)} = 0$.</li>
</ul>
<h3 id="interpretazione-intuitiva">Interpretazione Intuitiva</h3>
<ul>
<li>Quando i gradienti puntano nella <strong>stessa direzione</strong> in iterazioni successive, il termine $\lambda \cdot \mathbf{v}^{(t)}$ <strong>rafforza</strong> la velocit√† in quella direzione, rendendo l‚Äôavanzamento pi√π rapido.</li>
<li>Quando la direzione del gradiente <strong>cambia spesso</strong> (es. oscillazioni), il momentum <strong>smorza le variazioni</strong>, stabilizzando l‚Äôandamento e migliorando la convergenza.</li>
</ul>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                    const code = this.parentElement.querySelector('pre');
                    if (code) {
                        navigator.clipboard.writeText(code.innerText);
                        this.textContent = 'Copied!';
                        setTimeout(() => this.textContent = 'Copy', 2000);
                    }
                ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Funzione di costo</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Gradiente della funzione</span>
<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">y</span><span class="p">])</span>

<span class="c1"># GD semplice</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">steps</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">start</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># GD con momentum</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent_momentum</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">steps</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">start</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Parametri</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">optimum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>  <span class="c1"># punto di minimo</span>

<span class="c1"># Percorsi</span>
<span class="n">path_gd</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
<span class="n">path_mom</span> <span class="o">=</span> <span class="n">gradient_descent_momentum</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># Contorno della funzione</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># Setup figura allungata</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="n">axs</span><span class="p">,</span>
    <span class="p">[</span><span class="n">path_gd</span><span class="p">,</span> <span class="n">path_mom</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;(a) Without momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;(b) With momentum&#39;</span><span class="p">]</span>
<span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens_r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Starting point e Solution</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Starting Point&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">path</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Solution&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">))</span>

    <span class="c1"># Ottimo</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">optimum</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Optimum&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">optimum</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span>
                <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/momentum.png" alt="Momentum Gradient Descent"></p>
<p><em>Figura 1.3: La discesa del gradiente con momentum permette una traiettoria pi√π fluida e veloce verso il minimo, evitando oscillazioni e rallentamenti dovuti a curvature diverse nelle direzioni principali.</em></p>
<h3 id="derivazione-della-forma-chiusa-per-gd-con-momentum">Derivazione della forma chiusa per GD con Momentum</h3>
<p>L‚Äôobiettivo √® derivare una <strong>forma chiusa</strong> (non ricorsiva) dell‚Äôaggiornamento dei parametri al tempo $t+1$, in funzione di tutti i gradienti calcolati fino a quel momento. In questo modo possiamo analizzare in modo pi√π chiaro <strong>l‚Äôeffetto cumulativo del momentum</strong>, che combina i gradienti passati pesandoli secondo una <strong>decadimento geometrico</strong> controllato dall&rsquo; iperparametro $\lambda$. Questo permette di evidenziare come il metodo favorisca le direzioni persistenti nel tempo e smorzi le oscillazioni dovute a cambiamenti locali nel paesaggio della funzione di perdita.</p>
<p>Partiamo dalle <strong>equazioni ricorsive</strong> della discesa del gradiente con momentum:</p>
$$
\begin{cases}
\mathbf{v}^{(t+1)} = \lambda\,\mathbf{v}^{(t)} - \alpha \,\nabla \ell\bigl(\Theta^{(t)}\bigr),\\
\Theta^{(t+1)} = \Theta^{(t)} + \mathbf{v}^{(t+1)}.
\end{cases}
$$
<p>Vogliamo <strong>unrollare</strong> queste relazioni fino all‚Äôiterazione iniziale $\Theta^{(0)}$.</p>
<h4 id="1-espressione-ricorsiva-di-math_inline_112">1. Espressione ricorsiva di $\mathbf{v}^{(t+1)}$</h4>
<p>Applichiamo pi√π volte la definizione di $\mathbf{v}$:</p>
$$
\begin{aligned}
\mathbf{v}^{(1)} &= \lambda\,\mathbf{v}^{(0)} - \alpha\,\nabla \ell(\Theta^{(0)}),\\ 
\mathbf{v}^{(2)} &= \lambda\,\mathbf{v}^{(1)} - \alpha\,\nabla \ell(\Theta^{(1)})\\
&= \lambda \bigl(\lambda\,\mathbf{v}^{(0)} - \alpha\,\nabla \ell(\Theta^{(0)})\bigr)
  - \alpha\,\nabla \ell(\Theta^{(1)})\\
&= \lambda^2 \mathbf{v}^{(0)}
  - \alpha \bigl(\lambda\,\nabla \ell(\Theta^{(0)}) + \nabla \ell(\Theta^{(1)})\bigr),
\end{aligned}
$$
<p>e in generale, per $0 \le i \le t$:</p>
$$
\mathbf{v}^{(t+1)}
= \lambda^{\,t+1}\mathbf{v}^{(0)}
- \alpha \sum_{i=0}^{t} \lambda^{\,t-i}\,\nabla \ell\bigl(\Theta^{(i)}\bigr).
$$
<p>Spesso si assume $\mathbf{v}^{(0)}=\mathbf{0}$, da cui:</p>
$$
\mathbf{v}^{(t+1)}
= -\,\alpha \sum_{i=0}^{t} \lambda^{\,t-i}\,\nabla \ell\bigl(\Theta^{(i)}\bigr).
$$
<h4 id="2-unrolling-di-math_inline_116">2. Unrolling di $\Theta^{(t+1)}$</h4>
<p>Ora inseriamo $\mathbf{v}^{(t+1)}$ nell‚Äôaggiornamento di $\Theta$:</p>
$$
\begin{aligned}
\Theta^{(t+1)}
&= \Theta^{(t)} + \mathbf{v}^{(t+1)}\\
&= \Theta^{(t)} 
  - \alpha \sum_{i=0}^{t} \lambda^{\,t-i}\,\nabla \ell\bigl(\Theta^{(i)}\bigr).
\end{aligned}
$$
<p>Ripetendo ricorsivamente l‚Äôaggiornamento su $\Theta^{(t)}, \Theta^{(t-1)}, \dots, \Theta^{(0)}$, otteniamo:</p>
$$
\begin{aligned}
\Theta^{(t+1)}
&= \Theta^{(0)}
  - \alpha \sum_{k=0}^{t} \sum_{i=0}^{k} \lambda^{\,k-i}\,\nabla \ell\bigl(\Theta^{(i)}\bigr) \\
&= \Theta^{(0)}
  - \alpha \sum_{i=0}^{t} \Bigl(\sum_{k=i}^{t} \lambda^{\,k-i}\Bigr)\,\nabla \ell\bigl(\Theta^{(i)}\bigr).
\end{aligned}
$$
<h4 id="3-calcolo-della-somma-geometrica-interna">3. Calcolo della somma geometrica interna</h4>
<p>La somma interna $\displaystyle\sum_{k=i}^{t} \lambda^{\,k-i}$ √® una <strong>serie geometrica</strong> di ragione $\lambda$ e $t-i+1$ termini:</p>
$$
\sum_{k=i}^{t} \lambda^{\,k-i}
= \sum_{h=0}^{t-i} \lambda^{\,h}
= \frac{1 - \lambda^{\,t-i+1}}{1 - \lambda}.
$$
<h4 id="4-forma-finale">4. Forma finale</h4>
<p>Sostituendo nella formula di $\Theta^{(t+1)}$, otteniamo la forma chiusa:</p>
$$
\boxed{
\Theta^{(t+1)} 
= \Theta^{(0)} 
- \alpha \sum_{i=0}^{t} 
      \underbrace{\frac{1 - \lambda^{\,t-i+1}}{1 - \lambda}}_{\Gamma_i^t}
  \,\nabla \ell\bigl(\Theta^{(i)}\bigr).
}
$$
<p>Qui $\displaystyle\Gamma_i^t = \frac{1 - \lambda^{\,t+1-i}}{1 - \lambda}$ √® il <strong>fattore di accumulo</strong> che deriva dalla somma geometrica.</p>
<p>Questa espansione chiarisce perch√© il momentum aiuta a <strong>smussare oscillazioni</strong> e a <strong>favorire direzioni stabili</strong>, facilitando la convergenza pi√π rapida verso un minimo.</p>
<h3 id="confronto-con-gradient-descent-standard">Confronto con Gradient Descent Standard</h3>
<table>
<thead>
<tr>
<th>Metodo</th>
<th>Pro</th>
<th>Contro</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gradient Descent</strong></td>
<td>Preciso, semplice da implementare</td>
<td>Lento in presenza di vallate strette</td>
</tr>
<tr>
<td><strong>Momentum Gradient Descent</strong></td>
<td>Convergenza pi√π rapida e fluida</td>
<td>Richiede una variabile aggiuntiva ($\mathbf{v}$) e tuning di $\lambda$</td>
</tr>
</tbody>
</table>
<h3 id="osservazioni-finali">Osservazioni Finali</h3>
<ul>
<li>Il termine $\lambda$ controlla <strong>quanto &ldquo;lontano&rdquo; nel passato</strong> guardiamo per l‚Äôaccumulo di velocit√†. Valori troppo alti ($\lambda \approx 0.99$) possono causare overshooting, mentre valori bassi rendono il metodo simile al GD standard.</li>
<li>Il metodo con momentum √® la base di molte varianti moderne dell&rsquo;ottimizzazione, tra cui <strong>Nesterov Accelerated Gradient (NAG)</strong> e <strong>Adam</strong>.</li>
</ul>
<p>In sintesi, il momentum fornisce un <strong>bilanciamento intelligente tra memoria del passato e reattivit√† al presente</strong>, migliorando l‚Äôefficienza di convergenza e la stabilit√† numerica della discesa del gradiente.</p>
<h2 id="limiti-superiori-asintotici-convergenza-di-gd-e-sgd">Limiti Superiori Asintotici: Convergenza di GD e SGD</h2>
<p>Per problemi <strong>convessi</strong> (dove la funzione di loss ha un solo minimo globale), possiamo analizzare quanto velocemente i metodi di discesa del gradiente si avvicinano al minimo ottimo.</p>
<p>Assumiamo di voler trovare un punto $\Theta$ tale che la <strong>loss</strong> ottenuta sia entro una precisione $\rho > 0$ dall</p>
<p>dove:
- $\ell(f_\Theta)$ √® la loss del modello corrente,
- $\ell(f^*)$ √® la loss ottima (raggiunta in teoria dal miglior modello),
- $\rho$ √® l&rsquo;accuratezza desiderata.</p>
<h3 id="notazione">üìå Notazione</h3>
<ul>
<li>$n$ = numero di esempi nel dataset di training  </li>
<li>$d$ = numero di parametri (dimensione di $\Theta$)  </li>
<li>$\kappa$ = <strong>numero di condizionamento</strong>, ovvero $\kappa = L/\mu$, dove:</li>
<li>$L$ √® la <strong>costante di Lipschitz</strong> del gradiente: $\|\nabla \ell(\Theta_1) - \nabla \ell(\Theta_2)\| \le L \|\Theta_1 - \Theta_2\|$</li>
<li>$\mu$ √® la <strong>costante di forte convessit√†</strong>: $\ell(\Theta) \ge \ell(f^*) + \frac{\mu}{2}\|\Theta - \Theta^*\|^2$</li>
<li>$\nu$ = varianza del rumore stocastico nel gradiente, rilevante per SGD</li>
</ul>
<h3 id="costo-computazionale-per-iterazione">‚öôÔ∏è Costo Computazionale per Iterazione</h3>
<table>
<thead>
<tr>
<th>Metodo</th>
<th>Costo per iterazione</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GD</strong></td>
<td>$O(n\,d)$</td>
</tr>
<tr>
<td><strong>SGD</strong></td>
<td>$O(d)$</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>GD</strong>: calcola il gradiente <strong>esatto</strong>, sommando i contributi di tutti i $n$ esempi.</li>
<li><strong>SGD</strong>: usa un <strong>solo</strong> esempio (o minibatch), abbattendo il costo computazionale per iterazione.</li>
</ul>
<h3 id="numero-di-iterazioni-per-raggiungere-precisione-math_inline_147">üìà Numero di Iterazioni per Raggiungere Precisione $\rho$</h3>
<table>
<thead>
<tr>
<th>Metodo</th>
<th>Iterazioni necessarie</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GD</strong></td>
<td>$O\left(\kappa \log \frac{1}{\rho}\right)$</td>
</tr>
<tr>
<td><strong>SGD</strong></td>
<td>$O\left(\frac{\nu \kappa^2}{\rho}\right) + o\left(\frac{1}{\rho}\right)$</td>
</tr>
</tbody>
</table>
<h4 id="convergenza-di-gd-discesa-del-gradiente">‚ú≥Ô∏è Convergenza di GD (Discesa del Gradiente)</h4>
<p>Se $\ell$ √® fortemente convessa e ha gradiente Lipschitz, allora:</p>
$$
\ell(f_{\Theta^{(t)}}) - \ell(f^*) \le \left(1 - \frac{1}{\kappa} \right)^t \cdot (\ell(f_{\Theta^{(0)}}) - \ell(f^*)),
$$
<p>che converge <strong>esponenzialmente</strong> verso $\ell(f^*)$. Invertendo questa relazione, bastano:</p>
$$
t = O\left(\kappa \log \frac{1}{\rho} \right)
$$
<p>iterazioni per raggiungere precisione $\rho$.</p>
<h4 id="convergenza-di-sgd">‚ú≥Ô∏è Convergenza di SGD</h4>
<p>Nel caso stocastico, ogni passo √® pi√π &ldquo;rumoroso&rdquo;, quindi la convergenza √® pi√π lenta. Si pu√≤ dimostrare che:</p>
$$
\mathbb{E}[\ell(f_{\Theta^{(t)}})] - \ell(f^*) \le O\left( \frac{\nu \kappa^2}{t} \right),
$$
<p>dove $\nu$ riflette la varianza del gradiente stocastico. Per ottenere precisione $\rho$, servono:</p>
$$
t = O\left( \frac{\nu \kappa^2}{\rho} \right).
$$
<p>Quindi la <strong>convergenza √® sublineare</strong>: pi√π lenta, ma il costo per iterazione √® molto inferiore.</p>
<h3 id="confronto-finale">‚úÖ Confronto Finale</h3>
<ul>
<li><strong>GD</strong>: pi√π costoso per iterazione, ma converge <strong>molto pi√π velocemente</strong> (esponenzialmente in $\rho$).</li>
<li><strong>SGD</strong>: estremamente efficiente per iterazione, ma servono pi√π passi per avvicinarsi all&rsquo;ottimo.</li>
</ul>
<p>In pratica, <strong>SGD</strong> √® preferito nei grandi dataset (dove $n$ √® molto grande), mentre <strong>GD</strong> √® ideale per problemi pi√π piccoli o ben condizionati.</p>
<h2 id="criteri-di-arresto-per-la-discesa-del-gradiente">Criteri di arresto per la discesa del gradiente</h2>
<h2 id="conclusioni">Conclusioni</h2>
<p>La discesa del gradiente si conferma come uno degli algoritmi fondamentali nell&rsquo;ottimizzazione di modelli matematici e machine learning. Attraverso un&rsquo;analisi multidimensionale, emergono chiaramente diversi aspetti cruciali:</p>
<ol>
<li>
<p><strong>Natura Iterativa e Sfide</strong>:</p>
</li>
<li>
<p>La dipendenza dalle condizioni iniziali e la presenza di minimi locali in funzioni non convesse sottolineano l&rsquo;importanza di strategie di inizializzazione robuste.</p>
</li>
<li>
<p>I punti di sella, sebbene teoricamente problematici, risultano meno critici in pratica grazie all&rsquo;instabilit√† numerica e all&rsquo;alta dimensionalit√† degli spazi di parametri.</p>
</li>
<li>
<p><strong>Differenziabilit√† e Continuit√†</strong>:</p>
</li>
<li>
<p>La differenziabilit√† della funzione obiettivo √® un requisito essenziale per il calcolo del gradiente, con implicazioni pratiche nella scelta delle funzioni di attivazione e di loss.</p>
</li>
<li>
<p>Casi patologici come funzioni con derivate parziali discontinue evidenziano la necessit√† di verifiche analitiche preliminari.</p>
</li>
<li>
<p><strong>Aspetti Implementativi</strong>:</p>
</li>
<li>
<p>Il <em>learning rate</em> si rivela un iperparametro critico, con strategie come il decadimento dinamico e il <em>line search</em> che mitigano rischi di divergenza o convergenza lenta.</p>
</li>
<li>
<p>L&rsquo;eterogeneit√† delle curvature del terreno di ottimizzazione motiva l&rsquo;adozione di tecniche avanzate come il momentum, che accelerano la convergenza smorzando le oscillazioni.</p>
</li>
<li>
<p><strong>Trade-off Computazionali</strong>:</p>
</li>
<li>
<p>Il confronto tra Batch GD, SGD e Mini-Batch GD delinea un chiaro compromesso tra precisione, costo computazionale e rumore stocastico, con la variante Mini-Batch che rappresenta spesso il miglior bilanciamento per applicazioni su larga scala.</p>
</li>
<li>
<p>I limiti superiori asintotici rivelano come SGD sia preferibile in scenari <em>big data</em> nonostante una convergenza teorica pi√π lenta, grazie alla scalabilit√† indipendente dalla dimensione del dataset.</p>
</li>
<li>
<p><strong>Prospettive Moderne</strong>:</p>
</li>
<li>
<p>Estensioni come Nesterov Momentum e ottimizzatori adattativi (es. Adam) ereditano i principi della discesa del gradiente classica, integrando meccanismi di auto-regolazione per gestire paesaggi di loss complessi.</p>
</li>
</ol>
<p>In sintesi, la discesa del gradiente non √® solo un algoritmo ma un <em>framework concettuale</em> che unisce rigore matematico e pragmatismo computazionale. La sua efficacia deriva dall&rsquo;armonia tra teoria dell&rsquo;ottimizzazione, intuizione geometrica e adattamento alle sfide ingegneristiche, rendendolo uno strumento indispensabile nell&rsquo;era dei modelli ad alta dimensionalit√†.</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> mathematics, machine learning, linear algebra, calculus, statistics, model, data, neural</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'Discesa del Gradiente',
          page_location: 'http://localhost:3000/theory/math-for-ml/Ottimizzazione/Non-Lineare/Discesa del Gradiente'
        });
      }
    </script>
</body>
</html>