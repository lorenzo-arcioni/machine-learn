<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Attention: Parallelizzazione e Diversificazione dell'Attenzione | Deep Learning | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="deep learning, neural networks, CNN, RNN, transformers, model, learning">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Multi-Head Attention: Parallelizzazione e Diversificazione dell'Attenzione">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/deep-learning/Transformers/Attention/Multi-Head Attention">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Multi-Head Attention: Parallelizzazione e Diversificazione dell'Attenzione">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/deep-learning/Transformers/Attention/Multi-Head Attention">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Multi-Head Attention: Parallelizzazione e Diversificazione dell'Attenzione",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/deep-learning/Transformers/Attention/Multi-Head Attention",
      "datePublished": "2025-09-25T14:48:27.746Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/deep-learning/Transformers/Attention/Multi-Head Attention" class="react-redirect">ðŸš€ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>Multi-Head Attention: Parallelizzazione e Diversificazione dell'Attenzione</h1>
                <div class="meta">
                    <strong>Topic:</strong> Deep Learning | 
                    <strong>Updated:</strong> 25/09/2025
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
/* Blocchi di codice sempre visibili */
.code-visible {
    border: 1px solid #e5e7eb;
    border-radius: 12px;
    background: #f9fafb;
    margin: 16px 0;
}
.code-visible .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<h2 id="introduzione-e-motivazione">Introduzione e Motivazione</h2>
<p>Il meccanismo di <strong>self-attention</strong> che abbiamo esplorato Ã¨ potente, ma presenta una limitazione fondamentale: utilizza un singolo &ldquo;tipo&rdquo; di attenzione per catturare tutte le relazioni nella sequenza. Ãˆ come avere un unico paio di occhi per osservare una scena complessa &ndash; possiamo vedere tutto, ma da una sola prospettiva.</p>
<p>Consideriamo la frase: <em>&ldquo;Il professore di matematica che insegna alla scuola elementare ha spiegato il teorema.&rdquo;</em> In questa frase coesistono diversi tipi di relazioni:</p>
<ul>
<li><strong>Relazioni sintattiche</strong>: &ldquo;professore&rdquo; Ã¨ il soggetto di &ldquo;ha spiegato&rdquo;</li>
<li><strong>Relazioni semantiche</strong>: &ldquo;teorema&rdquo; Ã¨ collegato concettualmente a &ldquo;matematica&rdquo;</li>
<li><strong>Relazioni di modificazione</strong>: &ldquo;di matematica&rdquo; modifica &ldquo;professore&rdquo;</li>
<li><strong>Relazioni temporali</strong>: &ldquo;ha spiegato&rdquo; indica un&rsquo;azione passata</li>
</ul>
<p>Un singolo meccanismo di attention potrebbe non riuscire a catturare simultaneamente tutte queste sfumature. Qui entra in gioco la <strong>Multi-Head Attention</strong>: invece di avere una sola &ldquo;testa&rdquo; di attention, ne abbiamo multiple, ognuna specializzata nel catturare diversi aspetti delle relazioni nella sequenza.</p>
<h2 id="lintuizione-della-multi-head-attention">L&rsquo;Intuizione della Multi-Head Attention</h2>
<h3 id="lanalogia-degli-esperti-specializzati">L&rsquo;Analogia degli Esperti Specializzati</h3>
<p>Immaginiamo di dover analizzare un documento complesso. Invece di affidarci a un singolo esperto generico, potremmo consultare:</p>
<ul>
<li>Un <strong>linguista</strong> che analizza la struttura grammaticale</li>
<li>Un <strong>esperto di dominio</strong> che comprende il contenuto tecnico  </li>
<li>Un <strong>analista del discorso</strong> che cattura le connessioni logiche</li>
<li>Un <strong>esperto di stile</strong> che valuta il tono e il registro</li>
</ul>
<p>Ogni esperto porta una prospettiva diversa, e la loro combinazione fornisce una comprensione piÃ¹ ricca del testo. La Multi-Head Attention replica questo principio: ogni &ldquo;testa&rdquo; Ã¨ come un esperto specializzato che si concentra su aspetti diversi delle relazioni nella sequenza.</p>
<h3 id="diversificazione-delle-rappresentazioni">Diversificazione delle Rappresentazioni</h3>
<p>Dal punto di vista matematico, una singola testa di attention opera in un sottospazio specifico dello spazio delle caratteristiche. Le trasformazioni lineari $\mathbf{W}_q$, $\mathbf{W}_k$, e $\mathbf{W}_v$ definiscono questo sottospazio, determinando quali aspetti dell&rsquo;input vengono enfatizzati.</p>
<p>Con la Multi-Head Attention, ogni testa opera attraverso le proprie trasformazioni lineari, potenzialmente catturando:</p>
<ul>
<li><strong>Pattern locali</strong>: relazioni tra parole adiacenti</li>
<li><strong>Pattern globali</strong>: relazioni a lungo raggio</li>
<li><strong>Pattern semantici</strong>: similaritÃ  concettuali</li>
<li><strong>Pattern sintattici</strong>: strutture grammaticali</li>
</ul>
<h2 id="formulazione-matematica-della-multi-head-attention">Formulazione Matematica della Multi-Head Attention</h2>
<h3 id="architettura-delle-teste-multiple">Architettura delle Teste Multiple</h3>
<p>Consideriamo $h$ teste di attention parallele. Per ogni testa $i$ (con $i = 1, 2, \ldots, h$), definiamo trasformazioni lineari separate:</p>
$$\mathbf{Q}^{(i)} = \mathbf{W}_q^{(i)} \mathbf{X} + \mathbf{b}_q^{(i)} \mathbf{1}^T \in \mathbb{R}^{d_k \times N}$$
$$\mathbf{K}^{(i)} = \mathbf{W}_k^{(i)} \mathbf{X} + \mathbf{b}_k^{(i)} \mathbf{1}^T \in \mathbb{R}^{d_k \times N}$$
$$\mathbf{V}^{(i)} = \mathbf{W}_v^{(i)} \mathbf{X} + \mathbf{b}_v^{(i)} \mathbf{1}^T \in \mathbb{R}^{d_v \times N}$$
<p>dove:
- $\mathbf{W}_q^{(i)} \in \mathbb{R}^{d_k \times d}$, $\mathbf{W}_k^{(i)} \in \mathbb{R}^{d_k \times d}$, $\mathbf{W}_v^{(i)} \in \mathbb{R}^{d_v \times d}$ sono le matrici di peso della testa $i$
- $\mathbf{b}_q^{(i)} \in \mathbb{R}^{d_k}$, $\mathbf{b}_k^{(i)} \in \mathbb{R}^{d_k}$, $\mathbf{b}_v^{(i)} \in \mathbb{R}^{d_v}$ sono i vettori di bias della testa $i$
- $d_k$ e $d_v$ sono le dimensioni delle query/key e dei value rispettivamente</p>
<h3 id="calcolo-dellattention-per-ogni-testa">Calcolo dell&rsquo;Attention per Ogni Testa</h3>
<p>Ogni testa calcola indipendentemente la propria attention:</p>
$$\text{head}_i = \text{Attention}(\mathbf{Q}^{(i)}, \mathbf{K}^{(i)}, \mathbf{V}^{(i)}) = \mathbf{V}^{(i)} \cdot \text{SoftMax}\left(\frac{(\mathbf{K}^{(i)})^T\mathbf{Q}^{(i)}}{\sqrt{d_k}}\right)$$
<p>Il risultato $\text{head}_i \in \mathbb{R}^{d_v \times N}$ rappresenta l&rsquo;output della testa $i$, dove ogni colonna $n$ contiene la rappresentazione dell&rsquo;elemento in posizione $n$ secondo la prospettiva della testa $i$.</p>
<h3 id="concatenazione-e-proiezione-finale">Concatenazione e Proiezione Finale</h3>
<p>Gli output di tutte le teste vengono concatenati lungo la dimensione delle caratteristiche:</p>
$$\text{Concat} = \text{Concatenate}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h) \in \mathbb{R}^{(h \cdot d_v) \times N}$$
<p>Matematicamente, se indichiamo con $\mathbf{H}_i \in \mathbb{R}^{d_v \times N}$ l&rsquo;output della testa $i$, la concatenazione Ã¨:</p>
$$\text{Concat} = \begin{bmatrix}
\mathbf{H}_1 \\
\mathbf{H}_2 \\
\vdots \\
\mathbf{H}_h
\end{bmatrix} = \begin{bmatrix}
| & | & & | \\
\mathbf{h}_1^{(1)} & \mathbf{h}_2^{(1)} & \cdots & \mathbf{h}_N^{(1)} \\
| & | & & | \\
| & | & & | \\
\mathbf{h}_1^{(2)} & \mathbf{h}_2^{(2)} & \cdots & \mathbf{h}_N^{(2)} \\
| & | & & | \\
\vdots & \vdots & \ddots & \vdots \\
| & | & & | \\
\mathbf{h}_1^{(h)} & \mathbf{h}_2^{(h)} & \cdots & \mathbf{h}_N^{(h)} \\
| & | & & |
\end{bmatrix}$$
<p>dove $\mathbf{h}_n^{(i)} \in \mathbb{R}^{d_v}$ Ã¨ l&rsquo;output della testa $i$ per la posizione $n$.</p>
<p>Infine, una trasformazione lineare finale ricombina le informazioni da tutte le teste:</p>
$$\text{MultiHead}(\mathbf{X}) = \mathbf{W}_O \cdot \text{Concat} + \mathbf{b}_O \mathbf{1}^T$$
<p>dove $\mathbf{W}_O \in \mathbb{R}^{d_{model} \times (h \cdot d_v)}$ e $\mathbf{b}_O \in \mathbb{R}^{d_{model}}$ sono i parametri della proiezione finale.</p>
<blockquote>
<p>Ricordiamo che $d_{model}$ rappresenta la dimensione dello spazio degli embedding, mentre $h$ rappresenta il numero di teste.</p>
</blockquote>
<h2 id="la-scelta-delle-dimensioni-un-compromesso-cruciale">La Scelta delle Dimensioni: Un Compromesso Cruciale</h2>
<h3 id="il-principio-della-conservazione-computazionale">Il Principio della Conservazione Computazionale</h3>
<p>Nei Transformer standard, si adotta una strategia elegante per mantenere approssimativamente costante il costo computazionale rispetto alla single-head attention:</p>
$$d_k = d_v = \frac{d_{model}}{h}$$
<p>Questa scelta garantisce che:</p>
<ol>
<li><strong>Costo computazionale simile</strong>: Il costo di calcolare $h$ teste con dimensione $d_k = d_{model}/h$ Ã¨ comparabile al costo di calcolare una testa con dimensione $d_{model}$</li>
<li><strong>Conservazione dell&rsquo;informazione</strong>: La concatenazione ricrea uno spazio di dimensione $h \cdot (d_{model}/h) = d_{model}$</li>
</ol>
<h3 id="analisi-del-compromesso">Analisi del Compromesso</h3>
<p>Questa strategia implica un compromesso fondamentale:</p>
<p><strong>Guadagno in diversitÃ </strong>: Ogni testa opera attraverso le proprie proiezioni lineari specializzate, catturando pattern diversi.</p>
<p><strong>Perdita in capacitÃ  individuale</strong>: Ogni singola testa ha meno &ldquo;potenza rappresentativa&rdquo; rispetto a una testa che opera nell&rsquo;intero spazio $d_{model}$.</p>
<p>Il successo empirico dei Transformer suggerisce che il guadagno in diversitÃ  supera ampiamente la perdita in capacitÃ  individuale.</p>
<h3 id="interpretazione-geometrica">Interpretazione Geometrica</h3>
<p>Dal punto di vista geometrico, ogni testa proietta l&rsquo;input attraverso le proprie matrici di peso su sottospazi che possono sovrapporsi parzialmente. La concatenazione finale ricombina queste diverse prospettive in una rappresentazione nell&rsquo;intero spazio $\mathbb{R}^{d_{model}}$.</p>
<h2 id="diversi-tipi-di-attention-patterns">Diversi Tipi di Attention Patterns</h2>
<h3 id="pattern-locali-vs-globali">Pattern Locali vs Globali</h3>
<p>Attraverso l&rsquo;addestramento, diverse teste tendono a specializzarsi naturalmente:</p>
<p><strong>Teste locali</strong>: Si concentrano su relazioni tra parole vicine, catturando pattern sintattici locali come accordi soggetto-verbo o relazioni articolo-nome.</p>
<p><strong>Teste globali</strong>: Catturano dipendenze a lungo raggio, come la correferenza pronominale o relazioni tematiche che attraversano l&rsquo;intera frase.</p>
<h3 id="pattern-sintattici-vs-semantici">Pattern Sintattici vs Semantici</h3>
<p><strong>Teste sintattiche</strong>: Imparano a identificare strutture grammaticali, come:
- Relazioni di dipendenza sintattica
- Gerarchia delle frasi subordinate
- Pattern di reggenza verbale</p>
<p><strong>Teste semantiche</strong>: Si focalizzano su:
- SimilaritÃ  concettuale tra parole
- Relazioni tematiche (agente, paziente, strumento)
- Coerenza semantica globale</p>
<h3 id="evidenze-empiriche">Evidenze Empiriche</h3>
<p>Studi di interpretabilitÃ  hanno mostrato che nei Transformer pre-addestrati:</p>
<ul>
<li>Alcune teste si specializzano nel tracciare dipendenze sintattiche specifiche</li>
<li>Altre teste catturano pattern semantici ricorrenti</li>
<li>Teste nei layer piÃ¹ bassi tendono a focalizzarsi su pattern locali</li>
<li>Teste nei layer piÃ¹ alti catturano relazioni piÃ¹ astratte e globali</li>
</ul>
<h2 id="vantaggi-della-multi-head-attention">Vantaggi della Multi-Head Attention</h2>
<h3 id="1-ricchezza-rappresentazionale">1. Ricchezza Rappresentazionale</h3>
<p>La possibilitÃ  di catturare simultaneamente diversi tipi di relazioni rende le rappresentazioni piÃ¹ ricche e informative. Una singola parola puÃ² essere rappresentata considerando:</p>
<ul>
<li>La sua funzione sintattica locale</li>
<li>Il suo ruolo semantico globale</li>
<li>Le sue relazioni di dipendenza</li>
<li>Il suo contributo al significato generale</li>
</ul>
<h3 id="2-robustezza">2. Robustezza</h3>
<p>La diversificazione delle teste aumenta la robustezza del modello:</p>
<ul>
<li>Se una testa &ldquo;fallisce&rdquo; nel catturare un pattern importante, altre teste possono compensare</li>
<li>La ridondanza parziale tra teste diverse previene l&rsquo;overfitting a pattern specifici</li>
<li>La combinazione di prospettive diverse Ã¨ meno sensibile al rumore nei dati</li>
</ul>
<h3 id="3-interpretabilita">3. InterpretabilitÃ </h3>
<p>Ogni testa fornisce una &ldquo;vista&rdquo; interpretabile su ciÃ² che il modello ha appreso:</p>
<ul>
<li>Possiamo visualizzare i pattern di attention di ciascuna testa</li>
<li>L&rsquo;analisi delle teste aiuta a comprendere quali aspetti linguistici il modello cattura</li>
<li>La specializzazione delle teste fornisce insight sui meccanismi interni del modello</li>
</ul>
<h3 id="4-parallelizzazione">4. Parallelizzazione</h3>
<p>Tutte le teste possono calcolare l&rsquo;attention in parallelo (con l&rsquo;implementazione appropriata), permettendo:</p>
<ul>
<li>Parallelizzazione massima su hardware moderno</li>
<li>Scaling efficiente con il numero di teste</li>
<li>Ottimizzazione dell&rsquo;utilizzo della memoria</li>
</ul>
<h2 id="implementazione-multi-head-self-attention">Implementazione Multi-Head Self-Attention</h2>
<h3 id="panoramica-dellimplementazione">Panoramica dell&rsquo;Implementazione</h3>
<p>La nostra implementazione della <code>Multi-Head Self-Attention</code> traduce fedelmente la matematica teorica in codice PyTorch efficiente. Analizziamo ogni componente per comprendere come la teoria si trasforma in pratica ottimizzata.</p>
<h3 id="struttura-della-classe-e-inizializzazione">Struttura della Classe e Inizializzazione</h3>
<h4 id="definizione-della-classe-e-docstring">Definizione della Classe e Docstring</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementazione efficiente di Multi-Head Self-Attention.</span>

<span class="sd">    FORMALISMO:</span>
<span class="sd">    - Input: X âˆˆ R^(N Ã— d_model) (convenzione PyTorch)</span>
<span class="sd">    - Per ogni testa i:</span>
<span class="sd">      - Q^(i) = X @ W_q^(i)^T + b_q^(i) âˆˆ R^(N Ã— d_k)</span>
<span class="sd">      - K^(i) = X @ W_k^(i)^T + b_k^(i) âˆˆ R^(N Ã— d_k)</span>
<span class="sd">      - V^(i) = X @ W_v^(i)^T + b_v^(i) âˆˆ R^(N Ã— d_v)</span>
<span class="sd">      - head_i = Attention(Q^(i), K^(i), V^(i)) âˆˆ R^(N Ã— d_v)</span>
<span class="sd">    - Concat = Concatenate(head_1, ..., head_h) âˆˆ R^(N Ã— hÂ·d_v)</span>
<span class="sd">    - Output = Concat @ W_O^T + b_O âˆˆ R^(N Ã— d_model)</span>

<span class="sd">    Standard: d_k = d_v = d_model / h per bilanciamento computazionale</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>: La docstring chiarisce la convenzione PyTorch (N Ã— features) e mostra le dimensioni corrette per tutte le trasformazioni.</p>
<h4 id="parametri-del-costruttore">Parametri del Costruttore</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</code></pre></div>
</div>
</details>

<h4 id="gestione-delle-dimensioni">Gestione delle Dimensioni</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Dimensioni di default per bilanciamento computazionale</span>
<span class="k">if</span> <span class="n">d_k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
<span class="k">if</span> <span class="n">d_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">d_v</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

<span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_model deve essere divisibile per num_heads per dimensioni standard&quot;</span>

<span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
<span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>: Manteniamo il principio di bilanciamento computazionale con dimensioni $d_k = d_v = d_{model}/h$.</p>
<h4 id="implementazione-efficiente-con-proiezioni-unificate">Implementazione Efficiente con Proiezioni Unificate</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Proiezioni unificate per tutte le teste (implementazione efficiente)</span>
<span class="c1"># W_q_all âˆˆ R^(d_model Ã— hÂ·d_k), W_k_all âˆˆ R^(d_model Ã— hÂ·d_k), W_v_all âˆˆ R^(d_model Ã— hÂ·d_v)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">W_q_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">W_k_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="bp">self</span><span class="o">.</span><span class="n">W_v_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Proiezione finale: W_O âˆˆ R^(hÂ·d_v Ã— d_model)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">W_O</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>: 
- <strong>Efficienza</strong>: Una singola trasformazione lineare per tutte le teste invece di $h$ trasformazioni separate
- <strong>Equivalenza matematica</strong>: Il risultato Ã¨ identico ma molto piÃ¹ efficiente
- <strong>Gestione automatica</strong>: PyTorch gestisce automaticamente inizializzazione e bias</p>
<h4 id="inizializzazione-dei-pesi">Inizializzazione dei Pesi</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Inizializzazione Xavier/Glorot per stabilitÃ  numerica</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Inizializza i pesi per stabilitÃ  numerica&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W_q_all</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k_all</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v_all</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">]:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>: L&rsquo;inizializzazione Xavier Ã¨ standard per reti profonde e garantisce gradienti ben condizionati.</p>
<h3 id="metodo-forward-implementazione-efficiente">Metodo Forward: Implementazione Efficiente</h3>
<h4 id="preparazione-dellinput">Preparazione dell&rsquo;Input</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Input (batch_size, seq_len, d_model)</span>
<span class="sd">        return_attention: Se restituire i pesi di attention</span>

<span class="sd">    Returns:</span>
<span class="sd">        output: (batch_size, seq_len, d_model)</span>
<span class="sd">        attention_weights: (batch_size, num_heads, seq_len, seq_len) se richiesti</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<h4 id="calcolo-unificato-di-q-k-v">Calcolo Unificato di Q, K, V</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Calcolo efficiente di Q, K, V per tutte le teste</span>
<span class="c1"># Shape: (batch_size, seq_len, num_heads * d_k/d_v)</span>
<span class="n">Q_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q_all</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, num_heads * d_k)</span>
<span class="n">K_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k_all</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, num_heads * d_k)</span>
<span class="n">V_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v_all</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, num_heads * d_v)</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>: Una singola moltiplicazione matrice-matrice invece di $h$ moltiplicazioni separate.</p>
<h4 id="reshaping-per-teste-multiple">Reshaping per Teste Multiple</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Reshape per separare le teste: (batch_size, seq_len, num_heads, d_k/d_v)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">Q_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>

<span class="c1"># Riorganizza per calcolo parallelo: (batch_size, num_heads, seq_len, d_k/d_v)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, d_k)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, d_k)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, d_v)</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>: 
- <strong>Reshape</strong>: Separa logicamente le teste senza costo computazionale
- <strong>Transpose</strong>: Porta le teste nella dimensione del batch per parallelizzazione</p>
<h4 id="calcolo-parallelo-dellattention">Calcolo Parallelo dell&rsquo;Attention</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Calcolo parallelo degli attention scores per tutte le teste</span>
<span class="c1"># S âˆˆ R^(batch_size Ã— num_heads Ã— seq_len Ã— seq_len)</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>

<span class="c1"># Applicazione softmax lungo l&#39;ultima dimensione (normalizza per ogni query)</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Applicazione dei pesi ai values</span>
<span class="c1"># Output per tutte le teste: (batch_size, num_heads, seq_len, d_v)</span>
<span class="n">attention_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>:
- <strong>Parallelizzazione completa</strong>: Tutte le teste calcolate simultaneamente
- <strong>Softmax corretto</strong>: <code>dim=-1</code> normalizza lungo le keys per ogni query
- <strong>Efficienza</strong>: Sfrutta al massimo le capacitÃ  tensoriali di PyTorch</p>
<h4 id="concatenazione-e-proiezione-finale_1">Concatenazione e Proiezione Finale</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Riorganizza per concatenazione: (batch_size, seq_len, num_heads, d_v)</span>
<span class="n">attention_output</span> <span class="o">=</span> <span class="n">attention_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Concatenazione delle teste: (batch_size, seq_len, num_heads * d_v)</span>
<span class="n">concatenated</span> <span class="o">=</span> <span class="n">attention_output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>

<span class="c1"># Proiezione finale: (batch_size, seq_len, d_model)</span>
<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">(</span><span class="n">concatenated</span><span class="p">)</span>
</code></pre></div>
</div>
</details>

<p><strong>Analisi</strong>:
- <strong>Transpose + View</strong>: Concatenazione efficiente senza copia dei dati
- <strong>Contiguous</strong>: Necessario dopo transpose per garantire memoria contigua
- <strong>Proiezione finale</strong>: Combina le informazioni da tutte le teste</p>
<h4 id="gestione-delloutput">Gestione dell&rsquo;Output</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
</div>
</details>

<h3 id="implementazione-completa-e-ottimizzata">Implementazione Completa e Ottimizzata</h3>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementazione efficiente di Multi-Head Self-Attention.</span>

<span class="sd">    FORMALISMO:</span>
<span class="sd">    - Input: X âˆˆ R^(N Ã— d_model) (convenzione PyTorch)</span>
<span class="sd">    - Per ogni testa i:</span>
<span class="sd">      - Q^(i) = X @ W_q^(i)^T + b_q^(i) âˆˆ R^(N Ã— d_k)</span>
<span class="sd">      - K^(i) = X @ W_k^(i)^T + b_k^(i) âˆˆ R^(N Ã— d_k)</span>
<span class="sd">      - V^(i) = X @ W_v^(i)^T + b_v^(i) âˆˆ R^(N Ã— d_v)</span>
<span class="sd">      - head_i = Attention(Q^(i), K^(i), V^(i)) âˆˆ R^(N Ã— d_v)</span>
<span class="sd">    - Concat = Concatenate(head_1, ..., head_h) âˆˆ R^(N Ã— hÂ·d_v)</span>
<span class="sd">    - Output = Concat @ W_O^T + b_O âˆˆ R^(N Ã— d_model)</span>

<span class="sd">    Standard: d_k = d_v = d_model / h per bilanciamento computazionale</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="c1"># Dimensioni di default per bilanciamento computazionale</span>
        <span class="k">if</span> <span class="n">d_k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="k">if</span> <span class="n">d_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">d_v</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_model deve essere divisibile per num_heads per dimensioni standard&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>

        <span class="c1"># Proiezioni unificate per tutte le teste (implementazione efficiente)</span>
        <span class="c1"># W_q_all âˆˆ R^(d_model Ã— hÂ·d_k), W_k_all âˆˆ R^(d_model Ã— hÂ·d_k), W_v_all âˆˆ R^(d_model Ã— hÂ·d_v)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_q_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_k_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">W_v_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Proiezione finale: W_O âˆˆ R^(hÂ·d_v Ã— d_model)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Inizializzazione dei pesi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Inizializza i pesi per stabilitÃ  numerica&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W_q_all</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k_all</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v_all</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">]:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input (batch_size, seq_len, d_model)</span>
<span class="sd">            return_attention: Se restituire i pesi di attention</span>

<span class="sd">        Returns:</span>
<span class="sd">            output: (batch_size, seq_len, d_model)</span>
<span class="sd">            attention_weights: (batch_size, num_heads, seq_len, seq_len) se richiesti</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="c1"># Calcolo efficiente di Q, K, V per tutte le teste</span>
        <span class="c1"># Shape: (batch_size, seq_len, num_heads * d_k/d_v)</span>
        <span class="n">Q_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q_all</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, num_heads * d_k)</span>
        <span class="n">K_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k_all</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, num_heads * d_k)</span>
        <span class="n">V_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v_all</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, num_heads * d_v)</span>

        <span class="c1"># Reshape per separare le teste: (batch_size, seq_len, num_heads, d_k/d_v)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_all</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>

        <span class="c1"># Riorganizza per calcolo parallelo: (batch_size, num_heads, seq_len, d_k/d_v)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, d_k)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, d_k)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, d_v)</span>

        <span class="c1"># Calcolo parallelo degli attention scores per tutte le teste</span>
        <span class="c1"># S âˆˆ R^(batch_size Ã— num_heads Ã— seq_len Ã— seq_len)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>

        <span class="c1"># Applicazione softmax lungo l&#39;ultima dimensione (normalizza per ogni query)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Applicazione dei pesi ai values</span>
        <span class="c1"># Output per tutte le teste: (batch_size, num_heads, seq_len, d_v)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

        <span class="c1"># Riorganizza per concatenazione: (batch_size, seq_len, num_heads, d_v)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">attention_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Concatenazione delle teste: (batch_size, seq_len, num_heads * d_v)</span>
        <span class="n">concatenated</span> <span class="o">=</span> <span class="n">attention_output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>

        <span class="c1"># Proiezione finale: (batch_size, seq_len, d_model)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">(</span><span class="n">concatenated</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>


<span class="c1"># Funzione di utilitÃ  per testing</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_multi_head_attention</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test della Multi-Head Attention&quot;&quot;&quot;</span>
    <span class="c1"># Parametri di test</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="c1"># Crea il modulo</span>
    <span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="c1"># Input di test</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Test forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Output deve avere la stessa forma dell&#39;input&quot;</span>

    <span class="c1"># Test con attention weights</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention weights shape: </span><span class="si">{</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">expected_attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">expected_attn_shape</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Attention weights dovrebbero avere forma </span><span class="si">{</span><span class="n">expected_attn_shape</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># Verifica che i pesi sommino a 1 lungo l&#39;ultima dimensione</span>
    <span class="n">attn_sum</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">attn_sum</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">attn_sum</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span> <span class="s2">&quot;I pesi di attention dovrebbero sommare a 1&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;âœ“ Tutti i test sono passati!&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">test_multi_head_attention</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p>Questa implementazione fornisce una base solida per comprendere e sperimentare con la Multi-Head Attention, mantenendo la chiarezza concettuale pur essendo efficiente in pratica.</p>
<h2 id="analisi-della-complessita-computazionale">Analisi della ComplessitÃ  Computazionale</h2>
<h3 id="costo-per-singola-testa">Costo per Singola Testa</h3>
<p>Con le dimensioni standard $d_k = d_v = d_{model}/h$, ogni testa ha complessitÃ :</p>
$$O\left(N^2 \cdot \frac{d_{model}}{h} + N \cdot d_{model} \cdot \frac{d_{model}}{h}\right) = O\left(\frac{N^2 d_{model}}{h} + \frac{N d_{model}^2}{h}\right)$$
<h3 id="costo-totale">Costo Totale</h3>
<p>Per $h$ teste:</p>
$$h \cdot O\left(\frac{N^2 d_{model}}{h} + \frac{N d_{model}^2}{h}\right) = O(N^2 d_{model} + N d_{model}^2)$$
<p>Questo Ã¨ <strong>identico</strong> al costo della single-head attention con dimensione $d_{model}$, confermando la conservazione computazionale.</p>
<h3 id="costo-della-proiezione-finale">Costo della Proiezione Finale</h3>
<p>La moltiplicazione $\mathbf{W}_O \cdot \text{Concat}$ richiede $O(N \cdot d_{model}^2)$ operazioni, che Ã¨ dominato dal termine quadratico $O(N^2 d_{model})$ per sequenze lunghe.</p>
<h2 id="la-formula-completa-della-multi-head-attention">La Formula Completa della Multi-Head Attention</h2>
<p>Combinando tutti i componenti, otteniamo la formula completa:</p>
$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O$$
<p>dove:</p>
$$\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)$$
<p>e le matrici di peso sono:
- $\mathbf{W}_i^Q \in \mathbb{R}^{d_{model} \times d_k}$
- $\mathbf{W}_i^K \in \mathbb{R}^{d_{model} \times d_k}$<br />
- $\mathbf{W}_i^V \in \mathbb{R}^{d_{model} \times d_v}$
- $\mathbf{W}^O \in \mathbb{R}^{hd_v \times d_{model}}$</p>
<h2 id="implementazione-e-ottimizzazioni">Implementazione e Ottimizzazioni</h2>
<p>[Placeholder per implementazione Python della Multi-Head Attention]</p>
<h3 id="ottimizzazioni-pratiche">Ottimizzazioni Pratiche</h3>
<p><strong>Calcolo Batch</strong>: Tutte le teste possono essere calcolate in un unico batch, riorganizzando le matrici per sfruttare al meglio le operazioni tensoriali.</p>
<p><strong>Fusione delle Proiezioni</strong>: Le trasformazioni lineari per query, key e value di tutte le teste possono essere fuse in singole operazioni matriciali piÃ¹ grandi.</p>
<p><strong>Ottimizzazioni Hardware</strong>: L&rsquo;architettura Ã¨ ideale per GPU e TPU, che eccellono nelle operazioni matriciali parallele.</p>
<h2 id="varianti-e-estensioni">Varianti e Estensioni</h2>
<h3 id="sparse-multi-head-attention">Sparse Multi-Head Attention</h3>
<p>Per ridurre la complessitÃ  quadratica:
- <strong>Local Attention</strong>: Ogni posizione presta attenzione solo a una finestra locale
- <strong>Strided Attention</strong>: Pattern di attention con passi fissi
- <strong>Random Attention</strong>: Subset casuale di posizioni per l&rsquo;attention</p>
<h3 id="linformer-e-performer">Linformer e Performer</h3>
<p>Tecniche per approssimare la Multi-Head Attention con complessitÃ  lineare:
- Proiezioni a basso rango delle matrici key/value
- Approximazioni kernel-based della softmax
- Decoposition tensoriale delle matrici di attention</p>
<h3 id="cross-attention">Cross-Attention</h3>
<p>Variante dove query, key e value provengono da sequenze diverse:
- Utilizzata nei Transformer encoder-decoder
- Applicazioni in traduzione automatica
- Meccanismo per fondere informazioni da fonti diverse</p>
<h2 id="analisi-sperimentale-e-ablation-studies">Analisi Sperimentale e Ablation Studies</h2>
<h3 id="effetto-del-numero-di-teste">Effetto del Numero di Teste</h3>
<p>Studi empirici hanno mostrato:
- <strong>Poche teste</strong> (h=1,2): Prestazioni subottimali, rappresentazioni troppo limitate
- <strong>Numero ottimale</strong> (h=8,16): Bilanciamento ideale tra diversitÃ  e efficienza<br />
- <strong>Troppe teste</strong> (h&gt;16): Rendimenti decrescenti, possibile overfitting</p>
<h3 id="pattern-di-specializzazione">Pattern di Specializzazione</h3>
<p>Analisi delle teste addestrate rivelano:
- <strong>Specializzazione automatica</strong>: Emerge naturalmente durante l&rsquo;addestramento
- <strong>Ridondanza parziale</strong>: Alcune teste imparano pattern simili
- <strong>Robustezza</strong>: La rimozione di singole teste ha impatto limitato</p>
<h3 id="transfer-learning">Transfer Learning</h3>
<p>Le teste addestrate su un compito spesso si trasferiscono bene ad altri:
- Pattern sintattici sono generalmente trasferibili
- Teste semantiche possono richiedere fine-tuning specifico
- La struttura multi-head facilita l&rsquo;adattamento a nuovi domini</p>
<h2 id="limitazioni-e-soluzioni">Limitazioni e Soluzioni</h2>
<h3 id="limitazioni-principali">Limitazioni Principali</h3>
<p><strong>ComplessitÃ  quadratica</strong>: Rimane il problema fondamentale per sequenze molto lunghe.</p>
<p><strong>InterpretabilitÃ  limitata</strong>: Sebbene migliore della single-head, l&rsquo;interpretazione delle teste resta complessa.</p>
<p><strong>Overhead della concatenazione</strong>: La combinazione finale puÃ² diventare un bottleneck.</p>
<h3 id="direzioni-di-ricerca">Direzioni di Ricerca</h3>
<p><strong>Attention Sparse</strong>: Riduzione della complessitÃ  attraverso pattern di attention selettivi.</p>
<p><strong>Attention Hierarchical</strong>: Strutture gerarchiche per catturare dipendenze multi-scala.</p>
<p><strong>Learnable Attention Patterns</strong>: Apprendimento automatico dei pattern di attention ottimali.</p>
<h2 id="conclusioni-e-direzioni-future">Conclusioni e Direzioni Future</h2>
<p>La <strong>Multi-Head Attention</strong> rappresenta un&rsquo;evoluzione naturale e potente del meccanismo di attention base. La sua capacitÃ  di catturare simultaneamente diversi tipi di relazioni ha contribuito significativamente al successo dei Transformer e dei modelli linguistici moderni.</p>
<h3 id="punti-chiave">Punti Chiave</h3>
<ol>
<li><strong>Diversificazione</strong>: Multiple prospettive sulla stessa sequenza arricchiscono le rappresentazioni</li>
<li><strong>Efficienza</strong>: Mantenimento della complessitÃ  computazionale della single-head attention</li>
<li><strong>InterpretabilitÃ </strong>: Migliore comprensione dei pattern appresi dal modello</li>
<li><strong>Robustezza</strong>: Ridondanza e specializzazione aumentano la stabilitÃ </li>
</ol>
<h3 id="prospettive-future">Prospettive Future</h3>
<p>La ricerca futura probabilmente si concentrerÃ  su:</p>
<ul>
<li><strong>Scaling</strong>: Gestione efficiente di sequenze sempre piÃ¹ lunghe</li>
<li><strong>Specializzazione</strong>: Controllo esplicito della specializzazione delle teste  </li>
<li><strong>Adaptive</strong>: Numero dinamico di teste basato sul contenuto</li>
<li><strong>Cross-modal</strong>: Extension a modalitÃ  diverse (testo, immagini, audio)</li>
</ul>
<p>La Multi-Head Attention continua a essere un componente fondamentale dell&rsquo;architettura dei Transformer, e la sua comprensione approfondita Ã¨ essenziale per chiunque voglia lavorare con i modelli linguistici moderni o sviluppare nuove architetture basate sull&rsquo;attention.</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> deep learning, neural networks, CNN, RNN, transformers, model, learning</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/deep-learning/Transformers/Attention/Multi-Head Attention">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'Multi-Head Attention: Parallelizzazione e Diversificazione dell'Attenzione',
          page_location: 'http://localhost:3000/theory/deep-learning/Transformers/Attention/Multi-Head Attention'
        });
      }
    </script>
</body>
</html>