<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Berkeley Restaurant Project (BERP) Corpus | Natural Language Processing | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="NLP, natural language processing, text analysis, language models, model, data">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="The Berkeley Restaurant Project (BERP) Corpus">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/nlp/Modelli di Linguaggio/The Berkeley Restourant Project">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="The Berkeley Restaurant Project (BERP) Corpus">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/nlp/Modelli di Linguaggio/The Berkeley Restourant Project">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "The Berkeley Restaurant Project (BERP) Corpus",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/nlp/Modelli di Linguaggio/The Berkeley Restourant Project",
      "datePublished": "2025-11-24T18:40:44.889Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/nlp/Modelli di Linguaggio/The Berkeley Restourant Project" class="react-redirect">üöÄ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>The Berkeley Restaurant Project (BERP) Corpus</h1>
                <div class="meta">
                    <strong>Topic:</strong> Natural Language Processing | 
                    <strong>Updated:</strong> 24/11/2025
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
/* Blocchi di codice sempre visibili */
.code-visible {
    border: 1px solid #e5e7eb;
    border-radius: 12px;
    background: #f9fafb;
    margin: 16px 0;
}
.code-visible .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<h2 id="descrizione-generale">Descrizione Generale</h2>
<p>Il <strong>BERP</strong> (Berkeley Restaurant Project) √® un corpus utilizzato nell&rsquo;ambito del <strong>Natural Language Processing (NLP)</strong>, in particolare per modellare e analizzare il linguaggio in contesti legati al cibo e alla ristorazione.<br />
Il corpus contiene query poste dagli utenti, per esempio:<br />
- <em>I‚Äôm looking for Cantonese food</em><br />
- <em>I‚Äôd like to eat dinner someplace nearby</em><br />
- <em>Tell me about Chez Panisse</em><br />
- <em>I‚Äôm looking for a good place to eat breakfast</em>  </p>
<p>Questo dataset √® impiegato per sviluppare modelli probabilistici del linguaggio, permettendo di stimare la probabilit√† di frasi, analizzare le frequenze delle parole (unigrammi) e le associazioni tra di esse (bigrammi).</p>
<h2 id="modellazione-probabilistica-con-n-grammi">Modellazione Probabilistica con N-grammi</h2>
<h3 id="calcolo-della-probabilita-con-il-modello-bigram">Calcolo della Probabilit√† con il Modello Bigram</h3>
<p>Assumendo l&rsquo;indipendenza dei bigrammi (Markov Property) e applicando la regola della catena, la probabilit√† di una frase viene approssimata moltiplicando le probabilit√† condizionali dei singoli bigrammi.<br />
Per esempio, per la frase modificata:
$$
P(\langle s \rangle\text{I want Chinese food}) \approx P(I|\langle s \rangle) \cdot P(want|I) \cdot P(Chinese|want) \cdot P(food|Chinese) \cdot P(\langle /s \rangle|food)
$$</p>
<h2 id="tabelle-di-conteggio">Tabelle di Conteggio</h2>
<p>Le tabelle di conteggio sono utilizzate per calcolare le probabilit√† dei bigrammi e degli unigrammi. Scegliamo ora solo alcune (nella realt√† vanno scelte tutte) parole (unigrams, vettore $\mathbf u$) e le coppie di parole (bigrams, matrice $\mathbf B$), e contiamo il numero di volte che appaiono nel corpus.</p>
<p>Le parole selezionate sono:</p>
<ul>
<li><code>&lt;s&gt;</code> (inizio frase)</li>
<li><code>i</code></li>
<li><code>want</code></li>
<li><code>to</code></li>
<li><code>eat</code></li>
<li><code>chinese</code></li>
<li><code>food</code></li>
<li><code>lunch</code></li>
<li><code>spend</code></li>
<li><code>&lt;/s&gt;</code> (fine frase)</li>
</ul>
<h3 id="conteggio-degli-unigrammi">Conteggio degli Unigrammi</h3>
<table>
<thead>
<tr>
<th></th>
<th>$\langle s \rangle$</th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
<th>$\langle /s \rangle$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Count</td>
<td>8566</td>
<td>2816</td>
<td>1038</td>
<td>2711</td>
<td>829</td>
<td>193</td>
<td>1242</td>
<td>392</td>
<td>310</td>
<td>8566</td>
</tr>
</tbody>
</table>
<p>Chiameremo questo vettore $\mathbf{u}$.</p>
<h3 id="conteggio-dei-bigrammi">Conteggio dei Bigrammi</h3>
<p>In questa tabella, includiamo <code>&lt;s&gt;</code> come riga iniziale e <code>&lt;/s&gt;</code> come colonna finale:</p>
<table>
<thead>
<tr>
<th></th>
<th>$\langle s \rangle$</th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
<th>$\langle /s \rangle$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>$\langle s \rangle$</strong></td>
<td>0</td>
<td>1922</td>
<td>4</td>
<td>32</td>
<td>4</td>
<td>10</td>
<td>4</td>
<td>39</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td><strong>i</strong></td>
<td>0</td>
<td>1</td>
<td>908</td>
<td>0</td>
<td>12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>673</td>
<td>0</td>
<td>7</td>
<td>6</td>
<td>6</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>753</td>
<td>3</td>
<td>0</td>
<td>6</td>
<td>233</td>
<td>3</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>16</td>
<td>2</td>
<td>52</td>
<td>0</td>
<td>10</td>
</tr>
<tr>
<td><strong>chinese</strong></td>
<td>0</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>10</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>0</td>
<td>14</td>
<td>0</td>
<td>13</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>806</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>221</td>
</tr>
<tr>
<td><strong>spend</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>8</td>
</tr>
<tr>
<td><strong>$\langle /s \rangle$</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Chiameremo questa matrice $\mathbf{B}$.</p>
<h3 id="probabilita-dei-bigrammi-conteggio-normalizzato">Probabilit√† dei Bigrammi (conteggio normalizzato)</h3>
<p>Per ottenere le probabilit√† dei bigrammi, si divide il conteggio del bigramma per il conteggio dell&rsquo;unigramma del prefisso. Ad esempio, per il bigramma &ldquo;i want&rdquo; abbiamo:</p>
$$
P(\text{want} \mid \text{i}) = \frac{908}{2816} \approx 0.32
$$
<p>La matrice normalizzata $\mathbf{N}$ (contenente le probabilit√†) sar√† strutturata in modo analogo, includendo le colonne e righe per <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code>:</p>
<table>
<thead>
<tr>
<th></th>
<th>$\langle s \rangle$</th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
<th>$\langle /s \rangle$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>$\langle s \rangle$</strong></td>
<td>0.0</td>
<td>0.224375</td>
<td>0.000467</td>
<td>0.003736</td>
<td>0.000467</td>
<td>0.001167</td>
<td>0.000467</td>
<td>0.004553</td>
<td>0.000117</td>
<td>0.000000</td>
</tr>
<tr>
<td><strong>i</strong></td>
<td>0.0</td>
<td>0.000355</td>
<td>0.322443</td>
<td>0.000000</td>
<td>0.004261</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000710</td>
<td>0.000000</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>0.0</td>
<td>0.001927</td>
<td>0.000000</td>
<td>0.648362</td>
<td>0.000000</td>
<td>0.006744</td>
<td>0.005780</td>
<td>0.005780</td>
<td>0.000963</td>
<td>0.001927</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000738</td>
<td>0.277757</td>
<td>0.001107</td>
<td>0.000000</td>
<td>0.002213</td>
<td>0.085946</td>
<td>0.001107</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.019300</td>
<td>0.002413</td>
<td>0.062726</td>
<td>0.000000</td>
<td>0.012063</td>
</tr>
<tr>
<td><strong>chinese</strong></td>
<td>0.0</td>
<td>0.020725</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.512953</td>
<td>0.005181</td>
<td>0.000000</td>
<td>0.051813</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>0.0</td>
<td>0.011272</td>
<td>0.000000</td>
<td>0.010467</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.648953</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>0.0</td>
<td>0.002551</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.002551</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.563776</td>
</tr>
<tr>
<td><strong>spend</strong></td>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.003226</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.025806</td>
</tr>
<tr>
<td><strong>$\langle /s \rangle$</strong></td>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
</tbody>
</table>
<p><em>Nota:</em> I token <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code> sono inclusi solo nelle matrici normalizzate per evidenziare la probabilit√† di inizio e fine frase.</p>
<p>Questa matrice $\mathbf N$ √® ottenuta semplicemente calcolando:
$$
\mathbf N_{ij} = \frac{\mathbf B_{ij}}{\mathbf u_i}
$$</p>
<h2 id="calcolo-della-probabilita-di-frasi-specifiche">Calcolo della Probabilit√† di Frasi Specifiche</h2>
<p>Per stimare la probabilit√† di una frase con un modello bigramma, si moltiplicano le probabilit√† condizionali dei bigrammi, includendo i token di inizio (<code>&lt;s&gt;</code>) e fine (<code>&lt;/s&gt;</code>). In generale, per una frase:
$$
\text{frase} = \langle s \rangle \; w_1 \; w_2 \; \dots \; w_n \; \langle /s \rangle
$$
la probabilit√† stimata √®:
$$
P(\text{frase}) = P(w_1 \mid \langle s \rangle) \cdot P(w_2 \mid w_1) \cdots P(\langle /s \rangle \mid w_n)
$$</p>
<h3 id="frase-i-want-chinese-food">Frase: &ldquo;I want Chinese food&rdquo;</h3>
<p>Utilizziamo i valori aggiornati dalla matrice per stimare la probabilit√† (trasformando sempre le lettere in lower case):</p>
<ul>
<li>$P(i \mid \langle s \rangle) = 0.224375$  </li>
<li>$P(want \mid i) = 0.322443$  </li>
<li>$P(chinese \mid want) = 0.006744$  </li>
<li>$P(food \mid chinese) = 0.512953$  </li>
<li>$P(\langle /s \rangle \mid food) = 0.648953$  </li>
</ul>
<p>La probabilit√† della frase √®:
$$
\begin{aligned}
P(\langle s \rangle\, i\, want\, chinese\, food\, \langle /s \rangle) &= P(i \mid \langle s \rangle) \cdot P(want \mid i) \cdot P(chinese \mid want) \\
&\quad \cdot P(food \mid chinese) \cdot P(\langle /s \rangle \mid food) \\
&= 0.224375 \cdot 0.322443 \cdot 0.006744 \cdot 0.512953 \cdot 0.648953 \\
&\approx 0.000162
\end{aligned}
$$</p>
<h2 id="conclusioni-cosa-ci-insegnano-gli-n-grammi">Conclusioni: Cosa Ci Insegnano gli N-grammi</h2>
<p>Nonostante la semplicit√†, i modelli basati su N-grammi riescono a catturare informazioni interessanti riguardo al linguaggio:</p>
<ul>
<li><strong>Fatti Linguistici:</strong>  </li>
<li>$P(English \mid want) = 0$, che rappresenta un problema, in quanto non compare nel corpus il bigramma &ldquo;<em>want English</em>&rdquo;.</li>
<li>$P(Chinese \mid want) \approx 0.0067$  </li>
<li>
<p>$P(to \mid want)$ (valore elevato nei dati originali)</p>
</li>
<li>
<p><strong>Conoscenza del Mondo:</strong>  </p>
</li>
<li>$P(eat \mid to) \approx 0.2778$ (da altri esempi)  </li>
<li>
<p>$P(food \mid to) \approx 0$ (in certi casi)</p>
</li>
<li>
<p><strong>Sintassi:</strong>  </p>
</li>
<li>$P(want \mid spend) = 0$  </li>
<li>
<p>$P(I \mid \langle s \rangle) \approx 0.2244$</p>
</li>
<li>
<p><strong>Discorso:</strong><br />
  Le probabilit√† riflettono le relazioni contestuali e il flusso del discorso, evidenziando come alcuni bigrammi siano molto probabili (come quelli che iniziano con <code>&lt;s&gt;</code>) mentre altri risultano meno frequenti o addirittura impossibili.</p>
</li>
</ul>
<h2 id="laplace-smoothing">Laplace Smoothing</h2>
<p>Applichiamo ora il Laplace <span class="text-gray-600">Smoothing</span> alla matrice di conteggio dei bigrammi per risolvere il problema dei bigrammi non osservati nel corpus, che hanno come probabilit√† 0.</p>
<h3 id="1-aggiunta-del-contatore-per-il-laplace-smoothing">1. Aggiunta del Contatore per il Laplace Smoothing</h3>
<p>Per applicare il Laplace smoothing, aggiungiamo 1 a ciascuna cella della matrice $\mathbf{B}$:</p>
<table>
<thead>
<tr>
<th></th>
<th>$\langle s \rangle$</th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
<th>$\langle /s \rangle$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>$\langle s \rangle$</strong></td>
<td>1</td>
<td>1923</td>
<td>5</td>
<td>33</td>
<td>5</td>
<td>11</td>
<td>5</td>
<td>40</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td><strong>i</strong></td>
<td>1</td>
<td>2</td>
<td>909</td>
<td>1</td>
<td>13</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>674</td>
<td>1</td>
<td>8</td>
<td>7</td>
<td>7</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>754</td>
<td>4</td>
<td>1</td>
<td>7</td>
<td>234</td>
<td>4</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>17</td>
<td>3</td>
<td>53</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td><strong>chinese</strong></td>
<td>1</td>
<td>5</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>100</td>
<td>2</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>1</td>
<td>15</td>
<td>1</td>
<td>14</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>807</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>222</td>
</tr>
<tr>
<td><strong>spend</strong></td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td><strong>$\langle /s \rangle$</strong></td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 id="2-calcolo-delle-probabilita-smoothing">2. Calcolo delle Probabilit√† Smoothing</h3>
<p>Per ogni bigramma $(w_{n-1}, w_n)$ il Laplace smoothing prevede:</p>
$$
\mathbb P(w_n \mid w_{n-1}) = \frac{c(w_{n-1}, w_n) + 1}{c(w_{n-1}) + V}
$$
<p>dove:
- $c(w_{n-1}, w_n)$ √® il conteggio (gi√† incrementato di 1) per il bigramma;
- $c(w_{n-1})$ √® il totale dei conteggi per il contesto $w_{n-1}$ (ottenibile dal vettore delle frequenze degli unigrammi $\mathbf{u}$);
- $V$ √® la dimensione del vocabolario (in questo caso, $V=1997$).</p>
<p><strong>Esempio di Calcolo:</strong></p>
<p>Supponiamo di voler calcolare la probabilit√† condizionata del bigramma (&ldquo;i&rdquo;, &ldquo;want&rdquo;).<br />
Dalla riga relativa a &ldquo;i&rdquo; abbiamo:
- Valore incrementato per (&ldquo;i&rdquo;, &ldquo;want&rdquo;) = 908<br />
- Totale dei conteggi per il contesto &ldquo;i&rdquo;:<br />
  $$
  c("i") = \mathbf u_i = 2816.
  $$</p>
<p>Quindi:</p>
$$
\mathbb P(\text{"want"} \mid \text{"i"}) = \frac{\overbrace{909}^{908+1}}{2816 + 1997} \approx 0.19.
$$
<p>e quindi la probabilit√† $\mathbb P("i", "want") = \mathbb P(\text{"want"} \mid \text{"i"}) \cdot \mathbb P("i")$.</p>
<h3 id="3-costruzione-della-matrice-di-probabilita-smoothed-math_inline_52">3. Costruzione della Matrice di Probabilit√† Smoothed $\mathbf{B^*}$</h3>
<p>Una volta applicata la formula per ogni cella (per ogni bigramma), la matrice $\mathbf{B^*}$ conterr√† le probabilit√† smoothed:</p>
<table>
<thead>
<tr>
<th></th>
<th>$\langle s \rangle$</th>
<th>i</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>chinese</th>
<th>food</th>
<th>lunch</th>
<th>spend</th>
<th>$\langle /s \rangle$</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>$\langle s \rangle$</strong></td>
<td>0.000095</td>
<td>0.182051</td>
<td>0.000473</td>
<td>0.000095</td>
<td>0.000473</td>
<td>0.001041</td>
<td>0.000473</td>
<td>0.003787</td>
<td>0.000189</td>
<td>0.000095</td>
</tr>
<tr>
<td><strong>i</strong></td>
<td>0.000208</td>
<td>0.000416</td>
<td>0.188863</td>
<td>0.000208</td>
<td>0.002701</td>
<td>0.000208</td>
<td>0.000208</td>
<td>0.000208</td>
<td>0.000623</td>
<td>0.000208</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>0.000329</td>
<td>0.000988</td>
<td>0.000329</td>
<td>0.000329</td>
<td>0.000329</td>
<td>0.002636</td>
<td>0.002306</td>
<td>0.002306</td>
<td>0.000659</td>
<td>0.000988</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
<td>0.000501</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>0.000354</td>
<td>0.000354</td>
<td>0.000354</td>
<td>0.000354</td>
<td>0.000354</td>
<td>0.006016</td>
<td>0.001062</td>
<td>0.018754</td>
<td>0.000354</td>
<td>0.003892</td>
</tr>
<tr>
<td><strong>chinese</strong></td>
<td>0.000457</td>
<td>0.002283</td>
<td>0.000457</td>
<td>0.000457</td>
<td>0.000457</td>
<td>0.000457</td>
<td>0.045662</td>
<td>0.000913</td>
<td>0.000457</td>
<td>0.005023</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>0.000309</td>
<td>0.004631</td>
<td>0.000309</td>
<td>0.000309</td>
<td>0.000309</td>
<td>0.000309</td>
<td>0.000309</td>
<td>0.000309</td>
<td>0.000309</td>
<td>0.249151</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>0.000419</td>
<td>0.000837</td>
<td>0.000419</td>
<td>0.000419</td>
<td>0.000419</td>
<td>0.000419</td>
<td>0.000837</td>
<td>0.000419</td>
<td>0.000419</td>
<td>0.092926</td>
</tr>
<tr>
<td><strong>spend</strong></td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.000433</td>
<td>0.003901</td>
</tr>
<tr>
<td><strong>$\langle /s \rangle$</strong></td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
<td>0.000095</td>
</tr>
</tbody>
</table>
<p>In questo modo, le frasi che prima avevano una probabilit√† nulla (ma comunque non impossibili nel linguaggio naturale), ora hanno una probabilit√† non nulla.</p>
<h3 id="frase-i-want-to-want-to-eat-chinese-food">Frase: &ldquo;I want to want to eat Chinese food.&rdquo;</h3>
<p>Utilizziamo i valori aggiornati dalla matrice (trasformando tutte le parole in lower case) per stimare la probabilit√† della frase.<br />
Attenzione: nella tabella il token &ldquo;to&rdquo; √® indicato come &ldquo;ot&rdquo;. Quindi, nel calcolo, sostituiamo ‚Äúto‚Äù con ‚Äúot‚Äù dove necessario.</p>
<p>I passaggi sono i seguenti:</p>
<ul>
<li>$P(i \mid \langle s \rangle) = 0.182051$  </li>
<li>$P(want \mid i) = 0.188863$  </li>
<li>$P(to \mid want) = 0.000329$  </li>
<li>$P(want \mid to) = 0.000501$  </li>
<li>$P(to \mid want) = 0.000329$  </li>
<li>$P(eat \mid to) = 0.000501$  </li>
<li>$P(chinese \mid eat) = 0.006016$  </li>
<li>$P(food \mid chinese) = 0.045662$  </li>
<li>$P(\langle /s \rangle \mid food) = 0.249151$  </li>
</ul>
<p>La struttura della frase (includendo i token di inizio e fine frase) √®:</p>
$$
\langle s \rangle \; i \; want \; to \; want \; to \; eat \; chinese \; food \; \langle /s \rangle
$$
<p>La probabilit√† complessiva della frase √® data dal prodotto dei singoli passaggi:</p>
$$
\begin{aligned}
P(\langle s \rangle\, i\, want\, to\, want\, to\, eat\, chinese\, food\, \langle /s \rangle) &= P(i \mid \langle s \rangle) \cdot P(want \mid i) \cdot P(to \mid want) \\
&\quad \cdot P(want \mid to) \cdot P(to \mid want) \cdot P(eat \mid to) \\
&\quad \cdot P(chinese \mid eat) \cdot P(food \mid chinese) \cdot P(\langle /s \rangle \mid food) \\
&= 0.182051 \cdot 0.188863 \cdot 0.000329 \cdot 0.000501 \cdot 0.000329 \cdot 0.000501 \\
&\quad \cdot 0.006016 \cdot 0.045662 \cdot 0.249151 \\
&\approx 6.39 \times 10^{-20}
\end{aligned}
$$
<p>Questa frase, pur non essendo particolarmente sensata, √® corretta dal punto di vista del linguaggio. Tuttavia, utilizzando il modello bigramma senza smoothing, la probabilit√† stimata della frase sarebbe nulla.</p>
<p>Grazie allo smoothing, invece, il modello bigramma riesce a stimare la probabilit√† della frase in modo piuÃÄ corretto e sensato.</p>
<h2 id="argomenti-correlati">Argomenti Correlati</h2>
<ul>
<li><a href="/theory/nlp/Modelli di Linguaggio/Modelli di Linguaggio" class="text-blue-600 hover:underline">Modelli di Linguaggio</a></li>
<li><a href="/theory/nlp/Parole, Corpora, Tokenizzazione e Normalizzazione" class="text-blue-600 hover:underline">Parole, Corpora, Tokenizzazione e Normalizzazione</a>  </li>
<li><a href="/theory/nlp/Modelli di Linguaggio/Smoothing nei Modelli Linguistici" class="text-blue-600 hover:underline">Smoothing nei Modelli Linguistici</a>   </li>
<li><a href="/theory/nlp/Modelli di Linguaggio/Valutazione dei Modelli di Linguaggio" class="text-blue-600 hover:underline">Valutazione dei Modelli di Linguaggio</a></li>
</ul>
<h2 id="conclusione">Conclusione</h2>
<p>Questa √® una breve panoramica sul corpus BERP e sulle tecniche di modellazione del linguaggio basate sugli N-grammi, che evidenzia come questi metodi possano essere utilizzati per valutare e interpretare la probabilit√† di frasi in linguaggio naturale.</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> NLP, natural language processing, text analysis, language models, model, data</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/nlp/Modelli di Linguaggio/The Berkeley Restourant Project">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'The Berkeley Restaurant Project (BERP) Corpus',
          page_location: 'http://localhost:3000/theory/nlp/Modelli di Linguaggio/The Berkeley Restourant Project'
        });
      }
    </script>
</body>
</html>