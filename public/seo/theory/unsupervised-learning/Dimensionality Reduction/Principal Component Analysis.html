<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Principal Component Analysis (PCA) - Guida Completa | Unsupervised Learning | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="unsupervised learning, clustering, dimensionality reduction, model, data, learning">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Principal Component Analysis (PCA) - Guida Completa">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/unsupervised-learning/Dimensionality Reduction/Principal Component Analysis">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Principal Component Analysis (PCA) - Guida Completa">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/unsupervised-learning/Dimensionality Reduction/Principal Component Analysis">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Principal Component Analysis (PCA) - Guida Completa",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/unsupervised-learning/Dimensionality Reduction/Principal Component Analysis",
      "datePublished": "2026-01-15T00:29:00.285Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/unsupervised-learning/Dimensionality Reduction/Principal Component Analysis" class="react-redirect">üöÄ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>Principal Component Analysis (PCA) - Guida Completa</h1>
                <div class="meta">
                    <strong>Topic:</strong> Unsupervised Learning | 
                    <strong>Updated:</strong> 15/01/2026
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
/* Blocchi di codice sempre visibili */
.code-visible {
    border: 1px solid #e5e7eb;
    border-radius: 12px;
    background: #f9fafb;
    margin: 16px 0;
}
.code-visible .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<h2 id="indice">Indice</h2>
<ol>
<li><a href="#introduzione-e-motivazione">Introduzione e Motivazione</a></li>
<li><a href="#fondamenti-matematici">Fondamenti Matematici</a></li>
<li><a href="#lalgoritmo-pca-step-by-step">L&rsquo;Algoritmo PCA Step-by-Step</a></li>
<li><a href="#implementazione-in-python">Implementazione in Python</a></li>
<li><a href="#interpretazione-dei-risultati">Interpretazione dei Risultati</a></li>
<li><a href="#varianti-avanzate-della-pca">Varianti Avanzate della PCA</a></li>
<li><a href="#applicazioni-pratiche">Applicazioni Pratiche</a></li>
<li><a href="#vantaggi-e-limitazioni">Vantaggi e Limitazioni</a></li>
</ol>
<hr />
<h2 id="introduzione-e-motivazione">Introduzione e Motivazione</h2>
<p>La <strong>Principal Component Analysis (PCA)</strong> rappresenta una delle tecniche fondamentali e potenti nella cassetta degli algoritmi di machine learning. Non si tratta semplicemente di un algoritmo per ridurre le dimensioni dei dati, ma di un vero e proprio strumento matematico che ci permette di comprendere la struttura intrinseca dei nostri dataset.</p>
<p>Immaginate di trovarvi di fronte a un dataset con centinaia o migliaia di variabili. Come possiamo sperare di comprendere cosa ci dicono questi dati? Come possiamo visualizzarli? Come possiamo essere sicuri che non stiamo includendo informazioni ridondanti nei nostri modelli? La PCA risponde elegantemente a tutte queste domande.</p>
<p>La definizione formale ci dice che la PCA √® una <strong>tecnica di riduzione della dimensionalit√† basata sulla feature extraction</strong>, utilizzata per comprimere i dati preservando la maggior parte delle informazioni rilevanti. Ma cosa significa davvero questo in termini pratici?</p>
<h3 id="lidea-centrale">L&rsquo;Idea Centrale</h3>
<p>L&rsquo;obiettivo principale della PCA √® cristallino nella sua semplicit√† matematica, eppure profondo nelle sue implicazioni:</p>
<blockquote>
<p><strong>Trovare una rappresentazione dello spazio originale dei dati in un sistema di coordinate trasformato, chiamato &ldquo;componenti principali&rdquo;, che massimizzi la varianza (informazione) dei dati.</strong></p>
</blockquote>
<p>Questa definizione nasconde un&rsquo;intuizione geometrica bellissima. I nostri dati, quando hanno molte dimensioni, spesso non &ldquo;riempiono&rdquo; tutto lo spazio disponibile in modo uniforme. Invece, tendono a concentrarsi lungo certe direzioni specifiche. La PCA identifica proprio queste direzioni - quelle lungo cui i dati si &ldquo;allungano&rdquo; di pi√π, quelle che catturano la maggior parte della variabilit√†.</p>
<h3 id="connessione-con-la-manifold-hypothesis">Connessione con la Manifold Hypothesis</h3>
<p>La PCA √® strettamente collegata a un&rsquo;idea fondamentale nel machine learning: la <strong><a href="/theory/unsupervised-learning/Dimensionality Reduction/Manifold Hypothesis" class="text-blue-600 hover:underline">Manifold Hypothesis</a></strong>. Questa ipotesi suggerisce che i dati ad alta dimensionalit√† che osserviamo nel mondo reale in realt√† &ldquo;vivono&rdquo; su una superficie (manifold) di dimensionalit√† molto pi√π bassa immersa nello spazio ad alta dimensione.</p>
<p>Pensate alle immagini di volti umani. Ogni immagine 64x64 pixel √® tecnicamente un punto in uno spazio a 4096 dimensioni. Tuttavia, non tutti i possibili punti in questo spazio rappresentano volti realistici - solo una piccolissima frazione lo fa. I volti umani &ldquo;vivono&rdquo; su un manifold di dimensionalit√† molto pi√π bassa. La PCA, pur essendo limitata a manifold lineari, ci aiuta a trovare approssimazioni di questi spazi di dimensionalit√† ridotta.</p>
<h4 id="esempio-con-immagine-di-volto">Esempio con Immagine di Volto</h4>
<h1 id="visualizzazione-pca-su-unimmagine-di-volto">Visualizzazione PCA su un&rsquo;immagine di volto</h1>
<p>L&rsquo;immagine <code>face_pca_levels.png</code> mostra la ricostruzione di un volto utilizzando la PCA con diversi livelli di varianza cumulativa:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="err">!</span><span class="n">wget</span> <span class="o">-</span><span class="n">O</span> <span class="n">volto</span><span class="o">.</span><span class="n">jpg</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">img</span><span class="o">.</span><span class="n">freepik</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">free</span><span class="o">-</span><span class="n">photo</span><span class="o">/</span><span class="n">portrait</span><span class="o">-</span><span class="n">white</span><span class="o">-</span><span class="n">man</span><span class="o">-</span><span class="n">isolated_53876</span><span class="o">-</span><span class="mf">40306.</span><span class="n">jpg</span><span class="err">?</span><span class="n">semt</span><span class="o">=</span><span class="n">ais_hybrid</span><span class="o">&amp;</span><span class="n">w</span><span class="o">=</span><span class="mi">740</span><span class="o">&amp;</span><span class="n">q</span><span class="o">=</span><span class="mi">80</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">skimage</span><span class="w"> </span><span class="kn">import</span> <span class="n">io</span><span class="p">,</span> <span class="n">color</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">skimage.transform</span><span class="w"> </span><span class="kn">import</span> <span class="n">resize</span>

<span class="c1"># --- Step 1: Carica immagine ---</span>
<span class="n">img_path</span> <span class="o">=</span> <span class="s2">&quot;volto.jpg&quot;</span>  <span class="c1"># sostituisci con il percorso della tua immagine</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>

<span class="c1"># Converti in scala di grigi se necessario</span>
<span class="k">if</span> <span class="n">img</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
    <span class="n">img_gray</span> <span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">img_gray</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>  <span class="c1"># normalizza se gi√† in scala di grigi</span>

<span class="c1"># Ridimensiona per semplicit√† (opzionale)</span>
<span class="n">img_gray</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">anti_aliasing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># --- Step 2: Prepara la matrice X ---</span>
<span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">img_gray</span><span class="o">.</span><span class="n">shape</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">img_gray</span>  <span class="c1"># (h, w)</span>
<span class="n">X_flat</span> <span class="o">=</span> <span class="n">X</span>  <span class="c1"># gi√† in 2D: n_samples=h, n_features=w</span>

<span class="c1"># --- Step 3: Fit PCA completo ---</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_flat</span><span class="p">)</span>

<span class="c1"># --- Step 4: Calcola varianza cumulativa e numero componenti per livelli desiderati ---</span>
<span class="n">cum_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="n">levels</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>  <span class="c1"># varianza cumulativa desiderata</span>
<span class="n">max_components</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">X_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># massimo consentito</span>

<span class="n">components_for_levels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">cum_var</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_components</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">level</span> <span class="ow">in</span> <span class="n">levels</span>
<span class="p">]</span>

<span class="c1"># --- Step 5: Ricostruisci immagine per ogni livello ---</span>
<span class="n">reconstructed_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n_comp</span> <span class="ow">in</span> <span class="n">components_for_levels</span><span class="p">:</span>
    <span class="n">pca_n</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_comp</span><span class="p">)</span>
    <span class="n">X_reduced</span> <span class="o">=</span> <span class="n">pca_n</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_flat</span><span class="p">)</span>
    <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">pca_n</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">)</span>
    <span class="n">reconstructed_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X_reconstructed</span><span class="p">)</span>

<span class="c1"># --- Step 6: Mostra le immagini ---</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;100%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà95%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà85%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà75%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà50%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà25%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà10%&quot;</span><span class="p">,</span> <span class="s2">&quot;‚âà5% var&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">recon</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">reconstructed_images</span><span class="p">,</span> <span class="n">titles</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">recon</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c1"># Salva la figura</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="s2">&quot;face_pca_levels.png&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/face_pca_levels.png" alt="Immagini ricostruite per livelli di varianza cumulativa" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p><br></p>
<table>
<thead>
<tr>
<th>Livello di varianza</th>
<th>Descrizione</th>
</tr>
</thead>
<tbody>
<tr>
<td>100%</td>
<td>Immagine originale</td>
</tr>
<tr>
<td>‚âà95%</td>
<td>La maggior parte delle informazioni principali √® mantenuta, dettagli leggermente sfocati</td>
</tr>
<tr>
<td>‚âà85%</td>
<td>I dettagli cominciano a sparire</td>
</tr>
<tr>
<td>‚âà75%</td>
<td>Perdite visibili, immagine gravemente deformata</td>
</tr>
<tr>
<td>‚âà50% $\geq$</td>
<td>Informazioni quasi completamente perse, struttura irriconoscibile</td>
</tr>
</tbody>
</table>
<p>Questa visualizzazione permette di capire come i primi componenti principali catturino le caratteristiche pi√π rilevanti del volto e come la PCA riduca progressivamente i dettagli mantenendo solo la varianza principale.</p>
<h3 id="problemi-concreti-che-risolve-la-pca">Problemi Concreti che Risolve la PCA</h3>
<p><strong>1. La Maledizione della Dimensionalit√† (Curse of Dimensionality)</strong>
Quando il numero di dimensioni cresce, i dati diventano sempre pi√π &ldquo;sparsi&rdquo; - la distanza tra punti vicini aumenta esponenzialmente. Questo rende difficile per molti algoritmi di machine learning trovare pattern significativi. La PCA ci permette di lavorare in spazi di dimensionalit√† ridotta dove questi problemi sono meno severi.</p>
<p><strong>2. Visualizzazione dell&rsquo;Invisibile</strong>
Come possiamo visualizzare dati che esistono in 100 dimensioni? √à fisicamente impossibile. Ma se la PCA ci dice che il 95% della varianza √® catturata dalle prime 2-3 componenti, possiamo creare visualizzazioni 2D o 3D che mantengono la maggior parte dell&rsquo;informazione strutturale.</p>
<p><strong>3. Rumore e Informazioni Irrilevanti</strong>
Nei dati reali, non tutte le dimensioni sono ugualmente informative. Alcune potrebbero essere principalmente rumore, errori di misurazione, o informazioni ridondanti. La PCA agisce come un filtro naturale, concentrando l&rsquo;informazione significativa nei primi componenti e relegando il rumore negli ultimi.</p>
<p><strong>4. Efficienza Computazionale</strong>
Algoritmi che richiedono tempo $O(d^3)$ dove $d$ √® il numero di dimensioni possono diventare impraticabili. Ridurre $d$ da 1000 a 50 pu√≤ significare la differenza tra un&rsquo;analisi che richiede ore e una che richiede secondi.</p>
<p><strong>5. Multicollinearit√†</strong>
Quando le variabili sono fortemente correlate tra loro, molti algoritmi statistici diventano instabili. La PCA trasforma automaticamente i dati in un insieme di variabili ortogonali (non correlate), risolvendo elegantemente questo problema.</p>
<h3 id="intuizione-geometrica-profonda">Intuizione Geometrica Profonda</h3>
<p>Per comprendere veramente la PCA, √® essenziale sviluppare l&rsquo;intuizione geometrica. Immaginate di avere un dataset bidimensionale dove i punti formano una nuvola ellittica. L&rsquo;asse maggiore dell&rsquo;ellisse rappresenta la direzione di massima varianza - questa sarebbe la prima componente principale. L&rsquo;asse minore, perpendicolare al primo, rappresenta la seconda componente principale.</p>
<p>Ora estendete questa intuizione a dimensioni superiori. In uno spazio tridimensionale, potreste avere dati che si distribuiscono principalmente lungo un piano inclinato. I primi due componenti principali definirebbero questo piano, mentre il terzo componente (perpendicolare al piano) catturerebbe la varianza residua.</p>
<p>Questa trasformazione di coordinate non √® arbitraria - √® ottimale nel senso che massimizza la varianza catturata da ogni componente, soggetto al vincolo che tutti i componenti siano ortogonali tra loro.</p>
<hr />
<h2 id="fondamenti-matematici">Fondamenti Matematici</h2>
<p>La bellezza matematica della PCA risiede nella sua elegante connessione tra algebra lineare, statistica e geometria. Per comprendere appieno come funziona, dobbiamo costruire la teoria passo dopo passo, partendo dalle fondamenta.</p>
<h3 id="rappresentazione-del-dataset">Rappresentazione del Dataset</h3>
<p>Consideriamo un dataset $D = \{\vec{x}_i\}_{i=1}^N$ dove ogni osservazione $\vec{x}_i \in \mathbb{R}^d$. Qui:
- $N$ rappresenta il numero di osservazioni (campioni, righe)
- $d$ rappresenta il numero di variabili (dimensioni, colonne)</p>
<p>In forma matriciale, organizziamo questi dati in una matrice $\mathbf{X}$ di dimensione $N \times d$:</p>
$$\mathbf{X} = \begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,d} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,d} \\
\vdots & \vdots & \ddots & \vdots \\
x_{N,1} & x_{N,2} & \cdots & x_{N,d}
\end{bmatrix}$$
<p>Ogni riga $\vec{x}_i = [x_{i,1}, x_{i,2}, \ldots, x_{i,d}]$ rappresenta un&rsquo;osservazione completa, mentre ogni colonna $\mathbf{x}^{(j)} = [x_{1,j}, x_{2,j}, \ldots, x_{N,j}]^T$ rappresenta tutti i valori di una specifica variabile.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Impostazioni di stile moderno</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;muted&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>

<span class="c1"># Generiamo un dataset di esempio</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span>  <span class="c1"># 100 osservazioni, 3 variabili</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">70</span><span class="p">])</span>

<span class="c1"># Convertiamo in DataFrame per visualizzazione</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Et√†&#39;</span><span class="p">,</span> <span class="s1">&#39;Reddito&#39;</span><span class="p">,</span> <span class="s1">&#39;Peso&#39;</span><span class="p">])</span>

<span class="c1"># Grafico pairplot per vedere le relazioni</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">diag_kind</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">corner</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Distribuzione iniziale delle variabili&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="s2">&quot;pca-datat.png&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/pca-data.png" alt="Pairplot delle variabili" style="display: block; margin-left: auto; margin-right: auto;"></p>
<h3 id="il-problema-fondamentale-della-standardizzazione">Il Problema Fondamentale della Standardizzazione</h3>
<p>Prima di procedere con l&rsquo;analisi, dobbiamo affrontare un problema cruciale: le diverse variabili potrebbero essere misurate in unit√† completamente diverse. Pensate a un dataset che include et√† (anni), reddito (euro), e peso (kg). La variabile &ldquo;reddito&rdquo; avr√† naturalmente una varianza molto maggiore semplicemente per via delle unit√† di misura, non necessariamente perch√© √® pi√π &ldquo;importante&rdquo; per la struttura dei dati.</p>
<h4 id="standardizzazione-z-score">Standardizzazione Z-score</h4>
<p>Per ogni variabile $j \in \{1, 2, \ldots, d\}$, calcoliamo:</p>
<p><strong>Media campionaria:</strong>
$$\mu_j = \frac{1}{N} \sum_{i=1}^N x_{i,j}$$</p>
<p><strong>Deviazione standard campionaria (con correzione di Bessel):</strong>
$$\sigma_j = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (x_{i,j} - \mu_j)^2}$$</p>
<p>La <strong><a href="/theory/math-for-ml/Probabilit√†/Correzione di Bessel" class="text-blue-600 hover:underline">Correzione di Bessel</a></strong> (usando $N-1$ invece di $N$) √® fondamentale per ottenere una stima non distorta della varianza della popolazione quando lavoriamo con campioni.</p>
<p><strong>Trasformazione standardizzata:</strong>
$$z_{i,j} = \frac{x_{i,j} - \mu_j}{\sigma_j}$$</p>
<p>Dopo la standardizzazione, ogni variabile ha media 0 e varianza 1. Il dataset standardizzato $\mathbf{Z}$ ha la stessa struttura di $\mathbf{X}$, </p>
$$
Z = \begin{bmatrix}
z_{1,1} & z_{1,2} & \cdots & z_{1,d} \\
z_{2,1} & z_{2,2} & \cdots & z_{2,d} \\
\vdots & \vdots & \ddots & \vdots \\
z_{N,1} & z_{N,2} & \cdots & z_{N,d}
\end{bmatrix}
$$
<p>ma ora tutte le variabili sono sulla stessa scala.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Standardizzazione</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">df_z</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Visualizzazione dopo standardizzazione</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df_z</span><span class="p">,</span> <span class="n">diag_kind</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">corner</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Dati standardizzati (media 0, varianza 1)&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="s2">&quot;pca-standardization.png&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/pca-standardization.png" alt="Pairplot delle variabili standardizzate" style="display: block; margin-left: auto; margin-right: auto;"></p>
<h3 id="la-matrice-di-covarianza-cuore-della-pca">La Matrice di Covarianza: Cuore della PCA</h3>
<p>La <strong><a href="/theory/math-for-ml/Probabilit√†/Covarianza" class="text-blue-600 hover:underline">matrice di covarianza</a></strong> $\boldsymbol{\Sigma}$ √® l&rsquo;oggetto matematico centrale della PCA. Per il dataset standardizzato $\mathbf{Z}$, calcoliamo:</p>
$$\boldsymbol{\Sigma} = \frac{1}{N-1} \mathbf{Z}^T \mathbf{Z}$$
<p>Questa √® una matrice $d \times d$ dove ogni elemento $\Sigma_{j,k}$ rappresenta la covarianza campionaria tra le variabili $j$ e $k$:</p>
$$\Sigma_{j,k} = \frac{1}{N-1} \sum_{i=1}^N z_{i,j} z_{i,k}$$
<h4 id="proprieta-fondamentali-della-matrice-di-covarianza">Propriet√† Fondamentali della Matrice di Covarianza</h4>
<ol>
<li>
<p><strong>Simmetria</strong>: $\boldsymbol{\Sigma} = \boldsymbol{\Sigma}^T$, poich√© $\Sigma_{j,k} = \Sigma_{k,j}$</p>
</li>
<li>
<p><strong>Semi-definitezza positiva</strong>: Per ogni vettore $\mathbf{v} \in \mathbb{R}^d$, abbiamo $\mathbf{v}^T \boldsymbol{\Sigma} \mathbf{v} \geq 0$</p>
</li>
<li>
<p><strong>Interpretazione degli elementi</strong>:</p>
</li>
<li><strong>Diagonale</strong>: $\Sigma_{j,j} = \text{Var}(z^{(j)}) = 1$ (per dati standardizzati)</li>
<li><strong>Off-diagonale</strong>: $\Sigma_{j,k} = \text{Cov}(z^{(j)}, z^{(k)}) = \text{Corr}(x^{(j)}, x^{(k)})$</li>
</ol>
<p>Per dati standardizzati, la matrice di covarianza coincide con la <strong>matrice di correlazione</strong> dei dati originali.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Sigma</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Matrice di Covarianza&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;pca-covariance.png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<p><img src="/images/posts/pca-covariance.png" alt="Matrice di Covarianza" style="display: block; margin-left: auto; margin-right: auto;"></p>
<h5 id="perche-la-diagonale-della-matrice-di-covarianza-dei-dati-standardizzati-puo-essere-leggermente-diversa-da-1">Perch√© la diagonale della matrice di covarianza dei dati standardizzati pu√≤ essere leggermente diversa da 1?</h5>
<p>In Python, quando calcoliamo la matrice di covarianza con <code>np.cov(Z, rowvar=False)</code>, per default viene usata la <strong>deviazione standard campionaria non corretta (unbiased estimator)</strong>, cio√®:</p>
$$
\text{Cov}(z_j, z_j) = \frac{1}{N-1} \sum_{i=1}^{N} (z_{i,j} - \bar{z}_j)^2
$$
<ul>
<li>La standardizzazione di <code>StandardScaler</code> usa la deviazione standard con denominatore $N$ (popolazione), mentre  </li>
<li><code>np.cov</code> usa denominatore $N-1$ (correzione di Bessel).  </li>
</ul>
<p>Questo piccolo disallineamento genera valori leggermente maggiori di 1 sulla diagonale, ad esempio <strong>1.01</strong>.  </p>
<h3 id="il-problema-degli-autovalori-dove-nasce-la-pca">Il Problema degli Autovalori: Dove Nasce la PCA</h3>
<p>Il cuore matematico della PCA √® la risoluzione del problema agli autovalori per la matrice di covarianza:</p>
$$\boldsymbol{\Sigma} \mathbf{v}_j = \lambda_j \mathbf{v}_j$$
<p>dove $\lambda_j$ sono gli <strong>autovalori</strong> e $\mathbf{v}_j$ sono i corrispondenti <strong>autovettori</strong>.</p>
<h4 id="perche-questo-problema-e-cruciale">Perch√© Questo Problema √à Cruciale?</h4>
<p>Gli autovettori della matrice di covarianza hanno un&rsquo;interpretazione geometrica profonda:
- <strong>Gli autovettori</strong> definiscono le direzioni principali di variazione nei dati
- <strong>Gli autovalori</strong> misurano la quantit√† di varianza lungo ciascuna direzione</p>
<h4 id="derivazione-matematica-dettagliata">Derivazione Matematica Dettagliata</h4>
<p>Per risolvere il problema degli autovalori, cerchiamo i valori $\lambda$ per cui esiste un vettore non nullo $\mathbf{v}$ tale che:</p>
$$\boldsymbol{\Sigma} \mathbf{v} = \lambda \mathbf{v}$$
<p>Riscrivendo:
$$(\boldsymbol{\Sigma} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$$</p>
<p>Per ottenere una soluzione non banale ($\mathbf{v} \neq \mathbf{0}$), la matrice $(\boldsymbol{\Sigma} - \lambda \mathbf{I})$ deve essere singolare (non invertibile):</p>
$$\det(\boldsymbol{\Sigma} - \lambda \mathbf{I}) = 0$$
<p>Questa equazione, chiamata <strong>equazione caratteristica</strong>, √® un polinomio di grado $d$ in $\lambda$, quindi ammette al massimo $d$ soluzioni reali (che sono esattamente $d$ per matrici simmetriche).</p>
<h4 id="il-teorema-spettrale">Il Teorema Spettrale</h4>
<p>Poich√© $\boldsymbol{\Sigma}$ √® simmetrica, il <strong>Teorema Spettrale</strong> garantisce che:</p>
<ol>
<li>Tutti gli autovalori sono reali: $\lambda_1, \lambda_2, \ldots, \lambda_d \in \mathbb{R}$</li>
<li>Gli autovettori corrispondenti ad autovalori distinti sono ortogonali</li>
<li>Esiste una base ortonormale di autovettori</li>
</ol>
<p>Possiamo quindi scrivere la decomposizione spettrale:
$$\boldsymbol{\Sigma} = \mathbf{W} \boldsymbol{\Lambda} \mathbf{W}^T$$</p>
<p>dove:
- $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_d]$ √® la matrice degli autovettori ortonormali
- $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d)$ con $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_d \geq 0$</p>
<h3 id="connessione-profonda-con-la-singular-value-decomposition">Connessione Profonda con la <a href="/theory/math-for-ml/Algebra/Singular Value Decomposition" class="text-blue-600 hover:underline">Singular Value Decomposition</a></h3>
<p>Un approccio numericamente pi√π stabile per calcolare la PCA utilizza la <strong>Singular Value Decomposition (SVD)</strong> direttamente sulla matrice centrata dei dati.</p>
<p>Per la matrice centrata $\mathbf{Z}$ (di dimensione $N \times d$), la SVD √®:
$$\mathbf{Z} = \mathbf{U} \boldsymbol{\Sigma}_{SVD} \mathbf{V}^T$$</p>
<p>dove:
- $\mathbf{U}$ √® una matrice $N \times \min(N,d)$ ortogonale
- $\boldsymbol{\Sigma}_{SVD}$ √® una matrice diagonale $\min(N,d) \times \min(N,d)$ con valori singolari $\sigma_1 \geq \sigma_2 \geq \ldots \geq 0$
- $\mathbf{V}$ √® una matrice $d \times d$ ortogonale</p>
<h4 id="la-connessione-magica">La Connessione Magica</h4>
<p>La matrice di covarianza pu√≤ essere espressa usando la SVD:
$$\boldsymbol{\Sigma} = \frac{1}{N-1} \mathbf{Z}^T \mathbf{Z} = \frac{1}{N-1} \mathbf{V} \boldsymbol{\Sigma}_{SVD}^2 \mathbf{V}^T$$</p>
<p>Questo rivela che:
- <strong>I componenti principali sono le colonne di $\mathbf{V}$</strong>: $\mathbf{w}_j = \mathbf{v}_j$
- <strong>Gli autovalori sono correlati ai valori singolari</strong>: $\lambda_j = \frac{\sigma_j^2}{N-1}$</p>
<h3 id="interpretazione-geometrica-dellalgebra">Interpretazione Geometrica dell&rsquo;Algebra</h3>
<p>Ogni autovettore $\mathbf{w}_j$ definisce una direzione nello spazio originale delle variabili. L&rsquo;autovalore $\lambda_j$ ci dice quanto i dati si &ldquo;allungano&rdquo; lungo quella direzione.</p>
<ul>
<li><strong>Autovalore grande</strong>: I dati hanno molta variabilit√† lungo questa direzione</li>
<li><strong>Autovalore piccolo</strong>: I dati sono &ldquo;compressi&rdquo; lungo questa direzione</li>
<li><strong>Autovalore zero</strong>: I dati giacciono esattamente su un iperpiano perpendicolare a questa direzione</li>
</ul>
<p>La somma di tutti gli autovalori √® uguale alla varianza totale del dataset:
$$\sum_{j=1}^d \lambda_j = \text{tr}(\boldsymbol{\Sigma}) = \sum_{j=1}^d \text{Var}(z^{(j)}) = d$$</p>
<p>(per dati standardizzati, dove ogni variabile ha varianza 1).</p>
<hr />
<h2 id="lalgoritmo-pca-step-by-step">L&rsquo;Algoritmo PCA Step-by-Step</h2>
<p>Dopo aver costruito le fondamenta matematiche, possiamo ora descrivere l&rsquo;algoritmo PCA in modo rigoroso e completo. Ogni passaggio ha una giustificazione matematica precisa e un significato geometrico intuitivo.</p>
<h3 id="input-e-output-dellalgoritmo">Input e Output dell&rsquo;Algoritmo</h3>
<p><strong>Input:</strong>
- Matrice dati $\mathbf{X}$ di dimensione $N \times d$ (N osservazioni, d variabili)
- Numero di componenti desiderato $k \leq d$</p>
<p><strong>Output:</strong>
- Matrice dei componenti principali $\mathbf{W}_k$ di dimensione $d \times k$
- Dati proiettati $\mathbf{Z}_{PCA}$ di dimensione $N \times k$
- Autovalori $\boldsymbol{\lambda} = [\lambda_1, \lambda_2, \ldots, \lambda_k]$
- Informazioni per la ricostruzione (medie, deviazioni standard)</p>
<h3 id="algoritmo-dettagliato">Algoritmo Dettagliato</h3>
<h4 id="step-1-standardizzazione-del-dataset">Step 1: Standardizzazione del Dataset</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Per ogni variabile j = 1, 2, ..., d:
    Calcola la media: Œº‚±º = (1/N) Œ£·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢‚±º
    Calcola la dev. std: œÉ‚±º = ‚àö[(1/(N-1)) Œ£·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢‚±º - Œº‚±º)¬≤]

Per ogni osservazione i = 1, 2, ..., N e variabile j = 1, 2, ..., d:
    Standardizza: z·µ¢‚±º = (x·µ¢‚±º - Œº‚±º) / œÉ‚±º
</code></pre></div>
</div>
</details>

<p><strong>Perch√© questo step?</strong> La standardizzazione assicura che:
1. Tutte le variabili abbiano uguale &ldquo;peso&rdquo; nel calcolo della PCA
2. L&rsquo;algoritmo non sia dominato da variabili con scale numeriche maggiori
3. I risultati siano invarianti rispetto alle unit√† di misura</p>
<p><strong>Output di questo step:</strong> Matrice standardizzata $\mathbf{Z}$ e vettori di medie $\boldsymbol{\mu}$ e deviazioni standard $\boldsymbol{\sigma}$.</p>
<h4 id="step-2-calcolo-della-matrice-di-covarianza">Step 2: Calcolo della Matrice di Covarianza</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Calcola la matrice di covarianza: Œ£ = (1/(N-1)) Z^T Z
</code></pre></div>
</div>
</details>

<p><strong>Interpretazione matematica:</strong> Questa operazione calcola tutte le covarianze a coppie tra le variabili. L&rsquo;elemento $\Sigma_{jk}$ rappresenta quanto le variabili $j$ e $k$ tendono a variare insieme.</p>
<p><strong>Interpretazione geometrica:</strong> La matrice di covarianza codifica la &ldquo;forma&rdquo; della nuvola di punti nello spazio multidimensionale. Se immaginassimo i dati come un ellissoide multidimensionale, $\boldsymbol{\Sigma}$ ne descriverebbe la forma e l&rsquo;orientamento.</p>
<h4 id="step-3-decomposizione-agli-autovalori">Step 3: Decomposizione agli Autovalori</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Risolvi il problema agli autovalori: Œ£w‚±º = Œª‚±ºw‚±º
Ottieni autovalori: Œª‚ÇÅ, Œª‚ÇÇ, ..., Œª‚Çë
Ottieni autovettori: w‚ÇÅ, w‚ÇÇ, ..., w‚Çë
</code></pre></div>
</div>
</details>

<p><strong>Metodi numerici:</strong> In pratica, questo step viene eseguito usando algoritmi numerici ottimizzati:
- <strong>Metodo diretto</strong>: Decomposizione spettrale di $\boldsymbol{\Sigma}$
- <strong>Metodo SVD</strong>: SVD su $\mathbf{Z}$ (pi√π stabile numericamente)</p>
<p><strong>Perch√© la SVD √® preferibile?</strong> 
1. <strong>Stabilit√† numerica</strong>: Evita di calcolare esplicitamente $\mathbf{Z}^T\mathbf{Z}$, che pu√≤ amplificare errori numerici
2. <strong>Efficienza</strong>: Per $N \ll d$, la SVD pu√≤ essere pi√π veloce
3. <strong>Robustezza</strong>: Meno sensibile a condizionamento numerico della matrice</p>
<h4 id="step-4-ordinamento-e-selezione">Step 4: Ordinamento e Selezione</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Ordina autovalori in ordine decrescente: Œª‚ÇÅ ‚â• Œª‚ÇÇ ‚â• ... ‚â• Œª‚Çë ‚â• 0
Riordina autovettori di conseguenza: w‚ÇÅ, w‚ÇÇ, ..., w‚Çë
Seleziona i primi k autovettori: W_k = [w‚ÇÅ, w‚ÇÇ, ..., w‚Çñ]
</code></pre></div>
</div>
</details>

<p><strong>Criterio di ordinamento:</strong> Ordinare per autovalori decrescenti assicura che selezioniamo le direzioni di massima varianza. Questo √® il principio ottimalit√† della PCA.</p>
<p><strong>Metodi per scegliere k:</strong>
1. <strong>Soglia di varianza</strong>: Scegli $k$ tale che $\frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^d \lambda_j} \geq \alpha$ (es. $\alpha = 0.95$)
2. <strong>Criterio del gomito</strong>: Cerca il &ldquo;gomito&rdquo; nel grafico degli autovalori
3. <strong>Criterio di Kaiser</strong>: Mantieni solo $\lambda_j > 1$ (per dati standardizzati)
4. <strong>Cross-validation</strong>: Testa diverse k su un task downstream</p>
<h4 id="step-5-proiezione-dei-dati">Step 5: Proiezione dei Dati</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Proietta i dati: Z_PCA = Z √ó W_k
</code></pre></div>
</div>
</details>

<p><strong>Significato matematico:</strong> Questa moltiplicazione matriciale calcola le coordinate di ogni punto dati nel nuovo sistema di riferimento definito dai componenti principali.</p>
<p><strong>Interpretazione geometrica:</strong> Stiamo &ldquo;ruotando&rdquo; il sistema di coordinate originale in modo che i nuovi assi siano allineati con le direzioni di massima varianza. Ogni colonna di $\mathbf{Z}_{PCA}$ rappresenta le coordinate dei dati lungo un componente principale.</p>
<h4 id="perche-questa-proiezione-funziona">Perch√© Questa Proiezione Funziona?</h4>
<p>La proiezione $\mathbf{Z} \mathbf{W}_k$ √® ottimale in diversi sensi matematici:</p>
<ol>
<li>
<p><strong>Massimizzazione della Varianza</strong>: I primi $k$ componenti catturano il massimo possibile di varianza totale che pu√≤ essere catturata da $k$ direzioni ortogonali.</p>
</li>
<li>
<p><strong>Minimizzazione dell&rsquo;Errore di Ricostruzione</strong>: Se dovessimo ricostruire i dati originali usando solo $k$ componenti, questa scelta minimizza l&rsquo;errore quadratico medio di ricostruzione.</p>
</li>
<li>
<p><strong>Decorrelazione</strong>: I componenti principali sono ortogonali, quindi le nuove coordinate sono non correlate.</p>
</li>
</ol>
<h4 id="step-6-ricostruzione-opzionale">Step 6: Ricostruzione (Opzionale)</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Ricostruisci nello spazio standardizzato: Z_rec = Z_PCA √ó W_k^T
Ricostruisci nello spazio originale: X_rec = Z_rec ‚äô œÉ + Œº
</code></pre></div>
</div>
</details>

<p>dove $‚äô$ indica la moltiplicazione elemento per elemento (broadcasting) e l&rsquo;addizione si applica lungo le colonne.</p>
<p><strong>Errore di Ricostruzione:</strong>
$$\text{MSE} = \frac{1}{Nd} \|\mathbf{Z} - \mathbf{Z}_{rec}\|_F^2 = \frac{1}{Nd} \sum_{j=k+1}^d \lambda_j$$</p>
<p>Questo errore dipende solo dagli autovalori &ldquo;scartati&rdquo;, confermando che la PCA √® ottimale per l&rsquo;approssimazione a rango ridotto.</p>
<h3 id="complessita-computazionale">Complessit√† Computazionale</h3>
<ul>
<li><strong>Standardizzazione</strong>: $O(Nd)$</li>
<li><strong>Matrice di covarianza</strong>: $O(Nd^2)$</li>
<li><strong>Decomposizione agli autovalori</strong>: $O(d^3)$</li>
<li><strong>Proiezione</strong>: $O(Ndk)$</li>
</ul>
<p><strong>Complessit√† totale</strong>: $O(Nd^2 + d^3)$</p>
<p>Per $N \ll d$, √® pi√π efficiente usare la SVD direttamente su $\mathbf{Z}$, con complessit√† $O(N^2d + N^3)$.</p>
<h3 id="varianti-algoritmiche">Varianti Algoritmiche</h3>
<h4 id="pca-via-svd-numericamente-stabile">PCA via SVD (Numericamente Stabile)</h4>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">pca_via_svd</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">n_components</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    PCA calcolata tramite SVD - pi√π stabile numericamente</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># SVD della matrice centrata</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># I componenti principali sono le righe di Vt (colonne di V)</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">Vt</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>

    <span class="c1"># Autovalori dalla SVD</span>
    <span class="n">explained_variance</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Proiezione</span>
    <span class="n">X_transformed</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">components</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">components</span><span class="p">,</span> <span class="n">explained_variance</span><span class="p">,</span> <span class="n">X_transformed</span>
</code></pre></div>
</div>
</details>

<h4 id="pca-incrementale-per-grandi-dataset">PCA Incrementale (Per Grandi Dataset)</h4>
<p>Quando $N$ √® molto grande, possiamo usare algoritmi incrementali che processano i dati a batch:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Inizializza: Œº = 0, Œ£ = 0, n_seen = 0

Per ogni batch B:
    Aggiorna statistiche incrementalmente
    Aggiorna decomposizione agli autovalori
</code></pre></div>
</div>
</details>

<p>Questo approccio ha complessit√† di memoria $O(d^2)$ invece di $O(Nd)$.</p>
<h3 id="considerazioni-pratiche">Considerazioni Pratiche</h3>
<ol>
<li><strong>Centralizzazione vs Standardizzazione</strong>: </li>
<li>Se le variabili hanno scale simili, pu√≤ bastare centrare (sottrarre la media)</li>
<li>
<p>Se le scale differiscono significativamente, la standardizzazione √® essenziale</p>
</li>
<li>
<p><strong>Gestione dei Valori Mancanti</strong>:</p>
</li>
<li><strong>Rimozione completa</strong>: Elimina osservazioni con valori mancanti</li>
<li><strong>Imputazione</strong>: Sostituisci valori mancanti prima della PCA</li>
<li>
<p><strong>PCA probabilistica</strong>: Gestisce valori mancanti direttamente</p>
</li>
<li>
<p><strong>Robustezza agli Outlier</strong>:</p>
</li>
<li>La PCA standard √® sensibile agli outlier</li>
<li>Alternative: Robust PCA, PCA con outlier detection preliminare</li>
</ol>
<p>Il risultato di questo algoritmo √® una trasformazione ottimale che preserva il massimo dell&rsquo;informazione (varianza) nei primi $k$ componenti, fornendo una rappresentazione compatta ed efficace dei dati originali.</p>
<hr />
<h2 id="implementazione-in-python">Implementazione in Python</h2>
<p>Implementiamo ora la PCA da zero e con scikit-learn, esplorando ogni aspetto pratico con esempi dettagliati e visualizzazioni illuminanti.</p>
<h3 id="setup-e-librerie-fondamentali">Setup e Librerie Fondamentali</h3>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span><span class="p">,</span> <span class="n">make_classification</span><span class="p">,</span> <span class="n">fetch_olivetti_faces</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># Configurazione per plot pi√π belli e informativi</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s2">&quot;husl&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;savefig.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">300</span>

<span class="c1"># Per riproducibilit√†</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>
</div>
</details>

<h3 id="implementazione-manuale-completa">Implementazione Manuale Completa</h3>
<p>Costruiamo la PCA passo dopo passo per comprendere ogni dettaglio:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">pca_from_scratch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementazione completa della PCA da zero</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    X : array-like, shape (n_samples, n_features)</span>
<span class="sd">        Matrice dei dati di input</span>
<span class="sd">    n_components : int, optional (default=None)</span>
<span class="sd">        Numero di componenti principali da mantenere</span>
<span class="sd">        Se None, mantiene tutti i componenti</span>
<span class="sd">    standardize : bool, optional (default=True)</span>
<span class="sd">        Se True, standardizza i dati prima della PCA</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    dict con tutte le informazioni della PCA</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;üîç ANALISI PCA - Dataset </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2">√ó</span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;üìä Componenti richiesti: </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Step 1: Analisi preliminare dei dati</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà STATISTICHE ORIGINALI:&quot;</span><span class="p">)</span>
    <span class="n">means_orig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">stds_orig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Medie: min=</span><span class="si">{</span><span class="n">means_orig</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><span class="n">means_orig</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Std dev: min=</span><span class="si">{</span><span class="n">stds_orig</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><span class="n">stds_orig</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 2: Standardizzazione (se richiesta)</span>
    <span class="k">if</span> <span class="n">standardize</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ STANDARDIZZAZIONE:&quot;</span><span class="p">)</span>
        <span class="c1"># Salviamo i parametri per la ricostruzione</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Evitiamo divisioni per zero</span>
        <span class="n">stds</span><span class="p">[</span><span class="n">stds</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">X_processed</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">stds</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ‚úì Dati standardizzati (media‚âà0, std‚âà1)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Check - Media post-std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_processed</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Check - Std post-std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_processed</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ SOLO CENTRAGGIO (no standardizzazione):&quot;</span><span class="p">)</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>  <span class="c1"># Per mantenere compatibilit√†</span>
        <span class="n">X_processed</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">means</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ‚úì Dati centrati (media‚âà0)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Check - Media post-centraggio: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_processed</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 3: Calcolo matrice di covarianza</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üî¢ MATRICE DI COVARIANZA:&quot;</span><span class="p">)</span>
    <span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_processed</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Dimensione: </span><span class="si">{</span><span class="n">cov_matrix</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Traccia (varianza totale): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Determinante: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verifica propriet√† della matrice di covarianza</span>
    <span class="n">is_symmetric</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">eigenvals_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span>
    <span class="n">is_positive_semidefinite</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">eigenvals_check</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mf">1e-10</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Simmetrica: </span><span class="si">{</span><span class="n">is_symmetric</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Semi-definita positiva: </span><span class="si">{</span><span class="n">is_positive_semidefinite</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 4: Decomposizione agli autovalori</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ DECOMPOSIZIONE AGLI AUTOVALORI:&quot;</span><span class="p">)</span>
    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span>

    <span class="c1"># Ordina in ordine decrescente</span>
    <span class="n">idx_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idx_sorted</span><span class="p">]</span>
    <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">idx_sorted</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Autovalori calcolati: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Range autovalori: [</span><span class="si">{</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

    <span class="c1"># Verifica ortogonalit√† degli autovettori</span>
    <span class="n">orthogonality_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eigenvectors</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">eigenvectors</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_features</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Check ortogonalit√† autovettori: </span><span class="si">{</span><span class="n">orthogonality_check</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 5: Selezione componenti e calcolo varianza spiegata</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìä ANALISI VARIANZA SPIEGATA:&quot;</span><span class="p">)</span>
    <span class="n">total_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>
    <span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">eigenvalues</span> <span class="o">/</span> <span class="n">total_variance</span>
    <span class="n">cumulative_variance_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza totale: </span><span class="si">{</span><span class="n">total_variance</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">))):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Œª=</span><span class="si">{</span><span class="n">eigenvalues</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;var=</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%), &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;cum=</span><span class="si">{</span><span class="n">cumulative_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">cumulative_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

    <span class="c1"># Seleziona i primi n_components</span>
    <span class="n">eigenvalues_selected</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
    <span class="n">eigenvectors_selected</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ COMPONENTI SELEZIONATI:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Numero: </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza catturata: </span><span class="si">{</span><span class="n">cumulative_variance_ratio</span><span class="p">[</span><span class="n">n_components</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">cumulative_variance_ratio</span><span class="p">[</span><span class="n">n_components</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

    <span class="c1"># Step 6: Proiezione dei dati</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîÑ PROIEZIONE DEI DATI:&quot;</span><span class="p">)</span>
    <span class="n">X_transformed</span> <span class="o">=</span> <span class="n">X_processed</span> <span class="o">@</span> <span class="n">eigenvectors_selected</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Dati originali: </span><span class="si">{</span><span class="n">X_processed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Dati trasformati: </span><span class="si">{</span><span class="n">X_transformed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Check - Media componenti: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verifica che le componenti sono decorrelate</span>
    <span class="n">transformed_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">max_off_diagonal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">transformed_cov</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">transformed_cov</span><span class="p">))))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Check decorrelazione: max correlazione = </span><span class="si">{</span><span class="n">max_off_diagonal</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 7: Calcolo dell&#39;errore di ricostruzione (se k &lt; d)</span>
    <span class="k">if</span> <span class="n">n_components</span> <span class="o">&lt;</span> <span class="n">n_features</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîß ERRORE DI RICOSTRUZIONE:&quot;</span><span class="p">)</span>
        <span class="n">X_reconstructed_std</span> <span class="o">=</span> <span class="n">X_transformed</span> <span class="o">@</span> <span class="n">eigenvectors_selected</span><span class="o">.</span><span class="n">T</span>
        <span class="n">reconstruction_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X_processed</span> <span class="o">-</span> <span class="n">X_reconstructed_std</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">theoretical_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">[</span><span class="n">n_components</span><span class="p">:])</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">n_features</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MSE empirico: </span><span class="si">{</span><span class="n">reconstruction_error</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MSE teorico: </span><span class="si">{</span><span class="n">theoretical_error</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Differenza: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">reconstruction_error</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">theoretical_error</span><span class="p">)</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Ricostruzione nello spazio originale</span>
        <span class="k">if</span> <span class="n">standardize</span><span class="p">:</span>
            <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">X_reconstructed_std</span> <span class="o">*</span> <span class="n">stds</span> <span class="o">+</span> <span class="n">means</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">X_reconstructed_std</span> <span class="o">+</span> <span class="n">means</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">reconstruction_error</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">‚úÖ ANALISI PCA COMPLETATA&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Prepara i risultati</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># Dati trasformati e ricostruiti</span>
        <span class="s1">&#39;X_transformed&#39;</span><span class="p">:</span> <span class="n">X_transformed</span><span class="p">,</span>
        <span class="s1">&#39;X_reconstructed&#39;</span><span class="p">:</span> <span class="n">X_reconstructed</span><span class="p">,</span>
        <span class="s1">&#39;X_standardized&#39;</span><span class="p">:</span> <span class="n">X_processed</span><span class="p">,</span>

        <span class="c1"># Componenti principali</span>
        <span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">eigenvectors_selected</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>  <span class="c1"># Convenzione scikit-learn</span>
        <span class="s1">&#39;eigenvalues&#39;</span><span class="p">:</span> <span class="n">eigenvalues_selected</span><span class="p">,</span>
        <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">[:</span><span class="n">n_components</span><span class="p">],</span>
        <span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">:</span> <span class="n">cumulative_variance_ratio</span><span class="p">[:</span><span class="n">n_components</span><span class="p">],</span>

        <span class="c1"># Parametri per trasformazioni inverse</span>
        <span class="s1">&#39;means&#39;</span><span class="p">:</span> <span class="n">means</span><span class="p">,</span>
        <span class="s1">&#39;stds&#39;</span><span class="p">:</span> <span class="n">stds</span><span class="p">,</span>
        <span class="s1">&#39;standardize&#39;</span><span class="p">:</span> <span class="n">standardize</span><span class="p">,</span>

        <span class="c1"># Metriche di qualit√†</span>
        <span class="s1">&#39;reconstruction_error&#39;</span><span class="p">:</span> <span class="n">reconstruction_error</span><span class="p">,</span>
        <span class="s1">&#39;total_variance&#39;</span><span class="p">:</span> <span class="n">total_variance</span><span class="p">,</span>

        <span class="c1"># Diagnostiche</span>
        <span class="s1">&#39;all_eigenvalues&#39;</span><span class="p">:</span> <span class="n">eigenvalues</span><span class="p">,</span>
        <span class="s1">&#39;all_explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">,</span>
        <span class="s1">&#39;cov_matrix&#39;</span><span class="p">:</span> <span class="n">cov_matrix</span><span class="p">,</span>
        <span class="s1">&#39;n_components&#39;</span><span class="p">:</span> <span class="n">n_components</span><span class="p">,</span>
        <span class="s1">&#39;n_features_original&#39;</span><span class="p">:</span> <span class="n">n_features</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Funzioni di utilit√† per analisi e visualizzazione</span>
<span class="k">def</span><span class="w"> </span><span class="nf">analyze_pca_results</span><span class="p">(</span><span class="n">pca_results</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Analizza e visualizza i risultati della PCA in modo completo</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üîç ANALISI DETTAGLIATA DEI RISULTATI PCA&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Estrai informazioni principali</span>
    <span class="n">n_components</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;n_components&#39;</span><span class="p">]</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;n_features_original&#39;</span><span class="p">]</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;components&#39;</span><span class="p">]</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;eigenvalues&#39;</span><span class="p">]</span>
    <span class="n">explained_var</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">]</span>

    <span class="c1"># Analisi dei componenti principali</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìä INTERPRETAZIONE DEI COMPONENTI PRINCIPALI:&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">feature_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ COMPONENTE PRINCIPALE </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Autovalore: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza spiegata: </span><span class="si">{</span><span class="n">explained_var</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">explained_var</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

        <span class="c1"># Trova le feature pi√π importanti per questo componente</span>
        <span class="n">component_weights</span> <span class="o">=</span> <span class="n">components</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">abs_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">component_weights</span><span class="p">)</span>
        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">abs_weights</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Feature pi√π influenti:&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">))):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">component_weights</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     - </span><span class="si">{</span><span class="n">feature_names</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s2">+.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
                  <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="s1">&#39;positive&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;negative&#39;</span><span class="si">}</span><span class="s2"> contribution)&quot;</span><span class="p">)</span>

    <span class="c1"># Suggerimenti per l&#39;interpretazione</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üí° SUGGERIMENTI PER L&#39;INTERPRETAZIONE:&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.95</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ‚úÖ Eccellente: </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2"> componenti catturano &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% della varianza&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.80</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ‚ö†Ô∏è  Buono: </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2"> componenti catturano &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% della varianza&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      Considera di aumentare il numero di componenti se necessario&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   üîÑ Attenzione: </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2"> componenti catturano solo &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% della varianza&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;      Valuta se aumentare il numero di componenti&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_pca_analysis</span><span class="p">(</span><span class="n">pca_results</span><span class="p">,</span> <span class="n">X_original</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Crea una visualizzazione completa dei risultati PCA</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>

    <span class="c1"># Plot 1: Scree plot (Varianza spiegata per componente)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">n_all_components</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;all_explained_variance_ratio&#39;</span><span class="p">])</span>
    <span class="n">components_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="n">n_all_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">explained_var_plot</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;all_explained_variance_ratio&#39;</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">components_range</span><span class="p">,</span> <span class="n">explained_var_plot</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componente&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Spiegata&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìä Scree Plot</span><span class="se">\n</span><span class="s1">(Varianza per Componente)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Evidenzia i componenti selezionati</span>
    <span class="n">n_selected</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;n_components&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">n_selected</span> <span class="o">&lt;=</span> <span class="mi">20</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_selected</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
                   <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">n_selected</span><span class="si">}</span><span class="s1"> componenti selezionati&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Plot 2: Varianza cumulativa</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">cumulative_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;all_explained_variance_ratio&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">components_range</span><span class="p">,</span> <span class="n">cumulative_var</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% varianza&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;90% varianza&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componenti&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Cumulativa&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìà Varianza Cumulativa&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">)</span>

    <span class="c1"># Plot 3: Heatmap dei primi componenti principali</span>
    <span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">n_components_to_show</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;n_components&#39;</span><span class="p">])</span>
    <span class="n">components_for_heatmap</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;components&#39;</span><span class="p">][:</span><span class="n">n_components_to_show</span><span class="p">]</span>

    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">components_for_heatmap</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax3</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature Index&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Componente Principale&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;üéØ Pesi dei Primi </span><span class="si">{</span><span class="n">n_components_to_show</span><span class="si">}</span><span class="s1"> Componenti&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_components_to_show</span><span class="p">),</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components_to_show</span><span class="p">)])</span>

    <span class="c1"># Plot 4: Distribuzione autovalori</span>
    <span class="n">ax4</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">eigenvals_to_show</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;all_eigenvalues&#39;</span><span class="p">][:</span><span class="mi">15</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eigenvals_to_show</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">eigenvals_to_show</span><span class="p">,</span> 
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Componente&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Autovalore&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìä Distribuzione Autovalori&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

    <span class="c1"># Plot 5: Proiezione 2D (se possibile)</span>
    <span class="k">if</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;n_components&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">ax5</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">X_transformed</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;X_transformed&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scatter</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_transformed</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
                                <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">scatter</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax5</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üó∫Ô∏è Proiezione PCA (Colorata per Target)&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_transformed</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
                       <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üó∫Ô∏è Proiezione PCA (Prime 2 Componenti)&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC1 (</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s2">&quot;explained_variance_ratio&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% varianza)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC2 (</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s2">&quot;explained_variance_ratio&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% varianza)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Plot 6: Errore di ricostruzione vs numero di componenti</span>
    <span class="n">ax6</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">X_original</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Calcola l&#39;errore per diversi numeri di componenti</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X_original</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">max_components</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
        <span class="n">reconstruction_errors</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># PCA temporanea con k componenti</span>
            <span class="n">temp_pca</span> <span class="o">=</span> <span class="n">pca_from_scratch</span><span class="p">(</span><span class="n">X_original</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">reconstruction_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_pca</span><span class="p">[</span><span class="s1">&#39;reconstruction_error&#39;</span><span class="p">])</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">reconstruction_errors</span><span class="p">,</span> <span class="s1">&#39;go-&#39;</span><span class="p">,</span> 
                <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componenti&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE Ricostruzione&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üîß Errore di Ricostruzione&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>  <span class="c1"># Scala logaritmica per vedere meglio i dettagli</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Se non abbiamo i dati originali, mostra la distribuzione degli autovalori scartati</span>
        <span class="n">all_eigenvals</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;all_eigenvalues&#39;</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_eigenvals</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">all_eigenvals</span><span class="p">,</span> 
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;darkred&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;n_components&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> 
                   <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Soglia (</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s2">&quot;n_components&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> comp.)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Componente&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Autovalore (Varianza)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìâ Autovalori: Tenuti vs Scartati&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Esempio pratico con dataset Iris</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demo_pca_iris</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dimostra la PCA sul famoso dataset Iris con spiegazioni dettagliate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üå∏ DEMO PCA: DATASET IRIS&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Carica il dataset</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span>
    <span class="n">target_names</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;üìä Dataset caricato:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Campioni: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Feature: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Feature names: </span><span class="si">{</span><span class="n">feature_names</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Classi: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">target_names</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="c1"># Analisi esplorativa preliminare</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç ANALISI ESPLORATIVA:&quot;</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Statistiche descrittive:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Correlazioni tra feature:&quot;</span><span class="p">)</span>
    <span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">correlation_matrix</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

    <span class="c1"># Esegui PCA con la nostra implementazione</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ ESECUZIONE PCA (nostra implementazione):&quot;</span><span class="p">)</span>
    <span class="n">pca_results</span> <span class="o">=</span> <span class="n">pca_from_scratch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Analisi dei risultati</span>
    <span class="n">analyze_pca_results</span><span class="p">(</span><span class="n">pca_results</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>

    <span class="c1"># Confronta con scikit-learn</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîÑ CONFRONTO CON SCIKIT-LEARN:&quot;</span><span class="p">)</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">pca_sklearn</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X_pca_sklearn</span> <span class="o">=</span> <span class="n">pca_sklearn</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

    <span class="c1"># Verifica che i risultati siano equivalenti (a meno del segno)</span>
    <span class="n">diff_transformed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span>
        <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;X_transformed&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">X_pca_sklearn</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;X_transformed&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">X_pca_sklearn</span><span class="p">))</span>
    <span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Differenza max nei dati trasformati: </span><span class="si">{</span><span class="n">diff_transformed</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza spiegata (nostra): </span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza spiegata (sklearn): </span><span class="si">{</span><span class="n">pca_sklearn</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Visualizzazione completa</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_pca_analysis</span><span class="p">(</span><span class="n">pca_results</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;üå∏ Analisi PCA Completa - Dataset Iris&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># Plot aggiuntivo: confronto 2D tra originale e PCA</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="c1"># Plot originale (prime 2 feature)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target_names</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="n">i</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;üìä Spazio Originale</span><span class="se">\n</span><span class="s1">(Prime 2 Feature)&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Plot PCA</span>
    <span class="n">X_transformed</span> <span class="o">=</span> <span class="n">pca_results</span><span class="p">[</span><span class="s1">&#39;X_transformed&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target_names</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="n">i</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_transformed</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                   <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC1 (</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s2">&quot;explained_variance_ratio&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% var)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC2 (</span><span class="si">{</span><span class="n">pca_results</span><span class="p">[</span><span class="s2">&quot;explained_variance_ratio&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% var)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;üéØ Spazio PCA</span><span class="se">\n</span><span class="s1">(Prime 2 Componenti Principali)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">pca_results</span>

<span class="c1"># Test della demo</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Esegui la demo con Iris</span>
    <span class="n">iris_pca_results</span> <span class="o">=</span> <span class="n">demo_pca_iris</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<h3 id="implementazione-con-scikit-learn-best-practices">Implementazione con Scikit-Learn: Best Practices</h3>
<p>Mentre la nostra implementazione da zero ci aiuta a capire ogni dettaglio, nella pratica scikit-learn offre implementazioni ottimizzate e numericamente stabili:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span><span class="p">,</span> <span class="n">IncrementalPCA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pca_with_sklearn_complete</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">plot_analysis</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementazione completa PCA con scikit-learn e analisi approfondita</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üîß PCA CON SCIKIT-LEARN - IMPLEMENTAZIONE PROFESSIONALE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Step 1: Preprocessing con Pipeline</span>
    <span class="c1"># Un pipeline assicura che preprocessing e PCA siano applicati consistentemente</span>
    <span class="k">if</span> <span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Pipeline per riproducibilit√† e consistenza</span>
    <span class="n">pca_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
    <span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;üìä Configurazione Pipeline:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Standardizzazione: StandardScaler&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   PCA componenti: </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Dataset shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Fit e transform</span>
    <span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca_pipeline</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">pca_model</span> <span class="o">=</span> <span class="n">pca_pipeline</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;pca&#39;</span><span class="p">]</span>
    <span class="n">scaler_model</span> <span class="o">=</span> <span class="n">pca_pipeline</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;scaler&#39;</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">‚úÖ Trasformazione completata&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Output shape: </span><span class="si">{</span><span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza totale catturata: </span><span class="si">{</span><span class="n">pca_model</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 2: Analisi dettagliata dei risultati</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìà ANALISI COMPONENTI PRINCIPALI:&quot;</span><span class="p">)</span>

    <span class="c1"># Informazioni sui componenti</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">pca_model</span><span class="o">.</span><span class="n">components_</span>
    <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca_model</span><span class="o">.</span><span class="n">explained_variance_</span>
    <span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">pca_model</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Forma matrice componenti: </span><span class="si">{</span><span class="n">components</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Œª=</span><span class="si">{</span><span class="n">explained_variance</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;ratio=</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

    <span class="c1"># Step 3: Diagnostiche avanzate</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç DIAGNOSTICHE AVANZATE:&quot;</span><span class="p">)</span>

    <span class="c1"># Test di Kaiser (autovalori &gt; 1 per dati standardizzati)</span>
    <span class="n">kaiser_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">explained_variance</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Criterio di Kaiser: </span><span class="si">{</span><span class="n">kaiser_components</span><span class="si">}</span><span class="s2"> componenti (autovalori &gt; 1)&quot;</span><span class="p">)</span>

    <span class="c1"># Analisi del &quot;gomito&quot; (elbow method)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Calcola la seconda derivata per trovare il punto di gomito</span>
        <span class="n">second_derivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">elbow_candidate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">second_derivative</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>  <span class="c1"># +2 per offset delle derivate</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Punto gomito stimato: componente </span><span class="si">{</span><span class="n">elbow_candidate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Varianza cumulativa per diverse soglie</span>
    <span class="n">cumulative_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]:</span>
        <span class="n">n_components_threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumulative_variance</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">cumulative_variance</span><span class="p">[</span><span class="n">n_components_threshold</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">threshold</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% varianza: </span><span class="si">{</span><span class="n">n_components_threshold</span><span class="si">}</span><span class="s2"> componenti&quot;</span><span class="p">)</span>

    <span class="c1"># Step 4: Calcolo errore di ricostruzione</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîß ERRORE DI RICOSTRUZIONE:&quot;</span><span class="p">)</span>
    <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">pca_pipeline</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>

    <span class="c1"># MSE per feature</span>
    <span class="n">mse_per_feature</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_reconstructed</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mse_total</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_reconstructed</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MSE totale: </span><span class="si">{</span><span class="n">mse_total</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MSE per feature:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">mse</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mse_per_feature</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     Feature </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Errore teorico (somma degli autovalori scartati)</span>
    <span class="k">if</span> <span class="n">n_components</span> <span class="o">&lt;</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="c1"># Per calcolare l&#39;errore teorico, dobbiamo fare PCA completa</span>
        <span class="n">pca_full</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
        <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">pca_full</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

        <span class="n">theoretical_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pca_full</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">[</span><span class="n">n_components</span><span class="p">:])</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MSE teorico: </span><span class="si">{</span><span class="n">theoretical_error</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Differenza empirico-teorico: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mse_total</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">theoretical_error</span><span class="p">)</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Step 5: Analisi della stabilit√†</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ ANALISI STABILIT√Ä:&quot;</span><span class="p">)</span>

    <span class="c1"># Test bootstrap per valutare stabilit√† componenti</span>
    <span class="n">n_bootstrap</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">stability_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Eseguendo </span><span class="si">{</span><span class="n">n_bootstrap</span><span class="si">}</span><span class="s2"> test bootstrap...&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">bootstrap_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_bootstrap</span><span class="p">):</span>
        <span class="c1"># Campiona con replacement</span>
        <span class="n">bootstrap_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X_bootstrap</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">bootstrap_indices</span><span class="p">]</span>

        <span class="c1"># Applica PCA</span>
        <span class="n">pca_bootstrap</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
        <span class="p">])</span>
        <span class="n">pca_bootstrap</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_bootstrap</span><span class="p">)</span>

        <span class="c1"># Confronta i componenti principali (usando prodotto scalare)</span>
        <span class="n">bootstrap_components</span> <span class="o">=</span> <span class="n">pca_bootstrap</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;pca&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">components_</span>

        <span class="c1"># Calcola similarit√† con componenti originali</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
            <span class="c1"># Prodotto scalare (coseno se vettori normalizzati)</span>
            <span class="n">similarity</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">components</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">bootstrap_components</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="n">similarities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span>

        <span class="n">stability_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">similarities</span><span class="p">)</span>

    <span class="n">stability_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stability_scores</span><span class="p">)</span>
    <span class="n">mean_stability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">stability_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">std_stability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">stability_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Stabilit√† componenti principali:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">mean_stability</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">std_stability</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Interpretazione stabilit√†</span>
    <span class="n">stable_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mean_stability</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Componenti stabili (similarit√† &gt; 0.7): </span><span class="si">{</span><span class="n">stable_components</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_analysis</span><span class="p">:</span>
        <span class="c1"># Crea visualizzazione avanzata</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

        <span class="c1"># Plot 1: Scree plot con analisi del gomito</span>
        <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">n_plot</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">))</span>
        <span class="n">x_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_plot</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">explained_variance_ratio</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Componente Principale&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Spiegata&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìä Scree Plot con Analisi Gomito&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># Evidenzia punto di gomito se calcolato</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">second_derivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">second_derivative</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">elbow_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">second_derivative</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
                <span class="k">if</span> <span class="n">elbow_idx</span> <span class="o">&lt;</span> <span class="n">n_plot</span><span class="p">:</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">elbow_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                               <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Gomito: PC</span><span class="si">{</span><span class="n">elbow_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

        <span class="c1"># Plot 2: Varianza cumulativa con soglie</span>
        <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Aggiungi linee di soglia</span>
        <span class="n">thresholds</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                       <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">threshold</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componenti&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Cumulativa&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìà Varianza Cumulativa&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>

        <span class="c1"># Plot 3: Heatmap componenti principali</span>
        <span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">n_comp_heatmap</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">components</span><span class="p">[:</span><span class="n">n_comp_heatmap</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Componente Principale&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;üéØ Loadings Matrix</span><span class="se">\n</span><span class="s1">(Prime </span><span class="si">{</span><span class="n">n_comp_heatmap</span><span class="si">}</span><span class="s1"> PC)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_comp_heatmap</span><span class="p">),</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_comp_heatmap</span><span class="p">)])</span>

        <span class="c1"># Plot 4: Distribuzione autovalori</span>
        <span class="n">ax4</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">explained_variance</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kaiser_components</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                       <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Soglia Kaiser (Œª=1)&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Componente&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Autovalore&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìä Autovalori (Criterio Kaiser)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

        <span class="c1"># Plot 5: Stabilit√† dei componenti</span>
        <span class="n">ax5</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">x_stability</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean_stability</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_stability</span><span class="p">,</span> <span class="n">mean_stability</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std_stability</span><span class="p">,</span> 
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgreen&#39;</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                   <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soglia stabilit√† (0.7)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Componente Principale&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Stabilit√† (Cosine Similarity)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üéØ Stabilit√† Bootstrap&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>

        <span class="c1"># Plot 6: Errore di ricostruzione per feature</span>
        <span class="n">ax6</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">feature_indices</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mse_per_feature</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">feature_indices</span><span class="p">,</span> <span class="n">mse_per_feature</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE Ricostruzione&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üîß Errore per Feature&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

        <span class="c1"># Plot 7-12: Proiezioni 2D delle prime 6 combinazioni di componenti</span>
        <span class="k">if</span> <span class="n">n_components</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">plot_positions</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">)]</span>
            <span class="n">component_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>

            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">pos</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">component_pairs</span><span class="p">,</span> <span class="n">plot_positions</span><span class="p">)):</span>
                <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_components</span><span class="p">:</span>
                    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="o">*</span><span class="n">pos</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> vs PC</span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Se non abbiamo abbastanza componenti, mostra distribuzione di un componente</span>
                    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="o">*</span><span class="n">pos</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n_components</span><span class="p">:</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">)</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequenza&#39;</span><span class="p">)</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Distribuzione PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;üîç Analisi PCA Completa con Scikit-Learn&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># Restituisci risultati completi</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;pipeline&#39;</span><span class="p">:</span> <span class="n">pca_pipeline</span><span class="p">,</span>
        <span class="s1">&#39;pca_model&#39;</span><span class="p">:</span> <span class="n">pca_model</span><span class="p">,</span>
        <span class="s1">&#39;scaler_model&#39;</span><span class="p">:</span> <span class="n">scaler_model</span><span class="p">,</span>
        <span class="s1">&#39;X_transformed&#39;</span><span class="p">:</span> <span class="n">X_pca</span><span class="p">,</span>
        <span class="s1">&#39;X_reconstructed&#39;</span><span class="p">:</span> <span class="n">X_reconstructed</span><span class="p">,</span>
        <span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">components</span><span class="p">,</span>
        <span class="s1">&#39;explained_variance&#39;</span><span class="p">:</span> <span class="n">explained_variance</span><span class="p">,</span>
        <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_variance_ratio</span><span class="p">,</span>
        <span class="s1">&#39;cumulative_variance_ratio&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">),</span>
        <span class="s1">&#39;reconstruction_error_total&#39;</span><span class="p">:</span> <span class="n">mse_total</span><span class="p">,</span>
        <span class="s1">&#39;reconstruction_error_per_feature&#39;</span><span class="p">:</span> <span class="n">mse_per_feature</span><span class="p">,</span>
        <span class="s1">&#39;stability_scores&#39;</span><span class="p">:</span> <span class="n">stability_scores</span><span class="p">,</span>
        <span class="s1">&#39;stability_mean&#39;</span><span class="p">:</span> <span class="n">mean_stability</span><span class="p">,</span>
        <span class="s1">&#39;stability_std&#39;</span><span class="p">:</span> <span class="n">std_stability</span><span class="p">,</span>
        <span class="s1">&#39;kaiser_components&#39;</span><span class="p">:</span> <span class="n">kaiser_components</span><span class="p">,</span>
        <span class="s1">&#39;n_components&#39;</span><span class="p">:</span> <span class="n">n_components</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Esempio avanzato con dataset sintetico ad alta dimensionalit√†</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demo_high_dimensional_pca</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dimostra PCA su dataset sintetico ad alta dimensionalit√†</span>
<span class="sd">    per mostrare il potere della riduzione dimensionale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üöÄ DEMO PCA: DATASET AD ALTA DIMENSIONALIT√Ä&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Genera dataset sintetico con struttura intrinseca di bassa dimensionalit√†</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_classification</span>

    <span class="c1"># Dataset con molte feature ma struttura intrinseca semplice</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>           <span class="c1"># Numero campioni</span>
        <span class="n">n_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>            <span class="c1"># Feature totali</span>
        <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>          <span class="c1"># Feature realmente informative</span>
        <span class="n">n_redundant</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>           <span class="c1"># Feature ridondanti (combinazioni lineari delle informative)</span>
        <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>   <span class="c1"># Cluster per classe</span>
        <span class="n">class_sep</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>            <span class="c1"># Separazione tra classi</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;üìä Dataset generato:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Campioni: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Feature totali: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Feature informative: 5&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Feature ridondanti: 10&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Classi: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Aggiungi rumore gaussiano per rendere pi√π realistico</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">X_noisy</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">noise</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Rumore aggiunto: N(0, 0.1)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   SNR stimato: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Analizza la correlazione tra feature</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç ANALISI CORRELAZIONI:&quot;</span><span class="p">)</span>
    <span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">X_noisy</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="c1"># Trova coppie di feature altamente correlate</span>
    <span class="n">high_corr_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X_noisy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">correlation_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">:</span>
                <span class="n">high_corr_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">correlation_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Coppie con |correlazione| &gt; 0.7: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">high_corr_pairs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">high_corr_pairs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Top 5 correlazioni pi√π forti:&quot;</span><span class="p">)</span>
        <span class="n">sorted_pairs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">high_corr_pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">corr</span> <span class="ow">in</span> <span class="n">sorted_pairs</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     Feature </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Feature </span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">corr</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Esegui PCA completa per analisi</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ ESECUZIONE PCA COMPLETA:&quot;</span><span class="p">)</span>
    <span class="n">pca_full</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

    <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_noisy</span><span class="p">)</span>
    <span class="n">X_pca_full</span> <span class="o">=</span> <span class="n">pca_full</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

    <span class="n">explained_var_ratio</span> <span class="o">=</span> <span class="n">pca_full</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
    <span class="n">cumulative_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_var_ratio</span><span class="p">)</span>

    <span class="c1"># Trova numero ottimale di componenti per diverse soglie</span>
    <span class="n">thresholds</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]</span>
    <span class="n">optimal_components</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
        <span class="n">n_comp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumulative_var</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">optimal_components</span><span class="p">[</span><span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_comp</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">n_comp</span> <span class="o">/</span> <span class="n">n_features</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">threshold</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% varianza: </span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s2"> componenti &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;(riduzione: </span><span class="si">{</span><span class="n">reduction</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

    <span class="c1"># Analisi della &quot;curse of dimensionality&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìê ANALISI CURSE OF DIMENSIONALITY:&quot;</span><span class="p">)</span>

    <span class="c1"># Calcola distanze nell&#39;spazio originale vs PCA</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics.pairwise</span><span class="w"> </span><span class="kn">import</span> <span class="n">euclidean_distances</span>

    <span class="c1"># Campiona 100 punti per efficienza</span>
    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X_noisy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">X_sample</span> <span class="o">=</span> <span class="n">X_scaled</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">]</span>

    <span class="n">distances_original</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>

    <span class="c1"># Calcola per diversi numeri di componenti PCA</span>
    <span class="n">distances_pca</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">n_comp</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">n_comp</span> <span class="o">&lt;=</span> <span class="n">n_features</span><span class="p">:</span>
            <span class="n">pca_temp</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_comp</span><span class="p">)</span>
            <span class="n">X_pca_temp</span> <span class="o">=</span> <span class="n">pca_temp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
            <span class="n">X_pca_sample</span> <span class="o">=</span> <span class="n">X_pca_temp</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">]</span>
            <span class="n">distances_pca</span><span class="p">[</span><span class="n">n_comp</span><span class="p">]</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X_pca_sample</span><span class="p">)</span>

    <span class="c1"># Analizza la distribuzione delle distanze</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Statistiche distanze (campione di 100 punti):&quot;</span><span class="p">)</span>
    <span class="n">orig_distances_flat</span> <span class="o">=</span> <span class="n">distances_original</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">distances_original</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     Spazio originale (</span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s2">D): &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;Œº=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">orig_distances_flat</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;œÉ=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">orig_distances_flat</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n_comp</span><span class="p">,</span> <span class="n">dist_matrix</span> <span class="ow">in</span> <span class="n">distances_pca</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">dist_flat</span> <span class="o">=</span> <span class="n">dist_matrix</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">dist_matrix</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">preserved_variance</span> <span class="o">=</span> <span class="n">cumulative_var</span><span class="p">[</span><span class="n">n_comp</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     PCA </span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s2">D (</span><span class="si">{</span><span class="n">preserved_variance</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> var): &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Œº=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist_flat</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;œÉ=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dist_flat</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Visualizzazione avanzata</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

    <span class="c1"># Plot 1: Scree plot completo</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_var_ratio</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">explained_var_ratio</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Componente&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Spiegata&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìä Scree Plot Completo&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

    <span class="c1"># Evidenzia i primi componenti pi√π importanti</span>
    <span class="n">top_components</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">top_components</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Prime </span><span class="si">{</span><span class="n">top_components</span><span class="si">}</span><span class="s1"> PC&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Plot 2: Varianza cumulativa con soglie</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumulative_var</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cumulative_var</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">)</span>

    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="s1">&#39;brown&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">n_comp</span><span class="p">),</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">optimal_components</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">colors</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                   <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">threshold</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">% (</span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s1"> PC)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_comp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componenti&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Cumulativa&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìà Soglie di Varianza&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Plot 3: Heatmap correlazioni originali</span>
    <span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">correlation_matrix</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üîó Matrice Correlazioni</span><span class="se">\n</span><span class="s1">(Spazio Originale)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">)</span>

    <span class="c1"># Plot 4: Prime componenti principali (loadings)</span>
    <span class="n">ax4</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n_comp_show</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">pca_full</span><span class="o">.</span><span class="n">n_components_</span><span class="p">)</span>
    <span class="n">loadings</span> <span class="o">=</span> <span class="n">pca_full</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="n">n_comp_show</span><span class="p">]</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">loadings</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;üéØ Loadings Matrix</span><span class="se">\n</span><span class="s1">(Prime </span><span class="si">{</span><span class="n">n_comp_show</span><span class="si">}</span><span class="s1"> PC)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature Originale&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Componente Principale&#39;</span><span class="p">)</span>

    <span class="c1"># Plot 5-6: Proiezioni 2D con classi</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">n_comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_comp</span> <span class="o">&lt;=</span> <span class="n">pca_full</span><span class="o">.</span><span class="n">n_components_</span><span class="p">:</span>
            <span class="n">pca_temp</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_comp</span><span class="p">)</span>
            <span class="n">X_temp</span> <span class="o">=</span> <span class="n">pca_temp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">n_comp</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="c1"># Scatter plot 2D</span>
                <span class="k">for</span> <span class="n">class_val</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="n">class_val</span>
                    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_temp</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_temp</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                              <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Classe </span><span class="si">{</span><span class="n">class_val</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

                <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC1 (</span><span class="si">{</span><span class="n">pca_temp</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC2 (</span><span class="si">{</span><span class="n">pca_temp</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;üó∫Ô∏è Proiezione </span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s1">D</span><span class="se">\n</span><span class="s1">&#39;</span>
                         <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pca_temp</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% var)&#39;</span><span class="p">)</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Plot 7: Confronto distribuzioni distanze</span>
    <span class="n">ax7</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>

    <span class="c1"># Plot distribuzione per spazio originale</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">orig_distances_flat</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Originale (</span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s1">D)&#39;</span><span class="p">,</span>
             <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

    <span class="c1"># Plot per alcuni spazi PCA</span>
    <span class="n">colors_hist</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">n_comp</span><span class="p">,</span> <span class="n">dist_matrix</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">distances_pca</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="mi">3</span><span class="p">]):</span>
        <span class="n">dist_flat</span> <span class="o">=</span> <span class="n">dist_matrix</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">dist_matrix</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">dist_flat</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
                <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;PCA </span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s1">D (</span><span class="si">{</span><span class="n">cumulative_var</span><span class="p">[</span><span class="n">n_comp</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span>
                <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors_hist</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Distanza Euclidea&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Densit√†&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üìê Distribuzione Distanze&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Plot 8: Efficienza di compressione</span>
    <span class="n">ax8</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>

    <span class="c1"># Calcola rapporto di compressione per diversi livelli di qualit√†</span>
    <span class="n">compression_ratios</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">quality_levels</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)):</span>
        <span class="n">compression_ratio</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="n">n_features</span>
        <span class="n">quality</span> <span class="o">=</span> <span class="n">cumulative_var</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">compression_ratios</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compression_ratio</span><span class="p">)</span>
        <span class="n">quality_levels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">quality</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">compression_ratios</span><span class="p">,</span> <span class="n">quality_levels</span><span class="p">,</span> <span class="s1">&#39;go-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="c1"># Aggiungi punti di interesse</span>
    <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">optimal_components</span><span class="p">:</span>
            <span class="n">n_comp</span> <span class="o">=</span> <span class="n">optimal_components</span><span class="p">[</span><span class="n">threshold</span><span class="p">]</span>
            <span class="n">ratio</span> <span class="o">=</span> <span class="n">n_comp</span> <span class="o">/</span> <span class="n">n_features</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">threshold</span><span class="si">:</span><span class="s1">.0%</span><span class="si">}</span><span class="se">\n</span><span class="s1">(</span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s1"> PC)&#39;</span><span class="p">,</span> 
                        <span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="n">threshold</span><span class="p">),</span> 
                        <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Rapporto Compressione (k/d)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Qualit√† (Varianza Preservata)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;üóúÔ∏è Efficienza Compressione&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;üöÄ Analisi PCA: Dataset ad Alta Dimensionalit√†&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># Ritorna risultati per analisi successive</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;X_original&#39;</span><span class="p">:</span> <span class="n">X_noisy</span><span class="p">,</span>
        <span class="s1">&#39;X_scaled&#39;</span><span class="p">:</span> <span class="n">X_scaled</span><span class="p">,</span>
        <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
        <span class="s1">&#39;pca_model&#39;</span><span class="p">:</span> <span class="n">pca_full</span><span class="p">,</span>
        <span class="s1">&#39;optimal_components&#39;</span><span class="p">:</span> <span class="n">optimal_components</span><span class="p">,</span>
        <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">explained_var_ratio</span><span class="p">,</span>
        <span class="s1">&#39;correlation_matrix&#39;</span><span class="p">:</span> <span class="n">correlation_matrix</span>
    <span class="p">}</span>

<span class="c1"># Esempio con dataset reale: Faces (Olivetti)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demo_pca_faces</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dimostra PCA su immagini di volti per mostrare</span>
<span class="sd">    l&#39;applicazione alla computer vision</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üë§ DEMO PCA: DATASET VOLTI (OLIVETTI FACES)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Carica dataset Olivetti Faces</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_olivetti_faces</span><span class="p">(</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">X_faces</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">data</span>
        <span class="n">y_faces</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">target</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;üìä Dataset caricato:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Immagini: </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Pixel per immagine: </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Dimensioni immagine: 64x64 pixel&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Persone diverse: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_faces</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Immagini per persona: ~</span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_faces</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Analisi preliminare</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç ANALISI PRELIMINARE:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Range valori pixel: [</span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Media globale: </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Deviazione standard: </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Esegui PCA</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ ESECUZIONE PCA:&quot;</span><span class="p">)</span>

        <span class="c1"># Non standardizziamo per i volti perch√© i pixel hanno gi√† range simile</span>
        <span class="n">pca_faces</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>  <span class="c1"># Manteniamo 50 componenti principali</span>
        <span class="n">X_faces_pca</span> <span class="o">=</span> <span class="n">pca_faces</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_faces</span><span class="p">)</span>

        <span class="n">explained_var</span> <span class="o">=</span> <span class="n">pca_faces</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
        <span class="n">cumulative_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_var</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Componenti calcolati: </span><span class="si">{</span><span class="n">pca_faces</span><span class="o">.</span><span class="n">n_components_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Varianza catturata: </span><span class="si">{</span><span class="n">cumulative_var</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">cumulative_var</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

        <span class="c1"># Analisi delle &quot;eigenfaces&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üé≠ ANALISI EIGENFACES:&quot;</span><span class="p">)</span>
        <span class="n">eigenfaces</span> <span class="o">=</span> <span class="n">pca_faces</span><span class="o">.</span><span class="n">components_</span>

        <span class="c1"># Le eigenfaces sono i componenti principali rimodellati come immagini 64x64</span>
        <span class="n">eigenfaces_images</span> <span class="o">=</span> <span class="n">eigenfaces</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Eigenfaces generate: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenfaces_images</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Ogni eigenface rappresenta una direzione principale nello spazio dei volti&quot;</span><span class="p">)</span>

        <span class="c1"># Calcola quanto ogni persona √® rappresentata dai diversi componenti</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìä RAPPRESENTAZIONE PER COMPONENTI:&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eigenfaces</span><span class="p">))):</span>
            <span class="n">var_percentage</span> <span class="o">=</span> <span class="n">explained_var</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Eigenface </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">var_percentage</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% della varianza totale&quot;</span><span class="p">)</span>

        <span class="c1"># Visualizzazione completa</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

        <span class="c1"># Plot 1: Alcune immagini originali</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üñºÔ∏è CREAZIONE VISUALIZZAZIONI...&quot;</span><span class="p">)</span>

        <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">n_samples_show</span> <span class="o">=</span> <span class="mi">6</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples_show</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">face_img</span> <span class="o">=</span> <span class="n">X_faces</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">face_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Persona </span><span class="si">{</span><span class="n">y_faces</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;üñºÔ∏è Volti Originali&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

        <span class="c1"># Plot 2: Prime eigenfaces</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">7</span><span class="p">)</span>
            <span class="n">eigenface</span> <span class="o">=</span> <span class="n">eigenfaces_images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># Normalizza per visualizzazione</span>
            <span class="n">eigenface_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">eigenface</span> <span class="o">-</span> <span class="n">eigenface</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">eigenface</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">eigenface</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">eigenface_norm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Eigenface </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="se">\n</span><span class="s1">(</span><span class="si">{</span><span class="n">explained_var</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;üé≠ Prime 6 Eigenfaces&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.65</span><span class="p">)</span>

        <span class="c1"># Plot 3: Ricostruzioni con diversi numeri di componenti</span>
        <span class="c1"># Scegli un volto specifico per la dimostrazione</span>
        <span class="n">face_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">original_face</span> <span class="o">=</span> <span class="n">X_faces</span><span class="p">[</span><span class="n">face_idx</span><span class="p">]</span>

        <span class="n">reconstruction_components</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">n_comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reconstruction_components</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">13</span><span class="p">)</span>

            <span class="c1"># Ricostruisci usando solo i primi n_comp componenti</span>
            <span class="n">face_pca_coords</span> <span class="o">=</span> <span class="n">X_faces_pca</span><span class="p">[</span><span class="n">face_idx</span><span class="p">,</span> <span class="p">:</span><span class="n">n_comp</span><span class="p">]</span>
            <span class="n">face_reconstructed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">face_pca_coords</span><span class="p">,</span> <span class="n">eigenfaces</span><span class="p">[:</span><span class="n">n_comp</span><span class="p">])</span> <span class="o">+</span> <span class="n">pca_faces</span><span class="o">.</span><span class="n">mean_</span>

            <span class="c1"># Visualizza</span>
            <span class="n">face_img</span> <span class="o">=</span> <span class="n">face_reconstructed</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">face_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

            <span class="c1"># Calcola errore di ricostruzione</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">original_face</span> <span class="o">-</span> <span class="n">face_reconstructed</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">var_captured</span> <span class="o">=</span> <span class="n">cumulative_var</span><span class="p">[</span><span class="n">n_comp</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s1"> PC</span><span class="se">\n</span><span class="si">{</span><span class="n">var_captured</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% var</span><span class="se">\n</span><span class="s1">MSE: </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;üîß Ricostruzione Volto (Persona </span><span class="si">{</span><span class="n">y_faces</span><span class="p">[</span><span class="n">face_idx</span><span class="p">]</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="c1"># Grafico separato per analisi quantitativa</span>
        <span class="n">fig</span><span class="p">,</span> <span class="p">((</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">),</span> <span class="p">(</span><span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span><span class="p">))</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

        <span class="c1"># Scree plot</span>
        <span class="n">n_plot</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_var</span><span class="p">))</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_plot</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">explained_var</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="s1">&#39;bo-&#39;</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Componente Principale&#39;</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Spiegata&#39;</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;üìä Scree Plot - Eigenfaces&#39;</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

        <span class="c1"># Varianza cumulativa</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_plot</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cumulative_var</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="s1">&#39;ro-&#39;</span><span class="p">)</span>
        <span class="n">thresholds</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
            <span class="n">ax2</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                       <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">threshold</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
            <span class="c1"># Trova il numero di componenti necessario</span>
            <span class="n">n_comp_needed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumulative_var</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">n_comp_needed</span> <span class="o">&lt;=</span> <span class="n">n_plot</span><span class="p">:</span>
                <span class="n">ax2</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_comp_needed</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componenti&#39;</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Varianza Cumulativa&#39;</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;üìà Varianza Cumulativa&#39;</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># Errore di ricostruzione vs numero di componenti</span>
        <span class="n">component_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">51</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eigenfaces</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">reconstruction_errors</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">n_comp</span> <span class="ow">in</span> <span class="n">component_range</span><span class="p">:</span>
            <span class="c1"># Calcola errore medio di ricostruzione</span>
            <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">pca_faces</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_faces_pca</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_comp</span><span class="p">])</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X_faces</span> <span class="o">-</span> <span class="n">X_reconstructed</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">reconstruction_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

        <span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">component_range</span><span class="p">,</span> <span class="n">reconstruction_errors</span><span class="p">,</span> <span class="s1">&#39;go-&#39;</span><span class="p">)</span>
        <span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Numero Componenti&#39;</span><span class="p">)</span>
        <span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;MSE Ricostruzione&#39;</span><span class="p">)</span>
        <span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;üîß Errore di Ricostruzione&#39;</span><span class="p">)</span>
        <span class="n">ax3</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">ax3</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

        <span class="c1"># Proiezione 2D dei volti</span>
        <span class="n">ax4</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_faces_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_faces_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_faces</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;tab20&#39;</span><span class="p">)</span>
        <span class="n">ax4</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC1 (</span><span class="si">{</span><span class="n">explained_var</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
        <span class="n">ax4</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC2 (</span><span class="si">{</span><span class="n">explained_var</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
        <span class="n">ax4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;üó∫Ô∏è Proiezione 2D dei Volti&#39;</span><span class="p">)</span>
        <span class="n">ax4</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;üë§ Analisi Quantitativa PCA - Volti&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;pca_model&#39;</span><span class="p">:</span> <span class="n">pca_faces</span><span class="p">,</span> <span class="s1">&#39;X_transformed&#39;</span><span class="p">:</span> <span class="n">X_faces_pca</span><span class="p">,</span> <span class="s1">&#39;eigenfaces&#39;</span><span class="p">:</span> <span class="n">eigenfaces_images</span><span class="p">}</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Errore nel caricamento del dataset: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Dataset Olivetti Faces potrebbe non essere disponibile&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

<span class="c1"># Esempio semplificato di utilizzo</span>
<span class="k">def</span><span class="w"> </span><span class="nf">quick_pca_demo</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demo rapida e semplice della PCA&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚ö° DEMO PCA RAPIDA&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>

    <span class="c1"># Dataset Iris</span>
    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

    <span class="c1"># PCA con scikit-learn</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

    <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Varianza spiegata: </span><span class="si">{</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Plot semplice</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Dati Originali&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC1 (</span><span class="si">{</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC2 (</span><span class="si">{</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Dopo PCA&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Esegui demo semplice</span>
    <span class="n">quick_pca_demo</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<hr />
<h2 id="interpretazione-dei-risultati">Interpretazione dei Risultati</h2>
<p>La PCA non √® solo una trasformazione matematica - √® uno strumento di esplorazione che ci racconta storie sui nostri dati. Imparare a interpretare i risultati √® fondamentale per utilizzarla efficacemente.</p>
<h3 id="come-interpretare-i-componenti-principali">Come Interpretare i Componenti Principali</h3>
<p>I <strong>componenti principali</strong> non sono variabili casuali, ma combinazioni lineari significative delle variabili originali. Ogni componente ha un&rsquo;interpretazione geometrica e spesso anche semantica.</p>
<h4 id="i-loadings-la-chiave-dellinterpretazione">I Loadings: La Chiave dell&rsquo;Interpretazione</h4>
<p>I <strong>loadings</strong> (pesi) ci dicono quanto ogni variabile originale contribuisce a ciascun componente principale. Se il componente $j$ √® definito come:</p>
<p>$PC_j = w_{j1}x_1 + w_{j2}x_2 + \ldots + w_{jd}x_d$</p>
<p>allora $w_{ji}$ √® il loading della variabile $i$ nel componente $j$.</p>
<p><strong>Interpretazione pratica:</strong>
- <strong>Loading alto positivo</strong>: La variabile contribuisce fortemente e positivamente al componente
- <strong>Loading alto negativo</strong>: La variabile contribuisce fortemente ma negativamente
- <strong>Loading vicino a zero</strong>: La variabile ha poca influenza su questo componente</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Analisi dei loadings</span>
<span class="k">def</span><span class="w"> </span><span class="nf">analyze_loadings</span><span class="p">(</span><span class="n">pca_model</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Analizza e interpreta i loadings dei componenti principali&quot;&quot;&quot;</span>

    <span class="n">components</span> <span class="o">=</span> <span class="n">pca_model</span><span class="o">.</span><span class="n">components_</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">))):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üéØ COMPONENTE PRINCIPALE </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Varianza spiegata: </span><span class="si">{</span><span class="n">pca_model</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Ordina le feature per importanza</span>
        <span class="n">loadings</span> <span class="o">=</span> <span class="n">components</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">feature_importance</span> <span class="o">=</span> <span class="p">[(</span><span class="nb">abs</span><span class="p">(</span><span class="n">loading</span><span class="p">),</span> <span class="n">feature</span><span class="p">,</span> <span class="n">loading</span><span class="p">)</span> 
                            <span class="k">for</span> <span class="n">loading</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">loadings</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)]</span>
        <span class="n">feature_importance</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature pi√π influenti:&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">abs_loading</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">loading</span> <span class="ow">in</span> <span class="n">feature_importance</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
            <span class="n">direction</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span> <span class="k">if</span> <span class="n">loading</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;-&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">direction</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loading</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
</div>
</details>

<h3 id="scelta-del-numero-ottimale-di-componenti">Scelta del Numero Ottimale di Componenti</h3>
<p>Questa √® una delle decisioni pi√π importanti nell&rsquo;applicazione della PCA. Non esiste una risposta universale, ma diverse strategie:</p>
<h4 id="1-criterio-della-varianza-cumulativa">1. Criterio della Varianza Cumulativa</h4>
<p>Mantieni componenti fino a raggiungere una certa percentuale di varianza (tipicamente 80-95%).</p>
<h4 id="2-criterio-del-gomito-elbow-method">2. Criterio del Gomito (Elbow Method)</h4>
<p>Cerca il punto dove la varianza spiegata inizia a decrescere pi√π lentamente.</p>
<h4 id="3-criterio-di-kaiser">3. Criterio di Kaiser</h4>
<p>Per dati standardizzati, mantieni solo componenti con autovalore &gt; 1.</p>
<h4 id="4-cross-validation">4. Cross-Validation</h4>
<p>Valuta le performance su un task downstream (classificazione, clustering, etc.).</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">choose_components</span><span class="p">(</span><span class="n">pca_model</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Suggerisce il numero ottimale di componenti</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">explained_var</span> <span class="o">=</span> <span class="n">pca_model</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
    <span class="n">cumvar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_var</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;variance&#39;</span><span class="p">:</span>
        <span class="n">n_comp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumvar</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Per </span><span class="si">{</span><span class="n">threshold</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2"> varianza: </span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s2"> componenti&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kaiser&#39;</span><span class="p">:</span>
        <span class="n">eigenvals</span> <span class="o">=</span> <span class="n">pca_model</span><span class="o">.</span><span class="n">explained_variance_</span>
        <span class="n">n_comp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eigenvals</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Criterio Kaiser: </span><span class="si">{</span><span class="n">n_comp</span><span class="si">}</span><span class="s2"> componenti&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;elbow&#39;</span><span class="p">:</span>
        <span class="c1"># Calcola la seconda derivata</span>
        <span class="n">second_deriv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">explained_var</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">elbow</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">second_deriv</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Metodo gomito: ~</span><span class="si">{</span><span class="n">elbow</span><span class="si">}</span><span class="s2"> componenti&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">n_comp</span>
</code></pre></div>
</div>
</details>

<hr />
<h2 id="varianti-avanzate-della-pca">Varianti Avanzate della PCA</h2>
<p>La PCA classica √® solo l&rsquo;inizio. Esistono numerose varianti che estendono e migliorano il metodo base per situazioni specifiche.</p>
<h3 id="pca-whitening-sbiancamento">PCA Whitening (Sbiancamento)</h3>
<p>Il <strong>whitening</strong> trasforma i dati in modo che abbiano non solo media zero, ma anche covarianza uguale all&rsquo;identit√†. √à utile per algoritmi che assumono input decorrelati con varianza unitaria.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">pca_whitening</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PCA con whitening&quot;&quot;&quot;</span>

    <span class="c1"># PCA standard</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

    <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X_whitened</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

    <span class="c1"># Verifica: la covarianza dovrebbe essere circa l&#39;identit√†</span>
    <span class="n">cov_whitened</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_whitened</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Covarianza dopo whitening (dovrebbe ‚âà I):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max valore off-diagonale: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">cov_whitened</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cov_whitened</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pca</span><span class="p">,</span> <span class="n">X_whitened</span>
</code></pre></div>
</div>
</details>

<h3 id="incremental-pca">Incremental PCA</h3>
<p>Per dataset troppo grandi per stare in memoria, la <strong>Incremental PCA</strong> processa i dati a batch.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">incremental_pca_demo</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demo di Incremental PCA per grandi dataset&quot;&quot;&quot;</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">IncrementalPCA</span>

    <span class="c1"># PCA incrementale</span>
    <span class="n">ipca</span> <span class="o">=</span> <span class="n">IncrementalPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="c1"># Processa a batch</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">ipca</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Trasforma tutto il dataset</span>
    <span class="n">X_ipca</span> <span class="o">=</span> <span class="n">ipca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;IPCA completata: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">X_ipca</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Varianza spiegata: </span><span class="si">{</span><span class="n">ipca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ipca</span><span class="p">,</span> <span class="n">X_ipca</span>
</code></pre></div>
</div>
</details>

<h3 id="kernel-pca">Kernel PCA</h3>
<p>La <strong>Kernel PCA</strong> estende la PCA a trasformazioni non lineari usando il &ldquo;kernel trick&rdquo;.</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kernel_pca_demo</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demo di Kernel PCA per relazioni non lineari&quot;&quot;&quot;</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">KernelPCA</span>

    <span class="c1"># Kernel PCA</span>
    <span class="n">kpca</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">X_kpca</span> <span class="o">=</span> <span class="n">kpca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel PCA (</span><span class="si">{</span><span class="n">kernel</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">X_kpca</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">kpca</span><span class="p">,</span> <span class="n">X_kpca</span>
</code></pre></div>
</div>
</details>

<hr />
<h2 id="applicazioni-pratiche">Applicazioni Pratiche</h2>
<h3 id="computer-vision-riconoscimento-facciale">Computer Vision: Riconoscimento Facciale</h3>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="c1"># Esempio semplificato di eigenfaces</span>
<span class="k">def</span><span class="w"> </span><span class="nf">simple_face_recognition</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sistema semplificato di riconoscimento facciale&quot;&quot;&quot;</span>

    <span class="c1"># Simula dataset di volti</span>
    <span class="n">n_people</span><span class="p">,</span> <span class="n">n_images_per_person</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">64</span>

    <span class="c1"># Genera dati sintetici (in realt√† useresti immagini reali)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">faces_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">person_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_people</span><span class="p">):</span>
        <span class="c1"># Simula &quot;volto base&quot; per ogni persona</span>
        <span class="n">base_face</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">image_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">img_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_images_per_person</span><span class="p">):</span>
            <span class="c1"># Aggiungi variazioni (illuminazione, pose, etc.)</span>
            <span class="n">variation</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">image_size</span><span class="p">)</span>
            <span class="n">face</span> <span class="o">=</span> <span class="n">base_face</span> <span class="o">+</span> <span class="n">variation</span>
            <span class="n">faces_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">face</span><span class="p">)</span>
            <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">person_id</span><span class="p">)</span>

    <span class="n">X_faces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">faces_data</span><span class="p">)</span>
    <span class="n">y_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="c1"># PCA per estrazione feature</span>
    <span class="n">pca_faces</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">X_faces_pca</span> <span class="o">=</span> <span class="n">pca_faces</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_faces</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset volti: </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dopo PCA: </span><span class="si">{</span><span class="n">X_faces_pca</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compressione: </span><span class="si">{</span><span class="n">X_faces_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">X_faces</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pca_faces</span><span class="p">,</span> <span class="n">X_faces_pca</span><span class="p">,</span> <span class="n">y_labels</span>
</code></pre></div>
</div>
</details>

<h3 id="analisi-finanziaria-riduzione-del-rischio">Analisi Finanziaria: Riduzione del Rischio</h3>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">financial_pca_demo</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PCA per analisi di portafoglio finanziario&quot;&quot;&quot;</span>

    <span class="c1"># Simula dati di rendimenti azionari</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_days</span><span class="p">,</span> <span class="n">n_stocks</span> <span class="o">=</span> <span class="mi">252</span><span class="p">,</span> <span class="mi">50</span>  <span class="c1"># Un anno di dati per 50 azioni</span>

    <span class="c1"># Genera correlazioni realistiche</span>
    <span class="n">base_market</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_days</span><span class="p">)</span>
    <span class="n">stock_returns</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_stocks</span><span class="p">):</span>
        <span class="c1"># Ogni azione ha correlazione diversa con il mercato</span>
        <span class="n">market_exposure</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="o">+</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="n">specific_risk</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">market_exposure</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_days</span><span class="p">)</span>
        <span class="n">stock_return</span> <span class="o">=</span> <span class="n">market_exposure</span> <span class="o">*</span> <span class="n">base_market</span> <span class="o">+</span> <span class="n">specific_risk</span>
        <span class="n">stock_returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stock_return</span><span class="p">)</span>

    <span class="n">X_returns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stock_returns</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># giorni √ó azioni</span>

    <span class="c1"># PCA sui rendimenti</span>
    <span class="n">pca_finance</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
    <span class="n">returns_pca</span> <span class="o">=</span> <span class="n">pca_finance</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_returns</span><span class="p">)</span>

    <span class="c1"># Analisi dei fattori di rischio</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üí∞ ANALISI FATTORI DI RISCHIO:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Primo fattore (mercato): </span><span class="si">{</span><span class="n">pca_finance</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Primi 5 fattori: </span><span class="si">{</span><span class="n">pca_finance</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Il primo componente dovrebbe rappresentare il &quot;fattore mercato&quot;</span>
    <span class="n">market_factor</span> <span class="o">=</span> <span class="n">returns_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">pca_finance</span><span class="p">,</span> <span class="n">market_factor</span>
</code></pre></div>
</div>
</details>

<hr />
<h2 id="vantaggi-e-limitazioni">Vantaggi e Limitazioni</h2>
<h3 id="vantaggi-della-pca">‚úÖ Vantaggi della PCA</h3>
<ol>
<li><strong>Riduzione del Rumore</strong>: Concentra l&rsquo;informazione nei primi componenti</li>
<li><strong>Efficienza Computazionale</strong>: Riduce drasticamente le dimensioni</li>
<li><strong>Visualizzazione</strong>: Permette visualizzazioni 2D/3D di dati complessi</li>
<li><strong>Decorrelazione</strong>: Elimina la multicollinearit√†</li>
<li><strong>Interpretabilit√†</strong>: I componenti principali spesso hanno significato semantico</li>
</ol>
<h3 id="limitazioni-della-pca">‚ö†Ô∏è Limitazioni della PCA</h3>
<ol>
<li><strong>Linearit√†</strong>: Cattura solo relazioni lineari tra variabili</li>
<li><strong>Interpretabilit√†</strong>: I componenti possono essere difficili da interpretare</li>
<li><strong>Sensibilit√† agli Outlier</strong>: Valori estremi possono distorcere i risultati</li>
<li><strong>Standardizzazione Necessaria</strong>: Richiede scaling appropriato dei dati</li>
<li><strong>Perdita di Informazione</strong>: Alcuni pattern potrebbero andare persi</li>
</ol>
<h3 id="quando-non-usare-la-pca">Quando NON Usare la PCA</h3>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">when_not_to_use_pca</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Esempi di situazioni dove la PCA non √® appropriata&quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚ùå NON usare PCA quando:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. Le relazioni sono fortemente non lineari&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;2. Ogni feature ha significato specifico importante&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;3. Il dataset √® gi√† di bassa dimensionalit√†&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;4. La interpretabilit√† √® pi√π importante dell&#39;efficienza&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;5. I dati hanno molti outlier non trattati&quot;</span><span class="p">)</span>

    <span class="c1"># Esempio: dati non lineari</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
    <span class="n">X_circle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)])</span>
    <span class="n">X_circle</span> <span class="o">+=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Rumore</span>

    <span class="c1"># PCA su dati circolari</span>
    <span class="n">pca_circle</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X_circle_pca</span> <span class="o">=</span> <span class="n">pca_circle</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_circle</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_circle</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_circle</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Dati Originali</span><span class="se">\n</span><span class="s1">(Struttura Circolare)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_circle_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_circle_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Dopo PCA</span><span class="se">\n</span><span class="s1">(Struttura Persa)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC1 (</span><span class="si">{</span><span class="n">pca_circle</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PC2 (</span><span class="si">{</span><span class="n">pca_circle</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Mostra i componenti principali</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_circle</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_circle</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Disegna i componenti principali</span>
    <span class="n">mean_point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_circle</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">pca_circle</span><span class="o">.</span><span class="n">components_</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">comp</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">components</span><span class="p">,</span> <span class="n">pca_circle</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">mean_point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean_point</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                 <span class="n">comp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">),</span> <span class="n">comp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">),</span>
                 <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Componenti Principali</span><span class="se">\n</span><span class="s1">(Non Catturano la Circolarit√†)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç Per dati circolari:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Varianza spiegata PC1: </span><span class="si">{</span><span class="n">pca_circle</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Varianza spiegata totale: </span><span class="si">{</span><span class="n">pca_circle</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚û°Ô∏è Alternative: Kernel PCA, Manifold Learning (t-SNE, UMAP)&quot;</span><span class="p">)</span>

<span class="c1"># Esegui esempio</span>
<span class="n">when_not_to_use_pca</span><span class="p">()</span>
</code></pre></div>
</div>
</details>

<hr />
<h2 id="conclusioni">Conclusioni</h2>
<p>La Principal Component Analysis rappresenta uno strumento fondamentale nell&rsquo;arsenale di ogni data scientist. La sua eleganza matematica nasconde una potente capacit√† di rivelare la struttura nascosta dei dati, permettendoci di:</p>
<ul>
<li><strong>Comprendere</strong> la geometria intrinseca dei nostri dataset</li>
<li><strong>Visualizzare</strong> informazioni altrimenti impossibili da rappresentare  </li>
<li><strong>Ottimizzare</strong> le performance computazionali dei nostri algoritmi</li>
<li><strong>Filtrare</strong> il rumore preservando il segnale importante</li>
</ul>
<p>Tuttavia, come ogni strumento, la PCA ha i suoi limiti. La chiave del successo sta nel comprendere quando applicarla e come interpretarne i risultati. Ricorda sempre che la PCA √® un mezzo, non un fine: il tuo obiettivo finale determina se e come utilizzarla.</p>
<h3 id="prossimi-passi">Prossimi Passi</h3>
<p>Dopo aver padroneggiato la PCA, esplora:
- <strong><a href="/theory/math-for-ml/Algebra/Singular Value Decomposition" class="text-blue-600 hover:underline">Singular Value Decomposition</a></strong> per una comprensione pi√π profonda
- <strong>t-SNE e UMAP</strong> per riduzione dimensionale non lineare<br />
- <strong>Factor Analysis</strong> per modelli probabilistici pi√π sofisticati
- <strong>Independent Component Analysis</strong> per separazione di segnali</p>
<p>La PCA ti ha aperto le porte al mondo della riduzione dimensionale - ora hai le chiavi per esplorarlo completamente! üöÄ</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> unsupervised learning, clustering, dimensionality reduction, model, data, learning</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/unsupervised-learning/Dimensionality Reduction/Principal Component Analysis">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'Principal Component Analysis (PCA) - Guida Completa',
          page_location: 'http://localhost:3000/theory/unsupervised-learning/Dimensionality Reduction/Principal Component Analysis'
        });
      }
    </script>
</body>
</html>