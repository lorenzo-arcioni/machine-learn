<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metriche di Valutazione per Classificazione in Machine Learning | Introduction to Machine Learning | ML Theory</title>
    <meta name="description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta name="keywords" content="machine learning, introduction, basics, fundamentals, AI, model, data, neural">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Metriche di Valutazione per Classificazione in Machine Learning">
    <meta property="og:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    <meta property="og:url" content="http://localhost:3000/theory/introduction/Tipologie di Problemi/Classificazione/Classification Metrics">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Machine Learning Theory">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Metriche di Valutazione per Classificazione in Machine Learning">
    <meta name="twitter:description" content="pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:3000/theory/introduction/Tipologie di Problemi/Classificazione/Classification Metrics">
    
    <!-- Preload critical resources -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Metriche di Valutazione per Classificazione in Machine Learning",
      "description": "pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos {...",
      "url": "http://localhost:3000/theory/introduction/Tipologie di Problemi/Classificazione/Classification Metrics",
      "datePublished": "2026-01-16T01:12:55.375Z",
      "author": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      },
      "publisher": {
        "@type": "Organization",
        "name": "ML Theory Platform"
      }
    }
    </script>
    
    <!-- Critical CSS -->
    <style>
      body { 
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
        line-height: 1.6; 
        margin: 0; 
        padding: 20px;
        background: #fafafa;
      }
      .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: white;
        padding: 40px;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      }
      h1 { 
        color: #1a1a1a; 
        margin-bottom: 20px; 
        font-size: 2.5rem;
        line-height: 1.2;
      }
      .meta { 
        color: #666; 
        margin-bottom: 30px; 
        padding-bottom: 20px;
        border-bottom: 1px solid #eee;
      }
      .content h2, .content h3 { 
        color: #2c3e50; 
        margin-top: 40px; 
        margin-bottom: 16px;
      }
      .content p { 
        margin-bottom: 16px; 
        color: #333;
      }
      .content code { 
        background: #f8f9fa; 
        padding: 2px 6px; 
        border-radius: 4px; 
        font-size: 0.9em;
        color: #e83e8c;
      }
      .content pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        overflow-x: auto;
        border: 1px solid #e9ecef;
      }
      .react-redirect {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #007acc;
        color: white;
        padding: 10px 20px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 8px rgba(0,122,204,0.3);
        transition: transform 0.2s;
      }
      .react-redirect:hover {
        transform: translateY(-1px);
      }
      @media (max-width: 768px) { 
        body { padding: 10px; }
        .container { padding: 20px; }
        h1 { font-size: 2rem; }
        .react-redirect { position: static; display: block; text-align: center; margin-bottom: 20px; }
      }
    </style>
</head>
<body>
    <!-- Link per versione interattiva -->
    <a href="http://localhost:3000/theory/introduction/Tipologie di Problemi/Classificazione/Classification Metrics" class="react-redirect">üöÄ View Interactive Version</a>
    
    <div class="container">
        <article>
            <header>
                <h1>Metriche di Valutazione per Classificazione in Machine Learning</h1>
                <div class="meta">
                    <strong>Topic:</strong> Introduction to Machine Learning | 
                    <strong>Updated:</strong> 16/01/2026
                </div>
            </header>
            
            <div class="content">
                <style>pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #F00 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666 } /* Operator */
.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #9C6500 } /* Comment.Preproc */
.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.codehilite .gr { color: #E40000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #008400 } /* Generic.Inserted */
.codehilite .go { color: #717171 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #04D } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #687822 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */
.codehilite .no { color: #800 } /* Name.Constant */
.codehilite .nd { color: #A2F } /* Name.Decorator */
.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #00F } /* Name.Function */
.codehilite .nl { color: #767600 } /* Name.Label */
.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #BBB } /* Text.Whitespace */
.codehilite .mb { color: #666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666 } /* Literal.Number.Float */
.codehilite .mh { color: #666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #00F } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */

/* Styling per blocchi di codice */
.codehilite {
    background: transparent !important;
    border-radius: 8px;
    overflow: hidden;
}
.codehilite pre {
    background: transparent !important;
    margin: 0 !important;
    padding: 20px !important;
    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;
    font-size: 14px !important;
    line-height: 1.5 !important;
    white-space: pre !important;
    overflow-x: auto !important;
    color: inherit !important;
}
.codehilite code {
    background: transparent !important;
    padding: 0 !important;
    font-family: inherit !important;
}


.code-wrapper { 
    position: relative; 
}
.copy-button {
    position: absolute; 
    top: 12px; 
    right: 12px; 
    padding: 6px 12px; 
    font-size: 12px;
    cursor: pointer; 
    border: none; 
    border-radius: 4px; 
    background: rgba(255,255,255,0.9);
    color: #374151; 
    transition: all 0.2s ease;
    font-weight: 500;
}
.copy-button:hover { 
    background: rgba(255,255,255,1);
    transform: translateY(-1px);
}


details.code-container {
    border: 1px solid #e5e7eb; 
    border-radius: 12px; 
    background: #f9fafb;
    margin: 16px 0;
    transition: all 0.3s ease;
}
details.code-container summary {
    padding: 12px 16px;
    font-size: 14px; 
    color: #6b7280; 
    cursor: pointer; 
    outline: none; 
    user-select: none;
    font-weight: 500;
}
details.code-container[open] summary::after { 
    content: " (Hide Code)"; 
    color: #9ca3af; 
}
details.code-container:not([open]) summary::after { 
    content: " (Show Code)"; 
    color: #d1d5db; 
}
details.code-container .code-wrapper {
    padding: 0;
    margin: 0;
}
/* Blocchi di codice sempre visibili */
.code-visible {
    border: 1px solid #e5e7eb;
    border-radius: 12px;
    background: #f9fafb;
    margin: 16px 0;
}
.code-visible .code-wrapper {
    padding: 0;
    margin: 0;
}
</style>
<h2 id="indice">Indice</h2>
<ol>
<li><a href="#1-fondamenti-teorici">Fondamenti Teorici</a></li>
<li><a href="#2-matrice-di-confusione">Matrice di Confusione</a></li>
<li><a href="#3-teoria-delle-decisioni-e-loss-functions">Teoria delle Decisioni e Loss Functions</a></li>
<li><a href="#4-metriche-fondamentali">Metriche Fondamentali</a></li>
<li><a href="#5-curve-roc-e-analisi-delle-performance">Curve ROC e Analisi delle Performance</a></li>
<li><a href="#6-curve-precision-recall">Curve Precision-Recall</a></li>
<li><a href="#7-metriche-avanzate-e-robuste">Metriche Avanzate e Robuste</a></li>
<li><a href="#8-valutazione-probabilistica-e-calibrazione">Valutazione Probabilistica e Calibrazione</a></li>
<li><a href="#9-classificazione-multi-classe">Classificazione Multi-Classe</a></li>
<li><a href="#10-guida-pratica-alla-scelta-delle-metriche">Guida Pratica alla Scelta delle Metriche</a></li>
</ol>
<h2 id="1-fondamenti-teorici">1. Fondamenti Teorici</h2>
<h3 id="11-introduzione">1.1 Introduzione</h3>
<p>La valutazione di modelli di classificazione √® un problema fondamentale nel machine learning. Non esiste una singola metrica universale: la scelta dipende dal problema specifico, dalla distribuzione dei dati, e dai costi associati ai diversi tipi di errore.</p>
<p>Questo documento presenta una trattazione rigorosa e completa delle principali metriche di valutazione, partendo dai fondamenti della teoria delle decisioni bayesiane fino alle applicazioni pratiche.</p>
<h3 id="12-il-framework-della-teoria-delle-decisioni-bayesiane">1.2 Il Framework della Teoria delle Decisioni Bayesiane</h3>
<p>Nel contesto della teoria delle decisioni, un problema di classificazione pu√≤ essere formalizzato come un <strong>gioco contro la natura</strong>:</p>
<ol>
<li><strong>La natura</strong> sceglie uno stato (label) $y \in \mathcal{Y}$, sconosciuto a noi</li>
<li><strong>La natura</strong> genera un&rsquo;osservazione $x \in \mathcal{X}$, che possiamo osservare</li>
<li><strong>Noi</strong> scegliamo un&rsquo;azione $a$ da uno spazio di azioni $\mathcal{A}$</li>
<li><strong>Incorriamo</strong> in una perdita $L(y, a)$ che misura la discrepanza tra stato reale e azione scelta</li>
</ol>
<h4 id="objective-decision-rule-ottimale">Objective: Decision Rule Ottimale</h4>
<p>L&rsquo;obiettivo √® trovare una <strong>decision rule</strong> (o <strong>policy</strong>) $\delta: \mathcal{X} \rightarrow \mathcal{A}$ che minimizzi la perdita attesa:</p>
$$\delta^*(x) = \arg\min_{a \in \mathcal{A}} \mathbb{E}_{p(y|x)}[L(y, a)]$$
<p>Nell&rsquo;approccio <strong>bayesiano</strong>, dopo aver osservato $x$, l&rsquo;azione ottimale √® quella che minimizza la <strong>perdita attesa a posteriori</strong> (posterior expected loss):</p>
$$\rho(a|x) = \mathbb{E}_{p(y|x)}[L(y, a)] = \sum_{y \in \mathcal{Y}} L(y, a) \cdot p(y|x)$$
<p>Quindi, il <strong>Bayes estimator</strong> (o <strong>Bayes decision rule</strong>) √®:</p>
$$\delta^*(x) = \arg\min_{a \in \mathcal{A}} \rho(a|x) = \arg\min_{a \in \mathcal{A}} \sum_{y \in \mathcal{Y}} L(y, a) \cdot p(y|x)$$
<p><strong>Interpretazione intuitive</strong>: Il Bayes estimator ci dice: &ldquo;Data l&rsquo;osservazione $x$, scegli l&rsquo;azione che minimizza la perdita media che ti aspetti di subire, pesando ogni possibile stato reale $y$ per la sua probabilit√† a posteriori $p(y|x)$.&rdquo;</p>
<h4 id="principio-di-utilita-attesa-massima">Principio di Utilit√† Attesa Massima</h4>
<p>In economia, √® pi√π comune parlare di <strong>funzione di utilit√†</strong> $U(y, a) = -L(y, a)$. Il problema diventa:</p>
$$\delta^*(x) = \arg\max_{a \in \mathcal{A}} \mathbb{E}_{p(y|x)}[U(y, a)]$$
<p>Questo √® il <strong>principio di utilit√† attesa massima</strong>, che costituisce la base del comportamento razionale in condizioni di incertezza.</p>
<h3 id="13-rischio-e-generalizzazione">1.3 Rischio e Generalizzazione</h3>
<p>Il <strong>rischio</strong> (o rischio atteso) di una decision rule $\delta$ √® la perdita media sulla distribuzione dei dati:</p>
$$R(\delta) = \mathbb{E}_{(X,Y) \sim p(x,y)}[L(Y, \delta(X))] = \int_{\mathcal{X}} \int_{\mathcal{Y}} L(y, \delta(x)) \, p(x,y) \, dy \, dx$$
<p>Dobbiamo distinguere tre concetti fondamentali:</p>
<p><strong>Rischio Empirico</strong> (Training Risk):
$$\hat{R}_{\text{train}}(\delta) = \frac{1}{n} \sum_{i=1}^{n} L(y_i, \delta(x_i))$$</p>
<p>√à la perdita media calcolata sul training set. Tende a <strong>sottostimare</strong> il vero rischio (overfitting).</p>
<p><strong>Rischio di Generalizzazione</strong> (True Risk):
$$R_{\text{true}}(\delta) = \mathbb{E}_{(X,Y) \sim p_{\text{true}}(x,y)}[L(Y, \delta(X))]$$</p>
<p>√à il vero rischio sulla distribuzione sottostante (sconosciuta). √à quello che vogliamo davvero minimizzare.</p>
<p><strong>Rischio Empirico su Test Set</strong>:
$$\hat{R}_{\text{test}}(\delta) = \frac{1}{m} \sum_{j=1}^{m} L(y_j^{\text{test}}, \delta(x_j^{\text{test}}))$$</p>
<p>√à una stima non distorta di $R_{\text{true}}(\delta)$ se il test set √® indipendente dal training.</p>
<p><strong>Principio Fondamentale</strong>: Minimizziamo $\hat{R}_{\text{train}}(\delta)$ durante il training, ma valutiamo su $\hat{R}_{\text{test}}(\delta)$ per stimare $R_{\text{true}}(\delta)$.</p>
<h2 id="2-matrice-di-confusione">2. Matrice di Confusione</h2>
<h3 id="21-definizione-e-struttura">2.1 Definizione e Struttura</h3>
<p>La <strong>matrice di confusione</strong> (confusion matrix) √® la struttura fondamentale per calcolare tutte le metriche di classificazione binaria. Essa organizza le predizioni in base alla classe reale e alla classe predetta.</p>
<p>Per un problema di classificazione binaria, con:
- $y = 1$: classe <strong>positiva</strong>
- $y = 0$: classe <strong>negativa</strong></p>
<p>La matrice di confusione ha questa struttura:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Predetto Positivo</strong> ($\hat{y}=1$)</th>
<th><strong>Predetto Negativo</strong> ($\hat{y}=0$)</th>
<th><strong>Totale</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reale Positivo</strong> ($y=1$)</td>
<td><strong>TP</strong> (True Positive)</td>
<td><strong>FN</strong> (False Negative)</td>
<td>$P = TP + FN$</td>
</tr>
<tr>
<td><strong>Reale Negativo</strong> ($y=0$)</td>
<td><strong>FP</strong> (False Positive)</td>
<td><strong>TN</strong> (True Negative)</td>
<td>$N = TN + FP$</td>
</tr>
<tr>
<td><strong>Totale</strong></td>
<td>$P^* = TP + FP$</td>
<td>$N^* = TN + FN$</td>
<td>$n = P + N$</td>
</tr>
</tbody>
</table>
<h3 id="22-definizioni-rigorose">2.2 Definizioni Rigorose</h3>
<p>Dato un dataset di $n$ esempi $\{(x_i, y_i)\}_{i=1}^n$ e un classificatore che produce predizioni $\hat{y}_i$, definiamo:</p>
<p><strong>True Positive (TP)</strong>:
$$TP = |\{i : y_i = 1 \land \hat{y}_i = 1\}|$$
Numero di istanze positive correttamente classificate come positive.</p>
<p><strong>True Negative (TN)</strong>:
$$TN = |\{i : y_i = 0 \land \hat{y}_i = 0\}|$$
Numero di istanze negative correttamente classificate come negative.</p>
<p><strong>False Positive (FP)</strong>:
$$FP = |\{i : y_i = 0 \land \hat{y}_i = 1\}|$$
Numero di istanze negative erroneamente classificate come positive (<strong>Errore di Tipo I</strong>).</p>
<p><strong>False Negative (FN)</strong>:
$$FN = |\{i : y_i = 1 \land \hat{y}_i = 0\}|$$
Numero di istanze positive erroneamente classificate come negative (<strong>Errore di Tipo II</strong>).</p>
<p><strong>Mnemonico</strong>: La prima lettera (T/F) indica se la predizione √® corretta (True) o errata (False). La seconda lettera (P/N) indica cosa ha predetto il classificatore.</p>
<h3 id="23-relazioni-fondamentali">2.3 Relazioni Fondamentali</h3>
<p>Dalla matrice di confusione derivano identit√† fondamentali:</p>
<p><strong>Totale esempi</strong>:
$$n = TP + TN + FP + FN$$</p>
<p><strong>Esempi positivi reali</strong>:
$$P = TP + FN$$</p>
<p><strong>Esempi negativi reali</strong>:
$$N = TN + FP$$</p>
<p><strong>Esempi predetti come positivi</strong>:
$$P^* = TP + FP$$</p>
<p><strong>Esempi predetti come negativi</strong>:
$$N^* = TN + FN$$</p>
<p><strong>Prevalenza</strong> (proporzione di positivi):
$$\pi = \frac{P}{n} = \frac{TP + FN}{n}$$</p>
<h3 id="24-interpretazione-probabilistica">2.4 Interpretazione Probabilistica</h3>
<p>Possiamo interpretare i conteggi della matrice di confusione in termini probabilistici. Definiamo:</p>
<p><strong>Ipotesi</strong>:
- $H_0$: L&rsquo;istanza appartiene alla classe negativa ($y=0$)
- $H_1$: L&rsquo;istanza appartiene alla classe positiva ($y=1$)</p>
<p><strong>Decisioni</strong>:
- $D_0$: Classificare come negativo ($\hat{y}=0$)
- $D_1$: Classificare come positivo ($\hat{y}=1$)</p>
<p>Allora possiamo scrivere le probabilit√† condizionate:</p>
<p><strong>True Positive Rate (TPR)</strong>:
$$\text{TPR} = P(D_1 | H_1) = P(\hat{y}=1 | y=1) = \frac{TP}{P}$$
Probabilit√† di classificare correttamente un positivo.</p>
<p><strong>False Positive Rate (FPR)</strong>:
$$\text{FPR} = P(D_1 | H_0) = P(\hat{y}=1 | y=0) = \frac{FP}{N}$$
Probabilit√† di classificare erroneamente un negativo come positivo (Errore di Tipo I).</p>
<p><strong>True Negative Rate (TNR)</strong>:
$$\text{TNR} = P(D_0 | H_0) = P(\hat{y}=0 | y=0) = \frac{TN}{N}$$
Probabilit√† di classificare correttamente un negativo.</p>
<p><strong>False Negative Rate (FNR)</strong>:
$$\text{FNR} = P(D_0 | H_1) = P(\hat{y}=0 | y=1) = \frac{FN}{P}$$
Probabilit√† di classificare erroneamente un positivo come negativo (Errore di Tipo II).</p>
<p><strong>Relazioni complementari</strong>:
$$\text{TPR} + \text{FNR} = 1$$
$$\text{TNR} + \text{FPR} = 1$$</p>
<h3 id="25-esempio-rilevamento-di-malattie-della-tiroide">2.5 Esempio: Rilevamento di Malattie della Tiroide</h3>
<p>Consideriamo il problema di rilevare malattie della tiroide usando un dataset con 3428 pazienti nel test set, di cui 250 hanno una malattia tiroidea.</p>
<p><strong>Prima configurazione</strong> (soglia di default $\tau = 0.5$):</p>
<table>
<thead>
<tr>
<th></th>
<th>Predetto Normale</th>
<th>Predetto Malato</th>
<th>Totale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Realmente Normale</td>
<td>3177</td>
<td>1</td>
<td>3178</td>
</tr>
<tr>
<td>Realmente Malato</td>
<td>237</td>
<td>13</td>
<td>250</td>
</tr>
<tr>
<td>Totale</td>
<td>3414</td>
<td>14</td>
<td>3428</td>
</tr>
</tbody>
</table>
<p>Analisi:
- <strong>Accuracy</strong>: $(3177 + 13) / 3428 = 93.1\%$ (sembra buona!)
- <strong>Recall</strong>: $13 / 250 = 5.2\%$ (pessimo! Perdiamo il 95% dei malati)
- <strong>Precision</strong>: $13 / 14 = 92.9\%$ (alta, ma poche predizioni positive)</p>
<p>Questo classificatore √® <strong>praticamente inutile</strong>: un modello &ldquo;dummy&rdquo; che predice sempre &ldquo;normale&rdquo; otterrebbe accuracy del $92.7\%$, quasi identica!</p>
<p><strong>Seconda configurazione</strong> (soglia abbassata a $\tau = 0.15$):</p>
<table>
<thead>
<tr>
<th></th>
<th>Predetto Normale</th>
<th>Predetto Malato</th>
<th>Totale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Realmente Normale</td>
<td>3067</td>
<td>111</td>
<td>3178</td>
</tr>
<tr>
<td>Realmente Malato</td>
<td>165</td>
<td>85</td>
<td>250</td>
</tr>
<tr>
<td>Totale</td>
<td>3232</td>
<td>196</td>
<td>3428</td>
</tr>
</tbody>
</table>
<p>Analisi:
- <strong>Accuracy</strong>: $(3067 + 85) / 3428 = 91.9\%$ (leggermente diminuita)
- <strong>Recall</strong>: $85 / 250 = 34\%$ (migliorato significativamente!)
- <strong>Precision</strong>: $85 / 196 = 43.4\%$ (diminuita, ma accettabile)</p>
<p><strong>Conclusione</strong>: Il secondo modello √® probabilmente pi√π utile in pratica, nonostante l&rsquo;accuracy leggermente inferiore. Questo esempio illustra perch√© l&rsquo;accuracy da sola √® insufficiente per problemi sbilanciati.</p>
<h2 id="3-teoria-delle-decisioni-e-loss-functions">3. Teoria delle Decisioni e Loss Functions</h2>
<h3 id="31-loss-functions-e-bayes-estimators">3.1 Loss Functions e Bayes Estimators</h3>
<p>La scelta della <strong>loss function</strong> $L(y, a)$ determina quale azione √® ottimale. Diverse loss functions portano a diversi estimatori ottimali.</p>
<h4 id="311-0-1-loss-e-stima-map">3.1.1 0-1 Loss e Stima MAP</h4>
<p>La <strong>0-1 loss</strong> √® la pi√π semplice e naturale:</p>
$$L_{0-1}(y, a) = \mathbb{I}(y \neq a) = \begin{cases} 0 & \text{se } a = y \\ 1 & \text{se } a \neq y \end{cases}$$
<p><strong>Interpretazione</strong>: Penalizziamo ugualmente tutti gli errori, senza distinzione di tipo.</p>
<p><strong>Teorema 3.1</strong> (Bayes Estimator per 0-1 Loss):
<em>La 0-1 loss √® minimizzata dalla stima MAP (Maximum A Posteriori).</em></p>
<p><strong>Dimostrazione</strong>:</p>
<p>La perdita attesa a posteriori per l&rsquo;azione $a$ √®:</p>
$$\rho(a|x) = \sum_{y \in \mathcal{Y}} L_{0-1}(y,a) \cdot p(y|x) = \sum_{y \neq a} p(y|x) = 1 - p(a|x)$$
<p>Per minimizzare $\rho(a|x)$, dobbiamo massimizzare $p(a|x)$:</p>
$$\delta^*(x) = \arg\min_a \rho(a|x) = \arg\max_a p(a|x) = \arg\max_{y \in \mathcal{Y}} p(y|x)$$
<p>che √® esattamente la <strong>stima MAP</strong>. $\square$</p>
<p><strong>Corollario</strong>: Per classificazione binaria con 0-1 loss, la regola ottimale √®:</p>
$$\hat{y} = \begin{cases} 1 & \text{se } p(y=1|x) > 0.5 \\ 0 & \text{altrimenti} \end{cases}$$
<h4 id="312-loss-asimmetrica-e-costi-differenziati">3.1.2 Loss Asimmetrica e Costi Differenziati</h4>
<p>Nella pratica, i diversi tipi di errore hanno spesso costi diversi. Rappresentiamo questo con una <strong>matrice di loss</strong>:</p>
<table>
<thead>
<tr>
<th></th>
<th>$\hat{y}=1$</th>
<th>$\hat{y}=0$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$y=1$</td>
<td>$0$</td>
<td>$L_{FN}$</td>
</tr>
<tr>
<td>$y=0$</td>
<td>$L_{FP}$</td>
<td>$0$</td>
</tr>
</tbody>
</table>
<p>dove:
- $L_{FN}$: costo di un False Negative (mancata rilevazione)
- $L_{FP}$: costo di un False Positive (falso allarme)</p>
<p><strong>Teorema 3.2</strong> (Regola di Decisione Ottimale con Costi Asimmetrici):
<em>Sotto la matrice di loss asimmetrica, dovremmo classificare come positivo se e solo se:</em></p>
$$\frac{p(y=1|x)}{p(y=0|x)} > \frac{L_{FP}}{L_{FN}}$$
<p><strong>Dimostrazione</strong>:</p>
<p>Le perdite attese per le due azioni sono:</p>
$$\rho(\hat{y}=0|x) = L_{FN} \cdot p(y=1|x) + 0 \cdot p(y=0|x) = L_{FN} \cdot p(y=1|x)$$
$$\rho(\hat{y}=1|x) = 0 \cdot p(y=1|x) + L_{FP} \cdot p(y=0|x) = L_{FP} \cdot p(y=0|x)$$
<p>Scegliamo $\hat{y}=1$ quando $\rho(\hat{y}=1|x) < \rho(\hat{y}=0|x)$:</p>
$$L_{FP} \cdot p(y=0|x) < L_{FN} \cdot p(y=1|x)$$
<p>Dividendo entrambi i lati per $p(y=0|x)$ e $L_{FN}$:</p>
$$\frac{L_{FP}}{L_{FN}} < \frac{p(y=1|x)}{p(y=0|x)}$$
<p>$\square$</p>
<p><strong>Corollario (Soglia Ottimale)</strong>:
Se $L_{FN} = c \cdot L_{FP}$ con $c > 0$, la regola diventa: classificare come positivo se $p(y=1|x) > \tau^*$ dove:</p>
$$\tau^* = \frac{1}{1 + c} = \frac{L_{FP}}{L_{FP} + L_{FN}}$$
<p><strong>Esempi numerici</strong>:</p>
<ol>
<li>
<p><strong>Screening medico</strong>: $L_{FN} = 100$, $L_{FP} = 1$ (la mancata diagnosi √® 100 volte pi√π grave)
   $$\tau^* = \frac{1}{101} \approx 0.01$$
   Soglia molto bassa ‚Üí massimizziamo il recall.</p>
</li>
<li>
<p><strong>Anti-spam</strong>: $L_{FN} = 1$, $L_{FP} = 10$ (eliminare email legittima √® 10 volte peggio)
   $$\tau^* = \frac{10}{11} \approx 0.91$$
   Soglia alta ‚Üí massimizziamo la precision.</p>
</li>
</ol>
<h4 id="313-reject-option">3.1.3 Reject Option</h4>
<p>In applicazioni ad alto rischio (medicina, finanza), pu√≤ essere preferibile <strong>rifiutare</strong> di classificare esempi incerti piuttosto che rischiare errori gravi.</p>
<p>Formalizziamo l&rsquo;azione di rifiuto come $a = \text{reject}$ con costo $\lambda_r$, mentre gli errori di classificazione hanno costo $\lambda_s$ (substitution error).</p>
<p><strong>Teorema 3.3</strong> (Regola con Reject Option):
<em>L&rsquo;azione ottimale √®:</em></p>
$$\delta^*(x) = \begin{cases}
\arg\max_c p(y=c|x) & \text{se } \max_c p(y=c|x) \geq 1 - \frac{\lambda_r}{\lambda_s} \\
\text{reject} & \text{altrimenti}
\end{cases}$$
<p><strong>Dimostrazione</strong> (sketch):</p>
<p>Il costo atteso per classificare nella classe $c$ √®:</p>
$$\rho(\hat{y}=c|x) = \lambda_s \cdot P(\text{errore}|x) = \lambda_s \cdot (1 - p(y=c|x))$$
<p>Il costo per rifiutare √® costante: $\rho(\text{reject}|x) = \lambda_r$.</p>
<p>Conviene classificare se:</p>
$$\lambda_s \cdot (1 - p(y=c|x)) < \lambda_r$$
$$p(y=c|x) > 1 - \frac{\lambda_r}{\lambda_s}$$
<p>Scegliamo la classe con probabilit√† massima solo se supera questa soglia. $\square$</p>
<p><strong>Esempio</strong>: Se $\lambda_s = 10$ (errore costa 10) e $\lambda_r = 2$ (rifiuto costa 2):
$$\text{Soglia} = 1 - \frac{2}{10} = 0.8$$</p>
<p>Rifiutiamo di classificare se $\max_c p(y=c|x) < 0.8$.</p>
<h4 id="314-quadratic-loss-e-posterior-mean">3.1.4 Quadratic Loss e Posterior Mean</h4>
<p>Per problemi di regressione o quando lavoriamo con probabilit√†, la <strong>quadratic loss</strong> (o squared error) √® naturale:</p>
$$L_2(y, a) = (y - a)^2$$
<p><strong>Teorema 3.4</strong> (Bayes Estimator per Quadratic Loss):
<em>La $\ell_2$ loss √® minimizzata dalla media a posteriori.</em></p>
<p><strong>Dimostrazione</strong>:</p>
<p>La perdita attesa a posteriori √®:</p>
$$\rho(a|x) = \mathbb{E}[(y-a)^2|x] = \mathbb{E}[y^2|x] - 2a\mathbb{E}[y|x] + a^2$$
<p>Deriviamo rispetto ad $a$ e poniamo uguale a zero:</p>
$$\frac{\partial \rho(a|x)}{\partial a} = -2\mathbb{E}[y|x] + 2a = 0$$
$$\Rightarrow a^* = \mathbb{E}[y|x] = \int y \, p(y|x) \, dy$$
<p>Questa √® la <strong>stima MMSE (Minimum Mean Squared Error)</strong>. $\square$</p>
<p><strong>Applicazione in regressione</strong>: Per un modello lineare $p(y|x,w) = \mathcal{N}(y|w^Tx, \sigma^2)$, l&rsquo;estimatore MMSE √®:</p>
$$\hat{y}(x) = \mathbb{E}[y|x, \mathcal{D}] = x^T \mathbb{E}[w|\mathcal{D}]$$
<p>Basta usare la media a posteriori dei parametri.</p>
<h4 id="315-absolute-loss-e-posterior-median">3.1.5 Absolute Loss e Posterior Median</h4>
<p>La <strong>absolute loss</strong> (o $\ell_1$ loss) √® pi√π robusta agli outlier:</p>
$$L_1(y, a) = |y - a|$$
<p><strong>Teorema 3.5</strong> (Bayes Estimator per Absolute Loss):
<em>La $\ell_1$ loss √® minimizzata dalla mediana a posteriori.</em></p>
<p><strong>Dimostrazione</strong>:</p>
<p>La perdita attesa √®:</p>
$$\rho(a|x) = \int |y-a| p(y|x) dy = \int_{-\infty}^{a} (a-y) p(y|x) dy + \int_{a}^{\infty} (y-a) p(y|x) dy$$
<p>Deriviamo rispetto ad $a$. Usando la regola di Leibniz:</p>
$$\frac{\partial \rho(a|x)}{\partial a} = \int_{-\infty}^{a} p(y|x) dy - \int_{a}^{\infty} p(y|x) dy$$
$$= P(y \leq a|x) - P(y > a|x)$$
<p>Ponendo uguale a zero:</p>
$$P(y \leq a|x) = P(y > a|x) = \frac{1}{2}$$
<p>che √® la definizione di <strong>mediana</strong>. $\square$</p>
<p><strong>Perch√© la $\ell_1$ √® pi√π robusta?</strong> La $\ell_2$ loss penalizza quadraticamente le deviazioni, quindi un singolo outlier molto distante pu√≤ dominare la loss. La $\ell_1$ penalizza linearmente, riducendo l&rsquo;influenza degli outlier.</p>
<h2 id="4-metriche-fondamentali">4. Metriche Fondamentali</h2>
<h3 id="41-accuracy-accuratezza">4.1 Accuracy (Accuratezza)</h3>
<p>L&rsquo;<strong>accuracy</strong> √® la metrica pi√π semplice e intuitiva: misura la proporzione di predizioni corrette.</p>
<p><strong>Definizione</strong>:
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{n}$$</p>
<p><strong>Interpretazione probabilistica</strong>:
$$\text{Accuracy} = P(\hat{y} = y)$$
√à la probabilit√† che una predizione casuale sia corretta.</p>
<p><strong>Propriet√†</strong>:
- <strong>Range</strong>: $[0, 1]$, dove $1$ indica predizioni perfette
- <strong>Simmetrica</strong> rispetto alle classi
- <strong>Uguale peso</strong> a errori positivi e negativi</p>
<p><strong>Limitazione critica: Dataset Sbilanciati</strong></p>
<p>Consideriamo un problema di fraud detection dove solo l&lsquo;1% delle transazioni √® fraudolenta ($\pi = 0.01$).</p>
<p>Un classificatore &ldquo;dummy&rdquo; che <strong>predice sempre negativo</strong> ottiene:
$$\text{Accuracy}_{\text{dummy}} = \frac{0 + 0.99n}{n} = 0.99$$</p>
<p>Questo sembra eccellente, ma il modello √® completamente inutile! Non rileva nessuna frode.</p>
<p><strong>Teorema 4.1</strong> (Lower Bound su Accuracy per Classificatore Dummy):
<em>Un classificatore che predice sempre la classe maggioritaria ottiene accuracy pari alla prevalenza della classe maggioritaria:</em></p>
$$\text{Accuracy}_{\text{majority}} = \max(\pi, 1-\pi)$$
<p><strong>Conclusione</strong>: L&rsquo;accuracy √® <strong>inadeguata per dataset sbilanciati</strong>. Dobbiamo usare metriche che distinguano tra i diversi tipi di errore.</p>
<h3 id="42-precision-precisione">4.2 Precision (Precisione)</h3>
<p>La <strong>precision</strong> misura l&rsquo;affidabilit√† delle predizioni positive.</p>
<p><strong>Definizione</strong>:
$$\text{Precision} = \frac{TP}{TP + FP} = \frac{TP}{P^*}$$</p>
<p><strong>Interpretazione probabilistica</strong>:
$$\text{Precision} = P(y=1|\hat{y}=1)$$
&ldquo;Tra tutti i casi che ho predetto come positivi, qual √® la probabilit√† che siano realmente positivi?&rdquo;</p>
<p><strong>Interpretazione intuitiva</strong>: &ldquo;Quando il modello dice &lsquo;positivo&rsquo;, quanto possiamo fidarci?&rdquo;</p>
<p><strong>Quando √® critica</strong>: Scenari dove i <strong>falsi positivi sono costosi</strong>:</p>
<ol>
<li><strong>Spam detection</strong>: Classificare email legittime come spam pu√≤ far perdere comunicazioni importanti</li>
<li><strong>Diagnosi mediche aggressive</strong>: Prescrivere chemioterapia a pazienti sani</li>
<li><strong>Raccomandazioni</strong>: Raccomandare prodotti irrilevanti irrita l&rsquo;utente</li>
<li><strong>Allerte di sicurezza</strong>: Troppi falsi allarmi causano &ldquo;alarm fatigue&rdquo;</li>
</ol>
<p><strong>Complemento - False Discovery Rate (FDR)</strong>:
$$\text{FDR} = 1 - \text{Precision} = \frac{FP}{TP+FP}$$
Proporzione di &ldquo;scoperte&rdquo; che sono in realt√† false.</p>
<p><strong>Dipendenza dalla Prevalenza</strong></p>
<p>La precision dipende fortemente dalla prevalenza $\pi = P(y=1)$. Usando il teorema di Bayes:</p>
$$\text{Precision} = P(y=1|\hat{y}=1) = \frac{P(\hat{y}=1|y=1) \cdot P(y=1)}{P(\hat{y}=1)}$$
$$= \frac{\text{TPR} \cdot \pi}{\text{TPR} \cdot \pi + \text{FPR} \cdot (1-\pi)}$$
<p><strong>Esempio</strong>: Con TPR = 0.9, FPR = 0.1:
- Se $\pi = 0.5$: Precision = $\frac{0.9 \cdot 0.5}{0.9 \cdot 0.5 + 0.1 \cdot 0.5} = 0.9$
- Se $\pi = 0.01$: Precision = $\frac{0.9 \cdot 0.01}{0.9 \cdot 0.01 + 0.1 \cdot 0.99} \approx 0.08$</p>
<p>Con prevalenza bassa, anche un FPR modesto degrada drasticamente la precision!</p>
<h3 id="43-recall-sensibilita-true-positive-rate">4.3 Recall (Sensibilit√†, True Positive Rate)</h3>
<p>Il <strong>recall</strong> misura la capacit√† di identificare i positivi.</p>
<p><strong>Definizione</strong>:
$$\text{Recall} = \text{TPR} = \text{Sensitivity} = \frac{TP}{TP + FN} = \frac{TP}{P}$$</p>
<p><strong>Interpretazione probabilistica</strong>:
$$\text{Recall} = P(\hat{y}=1|y=1)$$
&ldquo;Tra tutti i casi realmente positivi, quale proporzione riesco a identificare?&rdquo;</p>
<p><strong>Interpretazione intuitiva</strong>: &ldquo;Quanto √® completa la mia rilevazione dei positivi?&rdquo;</p>
<p><strong>Quando √® critico</strong>: Scenari dove i <strong>falsi negativi sono costosi</strong>:</p>
<ol>
<li><strong>Screening medico</strong>: Non diagnosticare un tumore √® potenzialmente fatale</li>
<li><strong>Rilevamento frodi</strong>: Non bloccare una transazione fraudolenta causa perdite economiche</li>
<li><strong>Sistemi di sicurezza</strong>: Non rilevare un&rsquo;intrusione compromette la sicurezza</li>
<li><strong>Information retrieval</strong>: Non trovare documenti rilevanti limita l&rsquo;utilit√† del sistema</li>
</ol>
<p><strong>Complemento - False Negative Rate (FNR)</strong>:
$$\text{FNR} = \text{Miss Rate} = 1 - \text{Recall} = \frac{FN}{TP+FN}$$
Proporzione di positivi che &ldquo;perdiamo&rdquo; (manchiamo di rilevare).</p>
<p><strong>Indipendenza dalla Prevalenza</strong></p>
<p>A differenza della precision, il recall <strong>non dipende dalla prevalenza</strong> perch√© √® condizionato sulla classe reale:</p>
<p>$\text{Recall} = P(\hat{y}=1|y=1)$</p>
<p>Questa √® una probabilit√† condizionata su $y=1$, che dipende solo da $p(x|y=1)$ e dalla soglia di decisione, non da $P(y=1)$.</p>
<h3 id="44-specificity-true-negative-rate">4.4 Specificity (True Negative Rate)</h3>
<p>La <strong>specificity</strong> √® il &ldquo;recall per la classe negativa&rdquo;.</p>
<p><strong>Definizione</strong>:
$\text{Specificity} = \text{TNR} = \frac{TN}{TN + FP} = \frac{TN}{N}$</p>
<p><strong>Interpretazione probabilistica</strong>:
$\text{Specificity} = P(\hat{y}=0|y=0)$
&ldquo;Tra tutti i casi realmente negativi, quale proporzione riesco a identificare correttamente?&rdquo;</p>
<p><strong>Relazione con FPR</strong>:
$\text{FPR} = 1 - \text{Specificity} = \frac{FP}{FP + TN} = P(\hat{y}=1|y=0)$</p>
<p>Il FPR √® la probabilit√† di <strong>falso allarme</strong> (Errore di Tipo I nel testing d&rsquo;ipotesi).</p>
<p><strong>Importanza in medicina</strong>: In test diagnostici, specificity alta significa pochi falsi positivi, riducendo ansia ingiustificata e procedure invasive non necessarie.</p>
<h3 id="45-trade-off-precision-vs-recall">4.5 Trade-off Precision vs Recall</h3>
<p>Precision e recall sono tipicamente in <strong>trade-off</strong>: migliorare una tende a peggiorare l&rsquo;altra.</p>
<p><strong>Intuizione del trade-off</strong>:</p>
<p>Consideriamo un classificatore probabilistico che produce $p(y=1|x)$ e una soglia $\tau$:</p>
<p>$\hat{y} = \begin{cases} 1 & \text{se } p(y=1|x) > \tau \\ 0 & \text{altrimenti} \end{cases}$</p>
<p><strong>Abbassando la soglia</strong> $\tau$ (classifichiamo pi√π casi come positivi):
- ‚úÖ <strong>Recall aumenta</strong>: Catturiamo pi√π veri positivi
- ‚ùå <strong>Precision diminuisce</strong>: Includiamo anche pi√π falsi positivi</p>
<p><strong>Alzando la soglia</strong> $\tau$ (siamo pi√π selettivi):
- ‚úÖ <strong>Precision aumenta</strong>: Solo predizioni molto confidenti
- ‚ùå <strong>Recall diminuisce</strong>: Perdiamo alcuni veri positivi &ldquo;border-line&rdquo;</p>
<p><strong>Analisi formale</strong>:</p>
<p>Al variare di $\tau$:</p>
<p>$\tau \to 0: \quad \begin{cases} \text{Recall} \to 1 \\ \text{Precision} \to \pi \end{cases} \quad \text{(tutto positivo)}$</p>
<p>$\tau \to 1: \quad \begin{cases} \text{Recall} \to 0 \\ \text{Precision} \to 1 \end{cases} \quad \text{(tutto negativo)}$</p>
<p><strong>Esempio numerico</strong>:</p>
<p>Dataset: 100 positivi, 900 negativi. Modello produce score da 0 a 1.</p>
<table>
<thead>
<tr>
<th>Soglia $\tau$</th>
<th>TP</th>
<th>FP</th>
<th>FN</th>
<th>Precision</th>
<th>Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9</td>
<td>10</td>
<td>5</td>
<td>90</td>
<td>0.67</td>
<td>0.10</td>
</tr>
<tr>
<td>0.7</td>
<td>40</td>
<td>50</td>
<td>60</td>
<td>0.44</td>
<td>0.40</td>
</tr>
<tr>
<td>0.5</td>
<td>70</td>
<td>200</td>
<td>30</td>
<td>0.26</td>
<td>0.70</td>
</tr>
<tr>
<td>0.3</td>
<td>90</td>
<td>500</td>
<td>10</td>
<td>0.15</td>
<td>0.90</td>
</tr>
</tbody>
</table>
<p>Osserviamo chiaramente il trade-off: recall alta ‚Üí precision bassa, e viceversa.</p>
<h3 id="46-f-scores-armonizzare-precision-e-recall">4.6 F-Scores: Armonizzare Precision e Recall</h3>
<h4 id="461-f1-score-media-armonica">4.6.1 F1-Score: Media Armonica</h4>
<p>L&rsquo;<strong>F1-score</strong> combina precision e recall in una singola metrica bilanciata.</p>
<p><strong>Definizione</strong>:
$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}$</p>
<p><strong>Perch√© la media armonica?</strong></p>
<p>La media armonica di due numeri $a$ e $b$ √®:
$H(a,b) = \frac{2ab}{a+b}$</p>
<p>√à pi√π <strong>severa</strong> della media aritmetica quando i valori sono sbilanciati:
$H(a,b) \leq G(a,b) \leq A(a,b)$
dove $G$ √® la media geometrica e $A$ la media aritmetica.</p>
<p><strong>Esempio illustrativo</strong>:</p>
<table>
<thead>
<tr>
<th>Precision</th>
<th>Recall</th>
<th>Media Aritmetica</th>
<th>F1 (Media Armonica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9</td>
<td>0.9</td>
<td>0.90</td>
<td>0.90</td>
</tr>
<tr>
<td>0.9</td>
<td>0.5</td>
<td>0.70</td>
<td>0.64</td>
</tr>
<tr>
<td>0.9</td>
<td>0.1</td>
<td>0.50</td>
<td>0.18</td>
</tr>
<tr>
<td>0.5</td>
<td>0.5</td>
<td>0.50</td>
<td>0.50</td>
</tr>
</tbody>
</table>
<p>L&rsquo;F1 <strong>penalizza fortemente</strong> sistemi con una metrica molto bassa, anche se l&rsquo;altra √® alta.</p>
<p><strong>Propriet√† matematiche</strong>:</p>
<ol>
<li><strong>Range</strong>: $F_1 \in [0, 1]$</li>
<li><strong>Massimo</strong>: $F_1 = 1$ se e solo se $\text{Precision} = \text{Recall} = 1$</li>
<li><strong>Simmetria</strong>: $F_1(P, R) = F_1(R, P)$</li>
<li><strong>Monotonia</strong>: $F_1$ cresce se aumentiamo sia $P$ che $R$</li>
</ol>
<p><strong>Derivazione alternativa</strong>:</p>
<p>Possiamo scrivere:
$F_1 = \frac{1}{\frac{1}{2}\left(\frac{1}{P} + \frac{1}{R}\right)}$</p>
<p>L&rsquo;F1 √® la media armonica perch√© √® l&rsquo;inverso della media aritmetica degli inversi.</p>
<h4 id="462-f-beta-score-peso-asimmetrico">4.6.2 F-Beta Score: Peso Asimmetrico</h4>
<p>Il <strong>F-beta score</strong> generalizza l&rsquo;F1 permettendo di pesare diversamente recall e precision.</p>
<p><strong>Definizione</strong>:
$F_\beta = (1 + \beta^2) \cdot \frac{\text{Precision} \cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}$</p>
<p><strong>Interpretazione del parametro $\beta$</strong>:</p>
<ul>
<li>$\beta < 1$: <strong>Precision pesa di pi√π</strong> (enfasi su evitare falsi positivi)</li>
<li>$\beta = 1$: <strong>Peso uguale</strong> (F1-score standard)</li>
<li>$\beta > 1$: <strong>Recall pesa di pi√π</strong> (enfasi su catturare tutti i positivi)</li>
</ul>
<p><strong>Valori comuni</strong>:</p>
<p><strong>$F_{0.5}$</strong> (Precision vale il doppio):
$F_{0.5} = 1.25 \cdot \frac{P \cdot R}{0.25 \cdot P + R}$
Uso: Spam detection, dove falsi positivi sono molto costosi.</p>
<p><strong>$F_2$</strong> (Recall vale il doppio):
$F_2 = 5 \cdot \frac{P \cdot R}{4 \cdot P + R}$
Uso: Screening medico, dove falsi negativi sono molto costosi.</p>
<p><strong>Derivazione del peso di $\beta$</strong>:</p>
<p>Riscriviamo l&rsquo;F-beta in forma estesa:
$F_\beta = \frac{(1+\beta^2) \cdot TP}{(1+\beta^2) \cdot TP + \beta^2 \cdot FN + FP}$</p>
<p>Notiamo che:
- I falsi negativi (FN) sono pesati per $\beta^2$
- I falsi positivi (FP) sono pesati per $1$</p>
<p>Quindi $\beta^2$ √® il <strong>rapporto di importanza</strong> tra recall e precision:
$\beta^2 = \frac{\text{Importanza del Recall}}{\text{Importanza della Precision}}$</p>
<p><strong>Esempio numerico</strong>:</p>
<p>Consideriamo tre modelli con diverse caratteristiche:</p>
<table>
<thead>
<tr>
<th>Modello</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
<th>F0.5</th>
<th>F2</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0.90</td>
<td>0.50</td>
<td>0.64</td>
<td>0.75</td>
<td>0.56</td>
</tr>
<tr>
<td>B</td>
<td>0.50</td>
<td>0.90</td>
<td>0.64</td>
<td>0.56</td>
<td>0.75</td>
</tr>
<tr>
<td>C</td>
<td>0.70</td>
<td>0.70</td>
<td>0.70</td>
<td>0.70</td>
<td>0.70</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Modello A</strong>: Alta precision, basso recall ‚Üí F0.5 lo premia</li>
<li><strong>Modello B</strong>: Bassa precision, alto recall ‚Üí F2 lo premia  </li>
<li><strong>Modello C</strong>: Bilanciato ‚Üí performance costante su tutti gli F-score</li>
</ul>
<p><strong>Scelta di $\beta$ in base al dominio</strong>:</p>
<ol>
<li><strong>Medicina (screening)</strong>: $\beta = 2$ o superiore (priorit√† su recall)</li>
<li><strong>Spam filtering</strong>: $\beta = 0.5$ (priorit√† su precision)</li>
<li><strong>Information retrieval</strong>: $\beta = 1$ (bilanciamento)</li>
<li><strong>Fraud detection</strong>: $\beta = 1.5$ - $2$ (leggermente sbilanciato verso recall)</li>
</ol>
<h2 id="5-curve-roc-e-analisi-delle-performance">5. Curve ROC e Analisi delle Performance</h2>
<h3 id="51-introduzione-alle-curve-roc">5.1 Introduzione alle Curve ROC</h3>
<p>La <strong>curva ROC</strong> (Receiver Operating Characteristic) √® uno strumento fondamentale per valutare classificatori binari indipendentemente dalla scelta della soglia.</p>
<p><strong>Contesto storico</strong>: Le curve ROC furono sviluppate durante la Seconda Guerra Mondiale per analizzare segnali radar. Il nome &ldquo;Receiver Operating Characteristic&rdquo; deriva proprio dall&rsquo;analisi dei ricevitori radio.</p>
<h3 id="52-costruzione-della-curva-roc">5.2 Costruzione della Curva ROC</h3>
<p><strong>Definizione formale</strong>:</p>
<p>Data una famiglia di classificatori parametrizzati da una soglia $\tau \in [0,1]$:
$\hat{y}(\tau) = \mathbb{I}(p(y=1|x) > \tau)$</p>
<p>La curva ROC √® il grafico dei punti:
$\text{ROC}(\tau) = \big(\text{FPR}(\tau), \text{TPR}(\tau)\big)$</p>
<p>al variare di $\tau$ da 0 a 1.</p>
<p><strong>Coordinate</strong>:
- <strong>Asse X</strong>: False Positive Rate = $\frac{FP}{N}$
- <strong>Asse Y</strong>: True Positive Rate = $\frac{TP}{P}$</p>
<p><strong>Algoritmo di costruzione</strong>:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Input: Score s_i e label y_i per i = 1,...,n

1. Ordina gli esempi per score decrescente: s_1 ‚â• s_2 ‚â• ... ‚â• s_n
2. Inizializza: TP = 0, FP = 0
3. Per ogni soglia œÑ (prendendo s_i come soglie):
   a. Se y_i = 1: TP++
   b. Se y_i = 0: FP++
   c. Calcola: TPR = TP/P, FPR = FP/N
   d. Aggiungi punto (FPR, TPR) alla curva
4. Collega i punti per formare la curva
</code></pre></div>
</div>
</details>

<h3 id="53-interpretazione-e-punti-notevoli">5.3 Interpretazione e Punti Notevoli</h3>
<p><strong>Punti estremi</strong>:</p>
<p>$(0, 0)$: <strong>Origine</strong> - Soglia $\tau = 1$ ‚Üí tutto classificato come negativo
- TP = 0, FP = 0
- Classifier &ldquo;always negative&rdquo;</p>
<p>$(1, 1)$: <strong>Angolo in alto a destra</strong> - Soglia $\tau = 0$ ‚Üí tutto classificato come positivo
- TP = P, FP = N
- Classifier &ldquo;always positive&rdquo;</p>
<p>$(0, 1)$: <strong>Angolo in alto a sinistra</strong> - Classificatore perfetto
- TP = P, FP = 0
- Nessun errore</p>
<p><strong>Diagonale principale</strong> $y = x$: Classificatore casuale
- Prediction random con $P(\hat{y}=1) = p$
- In media: $\text{TPR} = p$, $\text{FPR} = p$</p>
<p><strong>Interpretazione geometrica</strong>:</p>
<ul>
<li><strong>Curve vicine all&rsquo;angolo (0,1)</strong>: Ottimo classificatore</li>
<li>Alto TPR con basso FPR</li>
<li>
<p>Buona separazione tra classi</p>
</li>
<li>
<p><strong>Curve vicine alla diagonale</strong>: Classificatore scarso</p>
</li>
<li>Nessun potere discriminante</li>
<li>
<p>Simile a indovinare casualmente</p>
</li>
<li>
<p><strong>Curve sotto la diagonale</strong>: Classificatore &ldquo;invertito&rdquo;</p>
</li>
<li>Performance peggiore del caso</li>
<li>Invertire le predizioni migliorerebbe il modello!</li>
</ul>
<p><strong>Propriet√† di monotonia</strong>:</p>
<p>La curva ROC √® <strong>monotona crescente</strong>: muovendoci lungo la curva da sinistra a destra (abbassando $\tau$), sia TPR che FPR aumentano (o restano costanti).</p>
<p><strong>Teorema 5.1</strong> (Monotonia della Curva ROC):
<em>Per un classificatore con score $s(x)$, se $\tau_1 < \tau_2$, allora:</em>
$\text{FPR}(\tau_1) \geq \text{FPR}(\tau_2) \quad \text{e} \quad \text{TPR}(\tau_1) \geq \text{TPR}(\tau_2)$</p>
<p><strong>Dimostrazione</strong>: Abbassando la soglia, classifichiamo pi√π esempi come positivi, quindi sia TP che FP possono solo aumentare (o restare costanti). $\square$</p>
<h3 id="54-area-under-the-curve-auc-roc">5.4 Area Under the Curve (AUC-ROC)</h3>
<p>L&rsquo;<strong>AUC</strong> (Area Under the ROC Curve) √® una metrica scalare che riassume la performance complessiva del classificatore.</p>
<p><strong>Definizione matematica</strong>:
$\text{AUC} = \int_0^1 \text{TPR}(t) \, d(\text{FPR}(t))$</p>
<p>dove $t$ varia lungo la curva (parametro di soglia).</p>
<p><strong>Range</strong>: $\text{AUC} \in [0, 1]$</p>
<p><strong>Interpretazione dei valori</strong>:</p>
<ul>
<li>$\text{AUC} = 1.0$: <strong>Perfetto</strong> - Separazione completa tra classi</li>
<li>$\text{AUC} = 0.9$: <strong>Eccellente</strong> - Ottima discriminazione</li>
<li>$\text{AUC} = 0.8$: <strong>Buono</strong> - Buona discriminazione</li>
<li>$\text{AUC} = 0.7$: <strong>Accettabile</strong> - Discriminazione discreta</li>
<li>$\text{AUC} = 0.5$: <strong>Nullo</strong> - Nessun potere discriminante (casuale)</li>
<li>$\text{AUC} < 0.5$: <strong>Invertito</strong> - Performance peggiore del caso</li>
</ul>
<p><strong>Teorema 5.2</strong> (Interpretazione Probabilistica dell&rsquo;AUC):
<em>L&rsquo;AUC √® la probabilit√† che un esempio positivo casuale abbia score maggiore di un esempio negativo casuale:</em></p>
<p>$\text{AUC} = P(s(X_+) > s(X_-))$</p>
<p>dove $X_+ \sim p(x|y=1)$ e $X_- \sim p(x|y=0)$.</p>
<p><strong>Dimostrazione</strong> (sketch):</p>
<p>Consideriamo tutti i possibili confronti tra un esempio positivo e uno negativo. Per ogni soglia $\tau$, contiamo:
- Quante coppie $(x_+, x_-)$ hanno $s(x_+) > \tau$ e $s(x_-) \leq \tau$</p>
<p>La curva ROC traccia esattamente questa proporzione. L&rsquo;integrale accumula tutti questi confronti, dando la frazione totale di coppie ordinate correttamente.</p>
<p>Formalmente, l&rsquo;AUC pu√≤ essere calcolata come:
$\text{AUC} = \frac{1}{P \cdot N} \sum_{i: y_i=1} \sum_{j: y_j=0} \mathbb{I}(s_i > s_j)$</p>
<p>dove $P$ √® il numero di positivi e $N$ di negativi. Questo √® esattamente una stima di $P(s(X_+) > s(X_-))$. $\square$</p>
<p><strong>Corollario</strong>: L&rsquo;AUC equivale alla statistica U del test di Mann-Whitney-Wilcoxon:
$\text{AUC} = \frac{U}{P \cdot N}$</p>
<p>dove $U$ √® la statistica U di Mann-Whitney.</p>
<p><strong>Calcolo pratico dell&rsquo;AUC</strong>:</p>
<p><strong>Metodo 1</strong> (Regola del trapezio):
$\text{AUC} \approx \sum_{i=1}^{n-1} \frac{1}{2}(\text{TPR}_i + \text{TPR}_{i+1}) \cdot (\text{FPR}_{i+1} - \text{FPR}_i)$</p>
<p><strong>Metodo 2</strong> (Conteggio di coppie concordanti):
$\text{AUC} = \frac{\#\{(i,j): y_i=1, y_j=0, s_i > s_j\}}{P \cdot N}$</p>
<h3 id="55-proprieta-fondamentali-dellauc">5.5 Propriet√† Fondamentali dell&rsquo;AUC</h3>
<p><strong>Propriet√† 5.1</strong> (Invarianza alla Scala):
L&rsquo;AUC dipende solo dall&rsquo;<strong>ordinamento</strong> degli score, non dai valori assoluti.</p>
<p>Se applichiamo una trasformazione monotona crescente $f$ agli score:
$\text{AUC}(f(s)) = \text{AUC}(s)$</p>
<p><strong>Implicazione</strong>: Possiamo confrontare modelli che producono score su scale diverse (e.g., probabilit√† vs logit vs distance).</p>
<p><strong>Propriet√† 5.2</strong> (Robustezza allo Sbilanciamento):
L&rsquo;AUC <strong>non dipende dalla prevalenza</strong> della classe positiva.</p>
<p>Se cambiamo la distribuzione di classe nel test set, l&rsquo;AUC rimane invariata (a patto che $p(x|y)$ non cambi).</p>
<p><strong>Dimostrazione</strong>: TPR e FPR sono entrambi condizionati su $y$:
$\text{TPR} = P(\hat{y}=1|y=1), \quad \text{FPR} = P(\hat{y}=1|y=0)$</p>
<p>Questi dipendono solo da $p(x|y=1)$, $p(x|y=0)$ e dalla soglia, non da $P(y)$. $\square$</p>
<p><strong>Propriet√† 5.3</strong> (Interpretazione come Ranking Metric):
L&rsquo;AUC misura la qualit√† del <strong>ranking</strong> prodotto dal classificatore:
- Un buon ranking mette esempi positivi in cima
- AUC alta ‚Üí la maggior parte dei positivi √® rankata sopra i negativi</p>
<p><strong>Limitazioni dell&rsquo;AUC</strong>:</p>
<ol>
<li>
<p><strong>Non fornisce informazioni sulla calibrazione</strong>: Due modelli con stesso AUC possono avere probabilit√† molto diverse</p>
</li>
<li>
<p><strong>Aggregazione su tutte le soglie</strong>: Pu√≤ mascherare performance scarse in regioni critiche</p>
</li>
<li>
<p><strong>Ottimizza per ranking globale</strong>: Pu√≤ non essere ottimale se ci interessa solo una specifica regione operativa (e.g., basso FPR)</p>
</li>
<li>
<p><strong>Sensibilit√† ridotta</strong>: Cambiamenti in regioni di bassa densit√† hanno stesso peso di regioni ad alta densit√†</p>
</li>
</ol>
<h3 id="56-operating-points-e-trade-offs">5.6 Operating Points e Trade-offs</h3>
<p><strong>Operating point</strong>: Un punto specifico sulla curva ROC corrispondente a una soglia $\tau$.</p>
<p><strong>Scelta dell&rsquo;operating point</strong>:</p>
<p>La curva ROC mostra tutti i possibili trade-off, ma dobbiamo scegliere un punto operativo specifico basato su:</p>
<ol>
<li>
<p><strong>Costi asimmetrici</strong>: Se $c = L_{FN}/L_{FP}$, cerchiamo il punto che minimizza:
   $\text{Cost}(\tau) = c \cdot \text{FNR}(\tau) + \text{FPR}(\tau)$</p>
</li>
<li>
<p><strong>Vincoli operativi</strong>: </p>
</li>
<li>&ldquo;FPR deve essere $\leq 0.05$&rdquo; ‚Üí scegli il punto con FPR massimo 0.05 e TPR massimo</li>
<li>
<p>&ldquo;Recall deve essere $\geq 0.9$&rdquo; ‚Üí scegli il punto con TPR minimo 0.9 e FPR minimo</p>
</li>
<li>
<p><strong>Youden&rsquo;s index</strong>: Massimizza la distanza dalla diagonale:
   $J = \text{TPR} - \text{FPR} = \text{Sensitivity} + \text{Specificity} - 1$
   Equivale a massimizzare l&rsquo;informedness.</p>
</li>
</ol>
<h3 id="57-equal-error-rate-eer">5.7 Equal Error Rate (EER)</h3>
<p>L&rsquo;<strong>Equal Error Rate</strong> √® il punto sulla curva ROC dove:
$\text{FPR}(\tau^*) = \text{FNR}(\tau^*) = \text{EER}$</p>
<p>Equivalentemente, dove:
$\text{FPR}(\tau^*) = 1 - \text{TPR}(\tau^*)$</p>
<p><strong>Interpretazione geometrica</strong>: Intersezione della curva ROC con la linea $y = 1 - x$.</p>
<p><strong>Propriet√†</strong>:
- Bilanciamento naturale tra i due tipi di errore
- Utile quando non abbiamo informazioni sui costi relativi
- EER basso indica performance migliore</p>
<p><strong>Calcolo</strong>: Cercare la soglia dove $|\text{FPR} - \text{FNR}|$ √® minimo.</p>
<h3 id="58-confronto-tra-modelli-con-roc">5.8 Confronto tra Modelli con ROC</h3>
<p><strong>Dominanza</strong>: Il modello A <strong>domina</strong> il modello B se:
$\text{TPR}_A(\tau) \geq \text{TPR}_B(\tau) \quad \forall \text{FPR}(\tau)$</p>
<p>In altre parole, la curva ROC di A √® sempre sopra (o coincide con) quella di B.</p>
<p>Se A domina B, allora certamente $\text{AUC}_A \geq \text{AUC}_B$.</p>
<p><strong>Curve che si intersecano</strong>: Se le curve ROC si intersecano, nessun modello domina l&rsquo;altro. La scelta dipende dalla regione operativa:
- Se operiamo a basso FPR (alta specificit√†), scegliamo il modello migliore in quella regione
- Se operiamo ad alto TPR (alta sensibilit√†), scegliamo il modello migliore in quella regione</p>
<p><strong>Esempio</strong>:
- Modello A: Migliore per FPR &lt; 0.1 (applicazioni dove FP sono molto costosi)
- Modello B: Migliore per FPR &gt; 0.1 (applicazioni dove vogliamo alto recall)</p>
<h2 id="6-curve-precision-recall">6. Curve Precision-Recall</h2>
<h3 id="61-motivazione-per-dataset-sbilanciati">6.1 Motivazione per Dataset Sbilanciati</h3>
<p>Quando la classe positiva √® <strong>rara</strong> (e.g., $P(y=1) \ll 0.5$), la curva ROC pu√≤ essere <strong>poco informativa</strong>:</p>
<p><strong>Problema con ROC per classi rare</strong>:</p>
<ol>
<li>Il numero di negativi $N$ √® molto grande</li>
<li>Anche un piccolo FPR corrisponde a <strong>molti falsi positivi</strong> in termini assoluti</li>
<li>La maggior parte della curva ROC √® compressa vicino all&rsquo;origine</li>
<li>Variazioni importanti nella precision sono mascherate</li>
</ol>
<p><strong>Esempio numerico</strong>:</p>
<p>Dataset: 10,000 esempi, 100 positivi (1%), 9,900 negativi.</p>
<p>Due classificatori:
- <strong>Modello A</strong>: TPR = 0.90, FPR = 0.02
- <strong>Modello B</strong>: TPR = 0.90, FPR = 0.05</p>
<p>Sulla curva ROC sembrano molto simili (stessa TPR, FPR simili).</p>
<p>Ma calcoliamo la precision:</p>
<p>$\text{Precision}_A = \frac{TP}{TP + FP} = \frac{90}{90 + (0.02 \times 9900)} = \frac{90}{288} \approx 0.31$</p>
<p>$\text{Precision}_B = \frac{90}{90 + (0.05 \times 9900)} = \frac{90}{585} \approx 0.15$</p>
<p>La precision di B √® <strong>met√†</strong> di quella di A! Ma questo non √® evidente nella curva ROC.</p>
<p><strong>Soluzione</strong>: La curva <strong>Precision-Recall</strong> focalizza l&rsquo;attenzione sui positivi, rendendola pi√π informativa per dataset sbilanciati.</p>
<h3 id="62-definizione-della-curva-pr">6.2 Definizione della Curva PR</h3>
<p><strong>Definizione</strong>: La curva Precision-Recall plotta:
$\text{PR}(\tau) = \big(\text{Recall}(\tau), \text{Precision}(\tau)\big)$</p>
<p>al variare della soglia $\tau$.</p>
<p><strong>Coordinate</strong>:
- <strong>Asse X</strong>: Recall = $\frac{TP}{P}$
- <strong>Asse Y</strong>: Precision = $\frac{TP}{TP + FP}$</p>
<p><strong>Costruzione</strong>:</p>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Input: Score s_i e label y_i per i = 1,...,n

1. Ordina per score decrescente: s_1 ‚â• s_2 ‚â• ... ‚â• s_n
2. Inizializza: TP = 0, FP = 0
3. Per ogni soglia œÑ:
   a. Aggiorna TP e FP
   b. Calcola: Recall = TP/P, Precision = TP/(TP+FP)
   c. Aggiungi punto (Recall, Precision)
</code></pre></div>
</div>
</details>

<h3 id="63-interpretazione-e-comportamento">6.3 Interpretazione e Comportamento</h3>
<p><strong>Punti notevoli</strong>:</p>
<p><strong>Alta soglia</strong> ($\tau \to 1$):
- Poche predizioni positive (solo le pi√π confidenti)
- Recall basso, Precision alta
- Punto in basso a destra della curva</p>
<p><strong>Bassa soglia</strong> ($\tau \to 0$):
- Molte predizioni positive
- Recall alto, Precision bassa (‚âà prevalenza)
- Punto in alto a sinistra della curva</p>
<p><strong>Baseline casuale</strong>:</p>
<p>Un classificatore casuale che predice positivo con probabilit√† $p$ ottiene:
$\text{Precision}_{\text{random}} = \frac{P}{n} = \pi$</p>
<p>indipendentemente da $p$ (in media). Quindi la baseline √® una <strong>linea orizzontale</strong> a $y = \pi$.</p>
<p><strong>Interpretazione</strong>: Una curva PR buona deve stare <strong>sopra</strong> questa baseline.</p>
<p><strong>Forma tipica</strong>: La curva PR tende a decrescere muovendosi da sinistra a destra (aumentando recall). Questo riflette il trade-off precision-recall.</p>
<h3 id="64-average-precision-ap">6.4 Average Precision (AP)</h3>
<p>L&rsquo;<strong>Average Precision</strong> riassume la curva PR in un singolo numero.</p>
<p><strong>Definizione</strong> (interpolata):
$\text{AP} = \sum_{k=1}^{n} (R_k - R_{k-1}) \cdot P_k$</p>
<p>dove $(P_k, R_k)$ sono precision e recall al $k$-esimo elemento rankat, ordinati per recall crescente.</p>
<p><strong>Interpretazione</strong>: Approssimazione dell&rsquo;area sotto la curva PR, pesando ogni livello di recall per quanto √® &ldquo;grande&rdquo; (quanto recall guadagniamo).</p>
<p><strong>Definizione alternativa</strong> (usata in PASCAL VOC):
$\text{AP} = \sum_{k=1}^{n} (R_k - R_{k-1}) \cdot P_{\text{interp}}(R_k)$</p>
<p>dove:
$P_{\text{interp}}(R_k) = \max_{R' \geq R_k} P(R')$</p>
<p>Questo usa la precision <strong>interpolata</strong> (massima raggiungibile per recall $\geq R_k$), rendendo la curva monotona.</p>
<p><strong>Relazione con Ranking</strong>:</p>
<p>$\text{AP} = \frac{1}{P} \sum_{k=1}^{n} P(k) \cdot \text{rel}(k)$</p>
<p>dove:
- $P(k)$ = precision at rank $k$
- $\text{rel}(k) = 1$ se l&rsquo;item al rank $k$ √® positivo, 0 altrimenti
- $P$ = numero totale di positivi</p>
<p><strong>Interpretazione</strong>: L&rsquo;AP √® la precision media su tutte le posizioni dove troviamo un positivo.</p>
<h3 id="65-precisionk-e-recallk">6.5 Precision@K e Recall@K</h3>
<p>In information retrieval e ranking systems, spesso ci interessano solo i top-K risultati.</p>
<p><strong>Precision@K</strong>:
$P@K = \frac{|\{i \in \text{top-}K : y_i = 1\}|}{K}$</p>
<p>Frazione di positivi tra i primi K elementi rankati.</p>
<p><strong>Recall@K</strong>:
$R@K = \frac{|\{i \in \text{top-}K : y_i = 1\}|}{P}$</p>
<p>Frazione di tutti i positivi catturati nei primi K elementi.</p>
<p><strong>Average Precision@K</strong>:
$AP@K = \frac{1}{\min(m, K)} \sum_{k=1}^{K} P(k) \cdot \text{rel}(k)$</p>
<p>dove $m$ √® il numero di positivi nel dataset.</p>
<p><strong>Uso tipico</strong>: 
- Motori di ricerca: P@10 (prime 10 ricerche)
- Sistemi di raccomandazione: P@20 (prime 20 raccomandazioni)
- Object detection: mAP@IoU (mean Average Precision a diversi IoU threshold)</p>
<h3 id="66-confronto-auc-roc-vs-auc-pr">6.6 Confronto AUC-ROC vs AUC-PR</h3>
<p><strong>Differenze fondamentali</strong>:</p>
<table>
<thead>
<tr>
<th>Aspetto</th>
<th>AUC-ROC</th>
<th>AUC-PR</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Assi</strong></td>
<td>TPR vs FPR</td>
<td>Precision vs Recall</td>
</tr>
<tr>
<td><strong>Focus</strong></td>
<td>Bilanciamento tra positivi e negativi</td>
<td>Solo classe positiva</td>
</tr>
<tr>
<td><strong>Baseline</strong></td>
<td>Diagonale (0.5)</td>
<td>Orizzontale (prevalenza $\pi$)</td>
</tr>
<tr>
<td><strong>Dipendenza da $\pi$</strong></td>
<td>Invariante</td>
<td>Dipendente</td>
</tr>
<tr>
<td><strong>Dataset bilanciati</strong></td>
<td>Ottimo</td>
<td>Equivalente</td>
</tr>
<tr>
<td><strong>Dataset sbilanciati</strong></td>
<td>Pu√≤ essere misleading</td>
<td>Pi√π informativa</td>
</tr>
<tr>
<td><strong>Interpretazione</strong></td>
<td>Ranking globale</td>
<td>Rilevanza dei positivi</td>
</tr>
</tbody>
</table>
<p><strong>Teorema 6.1</strong> (Sensibilit√† al Prior):
<em>Dato uno shift nella prevalenza da $\pi_{\text{train}}$ a $\pi_{\text{test}}$:</em>
- <em>L&rsquo;AUC-ROC rimane invariante</em>
- <em>L&rsquo;AUC-PR cambia proporzionalmente</em></p>
<p><strong>Dimostrazione</strong>:</p>
<p>ROC usa metriche condizionate su $y$:
$\text{TPR} = P(\hat{y}=1|y=1), \quad \text{FPR} = P(\hat{y}=1|y=0)$</p>
<p>Queste dipendono solo da $p(x|y)$, non da $P(y)$.</p>
<p>PR usa Precision che dipende esplicitamente dal prior:
$\text{Precision} = P(y=1|\hat{y}=1) = \frac{P(\hat{y}=1|y=1) \cdot P(y=1)}{P(\hat{y}=1)}$</p>
<p>Per Bayes:
$\text{Precision} = \frac{\text{TPR} \cdot \pi}{\text{TPR} \cdot \pi + \text{FPR} \cdot (1-\pi)}$</p>
<p>Cambiando $\pi$, la precision cambia, quindi cambia AUC-PR. $\square$</p>
<p><strong>Implicazione pratica</strong>: Se il test set ha prevalenza diversa dal training, AUC-PR sar√† diversa anche con stesso modello. Questo rende AUC-PR pi√π &ldquo;onesta&rdquo; per dataset molto sbilanciati.</p>
<p><strong>Quando usare quale</strong>:</p>
<ul>
<li><strong>ROC-AUC</strong>: </li>
<li>Dataset bilanciati o moderatamente sbilanciati</li>
<li>Interessa ranking generale</li>
<li>
<p>Vogliamo confrontare modelli indipendentemente dalla prevalenza</p>
</li>
<li>
<p><strong>PR-AUC</strong>:</p>
</li>
<li>Dataset fortemente sbilanciati ($\pi < 0.1$ o $\pi > 0.9$)</li>
<li>Focus sulla classe positiva rara</li>
<li>Information retrieval e detection tasks</li>
</ul>
<h2 id="7-metriche-avanzate-e-robuste">7. Metriche Avanzate e Robuste</h2>
<h3 id="71-matthews-correlation-coefficient-mcc">7.1 Matthews Correlation Coefficient (MCC)</h3>
<p>Il <strong>Matthews Correlation Coefficient</strong> √® considerato una delle metriche pi√π bilanciate e robuste per classificazione binaria.</p>
<p><strong>Definizione</strong>:
$\text{MCC} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$</p>
<p><strong>Derivazione</strong>: L&rsquo;MCC √® il <strong>coefficiente di correlazione di Pearson</strong> $\phi$ tra le variabili binarie $y$ (label reale) e $\hat{y}$ (predizione).</p>
<p>Per due variabili binarie, il coefficiente $\phi$ √®:
$\phi = \frac{n_{11}n_{00} - n_{10}n_{01}}{\sqrt{n_{1\cdot}n_{0\cdot}n_{\cdot1}n_{\cdot0}}}$</p>
<p>dove $n_{ij}$ √® la frequenza congiunta di $y=i$ e $\hat{y}=j$. Sostituendo con la notazione della confusion matrix:
- $n_{11} = TP$ (entrambi 1)
- $n_{00} = TN$ (entrambi 0)
- $n_{10} = FN$ ($y=1$, $\hat{y}=0$)
- $n_{01} = FP$ ($y=0$, $\hat{y}=1$)
- $n_{1\cdot} = TP + FN$ (totale reali positivi)
- $n_{0\cdot} = TN + FP$ (totale reali negativi)
- $n_{\cdot1} = TP + FP$ (totale predetti positivi)
- $n_{\cdot0} = TN + FN$ (totale predetti negativi)</p>
<p>Otteniamo esattamente la formula dell&rsquo;MCC.</p>
<p><strong>Range e Interpretazione</strong>:</p>
<p>$\text{MCC} \in [-1, +1]$</p>
<ul>
<li>$\text{MCC} = +1$: <strong>Perfetto</strong> - Predizione completamente corretta</li>
<li>$\text{MCC} = 0$: <strong>Casuale</strong> - Performance non migliore del caso</li>
<li>$\text{MCC} = -1$: <strong>Inverso perfetto</strong> - Predizioni completamente sbagliate (ma consistentemente)</li>
</ul>
<p><strong>Interpretazione come correlazione</strong>:
- MCC positivo: Associazione positiva tra predizioni e realt√†
- MCC vicino a 0: Nessuna associazione (predizioni random)
- MCC negativo: Associazione negativa (predizioni sistematicamente inverse)</p>
<p><strong>Propriet√† Fondamentali</strong>:</p>
<p><strong>Propriet√† 7.1</strong> (Simmetria):
$\text{MCC}(y, \hat{y}) = \text{MCC}(\neg y, \neg \hat{y})$</p>
<p>L&rsquo;MCC √® invariante rispetto allo scambio di classe (chiamare &ldquo;positivo&rdquo; quello che prima era &ldquo;negativo&rdquo;).</p>
<p><strong>Dimostrazione</strong>: Sotto lo scambio $y \leftrightarrow (1-y)$ e $\hat{y} \leftrightarrow (1-\hat{y})$:
- $TP \leftrightarrow TN$
- $FP \leftrightarrow FN$</p>
<p>Il numeratore diventa: $TN \cdot TP - FN \cdot FP = TP \cdot TN - FP \cdot FN$ (invariato).</p>
<p>Il denominatore √® simmetrico per costruzione. $\square$</p>
<p><strong>Propriet√† 7.2</strong> (Robustezza allo Sbilanciamento):
L&rsquo;MCC <strong>non favorisce la classe maggioritaria</strong> ed √® considerato la metrica pi√π affidabile per dataset sbilanciati.</p>
<p><strong>Confronto con Accuracy su Dataset Sbilanciato</strong>:</p>
<p>Esempio: 95 negativi, 5 positivi.</p>
<p><strong>Classificatore Dummy</strong> (sempre negativo):
- TP = 0, TN = 95, FP = 0, FN = 5
- Accuracy = $95/100 = 0.95$ (sembra ottimo!)
- MCC = $\frac{0 \cdot 95 - 0 \cdot 5}{\sqrt{0 \cdot 5 \cdot 95 \cdot 100}} = \frac{0}{0}$ (indefinito, o 0)</p>
<p><strong>Classificatore Bilanciato</strong>:
- TP = 4, TN = 90, FP = 5, FN = 1
- Accuracy = $94/100 = 0.94$ (leggermente peggio)
- MCC = $\frac{4 \cdot 90 - 5 \cdot 1}{\sqrt{9 \cdot 5 \cdot 95 \cdot 91}} \approx 0.60$ (molto meglio!)</p>
<p>L&rsquo;MCC riconosce correttamente che il secondo classificatore √® superiore.</p>
<p><strong>Relazione con altre metriche</strong>:</p>
<p>L&rsquo;MCC pu√≤ essere espresso in termini di TPR, TNR, PPV (Precision), NPV:</p>
<p>$\text{MCC} = \sqrt{\text{TPR} \cdot \text{TNR} \cdot \text{PPV} \cdot \text{NPV}} - \sqrt{\text{FNR} \cdot \text{FPR} \cdot \text{FOR} \cdot \text{FDR}}$</p>
<p>dove FOR = False Omission Rate, FDR = False Discovery Rate.</p>
<p><strong>Nota computazionale</strong>: Quando uno qualsiasi dei termini nel denominatore √® zero, l&rsquo;MCC √® indefinito (divisione per zero). In pratica, si assegna MCC = 0 in questi casi.</p>
<h3 id="72-cohens-kappa-math_inline_384">7.2 Cohen&rsquo;s Kappa ($\kappa$)</h3>
<p>Il <strong>Cohen&rsquo;s Kappa</strong> misura l&rsquo;accordo tra predizioni e realt√†, <strong>corretto per l&rsquo;accordo casuale</strong>.</p>
<p><strong>Definizione</strong>:
$\kappa = \frac{p_o - p_e}{1 - p_e}$</p>
<p>dove:
- $p_o = \frac{TP + TN}{n}$ √® l&rsquo;<strong>accuratezza osservata</strong>
- $p_e$ √® l&rsquo;<strong>accuratezza attesa per caso</strong></p>
<p><strong>Calcolo di $p_e$</strong> (Accordo Casuale Atteso):</p>
<p>Se $y$ e $\hat{y}$ fossero <strong>indipendenti</strong> ma con le stesse distribuzioni marginali:</p>
<p>$p_e = P(y = \hat{y}|\text{indipendenza})$</p>
<p>$= P(y=1) \cdot P(\hat{y}=1) + P(y=0) \cdot P(\hat{y}=0)$</p>
<p>$= \frac{TP + FN}{n} \cdot \frac{TP + FP}{n} + \frac{TN + FP}{n} \cdot \frac{TN + FN}{n}$</p>
<p>$= \frac{(TP+FN)(TP+FP) + (TN+FP)(TN+FN)}{n^2}$</p>
<p><strong>Interpretazione</strong>:</p>
<p>$\kappa = \frac{\text{Accordo Osservato} - \text{Accordo Casuale}}{1 - \text{Accordo Casuale}}$</p>
<ul>
<li>Numeratore: Quanto l&rsquo;accordo osservato supera il caso</li>
<li>Denominatore: Massimo miglioramento possibile rispetto al caso</li>
</ul>
<p><strong>Range</strong>:</p>
<p>$\kappa \in [-1, 1]$</p>
<p>ma tipicamente $\kappa \in [0, 1]$ per classificatori ragionevoli.</p>
<p><strong>Scala di Landis e Koch</strong> (interpretazione classica):</p>
<table>
<thead>
<tr>
<th>Kappa</th>
<th>Forza dell&rsquo;Accordo</th>
</tr>
</thead>
<tbody>
<tr>
<td>$< 0$</td>
<td>Peggiore del caso</td>
</tr>
<tr>
<td>$0.00 - 0.20$</td>
<td>Lieve</td>
</tr>
<tr>
<td>$0.21 - 0.40$</td>
<td>Discreto</td>
</tr>
<tr>
<td>$0.41 - 0.60$</td>
<td>Moderato</td>
</tr>
<tr>
<td>$0.61 - 0.80$</td>
<td>Sostanziale</td>
</tr>
<tr>
<td>$0.81 - 1.00$</td>
<td>Quasi perfetto</td>
</tr>
</tbody>
</table>
<p><strong>Esempio di calcolo</strong>:</p>
<p>Dataset: 100 esempi, 60 positivi, 40 negativi.
Modello: TP = 50, TN = 30, FP = 10, FN = 10.</p>
<p>$p_o = \frac{50 + 30}{100} = 0.80$</p>
<p>$p_e = \frac{60 \cdot 60}{100^2} + \frac{40 \cdot 40}{100^2} = \frac{3600 + 1600}{10000} = 0.52$</p>
<p>$\kappa = \frac{0.80 - 0.52}{1 - 0.52} = \frac{0.28}{0.48} \approx 0.58$</p>
<p>Interpretazione: Accordo <strong>moderato</strong> (secondo Landis e Koch).</p>
<p><strong>Relazione con MCC</strong>:</p>
<p>Per problemi binari, MCC e Kappa sono correlati ma <strong>non identici</strong>. In generale:
- MCC √® preferito per la sua interpretazione come correlazione
- MCC ha migliori propriet√† matematiche
- Kappa √® pi√π usato in ambito medico/statistico per inter-rater agreement</p>
<p><strong>Differenza chiave</strong>: Kappa usa le distribuzioni marginali empiriche per calcolare $p_e$, mentre MCC √® una pura misura di correlazione.</p>
<h3 id="73-balanced-accuracy">7.3 Balanced Accuracy</h3>
<p>La <strong>balanced accuracy</strong> √® particolarmente utile per dataset sbilanciati, dando peso uguale a ciascuna classe.</p>
<p><strong>Definizione</strong>:
$\text{Balanced Accuracy} = \frac{1}{2}\left(\frac{TP}{TP+FN} + \frac{TN}{TN+FP}\right) = \frac{\text{TPR} + \text{TNR}}{2}$</p>
<p><strong>Equivalente</strong>:
$\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}$</p>
<p><strong>Motivazione</strong>: L&rsquo;accuracy standard pu√≤ essere dominata dalla classe maggioritaria. La balanced accuracy:
- Calcola accuracy per ciascuna classe separatamente
- Fa la media (non pesata) delle due</p>
<p><strong>Esempio illustrativo</strong>:</p>
<p>Dataset: 950 negativi, 50 positivi.</p>
<p><strong>Classificatore A</strong> (sempre negativo):
- Accuracy = $950/1000 = 0.95$
- Balanced Accuracy = $\frac{0 + 1}{2} = 0.50$ ‚Üê Rivela che √® casuale!</p>
<p><strong>Classificatore B</strong>:
- TP = 40, TN = 900, FP = 50, FN = 10
- Accuracy = $940/1000 = 0.94$
- Balanced Accuracy = $\frac{40/50 + 900/950}{2} = \frac{0.8 + 0.947}{2} \approx 0.87$</p>
<p>La balanced accuracy rivela correttamente che B √® molto migliore di A, anche se l&rsquo;accuracy semplice √® simile.</p>
<p><strong>Generalizzazione Multi-Classe</strong>:</p>
<p>$\text{Balanced Accuracy} = \frac{1}{C} \sum_{c=1}^{C} \frac{TP_c}{TP_c + FN_c}$</p>
<p>dove $C$ √® il numero di classi.</p>
<p><strong>Propriet√†</strong>:
- Range: $[0, 1]$
- Balanced Accuracy = 0.5 per classificatore casuale (binario)
- Non favorisce alcuna classe
- Pi√π interpretabile dell&rsquo;MCC per utenti non tecnici</p>
<p><strong>Confronto con Macro-F1</strong>:</p>
<p>Entrambe danno peso uguale alle classi, ma:
- Balanced Accuracy: Media di recall per classe
- Macro-F1: Media di F1 per classe (combina precision e recall)</p>
<h3 id="74-informedness-e-markedness">7.4 Informedness e Markedness</h3>
<p>Due metriche meno note ma teoricamente importanti.</p>
<p><strong>Informedness</strong> (Bookmaker Informedness):
$\text{Informedness} = \text{TPR} + \text{TNR} - 1 = \text{Sensitivity} + \text{Specificity} - 1$</p>
<p><strong>Interpretazione</strong>: 
- Quanto il classificatore √® pi√π informato del caso?
- Probabilit√† di decisione informata vs casuale
- Range: $[-1, 1]$ dove 0 = casuale</p>
<p><strong>Relazione</strong>: 
$\text{Informedness} = 2 \cdot \text{Balanced Accuracy} - 1$</p>
<p><strong>Markedness</strong>:
$\text{Markedness} = \text{PPV} + \text{NPV} - 1 = \text{Precision} + \text{NPV} - 1$</p>
<p>dove NPV (Negative Predictive Value) = $\frac{TN}{TN+FN}$.</p>
<p><strong>Interpretazione</strong>: Quanto sono &ldquo;marcate&rdquo; (affidabili) le predizioni?</p>
<p><strong>Teorema 7.1</strong> (Relazione MCC con Informedness e Markedness):
$\text{MCC} = \sqrt{\text{Informedness} \times \text{Markedness}}$</p>
<p>(quando tutti i termini sono definiti e non negativi)</p>
<p><strong>Dimostrazione</strong> (sketch):
$\text{Informedness} = \frac{TP}{P} + \frac{TN}{N} - 1$</p>
<p>$\text{Markedness} = \frac{TP}{P^*} + \frac{TN}{N^*} - 1$</p>
<p>Espandendo e semplificando usando le identit√† della confusion matrix, si ottiene che il loro prodotto geometrico √® correlato a MCC¬≤. $\square$</p>
<h2 id="8-valutazione-probabilistica-e-calibrazione">8. Valutazione Probabilistica e Calibrazione</h2>
<h3 id="81-introduzione">8.1 Introduzione</h3>
<p>Molti classificatori producono <strong>probabilit√†</strong> $p(y=1|x)$ anzich√© solo label binari. √à importante valutare:
1. <strong>Discriminazione</strong>: Il modello separa bene le classi? (ROC, PR)
2. <strong>Calibrazione</strong>: Le probabilit√† predette riflettono le vere frequenze?</p>
<p>Un modello pu√≤ avere ottima discriminazione (AUC alta) ma pessima calibrazione.</p>
<p><strong>Esempio</strong>: Un modello che produce sempre $p=0.9$ per positivi e $p=0.1$ per negativi ha:
- Perfetta discriminazione (AUC = 1)
- Pessima calibrazione (le probabilit√† non riflettono l&rsquo;incertezza reale)</p>
<h3 id="82-log-loss-cross-entropy-loss">8.2 Log Loss (Cross-Entropy Loss)</h3>
<p>La <strong>log loss</strong> valuta la qualit√† delle probabilit√† predette.</p>
<p><strong>Definizione</strong> (Binaria):
$\mathcal{L}_{\text{log}} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]$</p>
<p>dove $p_i = P(y_i=1|x_i)$ √® la probabilit√† predetta.</p>
<p><strong>Derivazione</strong>: La log loss √® l&rsquo;<strong>entropia incrociata</strong> tra distribuzione empirica e predetta:</p>
<p>$H(q, p) = -\mathbb{E}_{y \sim q}[\log p(y|x)]$</p>
<p>Per label binari deterministici: $q(y=1|x) = y$ (0 o 1):
$H = -y \log p - (1-y) \log(1-p)$</p>
<p><strong>Propriet√†</strong>:</p>
<ol>
<li><strong>Range</strong>: $[0, +\infty)$ dove 0 indica probabilit√† perfette</li>
<li><strong>Penalizzazione logaritmica</strong>: Predizioni confidenti ma sbagliate sono penalizzate esponenzialmente</li>
<li><strong>Proper scoring rule</strong>: Minimizzata dalle vere probabilit√†</li>
</ol>
<p><strong>Proper Scoring Rule</strong>: Una metrica √® &ldquo;proper&rdquo; se √® ottimizzata predicendo le vere probabilit√†:
$\mathbb{E}_{y \sim p^*}[S(y, p)] \geq \mathbb{E}_{y \sim p^*}[S(y, q)] \quad \forall q$</p>
<p>con uguaglianza solo se $q = p^*$.</p>
<p><strong>Esempi di penalizzazione</strong>:</p>
<table>
<thead>
<tr>
<th>Vera Classe</th>
<th>Probabilit√† Predetta</th>
<th>Log Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>$y=1$</td>
<td>$p=0.99$</td>
<td>$-\log(0.99) \approx 0.01$</td>
</tr>
<tr>
<td>$y=1$</td>
<td>$p=0.9$</td>
<td>$-\log(0.9) \approx 0.11$</td>
</tr>
<tr>
<td>$y=1$</td>
<td>$p=0.5$</td>
<td>$-\log(0.5) \approx 0.69$</td>
</tr>
<tr>
<td>$y=1$</td>
<td>$p=0.1$</td>
<td>$-\log(0.1) \approx 2.30$</td>
</tr>
<tr>
<td>$y=1$</td>
<td>$p=0.01$</td>
<td>$-\log(0.01) \approx 4.61$</td>
</tr>
</tbody>
</table>
<p>Notare la <strong>penalizzazione esponenziale</strong>: predire $p=0.01$ quando $y=1$ costa 460 volte pi√π che predire $p=0.99$!</p>
<p><strong>Collegamento con Maximum Likelihood</strong>:</p>
<p>Minimizzare la log loss √® equivalente a massimizzare la log-likelihood:
$\arg\min_\theta \mathcal{L}_{\text{log}} = \arg\max_\theta \sum_{i=1}^{n} \log p(y_i|x_i, \theta)$</p>
<p>Questo √® il principio del <strong>Maximum Likelihood Estimation (MLE)</strong>.</p>
<p><strong>Multi-Classe</strong>:
$\mathcal{L}_{\text{log}} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{C} y_{ic} \log(p_{ic})$</p>
<p>dove $y_{ic} = 1$ se $y_i = c$, altrimenti 0 (one-hot encoding).</p>
<h3 id="83-brier-score">8.3 Brier Score</h3>
<p>Il <strong>Brier score</strong> misura l&rsquo;errore quadratico delle probabilit√†.</p>
<p><strong>Definizione</strong> (Binaria):
$\text{BS} = \frac{1}{n} \sum_{i=1}^{n} (p_i - y_i)^2$</p>
<p>dove $y_i \in \{0, 1\}$.</p>
<p><strong>Derivazione</strong>: √à semplicemente il <strong>Mean Squared Error (MSE)</strong> tra probabilit√† predette e label binari.</p>
<p><strong>Range</strong>: $[0, 1]$ dove 0 indica probabilit√† perfette.</p>
<p><strong>Decomposizione di Murphy</strong>:</p>
<p>Il Brier score pu√≤ essere decomposto in tre componenti interpretabili:</p>
<p>$\text{BS} = \text{Reliability} - \text{Resolution} + \text{Uncertainty}$</p>
<p>dove:</p>
<p><strong>Uncertainty</strong> (varianza intrinseca):
$\text{Uncertainty} = \bar{y}(1 - \bar{y})$
dove $\bar{y}$ √® la prevalenza. Non controllabile dal modello.</p>
<p><strong>Resolution</strong> (capacit√† di separare):
$\text{Resolution} = \frac{1}{n} \sum_{k=1}^{K} n_k(\bar{y}_k - \bar{y})^2$
Quanto bene il modello separa esempi con diverse probabilit√† vere. Vogliamo massimizzarlo.</p>
<p><strong>Reliability</strong> (calibrazione):
$\text{Reliability} = \frac{1}{n} \sum_{k=1}^{K} n_k(\bar{y}_k - \bar{p}_k)^2$
Deviazione tra probabilit√† predette e frequenze osservate. Vogliamo minimizzarlo.</p>
<p><strong>Interpretazione</strong>: 
- Un buon modello ha <strong>alta resolution</strong> (separa bene) e <strong>bassa reliability</strong> (ben calibrato)
- BS basso indica entrambe le propriet√†</p>
<p><strong>Confronto Log Loss vs Brier Score</strong>:</p>
<table>
<thead>
<tr>
<th>Aspetto</th>
<th>Log Loss</th>
<th>Brier Score</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Penalizzazione</strong></td>
<td>Logaritmica (severa)</td>
<td>Quadratica (moderata)</td>
</tr>
<tr>
<td><strong>Range</strong></td>
<td>$[0, \infty)$</td>
<td>$[0, 1]$</td>
</tr>
<tr>
<td><strong>Proper scoring</strong></td>
<td>S√¨</td>
<td>S√¨</td>
</tr>
<tr>
<td><strong>Interpretabilit√†</strong></td>
<td>Meno intuitiva</td>
<td>Pi√π intuitiva (MSE)</td>
</tr>
<tr>
<td><strong>Sensibilit√† a errori</strong></td>
<td>Molto alta</td>
<td>Moderata</td>
</tr>
<tr>
<td><strong>Decomponibile</strong></td>
<td>No (direttamente)</td>
<td>S√¨ (Murphy)</td>
</tr>
</tbody>
</table>
<p><strong>Quando usare quale</strong>:
- <strong>Log Loss</strong>: Training di modelli (gradient-based), quando predizioni molto sbagliate devono essere evitate
- <strong>Brier Score</strong>: Valutazione finale, quando vogliamo interpretabilit√† e decomposizione</p>
<h3 id="84-calibrazione-delle-probabilita">8.4 Calibrazione delle Probabilit√†</h3>
<p>Un modello √® <strong>ben calibrato</strong> (o <strong>affidabile</strong>) se le probabilit√† predette riflettono le vere frequenze:</p>
<p><strong>Definizione Formale</strong>:
$P(y=1 | p(y=1|x) = q) = q \quad \forall q \in [0,1]$</p>
<p><strong>Interpretazione</strong>: &ldquo;Tra tutti gli esempi a cui assegno probabilit√† $q$, una frazione $q$ dovrebbe essere effettivamente positiva.&rdquo;</p>
<p><strong>Esempio</strong>:
- Se predico $p=0.7$ per 100 esempi, circa 70 dovrebbero essere realmente positivi
- Se predico $p=0.3$ per 50 esempi, circa 15 dovrebbero essere realmente positivi</p>
<h4 id="841-reliability-diagram-calibration-plot">8.4.1 Reliability Diagram (Calibration Plot)</h4>
<p>Il <strong>reliability diagram</strong> visualizza la calibrazione.</p>
<p><strong>Procedura</strong>:</p>
<ol>
<li>
<p><strong>Binning</strong>: Dividi le predizioni in $B$ bin basati su $p_i$ (e.g., $B=10$ bin di ampiezza 0.1)</p>
</li>
<li>
<p><strong>Per ogni bin $b$</strong>:</p>
</li>
<li>Calcola <strong>probabilit√† media predetta</strong>: $\bar{p}_b = \frac{1}{|B_b|} \sum_{i \in B_b} p_i$</li>
<li>
<p>Calcola <strong>frazione empirica di positivi</strong>: $\bar{y}_b = \frac{1}{|B_b|} \sum_{i \in B_b} y_i$</p>
</li>
<li>
<p><strong>Plot</strong>: $\bar{y}_b$ (asse Y) vs $\bar{p}_b$ (asse X)</p>
</li>
</ol>
<p><strong>Interpretazione</strong>:</p>
<ul>
<li><strong>Diagonale perfetta</strong> ($\bar{y}_b = \bar{p}_b$ per ogni bin): Calibrazione perfetta</li>
<li><strong>Sopra la diagonale</strong>: Modello <strong>sotto-confidente</strong> (predice probabilit√† troppo basse)</li>
<li><strong>Sotto la diagonale</strong>: Modello <strong>sovra-confidente</strong> (predice probabilit√† troppo alte)</li>
<li><strong>Forma a S</strong>: Modello sovra-confidente alle estremit√†, sotto-confidente al centro</li>
</ul>
<p><strong>Esempio</strong>:</p>
<p>Bin $[0.8, 0.9]$:
- $\bar{p}_b = 0.85$ (probabilit√† media predetta)
- $\bar{y}_b = 0.95$ (frazione reale di positivi)
- Interpretazione: Il modello √® sotto-confidente in questa regione</p>
<h4 id="842-expected-calibration-error-ece">8.4.2 Expected Calibration Error (ECE)</h4>
<p>L&rsquo;<strong>ECE</strong> quantifica numericamente la deviazione dalla calibrazione perfetta.</p>
<p><strong>Definizione</strong>:
$\text{ECE} = \sum_{b=1}^{B} \frac{|B_b|}{n} |\bar{y}_b - \bar{p}_b|$</p>
<p>dove:
- $B$ = numero di bin
- $|B_b|$ = numero di esempi nel bin $b$
- $\bar{y}_b$ = frazione empirica di positivi nel bin
- $\bar{p}_b$ = probabilit√† media predetta nel bin</p>
<p><strong>Interpretazione</strong>: Media pesata della deviazione assoluta dalla calibrazione perfetta.</p>
<p><strong>Propriet√†</strong>:
- Range: $[0, 1]$
- ECE = 0 indica calibrazione perfetta
- Usa errore <strong>assoluto</strong> (pi√π robusto del quadratico)</p>
<p><strong>Maximum Calibration Error (MCE)</strong>:
$\text{MCE} = \max_{b=1,\ldots,B} |\bar{y}_b - \bar{p}_b|$</p>
<p>Misura la <strong>peggiore</strong> deviazione locale dalla calibrazione.</p>
<p><strong>Scelta del numero di bin</strong>: Tipicamente $B \in \{10, 15, 20\}$. Troppo pochi ‚Üí scarsa risoluzione. Troppi ‚Üí bin con pochi esempi (stime instabili).</p>
<h4 id="843-metodi-di-calibrazione">8.4.3 Metodi di Calibrazione</h4>
<p>Se un modello ha buona discriminazione ma scarsa calibrazione, possiamo <strong>post-processare</strong> le probabilit√†.</p>
<p><strong>Platt Scaling</strong> (Regressione Logistica):</p>
<p>Applica una trasformazione logistica agli score:
$p_{\text{calib}}(y=1|x) = \frac{1}{1 + e^{-(a \cdot s(x) + b)}}$</p>
<p>dove:
- $s(x)$ √® lo score non calibrato del modello
- $a, b$ sono parametri appresi su un <strong>validation set</strong></p>
<p><strong>Procedura</strong>:
1. Genera score $s_i$ per il validation set
2. Fit regressione logistica: $y_i \sim \text{Logistic}(a \cdot s_i + b)$
3. Applica la trasformazione agli score futuri</p>
<p><strong>Quando usare</strong>: Funziona bene quando la relazione score-probabilit√† √® monotona e approssimativamente sigmoidale (comune per SVM, Naive Bayes).</p>
<p><strong>Isotonic Regression</strong>:</p>
<p>Apprende una funzione <strong>monotona crescente</strong> non-parametrica $f: \mathbb{R} \to [0,1]$:
$p_{\text{calib}}(y=1|x) = f(s(x))$</p>
<p><strong>Procedura</strong>:
1. Ordina validation set per score crescente
2. Trova la funzione a gradini monotona che minimizza MSE con i label
3. Applica $f$ agli score futuri</p>
<p><strong>Quando usare</strong>: Pi√π flessibile di Platt, funziona per relazioni non-sigmoidali. Richiede pi√π dati per evitare overfitting.</p>
<p><strong>Temperature Scaling</strong> (per Neural Networks):</p>
<p>Scala i <strong>logit</strong> con un parametro temperatura $T$:
$p_i^{\text{calib}} = \frac{e^{z_i/T}}{\sum_{j=1}^{C} e^{z_j/T}}$</p>
<p>dove $z_i$ sono i logit (output pre-softmax).</p>
<p><strong>Effetto di $T$</strong>:
- $T > 1$: <strong>&ldquo;Smoothing&rdquo;</strong> ‚Üí probabilit√† meno confidenti (pi√π disperse)
- $T < 1$: <strong>&ldquo;Sharpening&rdquo;</strong> ‚Üí probabilit√† pi√π confidenti (pi√π concentrate)
- $T = 1$: Nessun cambiamento</p>
<p><strong>Apprendimento</strong>: Trova $T$ che minimizza log loss sul validation set (tipicamente con grid search o gradient descent).</p>
<p><strong>Vantaggi</strong>: Mantiene l&rsquo;ordinamento relativo delle classi, singolo parametro globale, preserva accuracy.</p>
<p><strong>Confronto metodi</strong>:</p>
<table>
<thead>
<tr>
<th>Metodo</th>
<th>Parametri</th>
<th>Flessibilit√†</th>
<th>Requisiti Dati</th>
<th>Uso Tipico</th>
</tr>
</thead>
<tbody>
<tr>
<td>Platt</td>
<td>2</td>
<td>Bassa (sigmoid)</td>
<td>Moderati</td>
<td>SVM, Naive Bayes</td>
</tr>
<tr>
<td>Isotonic</td>
<td>Molti (piecewise)</td>
<td>Alta</td>
<td>Molti</td>
<td>Alberi, ensemble</td>
</tr>
<tr>
<td>Temperature</td>
<td>1</td>
<td>Bassa (scala)</td>
<td>Pochi</td>
<td>Neural networks</td>
</tr>
</tbody>
</table>
<h3 id="85-decisioni-ottimali-con-costi-asimmetrici">8.5 Decisioni Ottimali con Costi Asimmetrici</h3>
<h4 id="851-framework-del-rischio-bayesiano">8.5.1 Framework del Rischio Bayesiano</h4>
<p>Abbiamo visto nella Sezione 3 che la decision rule ottimale minimizza il rischio atteso. Approfondiamo ora come utilizzare questo framework in pratica.</p>
<p><strong>Rischio Atteso</strong> per soglia $\tau$:
$$R(\tau) = L_{FN} \cdot \text{FNR}(\tau) \cdot \pi + L_{FP} \cdot \text{FPR}(\tau) \cdot (1-\pi)$$</p>
<p>dove $\pi = P(Y=1)$ √® la prevalenza.</p>
<p><strong>Teorema 8.1</strong> (Soglia Ottimale per Costi Asimmetrici):
<em>Data loss matrix con costi $L_{FP}$ e $L_{FN}$, la soglia ottimale √®:</em></p>
$$\tau^* = \frac{L_{FP} \cdot (1-\pi)}{L_{FP} \cdot (1-\pi) + L_{FN} \cdot \pi}$$
<p><strong>Dimostrazione</strong>:</p>
<p>Dal Teorema 3.2, classifichiamo come positivo quando:
$$\frac{p(y=1|x)}{p(y=0|x)} > \frac{L_{FP}}{L_{FN}}$$</p>
<p>Riscrivendo in termini di $p(y=1|x) = p$:
$$\frac{p}{1-p} > \frac{L_{FP}}{L_{FN}}$$</p>
<p>Risolvendo per $p$:
$$p > \frac{L_{FP}}{L_{FP} + L_{FN}}$$</p>
<p>Questa √® la soglia ottimale quando $\pi = 0.5$. Per prevalenza arbitraria, la soglia diventa:
$$\tau^* = \frac{L_{FP} \cdot (1-\pi)}{L_{FP} \cdot (1-\pi) + L_{FN} \cdot \pi}$$</p>
<p>$\square$</p>
<p><strong>Casi speciali</strong>:</p>
<ol>
<li>
<p><strong>Costi uguali</strong> ($L_{FP} = L_{FN} = 1$):
   $$\tau^* = \frac{1-\pi}{1-\pi+\pi} = 1-\pi$$</p>
</li>
<li>
<p><strong>Prevalenza bilanciata</strong> ($\pi = 0.5$):
   $$\tau^* = \frac{L_{FP}}{L_{FP} + L_{FN}}$$</p>
</li>
</ol>
<p><strong>Esempio pratico</strong>:</p>
<p>Screening medico: $L_{FN} = 1000$ (vita a rischio), $L_{FP} = 1$ (test aggiuntivo), $\pi = 0.01$.</p>
$$\tau^* = \frac{1 \cdot 0.99}{1 \cdot 0.99 + 1000 \cdot 0.01} = \frac{0.99}{10.99} \approx 0.09$$
<p>Soglia molto bassa ‚Üí massimizziamo la sensibilit√†, accettando molti falsi positivi.</p>
<h3 id="852-cost-sensitive-learning">8.5.2 Cost-Sensitive Learning</h3>
<p>Invece di ottimizzare la soglia post-hoc, possiamo integrare i costi <strong>durante il training</strong>.</p>
<p><strong>Weighted Loss</strong>:
$$\mathcal{L}_{\text{weighted}} = -\frac{1}{n}\sum_{i=1}^n \left[w_1 \cdot y_i\log(p_i) + w_0 \cdot (1-y_i)\log(1-p_i)\right]$$</p>
<p>dove i pesi sono proporzionali ai costi:
$$w_1 = L_{FN}, \quad w_0 = L_{FP}$$</p>
<p><strong>Class Rebalancing</strong>:
Alternativamente, possiamo ri-pesare le classi per compensare lo sbilanciamento:
$$w_c = \frac{n}{C \cdot n_c}$$
dove $C$ √® il numero di classi e $n_c$ il numero di esempi della classe $c$.</p>
<h2 id="9-classificazione-multi-classe">9. Classificazione Multi-Classe</h2>
<h3 id="91-estensione-della-matrice-di-confusione">9.1 Estensione della Matrice di Confusione</h3>
<p>Per $C$ classi, la matrice di confusione √® $C \times C$:</p>
$$\text{CM}[i,j] = \text{numero di esempi con classe reale } i \text{ predetti come } j$$
<p><strong>Diagonale</strong>: Predizioni corrette
<strong>Fuori diagonale</strong>: Errori</p>
<h3 id="92-metriche-per-classe">9.2 Metriche Per-Classe</h3>
<p>Per ogni classe $c$, definiamo:</p>
<p><strong>Precision per classe $c$</strong>:
$$\text{Precision}_c = \frac{TP_c}{TP_c + FP_c} = \frac{\text{CM}[c,c]}{\sum_i \text{CM}[i,c]}$$</p>
<p><strong>Recall per classe $c$</strong>:
$$\text{Recall}_c = \frac{TP_c}{TP_c + FN_c} = \frac{\text{CM}[c,c]}{\sum_j \text{CM}[c,j]}$$</p>
<p><strong>F1 per classe $c$</strong>:
$$F1_c = \frac{2 \cdot \text{Precision}_c \cdot \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}$$</p>
<h3 id="93-aggregazione-macro-vs-micro-vs-weighted">9.3 Aggregazione: Macro vs Micro vs Weighted</h3>
<p><strong>Macro-Average</strong> (media semplice):
$$\text{Macro-Precision} = \frac{1}{C}\sum_{c=1}^C \text{Precision}_c$$</p>
<p><strong>Interpretazione</strong>: Ogni classe ha peso uguale, indipendentemente dalla sua frequenza.
<strong>Uso</strong>: Dataset bilanciati, tutte le classi sono ugualmente importanti.</p>
<p><strong>Micro-Average</strong> (aggregazione globale):
$$\text{Micro-Precision} = \frac{\sum_{c=1}^C TP_c}{\sum_{c=1}^C (TP_c + FP_c)}$$</p>
<p><strong>Interpretazione</strong>: Ogni esempio ha peso uguale.
<strong>Uso</strong>: Dataset sbilanciati, classi maggioritarie sono pi√π importanti.</p>
<p><strong>Weighted-Average</strong> (pesata per frequenza):
$$\text{Weighted-Precision} = \sum_{c=1}^C \frac{n_c}{n} \cdot \text{Precision}_c$$</p>
<p><strong>Interpretazione</strong>: Peso proporzionale alla dimensione della classe.
<strong>Uso</strong>: Compromesso tra macro e micro.</p>
<p><strong>Esempio</strong>:</p>
<p>3 classi: A (100 esempi), B (50 esempi), C (10 esempi)</p>
<table>
<thead>
<tr>
<th>Classe</th>
<th>Precision</th>
<th>Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0.90</td>
<td>0.85</td>
</tr>
<tr>
<td>B</td>
<td>0.80</td>
<td>0.75</td>
</tr>
<tr>
<td>C</td>
<td>0.50</td>
<td>0.40</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Macro-Precision</strong> = $(0.90 + 0.80 + 0.50)/3 = 0.73$</li>
<li><strong>Micro-Precision</strong> = $(90 + 40 + 5)/(100 + 50 + 10) = 0.84$</li>
<li><strong>Weighted-Precision</strong> = $0.90 \cdot \frac{100}{160} + 0.80 \cdot \frac{50}{160} + 0.50 \cdot \frac{10}{160} = 0.85$</li>
</ul>
<h3 id="94-one-vs-rest-e-one-vs-one">9.4 One-vs-Rest e One-vs-One</h3>
<p><strong>One-vs-Rest (OvR)</strong>:
- Per ogni classe $c$, creiamo un problema binario: classe $c$ vs tutte le altre
- Calcoliamo metriche binarie per ciascun problema
- Aggreghiamo con macro/micro/weighted</p>
<p><strong>One-vs-One (OvO)</strong>:
- Per ogni coppia di classi $(c_i, c_j)$, creiamo un classificatore binario
- Totale: $\binom{C}{2} = \frac{C(C-1)}{2}$ classificatori
- Utile per SVM multi-classe</p>
<h3 id="95-matthews-correlation-coefficient-multi-classe">9.5 Matthews Correlation Coefficient Multi-Classe</h3>
<p>L&rsquo;MCC pu√≤ essere esteso al caso multi-classe:</p>
$$\text{MCC} = \frac{\sum_{k,l,m} C_{kk}C_{lm} - C_{kl}C_{mk}}{\sqrt{\sum_k\left(\sum_l C_{kl}\right)\left(\sum_{k'\neq k}\sum_{l'}C_{k'l'}\right)} \cdot \sqrt{\sum_k\left(\sum_l C_{lk}\right)\left(\sum_{k'\neq k}\sum_{l'}C_{l'k'}\right)}}$$
<p>dove $C$ √® la matrice di confusione.</p>
<p><strong>Interpretazione</strong>: Generalizzazione del coefficiente di correlazione al caso multi-classe.</p>
<p><strong>Range</strong>: $[-1, +1]$ come nel caso binario.</p>
<h2 id="10-guida-pratica-alla-scelta-delle-metriche">10. Guida Pratica alla Scelta delle Metriche</h2>
<h3 id="101-albero-decisionale">10.1 Albero Decisionale</h3>
<details class="code-container">
<summary>Code</summary>
<div class="code-wrapper">
<button class="copy-button" onclick="
                const code = this.parentElement.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.innerText);
                    this.textContent = 'Copied!';
                    setTimeout(() => this.textContent = 'Copy', 2000);
                }
            ">Copy</button>
<div class="codehilite"><pre><span></span><code>Dataset bilanciato?
‚îú‚îÄ S√¨
‚îÇ  ‚îú‚îÄ Interessa solo accuracy? ‚Üí Accuracy, Balanced Accuracy
‚îÇ  ‚îî‚îÄ Serve probabilit√†? ‚Üí Log Loss, Brier Score, Calibration
‚îÇ
‚îî‚îÄ No (sbilanciato)
   ‚îú‚îÄ Qual √® la classe di interesse?
   ‚îÇ  ‚îú‚îÄ Classe rara (positiva)
   ‚îÇ  ‚îÇ  ‚îú‚îÄ FN molto costosi? ‚Üí Recall, F2, PR-AUC
   ‚îÇ  ‚îÇ  ‚îú‚îÄ FP molto costosi? ‚Üí Precision, F0.5
   ‚îÇ  ‚îÇ  ‚îî‚îÄ Bilanciamento? ‚Üí F1, MCC
   ‚îÇ  ‚îÇ
   ‚îÇ  ‚îî‚îÄ Entrambe le classi importanti ‚Üí Balanced Accuracy, MCC, Cohen&#39;s Kappa
   ‚îÇ
   ‚îî‚îÄ Serve valutazione threshold-independent? ‚Üí ROC-AUC (se moderatamente sbilanciato), PR-AUC (se molto sbilanciato)
</code></pre></div>
</div>
</details>

<h3 id="102-raccomandazioni-per-dominio">10.2 Raccomandazioni per Dominio</h3>
<p><strong>Medicina (Screening)</strong>:
- <strong>Primarie</strong>: Recall, Sensitivity, F2
- <strong>Secondarie</strong>: Specificity, PR-AUC
- <strong>Perch√©</strong>: FN (mancata diagnosi) sono critici</p>
<p><strong>Medicina (Diagnostica Definitiva)</strong>:
- <strong>Primarie</strong>: Balanced Accuracy, MCC, F1
- <strong>Secondarie</strong>: Specificity, PPV
- <strong>Perch√©</strong>: Bilanciamento tra evitare trattamenti inutili e non perdere malati</p>
<p><strong>Fraud Detection</strong>:
- <strong>Primarie</strong>: Precision@K, PR-AUC, F1.5
- <strong>Secondarie</strong>: Recall, ROC-AUC
- <strong>Perch√©</strong>: FN costosi (perdite economiche), ma serve precision ragionevole</p>
<p><strong>Spam Filtering</strong>:
- <strong>Primarie</strong>: Precision, F0.5
- <strong>Secondarie</strong>: FPR, Specificity
- <strong>Perch√©</strong>: FP (email legittime in spam) sono inaccettabili</p>
<p><strong>Information Retrieval</strong>:
- <strong>Primarie</strong>: MAP (Mean Average Precision), NDCG, Precision@K
- <strong>Secondarie</strong>: Recall@K, F1
- <strong>Perch√©</strong>: Focus su top-K risultati e qualit√† del ranking</p>
<p><strong>Computer Vision (Classification)</strong>:
- <strong>Bilanciato</strong>: Top-1 Accuracy, Top-5 Accuracy
- <strong>Sbilanciato</strong>: Macro-F1, Per-class metrics
- <strong>Perch√©</strong>: Dipende dal numero e bilanciamento delle classi</p>
<p><strong>Sentiment Analysis / NLP</strong>:
- <strong>Primarie</strong>: Macro-F1, Weighted-F1
- <strong>Secondarie</strong>: Per-class Precision/Recall, Confusion Matrix
- <strong>Perch√©</strong>: Classi spesso sbilanciate, tutte le sentiment importanti</p>
<h3 id="103-checklist-di-valutazione">10.3 Checklist di Valutazione</h3>
<p>Prima di scegliere le metriche, rispondi a:</p>
<ol>
<li><strong>Dataset √® bilanciato?</strong></li>
<li>[ ] S√¨ (Accuracy OK)</li>
<li>
<p>[ ] No (evitare Accuracy)</p>
</li>
<li>
<p><strong>Costi asimmetrici?</strong></p>
</li>
<li>[ ] FP pi√π costosi ‚Üí enfatizza Precision</li>
<li>[ ] FN pi√π costosi ‚Üí enfatizza Recall</li>
<li>
<p>[ ] Bilanciati ‚Üí F1, MCC</p>
</li>
<li>
<p><strong>Soglia fissa o variabile?</strong></p>
</li>
<li>[ ] Fissa ‚Üí metriche a soglia fissata (Precision, Recall, F1)</li>
<li>
<p>[ ] Variabile ‚Üí curve (ROC, PR)</p>
</li>
<li>
<p><strong>Serve calibrazione?</strong></p>
</li>
<li>[ ] S√¨ ‚Üí Log Loss, Brier Score, ECE, Reliability Diagram</li>
<li>
<p>[ ] No ‚Üí solo discriminazione</p>
</li>
<li>
<p><strong>Multi-classe?</strong></p>
</li>
<li>[ ] Macro (classi ugualmente importanti)</li>
<li>[ ] Micro (esempi ugualmente importanti)</li>
<li>[ ] Weighted (compromesso)</li>
</ol>
<h3 id="104-metriche-da-riportare-sempre">10.4 Metriche da Riportare Sempre</h3>
<p><strong>Minimo indispensabile</strong>:
1. Matrice di confusione (visualizzazione completa)
2. Almeno 2 metriche complementari (e.g., Precision + Recall, o F1 + MCC)
3. Curva appropriata (ROC o PR) con AUC</p>
<p><strong>Report completo</strong>:
1. Confusion matrix
2. Precision, Recall, F1
3. ROC curve + AUC-ROC
4. PR curve + AUC-PR (se sbilanciato)
5. MCC o Cohen&rsquo;s Kappa
6. Calibration plot + ECE (se probabilistico)
7. Per-class metrics (se multi-classe)</p>
<h3 id="105-errori-comuni-da-evitare">10.5 Errori Comuni da Evitare</h3>
<p><strong>‚ùå Usare solo Accuracy su dataset sbilanciato</strong>
- Un modello dummy pu√≤ avere accuracy alta</p>
<p><strong>‚ùå Ignorare la calibrazione</strong>
- AUC alta non implica probabilit√† ben calibrate</p>
<p><strong>‚ùå Ottimizzare solo una metrica</strong>
- Trade-off impliciti possono nascondere problemi</p>
<p><strong>‚ùå Non considerare i costi reali</strong>
- FP e FN raramente hanno stesso costo</p>
<p><strong>‚ùå Confrontare modelli con metriche diverse</strong>
- Usare stesse metriche per confronti fair</p>
<p><strong>‚ùå Dimenticare intervalli di confidenza</strong>
- Report puntuale senza incertezza √® fuorviante</p>
<p><strong>‚ùå Usare test set per tuning</strong>
- Porta a overfitting ottimistico</p>
<p><strong>‚úÖ Best Practices</strong>:
1. Sempre riportare confusion matrix
2. Usare multiple metriche complementari
3. Considerare i costi del dominio applicativo
4. Validare calibrazione se si usano probabilit√†
5. Report con confidence intervals (bootstrap o cross-validation)
6. Mantenere test set completamente holdout</p>
<h2 id="riferimenti-e-risorse">Riferimenti e Risorse</h2>
<p><strong>Paper fondamentali</strong>:
- Provost, F., Fawcett, T. (2001). &ldquo;Robust Classification for Imprecise Environments&rdquo;
- Davis, J., Goadrich, M. (2006). &ldquo;The Relationship Between Precision-Recall and ROC Curves&rdquo;
- Chicco, D., Jurman, G. (2020). &ldquo;The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation&rdquo;</p>
<p><strong>Libri consigliati</strong>:
- Murphy, K. P. (2022). &ldquo;Probabilistic Machine Learning: An Introduction&rdquo;
- Hastie, T., Tibshirani, R., Friedman, J. (2009). &ldquo;The Elements of Statistical Learning&rdquo;
- Bishop, C. M. (2006). &ldquo;Pattern Recognition and Machine Learning&rdquo;</p>
<p><strong>Strumenti software</strong>:
- <code>sklearn.metrics</code> (Python): Implementazione completa
- <code>ROCR</code> (R): Visualizzazione ROC/PR
- <code>calibration</code> (Python): Post-processing calibration</p>
            </div>
            
            <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
                <p><strong>Keywords:</strong> machine learning, introduction, basics, fundamentals, AI, model, data, neural</p>
                <p><small>This is the SEO-optimized version. <a href="http://localhost:3000/theory/introduction/Tipologie di Problemi/Classificazione/Classification Metrics">Click here for the interactive experience</a>.</small></p>
            </footer>
        </article>
    </div>
    
    <!-- Vercel Analytics (opzionale) -->
    <script>
      // Track SEO page views
      if (window.gtag) {
        gtag('config', 'GA_TRACKING_ID', {
          page_title: 'Metriche di Valutazione per Classificazione in Machine Learning',
          page_location: 'http://localhost:3000/theory/introduction/Tipologie di Problemi/Classificazione/Classification Metrics'
        });
      }
    </script>
</body>
</html>