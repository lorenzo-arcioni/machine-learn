{
  "title": "The Berkeley Restaurant Project (BERP) Corpus",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"descrizione-generale\">Descrizione Generale</h2>\n<p>Il <strong>BERP</strong> (Berkeley Restaurant Project) è un corpus utilizzato nell&rsquo;ambito del <strong>Natural Language Processing (NLP)</strong>, in particolare per modellare e analizzare il linguaggio in contesti legati al cibo e alla ristorazione.<br />\nIl corpus contiene query poste dagli utenti, per esempio:<br />\n- <em>I’m looking for Cantonese food</em><br />\n- <em>I’d like to eat dinner someplace nearby</em><br />\n- <em>Tell me about Chez Panisse</em><br />\n- <em>I’m looking for a good place to eat breakfast</em>  </p>\n<p>Questo dataset è impiegato per sviluppare modelli probabilistici del linguaggio, permettendo di stimare la probabilità di frasi, analizzare le frequenze delle parole (unigrammi) e le associazioni tra di esse (bigrammi).</p>\n<h2 id=\"modellazione-probabilistica-con-n-grammi\">Modellazione Probabilistica con N-grammi</h2>\n<h3 id=\"calcolo-della-probabilita-con-il-modello-bigram\">Calcolo della Probabilità con il Modello Bigram</h3>\n<p>Assumendo l&rsquo;indipendenza dei bigrammi (Markov Property) e applicando la regola della catena, la probabilità di una frase viene approssimata moltiplicando le probabilità condizionali dei singoli bigrammi.<br />\nPer esempio, per la frase modificata:\n$$\nP(\\langle s \\rangle\\text{I want Chinese food}) \\approx P(I|\\langle s \\rangle) \\cdot P(want|I) \\cdot P(Chinese|want) \\cdot P(food|Chinese) \\cdot P(\\langle /s \\rangle|food)\n$$</p>\n<h2 id=\"tabelle-di-conteggio\">Tabelle di Conteggio</h2>\n<p>Le tabelle di conteggio sono utilizzate per calcolare le probabilità dei bigrammi e degli unigrammi. Scegliamo ora solo alcune (nella realtà vanno scelte tutte) parole (unigrams, vettore $\\mathbf u$) e le coppie di parole (bigrams, matrice $\\mathbf B$), e contiamo il numero di volte che appaiono nel corpus.</p>\n<p>Le parole selezionate sono:</p>\n<ul>\n<li><code>&lt;s&gt;</code> (inizio frase)</li>\n<li><code>i</code></li>\n<li><code>want</code></li>\n<li><code>to</code></li>\n<li><code>eat</code></li>\n<li><code>chinese</code></li>\n<li><code>food</code></li>\n<li><code>lunch</code></li>\n<li><code>spend</code></li>\n<li><code>&lt;/s&gt;</code> (fine frase)</li>\n</ul>\n<h3 id=\"conteggio-degli-unigrammi\">Conteggio degli Unigrammi</h3>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>$\\langle s \\rangle$</th>\n<th>i</th>\n<th>want</th>\n<th>to</th>\n<th>eat</th>\n<th>chinese</th>\n<th>food</th>\n<th>lunch</th>\n<th>spend</th>\n<th>$\\langle /s \\rangle$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Count</td>\n<td>8566</td>\n<td>2816</td>\n<td>1038</td>\n<td>2711</td>\n<td>829</td>\n<td>193</td>\n<td>1242</td>\n<td>392</td>\n<td>310</td>\n<td>8566</td>\n</tr>\n</tbody>\n</table>\n<p>Chiameremo questo vettore $\\mathbf{u}$.</p>\n<h3 id=\"conteggio-dei-bigrammi\">Conteggio dei Bigrammi</h3>\n<p>In questa tabella, includiamo <code>&lt;s&gt;</code> come riga iniziale e <code>&lt;/s&gt;</code> come colonna finale:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>$\\langle s \\rangle$</th>\n<th>i</th>\n<th>want</th>\n<th>to</th>\n<th>eat</th>\n<th>chinese</th>\n<th>food</th>\n<th>lunch</th>\n<th>spend</th>\n<th>$\\langle /s \\rangle$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>$\\langle s \\rangle$</strong></td>\n<td>0</td>\n<td>1922</td>\n<td>4</td>\n<td>32</td>\n<td>4</td>\n<td>10</td>\n<td>4</td>\n<td>39</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>i</strong></td>\n<td>0</td>\n<td>1</td>\n<td>908</td>\n<td>0</td>\n<td>12</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>want</strong></td>\n<td>0</td>\n<td>2</td>\n<td>0</td>\n<td>673</td>\n<td>0</td>\n<td>7</td>\n<td>6</td>\n<td>6</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td><strong>to</strong></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>2</td>\n<td>753</td>\n<td>3</td>\n<td>0</td>\n<td>6</td>\n<td>233</td>\n<td>3</td>\n</tr>\n<tr>\n<td><strong>eat</strong></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>16</td>\n<td>2</td>\n<td>52</td>\n<td>0</td>\n<td>10</td>\n</tr>\n<tr>\n<td><strong>chinese</strong></td>\n<td>0</td>\n<td>4</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>99</td>\n<td>1</td>\n<td>0</td>\n<td>10</td>\n</tr>\n<tr>\n<td><strong>food</strong></td>\n<td>0</td>\n<td>14</td>\n<td>0</td>\n<td>13</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>806</td>\n</tr>\n<tr>\n<td><strong>lunch</strong></td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>221</td>\n</tr>\n<tr>\n<td><strong>spend</strong></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>8</td>\n</tr>\n<tr>\n<td><strong>$\\langle /s \\rangle$</strong></td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>Chiameremo questa matrice $\\mathbf{B}$.</p>\n<h3 id=\"probabilita-dei-bigrammi-conteggio-normalizzato\">Probabilità dei Bigrammi (conteggio normalizzato)</h3>\n<p>Per ottenere le probabilità dei bigrammi, si divide il conteggio del bigramma per il conteggio dell&rsquo;unigramma del prefisso. Ad esempio, per il bigramma &ldquo;i want&rdquo; abbiamo:</p>\n$$\nP(\\text{want} \\mid \\text{i}) = \\frac{908}{2816} \\approx 0.32\n$$\n<p>La matrice normalizzata $\\mathbf{N}$ (contenente le probabilità) sarà strutturata in modo analogo, includendo le colonne e righe per <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code>:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>$\\langle s \\rangle$</th>\n<th>i</th>\n<th>want</th>\n<th>to</th>\n<th>eat</th>\n<th>chinese</th>\n<th>food</th>\n<th>lunch</th>\n<th>spend</th>\n<th>$\\langle /s \\rangle$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>$\\langle s \\rangle$</strong></td>\n<td>0.0</td>\n<td>0.224375</td>\n<td>0.000467</td>\n<td>0.003736</td>\n<td>0.000467</td>\n<td>0.001167</td>\n<td>0.000467</td>\n<td>0.004553</td>\n<td>0.000117</td>\n<td>0.000000</td>\n</tr>\n<tr>\n<td><strong>i</strong></td>\n<td>0.0</td>\n<td>0.000355</td>\n<td>0.322443</td>\n<td>0.000000</td>\n<td>0.004261</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000710</td>\n<td>0.000000</td>\n</tr>\n<tr>\n<td><strong>want</strong></td>\n<td>0.0</td>\n<td>0.001927</td>\n<td>0.000000</td>\n<td>0.648362</td>\n<td>0.000000</td>\n<td>0.006744</td>\n<td>0.005780</td>\n<td>0.005780</td>\n<td>0.000963</td>\n<td>0.001927</td>\n</tr>\n<tr>\n<td><strong>to</strong></td>\n<td>0.0</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000738</td>\n<td>0.277757</td>\n<td>0.001107</td>\n<td>0.000000</td>\n<td>0.002213</td>\n<td>0.085946</td>\n<td>0.001107</td>\n</tr>\n<tr>\n<td><strong>eat</strong></td>\n<td>0.0</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.019300</td>\n<td>0.002413</td>\n<td>0.062726</td>\n<td>0.000000</td>\n<td>0.012063</td>\n</tr>\n<tr>\n<td><strong>chinese</strong></td>\n<td>0.0</td>\n<td>0.020725</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.512953</td>\n<td>0.005181</td>\n<td>0.000000</td>\n<td>0.051813</td>\n</tr>\n<tr>\n<td><strong>food</strong></td>\n<td>0.0</td>\n<td>0.011272</td>\n<td>0.000000</td>\n<td>0.010467</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.648953</td>\n</tr>\n<tr>\n<td><strong>lunch</strong></td>\n<td>0.0</td>\n<td>0.002551</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.002551</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.563776</td>\n</tr>\n<tr>\n<td><strong>spend</strong></td>\n<td>0.0</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.003226</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.025806</td>\n</tr>\n<tr>\n<td><strong>$\\langle /s \\rangle$</strong></td>\n<td>0.0</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n</tr>\n</tbody>\n</table>\n<p><em>Nota:</em> I token <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code> sono inclusi solo nelle matrici normalizzate per evidenziare la probabilità di inizio e fine frase.</p>\n<p>Questa matrice $\\mathbf N$ è ottenuta semplicemente calcolando:\n$$\n\\mathbf N_{ij} = \\frac{\\mathbf B_{ij}}{\\mathbf u_i}\n$$</p>\n<h2 id=\"calcolo-della-probabilita-di-frasi-specifiche\">Calcolo della Probabilità di Frasi Specifiche</h2>\n<p>Per stimare la probabilità di una frase con un modello bigramma, si moltiplicano le probabilità condizionali dei bigrammi, includendo i token di inizio (<code>&lt;s&gt;</code>) e fine (<code>&lt;/s&gt;</code>). In generale, per una frase:\n$$\n\\text{frase} = \\langle s \\rangle \\; w_1 \\; w_2 \\; \\dots \\; w_n \\; \\langle /s \\rangle\n$$\nla probabilità stimata è:\n$$\nP(\\text{frase}) = P(w_1 \\mid \\langle s \\rangle) \\cdot P(w_2 \\mid w_1) \\cdots P(\\langle /s \\rangle \\mid w_n)\n$$</p>\n<h3 id=\"frase-i-want-chinese-food\">Frase: &ldquo;I want Chinese food&rdquo;</h3>\n<p>Utilizziamo i valori aggiornati dalla matrice per stimare la probabilità (trasformando sempre le lettere in lower case):</p>\n<ul>\n<li>$P(i \\mid \\langle s \\rangle) = 0.224375$  </li>\n<li>$P(want \\mid i) = 0.322443$  </li>\n<li>$P(chinese \\mid want) = 0.006744$  </li>\n<li>$P(food \\mid chinese) = 0.512953$  </li>\n<li>$P(\\langle /s \\rangle \\mid food) = 0.648953$  </li>\n</ul>\n<p>La probabilità della frase è:\n$$\n\\begin{aligned}\nP(\\langle s \\rangle\\, i\\, want\\, chinese\\, food\\, \\langle /s \\rangle) &= P(i \\mid \\langle s \\rangle) \\cdot P(want \\mid i) \\cdot P(chinese \\mid want) \\\\\n&\\quad \\cdot P(food \\mid chinese) \\cdot P(\\langle /s \\rangle \\mid food) \\\\\n&= 0.224375 \\cdot 0.322443 \\cdot 0.006744 \\cdot 0.512953 \\cdot 0.648953 \\\\\n&\\approx 0.000162\n\\end{aligned}\n$$</p>\n<h2 id=\"conclusioni-cosa-ci-insegnano-gli-n-grammi\">Conclusioni: Cosa Ci Insegnano gli N-grammi</h2>\n<p>Nonostante la semplicità, i modelli basati su N-grammi riescono a catturare informazioni interessanti riguardo al linguaggio:</p>\n<ul>\n<li><strong>Fatti Linguistici:</strong>  </li>\n<li>$P(English \\mid want) = 0$, che rappresenta un problema, in quanto non compare nel corpus il bigramma &ldquo;<em>want English</em>&rdquo;.</li>\n<li>$P(Chinese \\mid want) \\approx 0.0067$  </li>\n<li>\n<p>$P(to \\mid want)$ (valore elevato nei dati originali)</p>\n</li>\n<li>\n<p><strong>Conoscenza del Mondo:</strong>  </p>\n</li>\n<li>$P(eat \\mid to) \\approx 0.2778$ (da altri esempi)  </li>\n<li>\n<p>$P(food \\mid to) \\approx 0$ (in certi casi)</p>\n</li>\n<li>\n<p><strong>Sintassi:</strong>  </p>\n</li>\n<li>$P(want \\mid spend) = 0$  </li>\n<li>\n<p>$P(I \\mid \\langle s \\rangle) \\approx 0.2244$</p>\n</li>\n<li>\n<p><strong>Discorso:</strong><br />\n  Le probabilità riflettono le relazioni contestuali e il flusso del discorso, evidenziando come alcuni bigrammi siano molto probabili (come quelli che iniziano con <code>&lt;s&gt;</code>) mentre altri risultano meno frequenti o addirittura impossibili.</p>\n</li>\n</ul>\n<h2 id=\"laplace-smoothing\">Laplace Smoothing</h2>\n<p>Applichiamo ora il Laplace <span class=\"text-gray-600\">Smoothing</span> alla matrice di conteggio dei bigrammi per risolvere il problema dei bigrammi non osservati nel corpus, che hanno come probabilità 0.</p>\n<h3 id=\"1-aggiunta-del-contatore-per-il-laplace-smoothing\">1. Aggiunta del Contatore per il Laplace Smoothing</h3>\n<p>Per applicare il Laplace smoothing, aggiungiamo 1 a ciascuna cella della matrice $\\mathbf{B}$:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>$\\langle s \\rangle$</th>\n<th>i</th>\n<th>want</th>\n<th>to</th>\n<th>eat</th>\n<th>chinese</th>\n<th>food</th>\n<th>lunch</th>\n<th>spend</th>\n<th>$\\langle /s \\rangle$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>$\\langle s \\rangle$</strong></td>\n<td>1</td>\n<td>1923</td>\n<td>5</td>\n<td>33</td>\n<td>5</td>\n<td>11</td>\n<td>5</td>\n<td>40</td>\n<td>2</td>\n<td>1</td>\n</tr>\n<tr>\n<td><strong>i</strong></td>\n<td>1</td>\n<td>2</td>\n<td>909</td>\n<td>1</td>\n<td>13</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>3</td>\n<td>1</td>\n</tr>\n<tr>\n<td><strong>want</strong></td>\n<td>1</td>\n<td>3</td>\n<td>1</td>\n<td>674</td>\n<td>1</td>\n<td>8</td>\n<td>7</td>\n<td>7</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td><strong>to</strong></td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>3</td>\n<td>754</td>\n<td>4</td>\n<td>1</td>\n<td>7</td>\n<td>234</td>\n<td>4</td>\n</tr>\n<tr>\n<td><strong>eat</strong></td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>17</td>\n<td>3</td>\n<td>53</td>\n<td>1</td>\n<td>11</td>\n</tr>\n<tr>\n<td><strong>chinese</strong></td>\n<td>1</td>\n<td>5</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>100</td>\n<td>2</td>\n<td>1</td>\n<td>11</td>\n</tr>\n<tr>\n<td><strong>food</strong></td>\n<td>1</td>\n<td>15</td>\n<td>1</td>\n<td>14</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>807</td>\n</tr>\n<tr>\n<td><strong>lunch</strong></td>\n<td>1</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>222</td>\n</tr>\n<tr>\n<td><strong>spend</strong></td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>9</td>\n</tr>\n<tr>\n<td><strong>$\\langle /s \\rangle$</strong></td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-calcolo-delle-probabilita-smoothing\">2. Calcolo delle Probabilità Smoothing</h3>\n<p>Per ogni bigramma $(w_{n-1}, w_n)$ il Laplace smoothing prevede:</p>\n$$\n\\mathbb P(w_n \\mid w_{n-1}) = \\frac{c(w_{n-1}, w_n) + 1}{c(w_{n-1}) + V}\n$$\n<p>dove:\n- $c(w_{n-1}, w_n)$ è il conteggio (già incrementato di 1) per il bigramma;\n- $c(w_{n-1})$ è il totale dei conteggi per il contesto $w_{n-1}$ (ottenibile dal vettore delle frequenze degli unigrammi $\\mathbf{u}$);\n- $V$ è la dimensione del vocabolario (in questo caso, $V=1997$).</p>\n<p><strong>Esempio di Calcolo:</strong></p>\n<p>Supponiamo di voler calcolare la probabilità condizionata del bigramma (&ldquo;i&rdquo;, &ldquo;want&rdquo;).<br />\nDalla riga relativa a &ldquo;i&rdquo; abbiamo:\n- Valore incrementato per (&ldquo;i&rdquo;, &ldquo;want&rdquo;) = 908<br />\n- Totale dei conteggi per il contesto &ldquo;i&rdquo;:<br />\n  $$\n  c(\"i\") = \\mathbf u_i = 2816.\n  $$</p>\n<p>Quindi:</p>\n$$\n\\mathbb P(\\text{\"want\"} \\mid \\text{\"i\"}) = \\frac{\\overbrace{909}^{908+1}}{2816 + 1997} \\approx 0.19.\n$$\n<p>e quindi la probabilità $\\mathbb P(\"i\", \"want\") = \\mathbb P(\\text{\"want\"} \\mid \\text{\"i\"}) \\cdot \\mathbb P(\"i\")$.</p>\n<h3 id=\"3-costruzione-della-matrice-di-probabilita-smoothed-math_inline_52\">3. Costruzione della Matrice di Probabilità Smoothed $\\mathbf{B^*}$</h3>\n<p>Una volta applicata la formula per ogni cella (per ogni bigramma), la matrice $\\mathbf{B^*}$ conterrà le probabilità smoothed:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>$\\langle s \\rangle$</th>\n<th>i</th>\n<th>want</th>\n<th>to</th>\n<th>eat</th>\n<th>chinese</th>\n<th>food</th>\n<th>lunch</th>\n<th>spend</th>\n<th>$\\langle /s \\rangle$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>$\\langle s \\rangle$</strong></td>\n<td>0.000095</td>\n<td>0.182051</td>\n<td>0.000473</td>\n<td>0.000095</td>\n<td>0.000473</td>\n<td>0.001041</td>\n<td>0.000473</td>\n<td>0.003787</td>\n<td>0.000189</td>\n<td>0.000095</td>\n</tr>\n<tr>\n<td><strong>i</strong></td>\n<td>0.000208</td>\n<td>0.000416</td>\n<td>0.188863</td>\n<td>0.000208</td>\n<td>0.002701</td>\n<td>0.000208</td>\n<td>0.000208</td>\n<td>0.000208</td>\n<td>0.000623</td>\n<td>0.000208</td>\n</tr>\n<tr>\n<td><strong>want</strong></td>\n<td>0.000329</td>\n<td>0.000988</td>\n<td>0.000329</td>\n<td>0.000329</td>\n<td>0.000329</td>\n<td>0.002636</td>\n<td>0.002306</td>\n<td>0.002306</td>\n<td>0.000659</td>\n<td>0.000988</td>\n</tr>\n<tr>\n<td><strong>to</strong></td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n<td>0.000501</td>\n</tr>\n<tr>\n<td><strong>eat</strong></td>\n<td>0.000354</td>\n<td>0.000354</td>\n<td>0.000354</td>\n<td>0.000354</td>\n<td>0.000354</td>\n<td>0.006016</td>\n<td>0.001062</td>\n<td>0.018754</td>\n<td>0.000354</td>\n<td>0.003892</td>\n</tr>\n<tr>\n<td><strong>chinese</strong></td>\n<td>0.000457</td>\n<td>0.002283</td>\n<td>0.000457</td>\n<td>0.000457</td>\n<td>0.000457</td>\n<td>0.000457</td>\n<td>0.045662</td>\n<td>0.000913</td>\n<td>0.000457</td>\n<td>0.005023</td>\n</tr>\n<tr>\n<td><strong>food</strong></td>\n<td>0.000309</td>\n<td>0.004631</td>\n<td>0.000309</td>\n<td>0.000309</td>\n<td>0.000309</td>\n<td>0.000309</td>\n<td>0.000309</td>\n<td>0.000309</td>\n<td>0.000309</td>\n<td>0.249151</td>\n</tr>\n<tr>\n<td><strong>lunch</strong></td>\n<td>0.000419</td>\n<td>0.000837</td>\n<td>0.000419</td>\n<td>0.000419</td>\n<td>0.000419</td>\n<td>0.000419</td>\n<td>0.000837</td>\n<td>0.000419</td>\n<td>0.000419</td>\n<td>0.092926</td>\n</tr>\n<tr>\n<td><strong>spend</strong></td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.000433</td>\n<td>0.003901</td>\n</tr>\n<tr>\n<td><strong>$\\langle /s \\rangle$</strong></td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n<td>0.000095</td>\n</tr>\n</tbody>\n</table>\n<p>In questo modo, le frasi che prima avevano una probabilità nulla (ma comunque non impossibili nel linguaggio naturale), ora hanno una probabilità non nulla.</p>\n<h3 id=\"frase-i-want-to-want-to-eat-chinese-food\">Frase: &ldquo;I want to want to eat Chinese food.&rdquo;</h3>\n<p>Utilizziamo i valori aggiornati dalla matrice (trasformando tutte le parole in lower case) per stimare la probabilità della frase.<br />\nAttenzione: nella tabella il token &ldquo;to&rdquo; è indicato come &ldquo;ot&rdquo;. Quindi, nel calcolo, sostituiamo “to” con “ot” dove necessario.</p>\n<p>I passaggi sono i seguenti:</p>\n<ul>\n<li>$P(i \\mid \\langle s \\rangle) = 0.182051$  </li>\n<li>$P(want \\mid i) = 0.188863$  </li>\n<li>$P(to \\mid want) = 0.000329$  </li>\n<li>$P(want \\mid to) = 0.000501$  </li>\n<li>$P(to \\mid want) = 0.000329$  </li>\n<li>$P(eat \\mid to) = 0.000501$  </li>\n<li>$P(chinese \\mid eat) = 0.006016$  </li>\n<li>$P(food \\mid chinese) = 0.045662$  </li>\n<li>$P(\\langle /s \\rangle \\mid food) = 0.249151$  </li>\n</ul>\n<p>La struttura della frase (includendo i token di inizio e fine frase) è:</p>\n$$\n\\langle s \\rangle \\; i \\; want \\; to \\; want \\; to \\; eat \\; chinese \\; food \\; \\langle /s \\rangle\n$$\n<p>La probabilità complessiva della frase è data dal prodotto dei singoli passaggi:</p>\n$$\n\\begin{aligned}\nP(\\langle s \\rangle\\, i\\, want\\, to\\, want\\, to\\, eat\\, chinese\\, food\\, \\langle /s \\rangle) &= P(i \\mid \\langle s \\rangle) \\cdot P(want \\mid i) \\cdot P(to \\mid want) \\\\\n&\\quad \\cdot P(want \\mid to) \\cdot P(to \\mid want) \\cdot P(eat \\mid to) \\\\\n&\\quad \\cdot P(chinese \\mid eat) \\cdot P(food \\mid chinese) \\cdot P(\\langle /s \\rangle \\mid food) \\\\\n&= 0.182051 \\cdot 0.188863 \\cdot 0.000329 \\cdot 0.000501 \\cdot 0.000329 \\cdot 0.000501 \\\\\n&\\quad \\cdot 0.006016 \\cdot 0.045662 \\cdot 0.249151 \\\\\n&\\approx 6.39 \\times 10^{-20}\n\\end{aligned}\n$$\n<p>Questa frase, pur non essendo particolarmente sensata, è corretta dal punto di vista del linguaggio. Tuttavia, utilizzando il modello bigramma senza smoothing, la probabilità stimata della frase sarebbe nulla.</p>\n<p>Grazie allo smoothing, invece, il modello bigramma riesce a stimare la probabilità della frase in modo più corretto e sensato.</p>\n<h2 id=\"argomenti-correlati\">Argomenti Correlati</h2>\n<ul>\n<li><a href=\"/theory/nlp/Modelli di Linguaggio/Modelli di Linguaggio\" class=\"text-blue-600 hover:underline\">Modelli di Linguaggio</a></li>\n<li><a href=\"/theory/nlp/Parole, Corpora, Tokenizzazione e Normalizzazione\" class=\"text-blue-600 hover:underline\">Parole, Corpora, Tokenizzazione e Normalizzazione</a>  </li>\n<li><a href=\"/theory/nlp/Modelli di Linguaggio/Smoothing nei Modelli Linguistici\" class=\"text-blue-600 hover:underline\">Smoothing nei Modelli Linguistici</a>   </li>\n<li><a href=\"/theory/nlp/Modelli di Linguaggio/Valutazione dei Modelli di Linguaggio\" class=\"text-blue-600 hover:underline\">Valutazione dei Modelli di Linguaggio</a></li>\n</ul>\n<h2 id=\"conclusione\">Conclusione</h2>\n<p>Questa è una breve panoramica sul corpus BERP e sulle tecniche di modellazione del linguaggio basate sugli N-grammi, che evidenzia come questi metodi possano essere utilizzati per valutare e interpretare la probabilità di frasi in linguaggio naturale.</p>"
}