{
  "title": "üéØ Tecniche di Pesatura in NLP",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"frequencies-are-not-enough\">Frequencies Are Not Enough!</h2>\n<p>In Natural Language Processing (NLP), contare semplicemente la frequenza di co-occorrenza delle parole non √® sufficiente per ottenere rappresentazioni semanticamente significative.<br />\nInfatti, parole ad alta frequenza come articoli (&ldquo;the&rdquo;, &ldquo;a&rdquo;), pronomi (&ldquo;it&rdquo;, &ldquo;he&rdquo;) o preposizioni (&ldquo;in&rdquo;, &ldquo;on&rdquo;) possono comparire frequentemente accanto a molte parole senza apportare reale informazione semantica.</p>\n<p>‚û°Ô∏è <strong>Esempi</strong>:\n- ‚úîÔ∏è <em>Buona co-occorrenza</em>: la parola ‚Äúsugar‚Äù appare frequentemente vicino ad ‚Äúapricot‚Äù ‚Üí relazione semantica plausibile.\n- ‚ùå <em>Cattiva co-occorrenza</em>: parole funzionali come ‚Äúit‚Äù o ‚Äúthe‚Äù appaiono ovunque ‚Üí <strong>rumore</strong> semantico.</p>\n<p>üîé <strong>Conclusione</strong>: <strong>Serve una tecnica di pesatura</strong> che vada oltre il semplice conteggio delle occorrenze per <strong>valorizzare</strong> le co-occorrenze semantiche significative e <strong>penalizzare</strong> quelle banali.</p>\n<h2 id=\"tf-idf-term-frequency-inverse-document-frequency\">üìä TF-IDF (Term Frequency - Inverse Document Frequency)</h2>\n<p>Una tecnica robusta e ampiamente utilizzata per la pesatura delle parole nei documenti √® <strong>TF-IDF</strong>, proposta da Salton e Buckley (1988).</p>\n<p><strong>Idea di fondo</strong>:<br />\n- Premiare le parole <strong>specifiche</strong> di un documento.<br />\n- Penalizzare le parole <strong>comuni</strong> a tutti i documenti.</p>\n<h3 id=\"definizione\">Definizione</h3>\n<p>TF-IDF √® il <strong>prodotto</strong> di due componenti:</p>\n<ol>\n<li><strong>Term Frequency (tf)</strong>:  </li>\n<li>Misura quanto frequentemente una parola $t$ appare in un documento $d$.</li>\n<li>Spesso trasformato usando il logaritmo per attenuare l&rsquo;effetto delle parole molto frequenti.</li>\n<li>\n<p>Formula:<br />\n     $$\n     tf(t,d) = \\text{frequenza di } t \\text{ in } d = \\begin{cases}\n     1+ \\log_{10} count(t, d) & \\text{se } count(t, d) > 0, \\\\\n     0 & \\text{altrimenti}\n     \\end{cases}\n     $$</p>\n</li>\n<li>\n<p><strong>Inverse Document Frequency (idf)</strong>:  </p>\n</li>\n<li>Misura l‚Äôimportanza della parola in tutto il corpus.</li>\n<li>Penalizza le parole comuni in molti documenti.</li>\n<li>Formula:<br />\n     $$\n     idf(t) = \\log \\left( \\frac{N}{df(t)} \\right)\n     $$\n     dove $N$ √® il numero totale di documenti e $df(t)$ √® il numero di documenti contenenti il termine $t$.</li>\n</ol>\n<h3 id=\"interpretazione\">Interpretazione</h3>\n<ul>\n<li>Se una parola appare in <strong>molti documenti</strong>, il suo IDF √® <strong>basso</strong> ‚Üí <strong>poco informativa</strong>.</li>\n<li>Se una parola appare in <strong>pochi documenti</strong>, il suo IDF √® <strong>alto</strong> ‚Üí <strong>molto informativa</strong>.</li>\n</ul>\n<p>üìà <strong>TF-IDF finale</strong>:\n$$\ntf\\text{-}idf(t,d) = tf(t,d) \\cdot idf(t) = w_{t, d}\n$$</p>\n<p>I valori di $tf$ sono calcolati per ogni coppia di parola e documento, mentre $idf$ viene calcolato una sola volta per ogni parola nel corpus. Il valore $idf$ non dipende dal documento specifico, quindi si puoÃÄ calcolare una volta sola per ogni parola nel corpus.</p>\n<h2 id=\"esempio-di-calcolo-tf-idf\">üìù Esempio di Calcolo TF-IDF</h2>\n<p>Corpus:\n- d1: ‚ÄúFrodo accidentally stabbed Sam and then some orcs‚Äù\n- d2: ‚ÄúFrodo was stabbing regular orcs but never stabbed super orcs ‚Äì Uruk-Hais‚Äù\n- d3: ‚ÄúSam was having a barbecue with some friendly orcs‚Äù</p>\n<p>Calcolo:</p>\n<ul>\n<li>$tf(\\text{\"Frodo\"}, d1) = 1$</li>\n<li>$idf(\\text{\"Frodo\"}) = \\log_{10}\\left(\\frac{3}{2}\\right) \\approx 0.176$</li>\n<li>$tf\\text{-}idf(\\text{\"Frodo\"}, d1) = 1 \\times 0.176 = 0.176$</li>\n</ul>\n<p>Analogamente per altre parole.</p>\n<h2 id=\"pointwise-mutual-information-pmi\">üîé Pointwise Mutual Information (PMI)</h2>\n<p>Un&rsquo;altra tecnica fondamentale per valutare la co-occorrenza di parole √® la <strong>Pointwise Mutual Information (PMI)</strong>.</p>\n<p><strong>Idea</strong>:\n- Misura quanto √® <strong>informativa</strong> l&rsquo;associazione tra due parole rispetto all&rsquo;ipotesi di indipendenza statistica.</p>\n<p><strong>Formula PMI</strong>:\n$$\nPMI(w_1, w_2) = \\log \\left( \\frac{P(w_1, w_2)}{P(w_1) P(w_2)} \\right)\n$$\ndove:\n- $P(w_1, w_2)$ √® la probabilit√† congiunta delle due parole.\n- $P(w_1)$, $P(w_2)$ sono le probabilit√† marginali.</p>\n<p>üîµ <strong>Interpretazione</strong>:\n- PMI &gt; 0 ‚Üí le parole co-occorrono <strong>pi√π del previsto</strong>.\n- PMI &lt; 0 ‚Üí le parole co-occorrono <strong>meno del previsto</strong>.</p>\n<p>üìö <strong>PMI in NLP</strong>:\n- Utilizzato per costruire matrici di co-occorrenza pesate tra termini e contesti.\n- Stimato da modelli bigrammi/unigrammi.</p>\n<h2 id=\"positive-pmi-ppmi\">‚ûï Positive PMI (PPMI)</h2>\n<p>Poich√© valori negativi di PMI sono spesso poco affidabili (soprattutto su corpora piccoli), si utilizza una variante: <strong>Positive PMI (PPMI)</strong>.</p>\n<p><strong>Definizione</strong>:\n$$\nPPMI(w_1, w_2) = \\max(PMI(w_1, w_2), 0)\n$$</p>\n<p>üîé <strong>Vantaggi</strong>:\n- Ignora co-occorrenze meno frequenti di quanto atteso.\n- Focalizza solo su associazioni <strong>significative</strong>.</p>\n<h2 id=\"esercizio-ppmi-pointwise-mutual-information\">üèóÔ∏è Esercizio PPMI (Pointwise Mutual Information)</h2>\n<h3 id=\"introduzione\">üìã Introduzione</h3>\n<p>In questo esercizio, partiamo da una <strong>term-context matrix</strong> $F$, che contiene il numero di volte in cui una parola $w_i$ appare in un contesto $c_j$.</p>\n<p><strong>Obiettivo:</strong><br />\nCostruire la matrice <strong>PPMI</strong> (Positive Pointwise Mutual Information) seguendo questi passi:</p>\n<h3 id=\"step-1-matrice-dei-conteggi-math_inline_38\">üî¢ Step 1: Matrice dei conteggi $F(w, context)$</h3>\n<table>\n<thead>\n<tr>\n<th>parola</th>\n<th style=\"text-align: center;\">computer</th>\n<th style=\"text-align: center;\">data</th>\n<th style=\"text-align: center;\">pinch</th>\n<th style=\"text-align: center;\">result</th>\n<th style=\"text-align: center;\">sugar</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>apricot</strong></td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">1</td>\n</tr>\n<tr>\n<td><strong>pineapple</strong></td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">1</td>\n</tr>\n<tr>\n<td><strong>digital</strong></td>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr>\n<td><strong>information</strong></td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">6</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"step-2-calcolo-delle-probabilita\">üõ†Ô∏è Step 2: Calcolo delle probabilit√†</h3>\n<h4 id=\"a-calcolare-il-totale-complessivo-delle-occorrenze\">a) Calcolare il totale complessivo delle occorrenze</h4>\n$$\n\\text{Totale} = \\text{somma di tutte le celle di F}\n$$\n<p>Sommiamo tutti i numeri della tabella:</p>\n$$\n\\text{Totale} = (0 + 0 + 1 + 0 + 1) + (0 + 0 + 1 + 0 + 1) + (2 + 1 + 0 + 1 + 0) + (1 + 6 + 0 + 4 + 0)\n$$\n$$\n\\text{Totale} = 2 + 2 + 4 + 11 = 19\n$$\n<p>Quindi:</p>\n$$\n\\text{Totale} = 19\n$$\n<h4 id=\"b-calcolare-math_inline_39-probabilita-congiunta\">b) Calcolare $p(w, c)$ (probabilit√† congiunta)</h4>\n<p>Ogni elemento:</p>\n$$\np(w, c) = \\frac{\\text{conteggio}(w, c)}{19}\n$$\n<p>Per esempio:\n- $p(\\text{apricot, pinch}) = \\frac{1}{19} \\approx 0.0526$</p>\n<p>E cos√¨ via per tutte le celle.</p>\n<h4 id=\"c-calcolare-math_inline_41-probabilita-della-parola\">c) Calcolare $p(w)$ (probabilit√† della parola)</h4>\n<p>Sommiamo le righe:</p>\n<ul>\n<li>$p(\\text{apricot}) = \\frac{0 + 0 + 1 + 0 + 1}{19} = \\frac{2}{19} \\approx 0.105$</li>\n<li>$p(\\text{pineapple}) = \\frac{2}{19} \\approx 0.105$</li>\n<li>$p(\\text{digital}) = \\frac{2 + 1 + 0 + 1 + 0}{19} = \\frac{4}{19} \\approx 0.211$</li>\n<li>$p(\\text{information}) = \\frac{1 + 6 + 0 + 4 + 0}{19} = \\frac{11}{19} \\approx 0.579$</li>\n</ul>\n<h4 id=\"d-calcolare-math_inline_46-probabilita-del-contesto\">d) Calcolare $p(c)$ (probabilit√† del contesto)</h4>\n<p>Sommiamo le colonne:</p>\n<ul>\n<li>$p(\\text{computer}) = \\frac{0 + 0 + 2 + 1}{19} = \\frac{3}{19} \\approx 0.158$</li>\n<li>$p(\\text{data}) = \\frac{0 + 0 + 1 + 6}{19} = \\frac{7}{19} \\approx 0.368$</li>\n<li>$p(\\text{pinch}) = \\frac{1 + 1 + 0 + 0}{19} = \\frac{2}{19} \\approx 0.105$</li>\n<li>$p(\\text{result}) = \\frac{0 + 0 + 1 + 4}{19} = \\frac{5}{19} \\approx 0.263$</li>\n<li>$p(\\text{sugar}) = \\frac{1 + 1 + 0 + 0}{19} = \\frac{2}{19} \\approx 0.105$</li>\n</ul>\n<p>Quindi ora abbiamo la seguente tabella di probabilit√† congiunte $p(w, context)$:</p>\n<table>\n<thead>\n<tr>\n<th>parola</th>\n<th style=\"text-align: center;\">computer</th>\n<th style=\"text-align: center;\">data</th>\n<th style=\"text-align: center;\">pinch</th>\n<th style=\"text-align: center;\">result</th>\n<th style=\"text-align: center;\">sugar</th>\n<th style=\"text-align: center;\">$P(w)$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>apricot</strong></td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.11</td>\n</tr>\n<tr>\n<td><strong>pineapple</strong></td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.11</td>\n</tr>\n<tr>\n<td><strong>digital</strong></td>\n<td style=\"text-align: center;\">0.11</td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.21</td>\n</tr>\n<tr>\n<td><strong>information</strong></td>\n<td style=\"text-align: center;\">0.05</td>\n<td style=\"text-align: center;\">0.32</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.21</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.58</td>\n</tr>\n</tbody>\n</table>\n<p>Le probabilit√† marginali dei contesti $P(context)$ sono:</p>\n<table>\n<thead>\n<tr>\n<th>context</th>\n<th style=\"text-align: center;\">$P(c)$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>computer</td>\n<td style=\"text-align: center;\">0.16</td>\n</tr>\n<tr>\n<td>data</td>\n<td style=\"text-align: center;\">0.37</td>\n</tr>\n<tr>\n<td>pinch</td>\n<td style=\"text-align: center;\">0.11</td>\n</tr>\n<tr>\n<td>result</td>\n<td style=\"text-align: center;\">0.26</td>\n</tr>\n<tr>\n<td>sugar</td>\n<td style=\"text-align: center;\">0.11</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"step-3-calcolo-della-ppmi\">üî• Step 3: Calcolo della PPMI</h3>\n<p>Per ogni cella $(w, c)$:</p>\n<ol>\n<li><strong>Se</strong> il conteggio √® 0, PPMI = 0.</li>\n<li><strong>Altrimenti</strong>, calcoliamo:</li>\n</ol>\n$$\nPPMI(w, c) = \\max\\left( \\log_2\\left( \\frac{p(w,c)}{p(w) \\times p(c)} \\right), 0 \\right)\n$$\n<h4 id=\"esempio-di-calcolo-apricot-e-pinch\">Esempio di calcolo: &ldquo;apricot&rdquo; e &ldquo;pinch&rdquo;</h4>\n<ul>\n<li>$p(\\text{apricot, pinch}) = 0.0526$</li>\n<li>$p(\\text{apricot}) = 0.105$</li>\n<li>$p(\\text{pinch}) = 0.105$</li>\n</ul>\n<p>Allora:</p>\n$$\n\\frac{p(w,c)}{p(w)p(c)} = \\frac{0.0526}{0.105 \\times 0.105} \\approx \\frac{0.0526}{0.011025} \\approx 4.77\n$$\n<p>Poi:</p>\n$$\n\\log_2(4.77) \\approx 2.25\n$$\n<p>Infine:</p>\n$$\nPPMI(\\text{apricot, pinch}) = 2.25\n$$\n<h3 id=\"step-4-costruzione-della-matrice-finale-ppmi\">‚úÖ Step 4: Costruzione della matrice finale PPMI</h3>\n<p>Ripetiamo il procedimento per ogni cella della matrice.</p>\n<ul>\n<li>Dove il conteggio √® 0, scriviamo &ldquo;-&ldquo;.</li>\n<li>Dove il conteggio √® &gt;0, calcoliamo il valore come sopra.</li>\n</ul>\n<p>La matrice PPMI risultante √®:</p>\n<table>\n<thead>\n<tr>\n<th>parola</th>\n<th style=\"text-align: center;\">computer</th>\n<th style=\"text-align: center;\">data</th>\n<th style=\"text-align: center;\">pinch</th>\n<th style=\"text-align: center;\">result</th>\n<th style=\"text-align: center;\">sugar</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>apricot</strong></td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">2.25</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">2.25</td>\n</tr>\n<tr>\n<td><strong>pineapple</strong></td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">2.25</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">2.25</td>\n</tr>\n<tr>\n<td><strong>digital</strong></td>\n<td style=\"text-align: center;\">1.66</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">-</td>\n</tr>\n<tr>\n<td><strong>information</strong></td>\n<td style=\"text-align: center;\">0.00</td>\n<td style=\"text-align: center;\">0.57</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">-0.47</td>\n<td style=\"text-align: center;\">-</td>\n</tr>\n</tbody>\n</table>\n<p><em>(Nota: i valori negativi diventano 0 nel PPMI.)</em></p>\n<h3 id=\"conclusione\">üéØ Conclusione</h3>\n<p>Ora abbiamo la matrice <strong>PPMI</strong> costruita a partire <strong>dai conteggi</strong> iniziali!</p>\n<h2 id=\"problemi-di-pmi-e-ppmi\">üö® Problemi di PMI e PPMI</h2>\n<p>Nonostante la PMI e la PPMI siano strumenti potenti per pesare la co-occorrenza di parole, presentano <strong>alcuni problemi</strong> importanti da considerare.</p>\n<h3 id=\"problema-sovrastima-degli-eventi-rari\">‚ö†Ô∏è Problema: Sovrastima degli Eventi Rari</h3>\n<p>La PMI tende a <strong>sovrastimare</strong> le associazioni che coinvolgono parole o contesti <strong>molto rari</strong>.</p>\n<p>üìå <strong>Esempio</strong>:\n- Se una parola rara appare anche <strong>una sola volta</strong> con un certo contesto, il valore di PMI pu√≤ risultare <strong>molto alto</strong>, anche se quell&rsquo;associazione √® dovuta al caso. Questo porta a <strong>valori anomali</strong> che inquinano la rappresentazione semantica.</p>\n<p><strong>Motivo</strong>:\n- Se $P(w)$ o $P(c)$ sono molto piccoli, il denominatore nella formula di PMI √® minuscolo, facendo esplodere il valore del logaritmo.</p>\n<h2 id=\"soluzioni-correzione-della-probabilita\">üõ†Ô∏è Soluzioni: Correzione della Probabilit√†</h2>\n<p>Per mitigare questo problema, possiamo <strong>modificare le probabilit√†</strong>:</p>\n<h3 id=\"1-smoothing-delle-probabilita\">1. Smoothing delle Probabilit√†</h3>\n<p>Una tecnica √® il <strong>Laplace Smoothing</strong>, in cui si aggiunge una piccola quantit√† (ad esempio 1) a tutti i conteggi, per evitare zeri e ridurre l‚Äôimpatto dei rari.</p>\n<p>Formula di smoothing:</p>\n$$\ncount'(w, c) = count(w, c) + \\lambda\n$$\n<p>dove $\\lambda > 0$ √® una costante (tipicamente $\\lambda = 1$).</p>\n<p>Con questo trucco:\n- Le parole molto rare ricevono una <strong>penalizzazione</strong>.\n- Il denominatore della PMI diventa pi√π stabile.</p>\n<h3 id=\"2-ppmi-con-probabilita-modificate-math_inline_64\">2. PPMI con Probabilit√† Modificate ($PPMI^{\\alpha}$)</h3>\n<p>Una soluzione ancora pi√π raffinata proposta da <strong>Levy et al. (2015)</strong> consiste nel modificare la probabilit√† dei contesti $P(c)$.</p>\n<p><strong>Idea</strong>:\n- Innalzare i contesti rari artificialmente, rendendoli meno &ldquo;speciali&rdquo;.\n- Applicare un <strong>esponente $\\alpha$ (tipicamente $\\alpha=0.75$)</strong> sulle frequenze dei contesti.</p>\n<p>Formula modificata:</p>\n$$\nP^{\\alpha}(c) = \\frac{count(c)^{\\alpha}}{\\sum_{c'}count(c')^{\\alpha}}\n$$\n<p>e la <strong>PPMI corretta ($PPMI^{\\alpha}$)</strong> diventa:</p>\n$$\nPPMI^{\\alpha}(w, c) = \\max\\left( \\log_2 \\left( \\frac{P(w,c)}{P(w) \\cdot P^{\\alpha}(c)} \\right), 0 \\right)\n$$\n<h3 id=\"interpretazione_1\">üß† Interpretazione</h3>\n<ul>\n<li><strong>Se $c$ √® raro</strong>, $P^{\\alpha}(c)$ sar√† <strong>pi√π grande</strong> di $P(c)$ normale.</li>\n<li>Questo <strong>riduce</strong> il valore di PMI per associazioni sospette con contesti rari.</li>\n<li>I contesti molto frequenti (grandi $count(c)$) sono <strong>penalizzati meno</strong>.</li>\n</ul>\n<h3 id=\"esempio-numerico\">üìö Esempio Numerico</h3>\n<p>Supponiamo:</p>\n<ul>\n<li>Contesto $a$: $count(a) = 99$</li>\n<li>Contesto $b$: $count(b) = 1$</li>\n<li>Numero totale di contesti: $N = 100$</li>\n</ul>\n<p>Calcoliamo le probabilit√† normali:</p>\n$$\nP(a) = 0.99, \\quad P(b) = 0.01\n$$\n<p>Ora applichiamo $\\alpha = 0.75$:</p>\n<ul>\n<li>$count(a)^{0.75} = 99^{0.75} \\approx 31.4$</li>\n<li>$count(b)^{0.75} = 1^{0.75} = 1$</li>\n</ul>\n<p>Totale normalizzato:</p>\n$$\n\\sum_{c}count(c)^{0.75} = 31.4 + 1 = 32.4\n$$\n<p>Quindi:</p>\n$$\nP^{0.75}(a) = \\frac{31.4}{32.4} \\approx 0.97\n$$\n$$\nP^{0.75}(b) = \\frac{1}{32.4} \\approx 0.03\n$$\n<p>‚û°Ô∏è <strong>Conclusione</strong>: anche se $b$ era rarissimo, ora la sua probabilit√† effettiva non √® pi√π 0.01 ma circa 0.03, <strong>riducendo l&rsquo;esplosione di PMI</strong> su contesti rari.</p>\n<h2 id=\"riassunto-finale\">üß© Riassunto Finale</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Metodo</th>\n<th style=\"text-align: left;\">Vantaggi</th>\n<th style=\"text-align: left;\">Problemi</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>TF (Term Frequency)</strong></td>\n<td style=\"text-align: left;\">Semplice da calcolare; riflette l&rsquo;importanza locale di una parola in un documento</td>\n<td style=\"text-align: left;\">Non distingue tra parole informative e parole molto comuni (es. &ldquo;the&rdquo;, &ldquo;is&rdquo;)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>IDF (Inverse Document Frequency)</strong></td>\n<td style=\"text-align: left;\">Penalizza parole troppo comuni; migliora la discriminativit√† delle parole</td>\n<td style=\"text-align: left;\">Pu√≤ assegnare punteggi estremi a parole molto rare</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>TF-IDF</strong></td>\n<td style=\"text-align: left;\">Combina importanza locale (TF) e globale (IDF); migliora la qualit√† delle rappresentazioni testuali</td>\n<td style=\"text-align: left;\">Non cattura le relazioni tra le parole; insensibile alla semantica</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>PMI</strong></td>\n<td style=\"text-align: left;\">Misura precisa della forza di associazione tra parola e contesto</td>\n<td style=\"text-align: left;\">Sovrastima eventi rari; instabile su piccoli corpora</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>PPMI</strong></td>\n<td style=\"text-align: left;\">Ignora associazioni negative; maggiore robustezza rispetto a PMI</td>\n<td style=\"text-align: left;\">Comunque vulnerabile a rumore da contesti rari</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>PPMI$^{\\alpha}$</strong></td>\n<td style=\"text-align: left;\">Migliora la gestione dei contesti rari applicando smoothing parametrico; pi√π bilanciato</td>\n<td style=\"text-align: left;\">Richiede una scelta accurata del parametro $\\alpha$; maggiore complessit√† computazionale</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"conclusioni\">üß† Conclusioni</h2>\n<ul>\n<li><strong>TF-IDF</strong> √® estremamente efficace per la rappresentazione di documenti rispetto a corpus di testi.</li>\n<li><strong>PMI</strong> e <strong>PPMI</strong> sono strumenti potenti per analizzare la semantica fine delle relazioni tra parole.</li>\n<li>In NLP moderno, queste tecniche di pesatura rimangono fondamentali per la costruzione di rappresentazioni sparse e semantiche significative delle parole.</li>\n</ul>"
}