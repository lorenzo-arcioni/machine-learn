# Modelli di Linguaggio

I modelli di linguaggio sono sistemi di intelligenza artificiale addestrati per comprendere, generare e manipolare il linguaggio umano. Utilizzano architetture avanzate (come i **[[Transformers|transformers]]**) per prevedere sequenze di parole o caratteri basandosi sul contesto. In particolare, sono distribuzioni probabilistiche sulle sequenze di parole, che permettono di prevedere il prossimo token in base al contesto precedente.

Per un approfondimento sui linguaggi, [[Gerarchia di Chomsky|qui]].

## Perch√© Distribuzioni Probabilistiche?

### Limiti delle Grammatiche Formali
- **Modelli "Binari"**:  
  Le grammatiche formali (es. regolari, context-free) definiscono regole rigide per determinare se una frase √® *legal* o meno in una lingua (approccio **0/1**).  
  ‚Üí **Problema**: Il linguaggio naturale √® ambiguo, flessibile e dipendente dal contesto.  
  Esempio: *"Leggo un libro sul volo"* pu√≤ essere interpretato in modo diverso (lettura *su* un argomento vs. lettura *fisicamente sopra* un oggetto).

### Vantaggi dei Modelli Probabilistici
1. **Gestione dell'incertezza**:  
   Assegnano una **probabilit√†** a ogni frase/stringa, riflettendo la sua "naturalezza" o plausibilit√† nel contesto reale.  
   ‚Üí Utile per: disambiguazione, ranking di ipotesi, generazione fluida.

2. **Adattabilit√† al mondo reale**:  
   - Modellano variazioni linguistiche (dialetti, errori ortografici, slang).  
   - Tengono conto di correlazioni statistiche tra parole (es. *"caff√®"* ‚Üí alta probabilit√† di *"bere"* o *"tazzina"*).

3. **Fondamento per NLP moderno**:  
   Consentono di:  
   - Addestrare modelli su corpora non perfetti (es. web text con rumore).  
   - Ottimizzare task come traduzione o riconoscimento vocale tramite massimizzazione della likelihood.

> üìä **Esempio Pratico**:  
> Un modello probabilistico pu√≤ assegnare:  
> - P(*"Il gatto corre sul tetto"*) = 0.85  
> - P(*"Il tetto corre sul gatto"*) = 0.02  
> Pur essendo entrambe frasi *sintatticamente corrette*, la probabilit√† riflette la plausibilit√† semantica.

### Confronto Chiave
| **Approccio**         | Grammatiche Formali         | Modelli Probabilistici      |
|------------------------|-----------------------------|-----------------------------|
| **Output**             | Binario (accetta/rigetta)   | Probabilit√† continua        |
| **Flessibilit√†**       | Bassa (regole fisse)        | Alta (apprendimento dati)   |
| **Gestione Ambiguit√†** | Limitata                    | Ottimizzata                 |
| **Use Case**           | Compilatori, parser semplici| NLP, generazione testo      |

Prima di proseguire, √® bene aver compreso a pieno le [[Basi di Probabilit√†]].

## Previsione Probabilistica del completamento delle frasi
Un modello linguistico supporta la previsione del completamento di una frase:
- **Esempi**:
  - *Please turn off your cell _____*
  - *Your program does not ______*
  - I sistemi di input predittivo possono indovinare ci√≤ che stai scrivendo e suggerire opzioni di completamento.

### Approccio statistico alla previsione delle parole
L'obiettivo √® prevedere la parola successiva in una frase o correggere un errore ortografico utilizzando **probabilit√† condizionate** basate sul contesto precedente.

**Definizioni**:

- $w$: una data parola.

- $\mathbb{P}(w_1, \ldots, w_n)$: rappresenta la **probabilit√† congiunta** dell‚Äôintera sequenza di parole $(w_1, w_2, \dots, w_n)$, ovvero la probabilit√† di ottenere proprio questa specifica sequenza di parole in un dato contesto (per esempio, un modello di linguaggio).
- $\mathbb P(w_n | w_1, w_2, ..., w_{n-1})$: √® la probabilit√† che, data la sequenza di parole $w_1, \ldots, w_{n-1}$ (gi√† presenti nel contesto), la prossima parola sia $w_n$.

**Esempio**: Se consideriamo una frase come *"oggi piove molto"*, la probabilit√† congiunta $\mathbb{P}(\text{"oggi"}, \text{"piove"}, \text{"molto"})$ indica quanto questa sequenza sia comune nel linguaggio naturale.

**Esempio**: Se consideriamo una frase come *"the pen is on the"*, la probabilit√† che la prossima parola sia "table" √® $\mathbb P("table" | "the", "pen", "is", "on", "the")$.

### Stima delle frequenze relative
Per stimare queste probabilit√† su un **corpus molto ampio**:
1. **Probabilit√† congiunta** (sequenza completa). Si conta il numero totale di parole $N$:
   $$\mathbb P(w_1, ..., w_n) = \frac{\text{Conteggio della sequenza } w_1, ..., w_n}{\text{Conteggio della sequenza } w_1, ..., w_{n-1}} = \frac{C(w_1, ..., w_n)}{N}$$
2. **Probabilit√† condizionata** (parola successiva). Si conta quante volte una sequenza specifica occorre:
   $$\mathbb P(w_n | w_1, ..., w_{n-1}) = \frac{\text{Conteggio della sequenza } w_1, ..., w_n}{\text{Conteggio della sequenza } w_1, ..., w_{n-1}} = \frac{C(w_1, ..., w_{n-1}, w_n)}{C(w_1, ..., w_{n-1})}$$
   Questo metodo √® chiamato **stima della frequenza relativa**.

### Vantaggi e svantaggi della stima a frequenza relativa
**Vantaggi**:
- La stima a frequenza relativa √® una *Stima di Massima Verosimiglianza (MLE)*:
  - Dato un modello, l'MLE produce la probabilit√† massima ottenibile dai dati disponibili.

**Svantaggi**:
- Richiede un corpus **ESTREMAMENTE GRANDE** per stime accurate.
- Computazionalmente impraticabile per sequenze lunghe o contesti complessi.

Ci serve un modo pi√π efficiente per stimare $\mathbb{P}(w_1, \ldots, w_n)$.

### $N$-grams models

L'idea alla base dei modelli N-grams √® **semplificare il calcolo delle probabilit√† linguistiche** evitando di considerare l'intera storia del contesto. Si utilizza invece un'approssimazione basata sulla **propriet√† di Markov**:

> "La probabilit√† di una parola dipende solo dalle ultime $N-1$ parole precedenti"

**Formula generale**:
$$\mathbb P(w_n | w_1, ..., w_{n-1}) \approx P(w_n | w_{n-N+1}, ..., w_{n-1})$$

Utilizziamo l'indice $N-1$ perch√© $N$ rappresenta il numero di parole considerate, ma dato che una √® l'$n$-esima (quella che dobbiamo predire), dobbiamo considerare il contesto precedente di $N-1$ parole. 

Questo significa che per predire la prossima parola $w_n$, ci basiamo sulle $N-1$ precedenti, ovvero $w_{n-N+1}, ..., w_{n-1}$. Che intuitivamente ha senso, in quanto la parola $w_n$ sar√† molto pi√π fortemente influenzata dalle precedenti pi√π vicine che da quelle pi√π distanti.

Quindi, grazie alla **propriet√† di Markov**, abbiamo che 

$$
\mathbb{P}(w_1, \ldots, w_n) \approx \prod_{k=1}^n \mathbb P(w_k \mid w_{k-N+1}, \ldots, w_{k-1})
$$

Qui stiamo approssimando la probabilit√† congiunta usando un'ipotesi di dipendenza limitata. 

Cosa significa questa espressione?  
- Si tratta di un **prodotto** di probabilit√† condizionate.  
- Ogni termine $\mathbb{P}(w_k \mid w_{k-N+1}, \ldots, w_{k-1})$ rappresenta **la probabilit√† che la parola $w_k$ appaia, dato il contesto delle $N-1$ parole precedenti**.  
- Stiamo assumendo che la probabilit√† di una parola dipenda solo dalle ultime $N-1$ parole, e non dall'intera sequenza precedente.  

**Interpretazione intuitiva**:  
- Invece di considerare tutta la sequenza passata, **usiamo solo una finestra di dimensione $N-1$** per predire la parola successiva.  
- Questo semplifica i calcoli e rende il modello computazionalmente gestibile.  

**Esempio** (modello bigramma, $N=2$):  
Se vogliamo stimare la probabilit√† di *"oggi piove molto"*, e assumiamo che ogni parola dipenda solo dalla precedente (**modello bigramma**), la formula diventa:  

$$
\mathbb{P}(\text{"oggi"}, \text{"piove"}, \text{"molto"}) \approx \mathbb{P}(\text{"oggi"}) \cdot \mathbb{P}(\text{"piove"} \mid \text{"oggi"}) \cdot \mathbb{P}(\text{"molto"} \mid \text{"piove"})
$$

- **$\mathbb{P}(\text{"oggi"})$**: probabilit√† che inizi la frase con "oggi".  
- **$\mathbb{P}(\text{"piove"} \mid \text{"oggi"})$**: probabilit√† che "piove" segua "oggi".  
- **$\mathbb{P}(\text{"molto"} \mid \text{"piove"})$**: probabilit√† che "molto" segua "piove".  

In un **modello trigramma** ($N=3$), invece, avremmo:  

$$
\mathbb{P}(\text{"oggi"}, \text{"piove"}, \text{"molto"}) \approx \mathbb{P}(\text{"oggi"}) \cdot \mathbb{P}(\text{"piove"} \mid \text{"oggi"}) \cdot \mathbb{P}(\text{"molto"} \mid \text{"oggi"}, \text{"piove"})
$$

Qui, ogni parola dipende **da entrambe le precedenti**, anzich√© solo dall'ultima.  

### Perch√© questa approssimazione?
- **Motivazione computazionale**: Calcolare la probabilit√† esatta di una sequenza lunga √® **impraticabile** perch√©: 
  - Se il nostro vocabolario ha, per esempio, **20.000 parole**, allora una sequenza di 5 parole pu√≤ teoricamente assumere $20.000^5 = 3.2 \times 10^{23}$ combinazioni possibili! Questo significa che per stimare accuratamente tutte le probabilit√† congiunte necessarie, dovremmo raccogliere un **enorme numero di esempi** per coprire tutte le possibili frasi. Con questa approssimazione, invece, **riduciamo drasticamente la complessit√†**, poich√© stimiamo ogni parola solo in base a un numero **limitato** di parole precedenti.  
- **Motivazione linguistica**: In molti casi il contesto rilevante √® **localizzato** (es. in italiano "fare ___ colazione" richiede quasi sempre "la") e molti costrutti grammaticali si basano solo su "poche" parole precedenti. Questo significa che la **propriet√† di Markov** funziona bene per stimare le probabilit√† linguistiche.

### Differenze nei valori di $N$

Un aspetto fondamentale nei modelli basati su n-grammi √® il valore di $N$. Questo parametro determina quante parole precedenti vengono considerate nel calcolo della probabilit√† di una parola successiva.  

- **Un valore pi√π grande di $N$ implica che:**  
  - Il modello ha **pi√π informazioni sul contesto**, poich√© considera una finestra pi√π ampia di parole precedenti.  
  - Questo porta a una **maggiore capacit√† discriminativa**, cio√® il modello √® pi√π preciso nel prevedere la parola successiva in base a un contesto pi√π dettagliato.  
  - Tuttavia, **cresce il problema della scarsit√† dei dati** (*data sparseness*):  
    - Le combinazioni di parole diventano pi√π numerose, quindi molte sequenze potrebbero non comparire mai nel dataset di addestramento.  
    - Questo porta a difficolt√† nella stima delle probabilit√†, poich√© alcuni n-grammi potrebbero avere conteggi molto bassi o addirittura nulli.
    - Le tecniche di [[Smoothing nei Modelli Linguistici|smoothing]] diventano cruciali e complesse.

- **Un valore pi√π piccolo di $N$ implica che:**  
  - Il modello ha **meno precisione**, poich√© considera un contesto pi√π limitato.  
  - Tuttavia, ci sono **pi√π esempi nel dataset** che corrispondono a ciascun n-gramma.  
  - Questo rende le **stime probabilistiche pi√π affidabili**, poich√© √® meno probabile che ci siano sequenze con frequenza nulla.  

In pratica, c'√® un **compromesso** nella scelta di $N$:  
- Un valore pi√π grande di $N$ aiuta a catturare meglio la struttura del linguaggio ma aumenta il rischio di dati insufficienti.  
- Un valore pi√π piccolo riduce la precisione ma garantisce un modello pi√π stabile e generalizzabile.  

Per affrontare il problema della scarsit√† dei dati nei modelli con $N$ elevato, si utilizzano tecniche come **[[Smoothing nei Modelli Linguistici|smoothing]]**, **[[Backoff nei Modelli Linguistici|backoff]]**, **[[Interpolazione Lineare nei Modelli Linguistici|interpolazione]]** e **modelli neurali** come le reti ricorrenti (*RNN*) o i Transformer.

### Riassumendo
I modelli **N-gram** approssimano la probabilit√† di sequenze di parole utilizzando contesti limitati di $N-1$ parole precedenti.  Se consideriamo una sequenza di parole $w_{1}^n = w_1, w_2, ..., w_n$ con $n$ parole, abbiamo:
- **Regola della catena delle probabilit√†**:  
  $$
  P(w_1^n) = \prod_{k=1}^n P(w_k \mid w_1^{k-1})
  $$  
  Calcola la probabilit√† di una frase scomponendola in probabilit√† condizionate di ogni parola dato l'intero contesto precedente.  

- **Approssimazioni**:  
  - **Bigramma** ($N=2$): Considera solo la parola precedente:  
    $$
    P(w_1^n) = \prod_{k=1}^n P(w_k \mid w_{k-1})
    $$  
  - **N-gramma** ($N$ generico): Utilizza le ultime $N-1$ parole:  
    $$
    P(w_1^n) = \prod_{k=1}^n P(w_k \mid w_{k-N+1}^{k-1})
    $$  

#### Stima delle Probabilit√† con Frequenze Relative  
Le probabilit√† condizionate si stimano dai conteggi delle sequenze nel corpus:  
- **Bigramma**:  
  $$
  P(w_n \mid w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1})}
  $$  
  Esempio: Se "cane" appare 100 volte e "cane abbaia" 30 volte, $P(\text{abbaia} \mid \text{cane}) = 0.3$.  

- **N-gramma**:  
  $$
  P(w_n \mid w_{n-N+1}^{n-1}) = \frac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}
  $$  
  Esempio: Per un trigramma ($N=3$), $P(\text{mangia} \mid \text{il, cane}) = \frac{C(\text{il cane mangia})}{C(\text{il cane})}$.  

Questo approccio si basa sulla **frequenza relativa** delle sequenze, rendendolo semplice ma sensibile alla sparsit√† dei dati per $N$ elevati.

[[Legge di Zipf|Qui]] √® presente un approfondimento dettagliato sulla legge di Zipf. Che spiega la motivazione teorica dietro la sparsit√† dei dati nei modelli N-gram. La legge di Zipf mostra che in un corpus linguistico, la distribuzione delle parole segue una legge di potenza, dove poche parole (es. articoli, preposizioni) compaiono con frequenza estremamente alta, mentre la maggioranza delle parole √® rara.

## Limiti e Problematiche

I modelli linguistici basati su n-grammi presentano diverse limitazioni intrinseche:

### 1. Sparse Data e Zero Probability
- **N-grammi non osservati**:  
  Sequenze plausibili ma assenti nel training set ricevono probabilit√† zero:  
  $$P(w_n | w_{n-N+1}, ..., w_{n-1}) = 0 \quad \text{se } C(w_{n-N+1}, ..., w_n) = 0$$  
  ‚Üí **Impatto**:  
  - Frasi valide nel test set ottengono perplexity infinita.  
  - Impossibilit√† di generalizzare a combinazioni non viste (es. *"cane mangia kiwi"*).  
  > *Soluzione*: Tecniche di smoothing (approfondite in dettaglio [[Smoothing nei Modelli Linguistici|qui]]).

### 2. Finestra Contestuale Limitata
- **Dipendenza da N fissato**:
  Con un modello basato su n-gram, il contesto considerato √® limitato a $N-1$ parole.
  - Con N=3 (trigrammi), il modello ignora parole oltre le ultime 2:  
    *"Ieri ho visitato il museo egizio che ___"* ‚Üí Il contesto rilevante ("museo") potrebbe essere troppo lontano.  
  - **Esempio**: In *"La ragazza con gli occhiali da sole che ___"*, la scelta di "indossava" vs "rompe" dipende da "occhiali", non dalle ultime 2 parole ("che" e "sole").

### 3. Incapacit√† di Modellare Strutture Complesse
- **Dipendenza dall'ordine locale**:  
  Non catturano fenomeni linguistici che richiedono memoria a lungo termine:  
  - Accordi verbali: *"Le donne che hanno ___"* (richiede accordo plurale)  
  - Riferimenti anaforici: *"Marco disse a Luca di comprare il pane. Poi ___ usc√¨"* (chi usc√¨?)

- **Ambiguit√† lessicale**:  
  Non distinguono significati multipli in base al contesto globale:  
  $$P(\text{bank} | \text{river}) ‚âà P(\text{bank} | \text{money})$$  
  (manca comprensione semantica di "bank" come "sponda" vs "banca").

### 4. Overhead Computazionale
- **Crescita esponenziale dello spazio**:  
  Per un vocabolario di 50k parole:  
  - Bigrammi: $50k^2 = 2.5$ miliardi di parametri  
  - Trigrammi: $50k^3 = 125$ trilioni di parametri  
  ‚Üí **Problema**: Memorizzazione e query inefficienti anche per N moderati.

    ‚Üí **Soluzione**: Utilizzare tecniche di compressione (es. Huffman coding) e pruning.

### 5. Sensibilit√† al Corpus di Training
- **Bias statistici**:  
  Riproducono stereotipi presenti nei dati:  
  - *"L'infermiere ___"* ‚Üí Probabilit√† alta per "lei" (se il corpus ha prevalenza femminile nel ruolo)  
  - *"Il CEO di successo ___"* ‚Üí Associazioni di genere/culturali distorte  

- **Out-Of-Vocabulary (OOV)**:  
  Parole nuove (slang, nomi propri, errori) non presenti nel training set vengono gestite male:  
  $$
  \mathbb P(\text{"Il nuovo NFT"}) = 0 \quad \text{se "NFT" non √® nel vocabolario}
  $$

### 6. Apprendimento Superficiale
- **Modellano correlazioni, non causalit√†**:  
  Apprendono pattern statistici senza comprensione logica:  
  - *"Se piove, prendo l'ombrello"* ‚Üí Alta probabilit√†  
  - *"Se prendo l'ombrello, piove"* ‚Üí Probabilit√† simile (manca relazione causale)  

- **Assenza di world knowledge**:  
  Non integrano informazioni esterne:  
  $$P(\text{"Roma"} | \text{"La capitale d'Italia √®"}) = 0$$  
  Anche se "Roma" √® l'unica risposta corretta, il modello assegna probabilit√† basate solo sui bigrammi/trigrammi osservati.



## Argomenti Collegati
- [[Valutazione dei Modelli di Linguaggio]]
- [[Smoothing nei Modelli Linguistici]]
- [[Pruning e Compressione nei Modelli NLP]]
- [[Bias e Fairness nell'Elaborazione del Linguaggio Naturale]]

## Conclusione
I modelli linguistici basati su n-gram hanno rappresentato una tappa fondamentale nello sviluppo dell'NLP, ma presentano notevoli limitazioni: dalla gestione della sparsenza e del problema degli n-grammi non osservati, alla finestra contestuale limitata e alla difficolt√† nel modellare strutture linguistiche complesse. Inoltre, l'esponenziale crescita dello spazio delle combinazioni e la sensibilit√† ai bias del corpus di training evidenziano il bisogno di approcci pi√π sofisticati. Questi limiti hanno favorito l'evoluzione verso modelli neurali e architetture transformer, capaci di integrare contesto e conoscenza semantica in maniera pi√π efficace, aprendo la strada a progressi significativi nel campo dell'elaborazione del linguaggio naturale.
