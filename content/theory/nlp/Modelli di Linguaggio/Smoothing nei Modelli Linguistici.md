# Smoothing nei Modelli Linguistici  

## Introduzione  
Lo **smoothing** √® una tecnica fondamentale per gestire il problema dei **dati sparsi** nei modelli di linguaggio. Senza smoothing:  
- Gli **n-grammi non osservati** nel training ricevono probabilit√† zero, portando a **perplessit√† infinita** durante il test.  
- Il modello non pu√≤ generalizzare a sequenze plausibili ma mai viste.  

L'idea √® **ridistribuire la massa di probabilit√†** dagli n-grammi frequenti a quelli rari o assenti ("Rubare ai ricchi per dare ai poveri").  

In alcune tecniche, viene utilizzato il concetto di sconto (discounting), che ora illustreremo.

### Discounting

Uno **sconto** (discount) √® una tecnica usata per ridurre la massa di probabilit√† di un evento, riconoscendo che il conteggio osservato in un corpus limitato potrebbe essere sottostimato rispetto alla reale probabilit√† che quell'evento si verifichi in un corpus pi√π grande. Formalmente, per un n-gramma con conteggio $c$ si definisce il conteggio ridistribuito $c^*$ come:

$$
c^* = c - d \quad \text{con } d \in [0, c],
$$

dove $d$ √® il valore dello sconto. Il fattore di sconto relativo √® quindi:

$$
d_c = \frac{c^*}{c}.
$$

Quando calcoliamo la probabilit√† di un n-gramma (dato un contesto $h$), usiamo il conteggio ridistribuito al posto del conteggio grezzo:

$$
P(w|h) = \frac{c^*}{N(h)},
$$

dove $N(h)$ √® la somma totale dei conteggi degli n-grammi osservati per quel contesto.  

Questo approccio ha due scopi fondamentali:  
1. **Ridurre la sovrastima** degli n-grammi osservati frequentemente.  
2. **Riservare parte della massa probabilistica** per quegli n-grammi non osservati, i quali potranno essere poi distribuiti uniformemente (o secondo qualche altra strategia) tra tutti gli eventi "mai visti" per garantire che ricevano probabilit√† non nulle.

### Un Esempio Pratico

Immaginiamo un contesto $h$ in cui abbiamo i seguenti n-grammi con i relativi conteggi:

| n-gramma | Conteggio $c$ |
|----------|------------------|
| $w_1$ | 10               |
| $w_2$ | 5                |
| $w_3$ | 2                |
| $w_4$ | 0                |

Supponiamo di impostare uno sconto $d = 0.5$ per ciascun n-gramma osservato.  
I conteggi ridistribuiti $c^*$ diventeranno:  

- Per $w_1$: $c^* = 10 - 0.5 = 9.5$  
- Per $w_2$: $c^* = 5 - 0.5 = 4.5$  
- Per $w_3$: $c^* = 2 - 0.5 = 1.5$  
- $w_4$, che non √® mai stato osservato, non subisce discounting: la sua probabilit√† sar√† determinata tramite la massa di probabilit√† riservata agli eventi non visti.

La probabilit√† degli n-grammi osservati diventa quindi:

$$
P(w|h) = \frac{c - d}{N(h)},
$$

con $N(h) = 10 + 5 + 2 = 17$ (somma dei conteggi dei n-grammi osservati).

Per gli n-grammi non osservati (ad esempio $w_4$), si calcola una probabilit√† separata utilizzando la massa di probabilit√† riservata, che √® la somma degli sconti applicati:

$$
\text{Massa riservata} = \frac{d \cdot N_{\text{unici}}(h)}{N(h)},
$$

dove $N_{\text{unici}}(h)$ √® il numero di n-grammi visti almeno una volta per quel contesto. In questo esempio $N_{\text{unici}}(h) = 3$.

Quindi, la probabilit√† per un n-gramma non osservato potrebbe essere distribuita in base a questa massa:

$$
P(w_{\text{non-osservato}}|h) = \frac{0.5 \times 3}{17}.
$$

### Conclusioni

Il processo di discounting consente di:
- **Normalizzare** la probabilit√† complessiva mantenendo la somma pari a 1.
- Dare a quegli n-grammi che non sono mai stati osservati (ma che potrebbero verificarsi) una probabilit√† non nulla.
- Affrontare il problema dei dati sparsi rendendo il modello pi√π robusto e in grado di generalizzare a sequenze mai viste nel training.

Questo approccio √® essenziale per garantire che i modelli linguistici possano trattare con successo la variet√† e la rarit√† degli eventi presenti nei dati reali.


## Tecniche Principali  

### 1. **Laplace (Add-One) Smoothing**  
Il Laplace Smoothing, noto anche come add-one smoothing, √® una tecnica usata nei modelli di linguaggio probabilistici per gestire il problema degli zeri nelle stime di probabilit√†. Nei modelli basati su n-grammi, ad esempio, capita spesso che alcune combinazioni di parole non compaiano mai nel corpus di addestramento. Senza smoothing, queste combinazioni avrebbero probabilit√† pari a zero, il che pu√≤ compromettere gravemente la generazione o la valutazione di frasi.

Il Laplace Smoothing risolve questo problema aggiungendo 1 al conteggio di ogni possibile n-gramma. In pratica, anche gli n-grammi mai visti ottengono un conteggio minimo, evitando probabilit√† nulle. 

Sebbene semplice ed efficace per corpus piccoli, il Laplace Smoothing tende a sovrastimare la probabilit√† degli eventi rari, penalizzando quelli frequenti. Per questo motivo, in applicazioni avanzate si preferiscono metodi pi√π sofisticati come Good-Turing o Kneser-Ney smoothing. Tuttavia, il Laplace rimane una base utile per comprendere il concetto di smoothing nei modelli di linguaggio.

**Formula (Unigrammi):**  
$$
P_{\text{Laplace}}(w_i) = \frac{c(w_i) + 1}{N + V}
$$  
- $c(w_i)$: conteggio della parola $w_i$.  
- $N$: numero totale di token nel corpus.  
- $V$: dimensione del vocabolario. Questo semplicemente perch√© abbiamo aggiunto $+1$ per ogni parola.

**Formula generale per n-grammi**:  
Per un n-gramma $w_1, w_2, \dots, w_n$:  
$$
P_{\text{Laplace}}(w_n | w_1, \dots, w_{n-1}) = \frac{c(w_1, \dots, w_n) + 1}{\sum_{w}c(w_1, \dots, w_{n-1} w)+ 1} = \frac{c(w_1, \dots, w_n) + 1}{c(w_1, \dots, w_{n-1}) + V}
$$  
dove $c(w_1, \dots, w_{n-1})$ √® il conteggio del contesto $(w_1, \dots, w_{n-1})$ e $V$ la dimensione del vocabolario (sempre perch√© abbiamo aggiunto $+1$ per ogni n-gramma con prefix $(w_1, \dots, w_{n-1})$).

**Ridistribuzione dei conteggi**:

Possiamo ottenere i conteggi risultati dall'applicazione dello smoothing con la seguente formula:  
$$
P_{\text{Laplace}}(w_n | w_1, \dots, w_{n-1}) = \frac{c(w_1, \dots, w_n) + 1}{c(w_1, \dots, w_{n-1}) + V} = \frac{c^*(w_1, \dots, w_n)}{c(w_i, \ldots, w_{n-1})} \Rightarrow c^*(w_1, \dots, w_n) = \frac{(c(w_1, \dots, w_n) + 1) \cdot c(w_1, \dots, w_{n-1})}{c(w_1, \dots, w_{n-1}) + V}.
$$

In questo modo possiamo confrontare direttamente i conteggi ridistribuiti con quelli originali (MLE).

**Esempio**:  
Se $N=1000$ e $V=500$, un bigramma "gatto felice" con $c=3$ (e contesto "gatto" che appare 10 volte):  
$$
P_{\text{Laplace}} = \frac{3 + 1}{10 + 500} = \frac{4}{510} \approx 0.0078
$$  
Conteggio ridistribuito:  
$$
c^* = \frac{(3 + 1) \cdot 10}{10 + 500} = \frac{40}{510} \approx 0.078
$$

**Problema**:  
- Sovrastima degli eventi rari per $V$ grandi (es. $V=10^5$). Per un bigramma mai visto "gatto volante", con contesto "gatto" ($c=10$):  
$$
P_{\text{Laplace}} = \frac{0 + 1}{10 + 500} = \frac{1}{510} \approx 0.00196.
$$

---

### 2. Add-$k$ Smoothing

Un'alternativa all'add-one smoothing √® spostare una quantit√† minore di massa probabilistica dagli eventi osservati a quelli non osservati. Invece di aggiungere 1 a ogni conteggio, aggiungiamo un conteggio frazionario $0 \leq k \leq 1$. Questo algoritmo √® quindi chiamato add-$k$ smoothing.

$$
P_{Add-k}(w_n |w_1, \ldots, w_{n‚àí1}) = \frac{c(w_1, \ldots, w_n) + k}{c(w_1, \ldots, w_{n-1}) + kV} = \frac{c^*(w_1, \dots, w_n)}{c(w_i, \ldots, w_{n-1})} \Rightarrow c^*(w_1, \dots, w_n) = \frac{(c(w_1, \dots, w_n) + k) \cdot c(w_1, \dots, w_{n-1})}{c(w_1, \dots, w_{n-1}) + kV}.
$$

L'add-$k$ smoothing richiede che si abbia un metodo per scegliere $k$; questo pu√≤ essere fatto, ad esempio, ottimizzando su un devset. Sebbene l'add-$k$ sia utile per alcune attivit√† (inclusa la classificazione di testi), risulta comunque non funzionare bene per la modellazione linguistica, generando conteggi con varianze scarse e spesso sconti inappropriati.

---

### 3. **Good-Turing Smoothing**

#### **Definizione**  
Il **Good-Turing smoothing** √® una tecnica statistica fondamentale per stimare la probabilit√† di token rari o non osservati in un dataset. √à particolarmente utile nei modelli linguistici (ad esempio, per $n$-gram) perch√© permette di ridistribuire la massa probabilistica dagli token frequenti a quelli che non sono stati mai osservati, migliorando cos√¨ la robustezza del modello anche in presenza di dati scarsi.


#### **Formula Principale**  
Per un token osservato $k$ volte, la probabilit√† scontata √®:  
$$
P_{\text{GT}}(w) = \frac{k^*}{N}, \quad \text{dove } k^* = \frac{(k+1) \cdot N_{k+1}}{N_k},  
$$  
- $N_k$ = numero di token osservati **esattamente** $k$ volte nel corpus,  
- $N$ = numero totale di token osservati ($N = \sum_{k=1}^\infty k \cdot N_k$).  

**Probabilit√† per Token non osservati** ($k=0$):  
$$
P_{\text{GT}}(w_{\text{new}}) = \frac{N_1}{N}.  
$$

Ovviamente i token non osservati sono quelli che non sono stati mai osservati nel corpus (training set), ma che sono presenti nel vocabolario $V$.

#### Intuizione

L'idea fondamentale del Good-Turing smoothing √® quella di ‚Äúriutilizzare‚Äù il corpus come un set di validazione per stimare la probabilit√† sia dei token gi√† osservati sia di quelli che non abbiamo mai visto. La chiave del seguente ragionamento non √® pi√π la probabilit√† di un token di apparire in un testo, ma la probabilit√† che un certo token appaia con una certa frequenza. Quello che ci chiediamo √®: quale frequenza mi aspetto per il prossimo token? e non pi√π: quale probabilit√† mi aspetto per il prossimo token?

Immagina di avere un cesto di frutta e di voler prevedere quale frutto potresti trovare in pi√π, anche se non lo hai mai visto o l'hai visto pochissimo. Il Good-Turing smoothing √® una tecnica che ci aiuta proprio a fare questo: usa le informazioni sulle frequenze dei frutti per stimare la loro probabilit√†.

Assumiamo quindi di avere il seguente corpus $C$:

| Frutto | Frequenza |  
| --- | --- |  
| üçå | 5 |  
| üçé | 3 |  
| üçä | 2 |  
| üçí | 2 | 
| üçâ | 1 |
| üçá | 0 |

e il seguente vocabolario: 

$$
V = \{ \text{üçå}, \text{üçé}, \text{üçä}, \text{üçí}, \text{üçâ}, \text{üçá} \}
$$

In questo contesto, $N_0$ √® il numero di token osservati 0 volte ($N_0 = 1$ in questo caso), $N_1$ il numero di token osservati 1 volta, e cosi via.

Per stimare la probabilit√† di trovare un üçá nel mondo reale, il Good-Turing smoothing utilizza il seguente ragionamento: se il prossimo token fosse üçá, avrebbe molteplicit√† 1 nel corpus (perch√© avrei visto üçá per la prima volta). Quindi, se cos√¨ fosse, avrei che questa situazione ha probabilit√† $\frac{1}{13}$, perch√© nel corpus per ora ho solo un elemento con molteplicit√† $1$ (üçâ). E quindi un altro elemento di molteplicit√† $1$ ha probabilit√† $\frac{1}{13}$.

Questo ragionamento pu√≤ estendersi tranquillamente per i token che gi√† appaiono nel corpus. Considerando ad esempio il token üçí, e chiediamoci qual √® la probabilit√† che appaia di nuovo. Dato che üçí appare gi√† 2 volte ($k=2$), se incontrassimo un altro üçí, ne avremmo 3. Ora, la probabilit√† di apparire di nuovo di un token di frequenza 2 √® la stessa che ha un token di frequenza 3 di apparire nel corpus, che √®:
$$
\frac{(k+1) \cdot N_{k+1}}{N} = \frac{3 \cdot 1}{13} = \frac{3}{13}.
$$

Questo per√≤ non basta, perch√© questa √® la probabilit√† che un **generico** frutto con molteplicit√† 2 diventi di molteplicit√† 3, quindi (dato che noi vogliamo la probabilit√† di un unico token) dobbiamo dividere questa probabilit√† per il numero di frutti con molteplicit√† 2 nel corpus ($N_2 = 2$). Quindi, la probabilit√† per il token üçí diventa:

$$
\frac{(k+1) \cdot N_{k+1}}{N \cdot N_2} = \frac{3 \cdot 1}{13 \cdot 2} = \frac{3}{26}.
$$

In questo contesto, possiamo definire anche $k^*$ come il conteggio atteso di un token con molteplicit√† $k$ nel corpus $C$ come segue:


(Numero di volte che un token con molteplicit√† $k$ apparirebbe nel corpus se venisse osservato un'ulteriore volta) x (Numero di classi con la stessa (nuova) molteplicit√† nel corpus) / (Numero di classi che potenzialmente possono essere "promosse" a molteplicit√† $k+1$ nel corpus).

In formule, 

$$
k^* = (k+1) \cdot \frac{N_{k+1}}{N_k}.
$$

Intuitivamente, se:

- $N_{k+1} > N_k$, allora significa che la porzione delle frequenze che hanno molteplicit√† $k+1$ nel corpus, sono maggiori di quelle che hanno molteplicit√† $k$ nel corpus. E quindi un token con molteplicit√† $k$ pi√π probabilmente deve essere promosso a molteplicit√† $k+1$.
- $N_{k+1} = N_k$, allora significa che se osserviamo un nuovo token con molteplicit√† $k$, esso arriver√† a molteplicit√† $k+1$. Quindi √® come se il modello dicesse: "non ho evidenze per correggere il conteggio che ho ora, quindi mi limito ad aumentarlo di 1 in via cautelativa".
- $N_{k+1} < N_k$, allora significa che la porzione delle frequenze che hanno molteplicit√† $k$ nel corpus, sono maggiori di quelle che hanno molteplicit√† $k+1$ nel corpus. E quindi un token con molteplicit√† $k$ pi√π probabilmente rimarr√† con molteplicit√† $k$ invece di essere promosso a molteplicit√† $k+1$. 

Questo era un esempio di utilizzo in un unigramma, ma questo discorso vale per $N$-grammi in generale.


#### Limiti e Considerazioni

Il Good-Turing smoothing, pur essendo estremamente utile, presenta alcune limitazioni e aspetti da considerare:

1. **Instabilit√† quando $N_{k+1} = 0$**:  
   Se per un determinato $k$ non esistono Token osservati $k+1$ volte, la formula per $k^*$ non pu√≤ essere calcolata, rendendo il metodo inapplicabile in quei casi. In questo caso, si utilizzano metodi per stimare anche il valore di $N_{k+1}$ (e.g. [[regressione lineare|Regressione Lineare]]).

2. **Ridotta Efficacia per Token ad Alta Frequenza**:  
   Per Token molto frequenti (tipicamente per $k \geq 5$), il metodo pu√≤ risultare meno efficace, poich√© la stima diventa meno significativa.

3. **Complessit√† Computazionale**:  
   Calcolare $N_k$ per ogni valore di $k$ pu√≤ essere oneroso, soprattutto in corpus di grandi dimensioni. In tali contesti, possono essere necessarie semplificazioni o tecniche approssimative per rendere il calcolo computazionalmente gestibile.

---

### 4. **Absolute Discounting**

L'**Absolute Discounting** √® una tecnica di smoothing che applica uno **sconto fisso** $d$ a tutti gli n-grammi con conteggio positivo. L‚Äôidea di base √® simile al concetto generale di discounting: si sottrae una quantit√† fissa dal conteggio di ogni n-gramma osservato e si **ridistribuisce la massa probabilistica risparmiata** agli eventi non osservati.

#### **Formula**

Per un bigramma $w_{n-1}, w_n$ con conteggio $c(w_{n-1}, w_n)$, la probabilit√† scontata viene calcolata come:

$$
P_{\text{Abs}}(w_n | w_{n-1}) =
\frac{\max(c(w_{n-1}, w_n) - d, 0)}{c(w_{n-1})} + \lambda(w_{n-1}) \cdot P_{\text{backoff}}(w_n)
$$

- $d$: valore dello sconto (tipicamente tra 0.5 e 1.0, scelto empiricamente o stimato).
- $\lambda(w_{n-1})$: fattore di normalizzazione per il contesto $w_{n-1}$.
- $P_{\text{backoff}}(w_n)$: probabilit√† stimata da un modello di ordine inferiore (es. unigramma).

#### **Calcolo di $\lambda(w_{n-1})$**

Il termine $\lambda(w_{n-1})$ rappresenta **la massa di probabilit√† riassegnata** ai bigrammi non osservati. Si calcola come:

$$
\lambda(w_{n-1}) = \frac{d \cdot N_{+}(w_{n-1})}{c(w_{n-1})}
$$

dove:
- $N_{+}(w_{n-1})$ √® il numero di bigrammi diversi che iniziano con $w_{n-1}$ e hanno conteggio positivo.

#### **Esempio Pratico**

Supponiamo di avere il seguente contesto $w_{n-1} = \text{"gatto"}$ con questi bigrammi:

| Bigramma         | Conteggio |
|------------------|-----------|
| ("gatto", "mangia") | 5       |
| ("gatto", "corre")  | 3       |
| ("gatto", "salta")  | 2       |
| ("gatto", "parla")  | 0       |

Totale conteggi per "gatto":  
$$
c(\text{"gatto"}) = 5 + 3 + 2 = 10
$$

Applichiamo uno sconto $d = 0.75$. I conteggi scontati diventano:

- ("gatto", "mangia"): $5 - 0.75 = 4.25$  
- ("gatto", "corre"): $3 - 0.75 = 2.25$  
- ("gatto", "salta"): $2 - 0.75 = 1.25$

Numero di bigrammi osservati: $N_{+}(\text{"gatto"}) = 3$

Calcoliamo $\lambda(\text{"gatto"})$:

$$
\lambda(\text{"gatto"}) = \frac{0.75 \cdot 3}{10} = 0.225
$$

La probabilit√† per i bigrammi osservati diventa:

$$
P(\text{"mangia"}|\text{"gatto"}) = \frac{4.25}{10} = 0.425  
$$

La probabilit√† per un bigramma non osservato come ("gatto", "parla") sar√† determinata tramite backoff:

$$
P(\text{"parla"}|\text{"gatto"}) = 0.225 \cdot P_{\text{unigram}}(\text{"parla"})
$$

#### **Vantaggi**
- Pi√π accurato del Laplace/Add-$k$, in quanto riduce i conteggi solo per n-grammi **osservati**.
- √à una base del pi√π sofisticato **Kneser-Ney smoothing**.

#### **Limiti**
- Richiede un buon stimatore per $d$ (pu√≤ essere stimato da un dev set o con metodi come Good-Turing).
- Pu√≤ sottostimare gli n-grammi frequenti se $d$ √® scelto male.
- Viene utilizzato un modello di ordine inferiore per il backoff, e questo pu√≤ portare a problemi di generalizzazione.

### 5. **Kneser-Ney Smoothing (Stato dell'Arte)**  
Il Kneser-Ney smoothing √® considerato il metodo pi√π efficace per la modellazione linguistica con $n$-grammi, combinando **sconti dinamici** e una **probabilit√† di continuazione** per gestire contesti non osservati e ridurre il bias verso parole frequenti in contesti specifici.

#### **Formula Base**
Usando l'intuizione che deriva dall'Absolute Discounting e sostituendo la probabilit√† di un modello di ordine inferiore con una **probabilit√† di continuazione**, otteniamo (nel caso di un bigramma) la seguente formula:

$$
P_{\text{KN}}(w_i | w_{i-1}) = \underbrace{\frac{\max(c(w_{i-1}, w_i) - d, 0)}{c(w_{i-1})}}_{\text{Probabilit√† del bigramma scontato}} + \underbrace{\lambda(w_{i-1})}_\text{Fattore di interpolazione} \cdot \underbrace{P_{\text{cont}}(w_i)}_\text{Probabilit√† di continuazione}
$$  
- **$d$**: Fattore di sconto (tipicamente $d = 0.75$).  
- **$P_{\text{cont}}(w_i)$**: Probabilit√† di continuazione (quanti bigrammi completa $w_i$), definita come:  
  $$
  P_{\text{cont}}(w_i) = \frac{|\{w_{i-1} : c(w_{i-1}, w_i) > 0\}|}{|\{(w_{j-1}, w_j) : c(w_{j-1}, w_j) > 0\}|}
  $$  
  - Numeratore: Numero di contesti **diversi** in cui $w_i$ appare.  
  - Denominatore: Numero totale di bigrammi **diversi** osservati nel corpus.

- **$\lambda(w_{i-1})$**: Fattore di interpolazione per garantire che la somma delle probabilit√† sia 1:  
  $$
  \lambda(w_{i-1}) = \underbrace{\frac{d}{c(w_{i-1})}}_{\text{sconto normalizzato}} \cdot \overbrace{\underbrace{\underbrace{|\{w_i : c(w_{i-1}, w_i) > 0\}|}_{\text{Numero di bigrammi diversi in cui $w_i$ appare}}}_\text{Numero di volte che abbiamo applicato lo sconto}}^{\text{Numero di bigrammi scontati}}
  $$

In generale:

Per un generico n-gramma $w_{i-n+1}, \dots, w_{i-1}, w_i$:  

$$
P_{\text{KN}}(w_i | w_{i-n+1}^{i-1}) = \frac{\max\left(c(w_{i-n+1}^{i}) - d,\, 0\right)}{c(w_{i-n+1}^{i-1})} + \lambda(w_{i-n+1}^{i-1}) \cdot P_{\text{KN}}(w_i | w_{i-n+2}^{i-1})
$$

Dove:
- **Sconto** ($d$):  
  Valore fisso (es. $d = 0.75$).  
- **Fattore di interpolazione** ($\lambda$):  
  $$
  \lambda(w_{i-n+1}^{i-1}) = \frac{d \cdot |\{w_i : c(w_{i-n+1}^{i}) > 0\}|}{c(w_{i-n+1}^{i-1})}
  $$  
  dove $|\{w_i : c(w_{i-n+1}^{i}) > 0\}|$ √® il numero di **parole distinte** che seguono il contesto $w_{i-n+1}^{i-1}$.  

- **Probabilit√† di continuazione** (ricorsiva):  
  -  **Numeratore**: Contesti distinti $w_{i-n+1}$ per $w_{i-n+2}^{i}$.  
  - **Denominatore**: Totale n-grammi unici nel corpus. 
$$
P_{\text{KN}}(w_i | w_{i-n+2}^{i-1}) = \frac{\max\left(c(w_{i-n+2}^{i}) - d,\, 0\right)}{c(w_{i-n+2}^{i-1})} + \lambda(w_{i-n+2}^{i-1}) \cdot P_{\text{KN}}(w_i | w_{i-n+3}^{i-1}).
$$

Alla fine della ricorsione otteniamo la formula per gli unigrammi:

$$
P_{KN}(w) = \frac{\max(c(w) - d, 0)}{\underbrace{\sum_{w_i} c(w_i)}_\text{Somma totale dei conteggi di tutte le parole}} + \lambda(\epsilon) \frac{1}{V}
$$

Se vogliamo includere una parola sconosciuta `<UNK>`, la trattiamo semplicemente come una normale voce del vocabolario con conteggio pari a zero.  
Di conseguenza, la sua probabilit√† sar√† una distribuzione uniforme pesata dal fattore $\lambda$:

$$
P(<\!UNK\!>) = \lambda(\varepsilon) \cdot \frac{1}{V}
$$

dove:
- $\varepsilon$ √® la stringa vuota,
- $V$ √® la dimensione del vocabolario.

#### Intuizione per Sconto e Probabilit√† di Continuazione  
1. **Sconto (Discounting)**:  
   Riduce i conteggi degli $n$-grammi osservati per "riservare" massa probabilistica agli eventi non osservati.  
   Questo sconto penalizzer√† di meno il conteggio di parole molto frequenti (quelle di cui ci fidiamo di pi√π) e di pi√π il conteggio di parole poco frequenti (quelle di cui ci fidiamo di meno). 

2. **Probabilit√† di Continuazione**:  
   Misura quanto una parola $w_i$ √® **versatile** nell'apparire in contesti diversi.  
   - Penalizza parole come "Francisco" che appaiono spesso solo in contesti specifici (es. dopo "San").  
   - Premia parole come "the" o "di" che appaiono in molti contesti.  

#### Intuizione per $\lambda(w_{i-n+1}^{i-1})$

L'interpretazione intuitiva di $\lambda(w_{i-n+1}^{i-1})$ si articola in tre componenti principali:

1. **Sconto Normalizzato $\frac{d}{c(w_{i-n+1}^{i-1})}$:**  
   Questo termine rappresenta la frazione della probabilit√† totale associata al contesto $w_{i-n+1}^{i-1}$ che viene "tolta" per ciascun n-gramma osservato in quel contesto. Il parametro $d$ √® lo sconto fisso applicato, e dividendolo per $c(w_{i-n+1}^{i-1})$ (ovvero il numero totale di occorrenze del contesto $w_{i-n+1}^{i-1}$) si ottiene il **peso** o **quota** di probabilit√† ridotta per ogni occorrenza.

2. **Numero di n-grammi Scontati $|\{w_i : c(w_{i-n+1}^{i}) > 0\}|$:**  
   Questo termine conta il numero di n-grammi distinti che completano il contesto $w_{i-n+1}^{i-1}$ e che sono stati osservati almeno una volta. In altre parole, esso indica **quante volte lo sconto $d$ viene applicato** all'interno del contesto specificato, ovvero quante volte abbiamo "rimosso" una parte della probabilit√† dagli n-grammi osservati.

3. **Prodotto delle Due Componenti:**  
   Moltiplicando il **sconto normalizzato** per il **numero di n-grammi scontati**, si ottiene la **massa totale di probabilit√†** che √® stata sottratta dagli eventi osservati nel contesto $w_{i-n+1}^{i-1}$. Questa massa di probabilit√† viene poi utilizzata nel meccanismo di backoff (o interpolazione) per garantire che la somma complessiva delle probabilit√†, comprese quelle dei n-grammi non osservati, risulti pari a 1.

In sintesi, **$\lambda(w_{i-n+1}^{i-1})$** raccoglie il "peso" persa a causa dello sconto applicato a tutti gli n-grammi che seguono il contesto $w_{i-n+1}^{i-1}$, e tale massa viene poi ridistribuita al modello inferiore. Questo meccanismo assicura una distribuzione di probabilit√† completa e normalizzata anche quando alcuni n-grammi non sono stati osservati durante il training.

#### Dimostrazione che $\sum P_{\text{KN}}(w_i | w_{i-n+1}^{i-1}) = 1$

Consideriamo la formula per un bigramma:

$$
P_{\text{KN}}(w_i | w_{i-1}) = \frac{\max(c(w_{i-1}, w_i) - d, 0)}{c(w_{i-1})} + \lambda(w_{i-1}) \cdot P_{\text{cont}}(w_i)
$$

dove il fattore di interpolazione √® definito come

$$
\lambda(w_{i-1}) = \frac{d}{c(w_{i-1})} \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|.
$$

**Passo 1: Sommiamo $P_{\text{KN}}(w_i | w_{i-1})$ su tutti i possibili $w_i$:**

$$
\sum_{w_i} P_{\text{KN}}(w_i | w_{i-1}) = \sum_{w_i} \frac{\max(c(w_{i-1}, w_i) - d, 0)}{c(w_{i-1})} + \lambda(w_{i-1}) \sum_{w_i} P_{\text{cont}}(w_i)
$$

Sappiamo per certo che $P_{\text{cont}}(w_i)$ sia una distribuzione di probabilit√† valida, ovvero

$$
\sum_{w_i} P_{\text{cont}}(w_i) = 1.
$$

**Passo 2: Scomponiamo la somma per i bigrammi osservati.**

Per ogni $w_i$ tale che $c(w_{i-1}, w_i) > 0$ abbiamo:

$$
\max(c(w_{i-1}, w_i) - d, 0) = c(w_{i-1}, w_i) - d.
$$

Quindi, sommando su tutti i $w_i$ osservati otteniamo:

$$
\sum_{\{w_i: c(w_{i-1},w_i) > 0\}} \bigl[c(w_{i-1}, w_i)-d\bigr] 
= \left(\sum_{\{w_i: c(w_{i-1},w_i) > 0\}} c(w_{i-1}, w_i)\right) - d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|.
$$

Notiamo che

$$
\sum_{\{w_i: c(w_{i-1},w_i) > 0\}} c(w_{i-1}, w_i) = c(w_{i-1}),
$$

pertanto si ha:

$$
\sum_{\{w_i: c(w_{i-1},w_i) > 0\}} \bigl[c(w_{i-1}, w_i)-d\bigr] = c(w_{i-1}) - d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|.
$$

**Passo 3: Inseriamo il tutto nella sommatoria totale.**

Dividendo per $c(w_{i-1})$ si ottiene:

$$
\sum_{w_i} \frac{\max(c(w_{i-1},w_i)-d,0)}{c(w_{i-1})} 
= \frac{c(w_{i-1}) - d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|}{c(w_{i-1})}.
$$

Per la parte del backoff abbiamo:

$$
\lambda(w_{i-1}) \cdot \sum_{w_i} P_{\text{cont}}(w_i) = \lambda(w_{i-1}) \cdot 1 = \lambda(w_{i-1}).
$$

Quindi, la somma totale diventa:

$$
\sum_{w_i} P_{\text{KN}}(w_i | w_{i-1}) 
= \frac{c(w_{i-1}) - d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|}{c(w_{i-1})} + \lambda(w_{i-1}).
$$

**Passo 4: Verifica del vincolo di normalizzazione.**

Sostituendo la definizione di $\lambda(w_{i-1})$:

$$
\lambda(w_{i-1}) = \frac{d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|}{c(w_{i-1})},
$$

si ottiene:

$$
\sum_{w_i} P_{\text{KN}}(w_i | w_{i-1}) = \frac{c(w_{i-1}) - d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|}{c(w_{i-1})} + \frac{d \cdot |\{w_i : c(w_{i-1}, w_i) > 0\}|}{c(w_{i-1})} = \frac{c(w_{i-1})}{c(w_{i-1})} = 1.
$$

$\square$

**Conclusione per il Caso Generale (n-grammi):**

La stessa logica si estende al caso degli n-grammi tramite la formulazione ricorsiva:

$$
P_{\text{KN}}(w_i | w_{i-n+1}^{i-1}) = \frac{\max(c(w_{i-n+1}^{i}) - d,\,0)}{c(w_{i-n+1}^{i-1})} + \lambda(w_{i-n+1}^{i-1}) \cdot P_{\text{KN}}(w_i | w_{i-n+2}^{i-1}),
$$

con

$$
\lambda(w_{i-n+1}^{i-1}) = \frac{d \cdot |\{w_i : c(w_{i-n+1}^{i}) > 0\}|}{c(w_{i-n+1}^{i-1})}.
$$

**Argomentazione per Induzione:**

1. **Base dell'induzione (n = 2 ‚Äì bigrammi):**  
   Abbiamo dimostrato che

   $$
   \sum_{w_i} P_{\text{KN}}(w_i | w_{i-1}) = 1.
   $$

2. **Passo induttivo:**  
   Supponiamo che per un modello di ordine $n-1$ (cio√® con condizione $w_{i-n+2}^{i-1}$) la propriet√† di normalizzazione sia soddisfatta:

   $$
   \sum_{w_i} P_{\text{KN}}(w_i | w_{i-n+2}^{i-1}) = 1.
   $$

   Allora, considerando la formula ricorsiva per il modello di ordine $n$:

   $$
   \sum_{w_i} P_{\text{KN}}(w_i | w_{i-n+1}^{i-1}) = \sum_{w_i} \left[ \frac{\max(c(w_{i-n+1}^{i}) - d,\,0)}{c(w_{i-n+1}^{i-1})} \right] + \lambda(w_{i-n+1}^{i-1}) \sum_{w_i} P_{\text{KN}}(w_i | w_{i-n+2}^{i-1}).
   $$

   Utilizzando l'ipotesi induttiva $\sum_{w_i} P_{\text{KN}}(w_i | w_{i-n+2}^{i-1}) = 1$ e seguendo i medesimi passaggi del caso bigramma, si ottiene:

   $$
   \sum_{w_i} P_{\text{KN}}(w_i | w_{i-n+1}^{i-1}) = \frac{c(w_{i-n+1}^{i-1}) - d \cdot |\{w_i : c(w_{i-n+1}^{i}) > 0\}|}{c(w_{i-n+1}^{i-1})} + \lambda(w_{i-n+1}^{i-1}) = 1.
   $$

Pertanto, per induzione, la propriet√† di normalizzazione vale per qualsiasi ordine $n$:

$$
\sum_{w_i} P_{\text{KN}}(w_i | w_{i-n+1}^{i-1}) = 1.
$$

$\square$

#### **Variante del Kneser-Ney**  
1. **Modified Kneser-Ney**:  
   Usa **sconti differenziati** per conteggi $c = 1$, $c = 2$, e $c \geq 3$:  
   - $d_1 = 0.75$ (per $c=1$),  
   - $d_2 = 0.5$ (per $c=2$),  
   - $d_3 = 0.25$ (per $c \geq 3$).

#### **Vantaggi**  
1. **Gestione ottimale delle parole comuni**:  
   - "Francisco" avr√† bassa $P_{\text{cont}}$ perch√© appare solo dopo "San".  
   - "the" avr√† alta $P_{\text{cont}}$ perch√© appare in molti contesti.  

2. **Adattabilit√† a contesti sparsi**:  
   Usa informazioni degli $n$-grammi di ordine inferiore in modo pi√π efficace rispetto a Good-Turing.  

3. **Performance superiori**:  
   √à lo standard per modelli linguistici in task come traduzione automatica e riconoscimento vocale.  

#### **Limiti**  
1. **Complessit√† computazionale**:  
   Richiede il calcolo di $P_{\text{cont}}$ per tutte le parole e contesti, costoso per corpus di grandi dimensioni.  

2. **Scelta dei parametri**:  
   Il valore di $d$ e la variante (interpolated vs modified) influenzano significativamente i risultati.    

## Tabella di Confronto

| **Metodo**         | **Idee Chiave**                                                                 | **Vantaggi**                                  | **Svantaggi**                                                                 | **Casi d'Uso**                     |
|---------------------|---------------------------------------------------------------------------------|----------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------|
| **Laplace (Add-One)** | Aggiunge 1 al conteggio di ogni n-gramma per evitare probabilit√† zero.          | Semplice da implementare.                    | Sovrastima eventi rari, inefficace per vocabolari grandi ($V$ elevato).       | Corpus piccoli, prototipazione.     |
| **Add-$k$**          | Aggiunge un conteggio frazionario $k$ (es. 0.5) invece di 1.                   | Pi√π flessibile di Laplace.                   | Difficolt√† nella scelta di $k$, varianza elevata, sconti inappropriati.       | Classificazione testi, task specifici. |
| **Good-Turing**     | Ridistribuisce massa dagli eventi frequenti a quelli rari usando $N_k$.         | Fondamento teorico solido.                   | Instabile per $N_{k+1}=0$, complessit√† computazionale, inefficace per $k$ alti. | Corpus medi, modelli con sparsit√†.  |
| **Kneser-Ney**      | Combina sconti e probabilit√† di continuazione per gestire contesti.             | Gestione avanzata dei contesti, stato dell'arte. | Complessit√† implementativa, richiede calcolo di $P_{\text{cont}}$.            | Modelli linguistici avanzati (es. NLP moderno). |

## Conclusioni  
I metodi di smoothing risolvono il problema degli n-grammi non osservati o rari, ma con compromessi tra semplicit√† e accuratezza:  

1. **Laplace e Add-$k$** sono adatti per **scenari semplici** (corpus piccoli o prototipi), ma diventano rapidamente inefficaci con vocabolari ampi.  
2. **Good-Turing** offre una **base teorica rigorosa** per la ridistribuzione della massa probabilistica, ma la sua complessit√† e instabilit√† lo rendono poco pratico per corpus molto grandi.  
3. **Kneser-Ney** √® lo **stato dell'arte** per la modellazione linguistica, grazie alla combinazione di sconti dinamici e probabilit√† di continuazione, che penalizzano parole comuni in contesti specifici (es. "Francisco" dopo "San").  

**Raccomandazioni**:  
- Usare **Kneser-Ney** per task avanzati (es. riconoscimento vocale, traduzione automatica).  
- Optare per **Good-Turing** se √® necessaria una ridistribuzione teorica senza troppa complessit√†.  
- **Laplace/Add-$k$** sono utili solo in fase esplorativa o con dati limitati.  

In sintesi, la scelta dipende dal trade-off tra risorse computazionali, dimensione del corpus e necessit√† di precisione. Per applicazioni reali, Kneser-Ney rimane il gold standard nonostante la sua complessit√†.  
