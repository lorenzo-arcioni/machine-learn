# ğŸ§  Word2Vec: Una Spiegazione Dettagliata e Intuitiva

## Cos'Ã¨ Word2Vec?

Word2Vec Ã¨ una tecnica di **apprendimento non supervisionato** introdotta da **Tomas Mikolov** nel 2013 al Google Research. Serve a rappresentare le **parole** come **vettori continui** in uno spazio n-dimensionale, catturando **relazioni semantiche e sintattiche** tra parole.

### ğŸ§© PerchÃ© usare Word2Vec?

- PerchÃ© le **parole non sono numeri**, ma per addestrare modelli di Machine Learning abbiamo bisogno di rappresentazioni **numericamente significative**.
- Word2Vec permette di **mappare parole simili in vettori simili** nello spazio.
- Ãˆ alla base di molte applicazioni NLP: **motori di ricerca, chatbot, traduttori automatici, recommender system**, ecc.

## ğŸ“Œ Obiettivo: Embedded Meaning

Word2Vec impara a **prevedere il contesto** di una parola o la parola dato un contesto. A differenza del classico one-hot encoding (dove ogni parola Ã¨ un vettore ortogonale), Word2Vec assegna a ogni parola un **vettore denso** (dense vector) che ne **cattura il significato**. Ad esempio:

$$
\text{King} - \text{Man} + \text{Woman} \approx \text{Queen}.
$$

Questo tipo di **algebra semantica** Ã¨ possibile perchÃ© Word2Vec apprende delle **rappresentazioni distribuite** (distributed representations).

## ğŸ› ï¸ Architettura: CBOW vs Skip-Gram

Word2Vec ha due principali architetture:

### 1. CBOW (Continuous Bag of Words)

- Obiettivo: **Predire la parola centrale** dato il **contesto** (le parole attorno).
- Esempio:
  - Contesto: "il __ beve latte"
  - Target: "gatto"

**Funziona meglio con dataset piccoli e parole frequenti.**

### 2. Skip-Gram

- Obiettivo: **Predire il contesto** dato una parola centrale.
- Esempio:
  - Input: "gatto"
  - Output atteso: ["il", "beve", "latte"]

**Funziona meglio con dataset grandi e parole rare.**

## ğŸ“ Come funziona internamente?

### Step 1: One-Hot Encoding dell'input

Ogni parola Ã¨ rappresentata come un vettore lungo quanto il vocabolario (es: 10.000 parole), con uno 0 ovunque tranne un 1 nella posizione della parola.

Esempio:
```
gatto â†’ [0, 0, ..., 1, ..., 0]
```

### Step 2: Layer di embedding (matrice peso)

- Lâ€™input one-hot moltiplicato per una **matrice dei pesi W** restituisce il vettore dense.
- Se `W` ha dimensione `(vocab_size, embedding_dim)` â†’ otterrai un vettore `embedding_dim` (es. 100).

### Step 3: Output layer + Softmax

Nel caso di CBOW:
- Il contesto viene mediato â†’ moltiplicato per `W.T` â†’ passaggio a softmax â†’ si confronta con la parola target.
Nel caso di Skip-Gram:
- La parola target genera il vettore â†’ moltiplicato â†’ predice ogni parola nel contesto.

## ğŸ’¡ Intuizione geometrica

- Le **distanze cosine** tra vettori di parole simili sono piccole.
- I vettori non hanno significato assoluto, ma **relativo**: il significato emerge dalla posizione rispetto alle altre parole.

ğŸ“· ![Spazio vettoriale 2D](/img/word2vec_projection.png)

## âš™ï¸ Ottimizzazione: Negative Sampling e Hierarchical Softmax

### ğŸ§® Problema

Calcolare una softmax su un vocabolario da 100.000 parole Ã¨ costoso.

### ğŸ’¡ Soluzioni:

#### 1. Negative Sampling

- Invece di aggiornare **tutti i vettori**, si aggiornano solo quelli delle parole **corrette** e di alcune **parole negative scelte a caso**.

- Esempio:
  - Target: "gatto"
  - Negative samples: ["astronave", "carburatore", "banana"]

- Si applica una **logistic regression binaria**: vera parola = 1, parole campione = 0.

#### 2. Hierarchical Softmax

- Organizza le parole in un **albero binario Huffman**.
- Ogni parola Ã¨ una foglia, ogni predizione Ã¨ un **cammino da radice a foglia**.
- Riduce la complessitÃ  computazionale da `O(V)` a `O(log V)`.

## ğŸ§  Significato Semantico nei Vettori

Grazie a Word2Vec, i vettori imparati **catturano concetti** come:

- **Somiglianza semantica**: "gatto" vicino a "cane"
- **Relazioni analogiche**: "re - uomo + donna â‰ˆ regina"
- **Sinonimia**: "auto" vicino a "macchina"

ğŸ“· ![Analogies](/img/word2vec_analogies.png)

## ğŸ§ª QualitÃ  degli embeddings

### ğŸ” Metriche comuni:

- **Cosine Similarity**: misura quanto due vettori puntano nella stessa direzione.
- **Word Analogies**: test come "paris : france = tokyo : ?"
- **t-SNE / PCA**: visualizzazioni per ridurre la dimensionalitÃ  e mostrare cluster semantici.

## ğŸ“ˆ Training Tips

- Dataset piÃ¹ grande â†’ embedding migliore.
- Pulizia dei dati importante: rimuovere stopword puÃ² aiutare.
- Finestra (window size) bilancia semantica vs sintassi:
  - **Piccola (2-3)** â†’ sintassi
  - **Grande (5-10)** â†’ semantica

## ğŸ§° Implementazioni e librerie

- `gensim.models.Word2Vec` (Python, semplice e veloce)
- TensorFlow / PyTorch per implementazioni personalizzate
- FastText (Facebook): estende Word2Vec gestendo morfologia con n-grammi

## ğŸ§­ Limiti di Word2Vec

- StaticitÃ : ogni parola ha **un solo vettore**, anche se ha significati diversi.
- Mancanza di contesto: non Ã¨ sensibile alla frase.
- Non gestisce frasi nÃ© strutture sintattiche.

â¡ï¸ Per superare questi limiti: **ELMo, BERT, GPT** e altri modelli **contextual embeddings**.

## ğŸ“š Risorse utili

- Paper originale: "Efficient Estimation of Word Representations in Vector Space" â€“ Mikolov et al., 2013
- [Tutorial ufficiale Gensim](https://radimrehurek.com/gensim/)
- Dataset: Google News (300D), Wikipedia, Common Crawl

## âœ… Conclusione

Word2Vec ha rivoluzionato l'NLP permettendo una **rappresentazione densa e semantica** delle parole. Ãˆ stato il **ponte tra il bag-of-words e i language model profondi**. Comprendere a fondo Word2Vec Ã¨ essenziale per chiunque lavori con linguaggio naturale, reti neurali e intelligenza artificiale in generale.

ğŸ“ *"You shall know a word by the company it keeps" â€“ J.R. Firth (1957)*
