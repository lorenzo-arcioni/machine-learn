# üî§ Dense Word Embeddings

## üß† Concetti Chiave

I **word embeddings densi** sono rappresentazioni continue e distribuite delle parole in uno spazio vettoriale a bassa dimensione ($\mathbb{R}^d$). Raspresentano uno sviluppo fondamentale nell‚ÄôNLP moderno perch√© permettono ai modelli di **catturare somiglianze semantiche** tra le parole.

### ‚úÖ Obiettivo
> Mappare ogni parola $w \in \mathcal{V}$ (vocabolario) in un vettore continuo $\mathbf{w} \in \mathbb{R}^d$ con $d \ll |\mathcal{V}|$

## üîç Perch√© embeddings "densi"?

Con i [[Sparse Word Embeddings]] (come one-hot encoding o matrici sparse TF-IDF):

- üü• I vettori sono **sparsi**: la maggior parte delle componenti √® zero.
- üü• Non catturano **relazioni semantiche** (es. ‚Äúgatto‚Äù e ‚Äúfelino‚Äù sono ortogonali).
- üü• Non generalizzano bene.

Con i dense word embeddings:

- ‚úÖ Le parole **semanticamente simili** hanno vettori **vicini** (es. distanza coseno piccola)
- ‚úÖ Le rappresentazioni sono **dense**: tutti i valori sono reali e significativi
- ‚úÖ I modelli possono sfruttare **algebra lineare** semantica:  
  $\text{king} - \text{man} + \text{woman} \approx \text{queen}$

## ‚öôÔ∏è Formato matematico

Dati:

- Un vocabolario $\mathcal{V}$
- Ogni parola $w_i$ √® rappresentata da un vettore $\mathbf{w}_i \in \mathbb{R}^d$
- Una matrice $\mathbf{W} \in \mathbb{R}^{|\mathcal V| \times d}$ che contiene tutti gli embeddings come righe

Allora:

$$
\mathbf{W} = 
\begin{bmatrix}
\text{--------- }\mathbf{w}_1^\top \text{ ---------}\\
\text{--------- }\mathbf{w}_2^\top \text{ ---------}\\
\vdots \\
\text{--------- }\mathbf{w}_V^\top \text{ ---------}
\end{bmatrix}
$$

Con:
- $d$: dimensione dello spazio semantico (tipicamente 50-300)
- Ogni riga: un vettore di embedding per una parola

## üß™ Come si apprendono?

Esistono diversi approcci per apprendere embeddings:

### 1. üè∑Ô∏è Predictive: basati su modelli linguistici ([[Word2Vec]], FastText)

- Usano una rete neurale shallow per predire parole basate sul contesto o viceversa
- Si basano sull'ipotesi di distribuzionalit√†:
  > "Parole che appaiono in contesti simili hanno significati simili" (Harris, 1954)

#### üìå [[Word2Vec]] (CBOW e Skip-gram)

- **CBOW (Continuous Bag of Words)**: predice una parola dato il suo contesto
- **Skip-Gram**: predice le parole del contesto dato una parola centrale

##### Skip-Gram: formulazione

Dati $T$ token $(w_1, \dots, w_T)$, massimizziamo:

$$
\mathcal{L} = \sum_{t=1}^T \sum_{-c \le j \le c, j \neq 0} \log P(w_{t+j} \mid w_t)
$$

Dove $c$ √® la dimensione della finestra di contesto.

La probabilit√† condizionata √® modellata tramite softmax:

$$
P(w_o \mid w_i) = \frac{\exp\left(\mathbf{v}_{w_o}^\top \cdot \mathbf{v}_{w_i}\right)}{\sum_{w \in \mathcal{V}} \exp\left(\mathbf{v}_w^\top \cdot \mathbf{v}_{w_i}\right)}
$$

> ‚ö†Ô∏è Calcolo costoso per vocabolari grandi ‚Üí si usano trucchi: **Negative Sampling**, **Hierarchical Softmax**

### 2. üìä Count-based: basati su co-occorrenze (es. GloVe)

- Costruiscono una **matrice di co-occorrenza** globale $X \in \mathbb{R}^{V \times V}$, dove $X_{ij}$ √® quante volte $w_j$ appare nel contesto di $w_i$
- Idea: l‚Äôembedding di una parola deve catturare le **statistiche di co-occorrenza** con tutte le altre

#### GloVe: Global Vectors for Word Representation

Minimizza:

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) \left( \mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2
$$

- $\mathbf{w}_i$, $\tilde{\mathbf{w}}_j$: embeddings per parola e contesto
- $b_i$, $\tilde{b}_j$: bias
- $f(x)$: funzione peso per regolare l‚Äôinfluenza delle co-occorrenze molto frequenti

## üßÆ Propriet√† geometriche emergenti

- Le parole con significato simile formano **cluster** nello spazio $\mathbb{R}^d$
- Relazioni semantiche lineari diventano **operazioni vettoriali**
- Gli assi principali possono catturare dimensioni latenti come **genere**, **tempo**, **concretezza**

## üíæ Embeddings Pre-Addestrati

Sono disponibili embeddings gi√† addestrati su enormi corpora:

- **[[Word2Vec]]** (Google News)
- **GloVe** (Wikipedia + Gigaword)
- **FastText** (Facebook AI)
- **ELMo**, **BERT** (contextual embeddings ‚Üí pi√π avanzati)

Vantaggi:

- üöÄ Riutilizzabili in downstream tasks
- üìà Migliorano le performance anche su dataset piccoli

## üìâ Limitazioni

- Non contestualizzati: ogni parola ha **una sola rappresentazione**, anche se pu√≤ avere **pi√π significati** (es. *banca* come istituto finanziario o riva del fiume)
- Non aggiornabili in tempo reale durante training fine-tuning
- Hanno **bias** (di genere, razza, ecc.) appresi dal corpus

## üîö Conclusione

I word embeddings densi hanno rivoluzionato il NLP grazie alla loro capacit√† di:

- Rappresentare il significato distribuzionale in forma compatta
- Catturare similarit√† semantiche e sintattiche
- Abilitare tecniche avanzate di NLP (classificazione, similarit√†, clustering, ecc.)

Sono alla base dei modelli **contestualizzati** moderni (BERT, GPT), che ne estendono la filosofia.

### ‚úÖ Riassumendo, i principali vantaggi sono:

- **Compattezza**: ogni parola √® rappresentata da un vettore $d$-dimensionale, dove $d$ √® gestibile computazionalmente (tipicamente 100‚Äì300).
- **Semanticit√†**: parole simili semanticamente hanno vettori geometricamente vicini.
- **Operazioni vettoriali significative**: le differenze tra vettori spesso riflettono analogie semantiche coerenti.
- **Riutilizzabilit√†**: possono essere pre-addestrati su grandi corpora e impiegati in numerose applicazioni downstream.

### ‚ö†Ô∏è Tuttavia, presentano anche alcune criticit√†:

- La rappresentazione √® **statica**: ogni parola ha un solo embedding, indipendentemente dal contesto in cui appare.
- Sono soggetti a **bias** insiti nei dati di addestramento (sessuali, razziali, culturali, ecc.).
- Non gestiscono ambiguit√† lessicali, polisemia o variazioni contestuali in modo naturale.

Per queste ragioni, negli ultimi anni si √® evoluta una nuova generazione di rappresentazioni: gli **embeddings contestualizzati** (es. ELMo, BERT, GPT), i quali, invece di assegnare un unico vettore a ciascuna parola, generano una rappresentazione dinamica **dipendente dal contesto locale**.

