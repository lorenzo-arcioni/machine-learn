{
  "title": "Partitioning-based Clustering",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"definizione\">Definizione</h2>\n<p>Gli algoritmi di clustering basati su partizioni suddividono un dataset in <strong>$K$ gruppi distinti</strong>. Ogni punto dati appartiene a uno e un solo cluster, e i cluster vengono definiti in modo tale da minimizzare la distanza intra-cluster e massimizzare la distanza inter-cluster.</p>\n<p>Quindi abbiamo la seguente configurazione:</p>\n<ul>\n<li><strong>Input</strong>: Un insieme di $N$ data points e un numero $K$ (tale che $K < N$)</li>\n<li><strong>Goal</strong>: Trovare una partizione che ottimizza un certo criterio (funzione obiettivo).</li>\n<li><strong>Output</strong>: Una partizione degli $n$ data points in $K$ clusters.</li>\n</ul>\n<p>Esempio di partizione con $K=2$:\n    $C = [0, 1, 1, 0, 1, 0, \\cdots, 1, 0]$</p>\n<p>Ma quante sono le possibili partizioni di un insieme? Beh potenzialmente potremmo pensare di avere $K^N$ partizioni, perché per ogni punto possiamo scegliere $K$ clusters, e quindi $K \\cdot K \\cdot \\ \\cdots \\ \\cdot K$, $N$ volte. Ma in realtà molte di meno in quanto siamo interessati solamente a quelle dove ogni cluster contiene almeno $1$ data point, ed inoltre stiamo considerando anche le stesse partizioni più volte, come ad esempio:\n$$\\begin{align}\n\\{[1, 2], [3, 4]\\} \\rightarrow (0, 0, 1, 1)\\\\\n\\{[3, 4], [1, 2]\\} \\rightarrow (1, 1, 0, 0)\n\\end{align}.\n$$\nPossiamo però essere molto più precisi utilizzando il numero di Stirling per le partizioni:</p>\n$$\nS(K, N) = \\begin{Bmatrix}\nx\\\\\ny\n\\end{Bmatrix}\\approx \\frac{K^N}{K!} \\in O(K^N)\n\\\n$$\nQuindi, la ricerca della partizione ottima (i.e. , che minimizza la funzione obiettivo) è un problema intrattabile, dal punto di vista computazionale, per molte funzioni di costo. Ma possiamo benissimo cavarcela con delle euristiche, come il <a href=\"/theory/unsupervised-learning/Clustering/K-Means\" class=\"text-blue-600 hover:underline\">K-Means</a> , <span class=\"text-gray-600\">K-Medoids</span> e il <a href=\"/theory/unsupervised-learning/Clustering/K-means++\" class=\"text-blue-600 hover:underline\">K-Means++</a>.</p>\n<h2 id=\"un-framework-generico-per-il-clustering\">Un Framework Generico per il Clustering</h2>\n<h3 id=\"flat-hard-clustering\">Flat Hard Clustering</h3>\n<p>Il <strong>flat hard clustering</strong> è un metodo di clustering in cui i dati vengono suddivisi in gruppi <strong>non gerarchici</strong> (flat) e ogni punto appartiene <strong>esattamente a un solo cluster</strong> (hard), senza sovrapposizioni. Quindi, formalmente:</p>\n<ul>\n<li>$\\{\\vec x_1, \\vec x_2, \\cdot, \\vec x_N\\}$ i dati di input.</li>\n<li>$\\{C_1, C_2, \\cdots, C_K\\}$ cluster di output.</li>\n<li>\n<p>$\\vec \\Theta_k$ è il <strong>rappresentante</strong> del cluster $C_k$.\nDove per rappresentante si intende un valore (o un insieme di valori) che descrive in modo sintetico le caratteristiche del cluster $C_k$. Può essere, ad esempio: </p>\n</li>\n<li>\n<p><strong>Il centroide</strong>: nel caso di algoritmi come <strong>K-Means</strong>, è il punto medio dei dati all&rsquo;interno del cluster $C_k$, calcolato come: $$ \\vec{\\Theta}_k = \\frac{1}{|C_k|} \\sum_{\\vec{x}_i \\in C_k} \\vec{x}_i $$\n</li>\n<li><strong>Il medoid</strong>: per algoritmi come <strong>K-Medoids</strong>, è l&rsquo;elemento del cluster $C_k$ che minimizza la somma delle distanze con gli altri punti del cluster. </li>\n<li><strong>Una distribuzione probabilistica</strong>: in approcci più avanzati (ad esempio clustering bayesiano), $\\vec{\\Theta}_k$ può rappresentare i parametri di una distribuzione (es. media e varianza di una gaussiana). In generale, il rappresentante $\\vec{\\Theta}_k$ è scelto per ottimizzare un criterio (ad esempio, la minimizzazione della varianza intra-cluster).</li>\n</ul>\n<h3 id=\"funzione-obiettivo\">Funzione Obiettivo</h3>\n<p>La funzione obiettivo è in grado di dirci quanto il nostro clustering è ottimale:\n$$\\large\nL(A, \\Theta) = \\sum_{n=1}^N \\sum_{k=1}^K \\alpha_{n, k} \\cdot \\delta(\\vec x_n, \\vec \\Theta_k)\n$$\nDove:</p>\n<ul>\n<li>$A$ è una matrice $N \\times K$ tale che\n$$\n\\alpha_{n, k} = \\begin{cases}\n1 \\quad \\text{se } \\vec x_n \\text{ è assegnato al cluster } k\\\\\n\\\\\n0 \\quad \\text{altrimenti}\n\\end{cases}.\n$$</li>\n<li>$\\Theta = \\{\\vec \\Theta_1, \\cdots, \\vec \\Theta_K\\}$ è l&rsquo;insieme dei rappresentanti dei clusters.</li>\n<li>$\\delta(\\vec x_n, \\vec \\Theta_k)$ è una funzione che misura la distanza tra $\\vec x_n$ e $\\vec \\Theta_k$.\nQuindi\n$$\nA^*, \\Theta^* = \\argmin_{A, \\Theta} L(A, \\Theta),\n$$\nma come abbiamo già visto, la soluzione a questo problema è $NP$-Hard, perché esplora tutte le possibili partizioni $O(K^N)$.</li>\n</ul>\n<p>Però, possiamo comunque trovare una soluzione non-convessa data l&rsquo;assegnazione discreta dei valori nella matrice $A$ (minimi locali multipli). Tuttavia, in questo modo, non possiamo sfruttare le proprietà delle funzioni convesse, come quella di trovare un minimo globale.</p>\n<p>La matrice $A$ infatti è composta da valori discreti ($0$ o $1$), quindi il problema è combinatorio e non può essere trattato come un problema di ottimizzazione continua. \nNon possiamo utilizzare tecniche analitiche, come il calcolo differenziale, per trovare facilmente il minimo. </p>\n<p>$L(A, \\Theta)$ non è convessa rispetto a $A$ e $\\Theta$, quindi può avere <strong>minimi locali multipli</strong>. Non esiste garanzia di trovare il minimo globale utilizzando metodi iterativi.</p>\n<h3 id=\"soluzione-iterativa-algoritmo-di-lloyd-forgy\">Soluzione Iterativa <a href=\"/theory/unsupervised-learning/Clustering/Algoritmo di Lloyd-Forgy\" class=\"text-blue-600 hover:underline\">Algoritmo di Lloyd-Forgy</a></h3>\n<p>Questo algoritmo non garantisce una soluzione ottima, perché potremmo fermarci ad un minimo locale o ad un punto di sella. L&rsquo;algoritmo si compone di due steps:\n- <strong>Assegnazione</strong>:\n    Minimizzare $L$ rispetto ad $A$ fissando $\\Theta$. Infatti non possiamo calcolare il gradiente di $L$ in quanto $A$ è una matrice discreta.\n    Intuitivamente, dato un set (fisso) di rappresentanti, $L$ è minimizzata se ogni data point $\\vec x_n$ è assegnato al cluster del più vicino rappresentante (ad $\\vec x_n$).\n    <strong>Remark:</strong> $L$ è la somma di tutte le distanze da ogni data point al suo rappresentante.\n    Quindi,</p>\n$$\n\\large\n\\alpha_{n, k} = \\begin{cases}\n1 \\quad \\text{se } \\delta(\\vec x_n, \\vec \\Theta_k) = \\min_{1 \\leq j \\leq K} \\{\\delta(\\vec x_n, \\vec \\Theta_j)\\} \\\\\n0 \\quad \\text{altrimenti}\n\\end{cases}.\n$$\n<ul>\n<li><strong>Ottimizzazione</strong>:\n    Minimizzare $L$ rispetto a $\\Theta$ fissando $A$. Se fissiamo $A$, possiamo tranquillamente calcolare il gradiente di $L$ (in quanto ora $L(\\vec \\Theta_1, \\cdots, \\vec \\Theta_K; A)$ dipende solo dal parametro $\\Theta$), porlo $= \\vec 0$ e risolvere per $\\Theta$.\n$$\n\\large\n\\begin{align*}\n\\nabla L(\\Theta; A) &= \\left(\\frac{\\partial L(\\Theta; A)}{\\partial \\vec \\Theta_1}, \\cdots,  \\frac{\\partial L(\\Theta; A)}{\\partial \\vec \\Theta_K} \\right)^T\\\\\n&= \\left(\\frac{\\partial L(\\vec \\Theta_1, \\cdots, \\vec \\Theta_K; A)}{\\partial \\vec \\Theta_1}, \\cdots,  \\frac{\\partial L(\\vec \\Theta_1, \\cdots, \\vec \\Theta_K; A)}{\\partial \\vec \\Theta_K} \\right)^T\n\\end{align*}\n$$ \nQuindi,\n$$\n\\nabla L(\\Theta; A) = \\vec 0 \\iff \\frac{\\partial L(\\vec \\Theta_1, \\cdots, \\vec \\Theta_K; A)}{\\partial \\vec \\Theta_j} = \\underbrace{\\frac{\\partial L}{\\partial \\vec \\Theta_j}}_\\text{per semplificare la notazione} = 0 \\quad \\forall j \\in [K].\n$$ \nCalcoliamo ora\n$$\n\\frac{\\partial L}{\\partial \\vec \\Theta_j} = \\frac{\\partial}{\\partial \\vec \\Theta_j} \\sum_{n=1}^N \\sum_{k=1}^K \\alpha_{n, k} \\cdot \\delta(\\vec x_n, \\vec \\Theta_k) = \\frac{\\partial}{\\partial \\vec \\Theta_j} \\sum_{n=1}^N \\alpha_{n, k} \\cdot \\delta(\\vec x_n, \\vec \\Theta_j)\n$$\ne poi non ci resta che risolvere\n$$\n\\frac{\\partial}{\\partial \\vec \\Theta_j} \\sum_{n=1}^N \\alpha_{n, k} \\cdot \\delta(\\vec x_n, \\vec \\Theta_j) = 0\n$$\nche ovviamente, ora che $\\alpha_{n, k}$ è una costante, dipende dalla funzione $\\delta$.</li>\n</ul>\n<h2 id=\"algoritmi-principali\">Algoritmi principali</h2>\n<ul>\n<li><a href=\"/theory/unsupervised-learning/Clustering/K-Means\" class=\"text-blue-600 hover:underline\">K-Means</a>: Suddivide i dati in $K$ cluster minimizzando la somma delle distanze quadrate tra i punti e i centroidi.</li>\n<li><span class=\"text-gray-600\">K-Medoids</span>: Simile a K-Means, ma utilizza punti effettivi del dataset come rappresentanti dei cluster (medoids) per ridurre l’impatto degli outlier.</li>\n<li><a href=\"/theory/unsupervised-learning/Clustering/K-means++\" class=\"text-blue-600 hover:underline\">K-Means++</a>:</li>\n</ul>\n<h2 id=\"limitazioni\">Limitazioni</h2>\n<ul>\n<li>Richiede di definire il numero di cluster ($K$) a priori.</li>\n<li>Sensibile agli outlier, che possono influenzare i centroidi.</li>\n</ul>\n<h2 id=\"applicazioni\">Applicazioni</h2>\n<ul>\n<li>Segmentazione clienti in ambito marketing. </li>\n<li>Clusterizzazione di documenti in <span class=\"text-gray-600\">Natural Language Processing</span>. </li>\n<li>Compressione di immagini (es. riduzione di colori usando K-Means).</li>\n</ul>"
}