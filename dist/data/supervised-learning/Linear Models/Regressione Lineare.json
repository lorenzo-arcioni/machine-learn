{
  "title": "Regressione Lineare",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>La <strong>Regressione Lineare</strong> √® un modello statistico utilizzato per descrivere la relazione tra una variabile dipendente (target) e una o pi√π variabili indipendenti (predittori). Assume una relazione lineare tra le variabili e minimizza l&rsquo;errore quadratico medio. Quindi la regressione lineare studia la dipendenza in media tra fenomeni, cercando una funzione che esprima tale dipendenza in modo lineare.</p>\n<h2 id=\"1-formulazione-generale\"><strong>1. Formulazione Generale</strong></h2>\n<p>Assumiamo di avere un dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, dove $x_i \\in \\mathbb R$ sono le variabili indipendenti e $y_i \\in \\mathbb R$ la variabile dipendente.\nUn modello di regressione lineare ha la forma:</p>\n$$\ny_i = f(\\mathbf{x}_i) + \\epsilon_i = \\mathbf{x}_i^T \\mathbf{w} + \\epsilon_i = 1 \\cdot w_0 + x_iw_1 + \\epsilon_i\n$$\n<p>Dove:</p>\n<ul>\n<li>$y_i \\in \\mathbb R$ √® la variabile target,</li>\n<li>$\\mathbf{x}_i = \\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix}$, dove gli $x_i$ sono le variabili indipendenti (features),</li>\n<li>$\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix} \\in \\mathbb R^2$ sono i coefficienti del modello (parametri da stimare),</li>\n<li>$w_0$ √® l&rsquo;intercetta, che rappresenta il valore previsto di $y$ quando $x = 0$,</li>\n<li>$w_1$ rappresenta la pendenza, che rappresenta quanto $y$ cresce per ogni unita di aumento di $x$.</li>\n<li>$1$ √® il cosi detto bias,</li>\n<li>$\\epsilon_i$ √® l&rsquo;errore (rumore) che segue una <a href=\"/theory/math-for-ml/Probabilit√†/Distribuzioni/Distribuzione Normale\" class=\"text-blue-600 hover:underline\">distribuzione normale</a> $\\mathcal{N}(0, \\sigma^2)$.</li>\n</ul>\n<hr />\n<h3 id=\"approfondimento-sul-rumore\">Approfondimento sul rumore</h3>\n<p>Il termine $\\epsilon_i$ incapsula <strong>tutte</strong> le fonti di variabilit√† non spiegate dal modello: rumore di misura, fattori non osservati, approssimazioni del modello, ecc. L&rsquo;ipotesi sulla distribuzione di $\\epsilon_i$ √® quindi l&rsquo;ipotesi fondamentale che determina sia la funzione di verosimiglianza sia le propriet√† inferenziali.</p>\n<p>Se il rumore osservato √® il risultato della <strong>somma di molti piccoli contributi indipendenti</strong> (errori di misura multipli, micro‚Äëfluttuazioni, componenti di variabili non modellate), il <strong>Teorema del Limite Centrale</strong> (CLT) dice che la somma (appropriatamente normalizzata) tende in distribuzione a una normale.</p>\n<p>Quindi, dal punto di vista modellistico, <strong>assumere $\\epsilon_i$ normale √® spesso una buona approssimazione naturale</strong>. Per la dimostrazione formale e le condizioni precise (Lindeberg, Lindeberg‚ÄìFeller, indipendenza, varianze finite, ecc.) vedi la nota <strong><span class=\"text-gray-600\">Teorema del Limite Centrale</span></strong>.</p>\n<p><strong>Nota sui limiti del CLT:</strong> il CLT richiede (in qualche forma) che i contributi abbiano varianza finita e che non ci siano dipendenze troppo forti; in presenza di code pesanti con varianza infinita (es. alcune leggi di potenza) o di forti dipendenze, il risultato pu√≤ non essere la normale ma una legge stabile diversa (es. L√©vy stable).</p>\n<p>Un&rsquo;altra giustificazione elegante √® il <strong><span class=\"text-gray-600\">Principio di Massima Entropia</span></strong>: tra tutte le distribuzioni con media e varianza fissate, quella che massimizza l&rsquo;entropia √® la normale. Pertanto la normale √® la scelta ‚Äúmeno informativa‚Äù coerente con conoscere solo media e varianza del rumore.</p>\n<hr />\n<p>Quindi possiamo riscrivere il modello in forma matriciale come:</p>\n$$\n\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\mathbf{\\epsilon}\n$$\n<p>Dove:</p>\n<ul>\n<li>$\\mathbf{y}$√® il vettore delle variabili dipendenti $n \\times 1$,</li>\n<li>$\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$√® la matrice delle variabili indipendenti $n \\times 2$,</li>\n<li>$\\mathbf{w}$√® il vettore dei parametri $2 \\times 1$,</li>\n<li>$\\mathbf{\\epsilon}$ √® il vettore degli errori $n \\times 1$.</li>\n</ul>\n<p>Nel caso pi√π generale (<strong>regressione multipla</strong>), considerando il dataset diventa $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$. In questo caso, la formula vettoriale diventa:</p>\n$$\ny_i = \\mathbf{x}_i^T \\mathbf{w} + \\mathbf{\\epsilon}_i\n$$\n<p>In forma matriciale:</p>\n$$\n\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\mathbf{\\epsilon}\n$$\n<p>Dove:</p>\n<ul>\n<li>$\\mathbf{y}$√® il vettore delle variabili dipendenti $n \\times 1$,\n$$\n\\mathbf{X} = \\begin{bmatrix}\n1 & \\text{------} \\mathbf{x}_1^T \\text{------} \\\\\n1 & \\text{------} \\mathbf{x}_2^T \\text{------} \\\\\n\\vdots & \\vdots \\\\\n1 & \\text{------} \\mathbf{x}_n^T \\text{------}\n\\end{bmatrix}\n$$ \n√® la matrice dei dati con dimensioni $n \\times (m+1)$, dove $m$ sono le variabili indipendenti.</li>\n<li>$\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix} \\in \\mathbb R^{m+1}$ sono i coefficienti del modello (parametri da stimare),</li>\n<li>$\\boldsymbol{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\in \\mathbb R^n$ sono i rumori (errore) che seguono una distribuzione normale $\\mathcal{N}(0, \\sigma^2)$.</li>\n</ul>\n<p>Nel caso <strong>multivariato</strong>, il dataset diventa $\\mathcal{D} = \\{(\\mathbf{x}_i, \\mathbf y_i)\\}_{i=1}^n$, dove $\\mathbf x_i \\in \\mathbb R^m$ sono i vettori di variabili indipendenti e $\\mathbf y_i \\in \\mathbb R^p$ i vettori di variabili dipendenti. In questo caso, la formula vettoriale diventa:</p>\n$$\n\\mathbf{y}_i = \\mathbf{x}_i^T \\mathbf{W} + \\mathbf{\\large \\epsilon}_i\n$$\n<p>Volendo quindi rappresentare il nostro modello di regressione lineare in forma matriciale (considerando l&rsquo;intero dataset), possiamo definire:</p>\n$$\n\\underbrace{\\begin{bmatrix}\n\\text{------} \\mathbf{y}_1^\\top \\text{------} \\\\\n\\text{------} \\mathbf{y}_2^\\top \\text{------} \\\\\n\\vdots \\\\\n\\text{------} \\mathbf{y}_n^\\top \\text{------}\n\\end{bmatrix}}_{\\large \\mathbf{Y}}\n=\n\\underbrace{\\begin{bmatrix}\n1 \\ |\\text{------} \\mathbf{x}_1^\\top \\text{------}\\\\\n1 \\ |\\text{------} \\mathbf{x}_2^\\top \\text{------}\\\\\n\\vdots \\\\\n1 \\ |\\text{------} \\mathbf{x}_n^\\top \\text{------}\n\\end{bmatrix}}_{\\large \\mathbf{X}}\n\\underbrace{\\begin{bmatrix}\n| & | &  & | \\\\\n\\mathbf{w}_1 & \\mathbf{w}_2 & \\dots & \\mathbf{w}_p \\\\\n| & | &  & | \n\\end{bmatrix}}_{\\large \\mathbf{W}}\n+ \n\\underbrace{\\begin{bmatrix}\n\\text{------} \\boldsymbol{\\epsilon}_1^\\top \\text{------} \\\\\n\\text{------} \\boldsymbol{\\epsilon}_2^\\top \\text{------} \\\\\n\\vdots \\\\\n\\text{------} \\boldsymbol{\\epsilon}_n^\\top \\text{------}\n\\end{bmatrix}}_{\\large{\\boldsymbol \\epsilon}}\n$$\n<p>Dove:\n- $\\mathbf{Y}$ √® la matrice delle variabili dipendenti $n \\times p$,\n- $\\mathbf{X}$ √® la matrice delle variabili indipendenti $n \\times (m+1)$,\n- $\\mathbf{W}$ √® la matrice dei coefficienti $(m+1) \\times p$, e $\\mathbf w_i = \\begin{bmatrix} w_{0,i} \\\\ w_{1,i} \\\\ \\vdots \\\\ w_{m,i} \\end{bmatrix}$ con $i \\in [p]$,\n- $\\large{\\boldsymbol \\epsilon}$ √® la matrice degli errori $n \\times p$, e $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$ con $i \\in [p]$.</p>\n<h2 id=\"2-assunzioni-della-regressione-lineare\"><strong>2. Assunzioni della Regressione Lineare</strong></h2>\n<ol>\n<li><strong>Linearit√†</strong>: La relazione tra le variabili indipendenti e la variabile dipendente √® lineare. Quindi $\\mathbf y$ si puoÃÄ scrivere come una combinazione lineare delle variabili indipendenti $\\mathbf x$ e degli errori: $\\mathbf y = \\mathbf x^T \\mathbf w + \\large{\\mathbf \\epsilon}$.</li>\n<li><strong>Indipendenza e Normalit√†</strong>: Gli errori $\\epsilon_i$ sono indipendenti tra loro e seguono una distribuzione normale $\\epsilon_i \\sim\\mathcal{N}(0, \\sigma^2)$. Questo implica che le variabili dipendenti sono distribuite normalmente con:<ul>\n<li>Media: $\\mathbb E[\\mathbf y | \\mathbf x] = f(\\mathbf x) = \\mathbf x^T \\mathbf w$.</li>\n<li><strong>Proof:</strong>\n    $\\mathbb E[\\mathbf y | \\mathbf  x] = \\mathbb E[f(\\mathbf x) + \\epsilon | \\mathbf x] = \\underbrace{\\mathbb E[f(\\mathbf x)|\\mathbf x]}_\\text{Una volta fissato x, f(x) √® deterministico, quindi = f(x)} + \\underbrace{\\mathbb E[\\epsilon |\\mathbf x]}_\\text{epsilon non dipende da x, quindi = 0} = f(\\mathbf x) + 0 = f(\\mathbf x). \\ \\square$</li>\n<li>Varianza: $\\mathbb V[\\mathbf y | \\mathbf x] = \\sigma^2$</li>\n<li><strong>Proof:</strong> $\\mathbb V[\\mathbf y | \\mathbf x] = \\mathbb V[f(\\mathbf x) + \\epsilon | \\mathbf x] = \\underbrace{\\mathbb V[f(\\mathbf x)|\\mathbf x]}_\\text{f(x) √® una costante, quindi = 0} + \\mathbb V[\\epsilon |\\mathbf x] + 2 \\cdot \\underbrace{\\mathbb Cov[\\underbrace{f(\\mathbf x)}_\\text{√® costante dato x}, \\epsilon | \\mathbf x]}_\\text{=0} = 0 + \\sigma^2 + 0 = \\sigma^2. \\ \\square$</li>\n</ul>\n</li>\n</ol>\n<p>Da questo si ottiene che:\n   $$\n   y_i | \\mathbf x_i, \\mathbf w \\sim \\mathcal{N}(f(\\mathbf x_i), \\sigma^2).\n   $$\n3. <strong>Assenza di multicollinearit√†</strong>: Le variabili indipendenti non devono essere linearmente dipendenti tra loro.<br />\n   - Se esiste una relazione lineare tra alcune colonne della matrice <strong>$\\mathbf{X}$</strong>, allora la matrice $\\mathbf{X}^T \\mathbf{X}$ diventa <strong>singolare</strong> (cio√® non invertibile). Questo √® problematico perch√© nella stima dei parametri con il metodo dei minimi quadrati ordinari (OLS), l&rsquo;espressione<br />\n     $$\n     \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n     $$\n     richiede che $(\\mathbf{X}^T \\mathbf{X})^{-1}$ esista, il che non √® possibile se $\\mathbf{X}^T \\mathbf{X}$ √® singolare.\n  In <a href=\"/theory/supervised-learning/Linear Models/Multicollinearit√†\" class=\"text-blue-600 hover:underline\">questa</a> nota viene approfondito il problema della multicollinearit√†.\n   - Per risolvere la multicollinearit√† si possono adottare strategie come:\n     - Rimuovere una delle variabili altamente correlate.\n     - Utilizzare metodi di regressione penalizzata come <strong>Ridge Regression</strong> o <strong>Lasso</strong>.\n     - Applicare una <strong>PCA (Principal Component Analysis)</strong> per trasformare le variabili indipendenti in nuove variabili non correlate.</p>\n<p>Le assunzioni della regressione lineare sono importanti per garantire la robustezza del modello e la sua applicabilita in situazioni reali.</p>\n<h2 id=\"3-stima-dei-parametri\"><strong>3. Stima dei Parametri</strong></h2>\n<p>La stima dei coefficienti $\\mathbf{w}$ nella regressione lineare √® basata sulla minimizzazione dell&rsquo;errore quadratico medio (<strong>MSE - Mean Squared Error</strong>), che deriva direttamente dal principio della <strong>massima verosimiglianza</strong> sotto l&rsquo;assunzione di rumore gaussiano.</p>\n<h3 id=\"31-assunzione-di-rumore-gaussiano\"><strong>3.1. Assunzione di Rumore Gaussiano</strong></h3>\n<p>Si assume che il rumore $\\epsilon_i$ in ogni osservazione sia distribuito secondo una <strong>normale con media zero e varianza costante</strong> $\\sigma^2$:</p>\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n$$\n<p>Quindi </p>\n$$\np(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{\\epsilon_i^2}{2\\sigma^2} \\right)\n$$\n<p>e dato che $\\epsilon = y_i - \\mathbf{x}_i^\\top \\mathbf{w}$, sostituendo abbiamo:</p>\n$$\np(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2}{2\\sigma^2} \\right)\n$$\n<p>Quindi, la distribuzione condizionale della variabile dipendente $y_i$, dato l&rsquo;input $x_i$, √® anch&rsquo;essa gaussiana:</p>\n$$\ny_i \\mid \\mathbf x_i, \\mathbf{w} \\sim \\mathcal{N}(\\mathbf{x}_i^\\top \\mathbf{w}, \\sigma^2).\n$$\n<h3 id=\"32-costruzione-della-funzione-di-verosimiglianza\"><strong>3.2. Costruzione della Funzione di Verosimiglianza</strong></h3>\n<p>Dati $N$ esempi indipendenti $\\{(\\mathbf x_i, y_i)\\}_{i=1}^{N}$, la <strong>funzione di verosimiglianza</strong> √® il prodotto delle probabilit√† condizionali di tutte le osservazioni:</p>\n$$\nL(\\mathbf{w}) = \\prod_{i=1}^{N} p(y_i \\mid \\mathbf x_i, \\mathbf{w})\n$$\n<p>Sostituendo la distribuzione gaussiana:</p>\n$$\nL(\\mathbf{w}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2}{2\\sigma^2} \\right)\n$$\n<h3 id=\"33-log-verosimiglianza\"><strong>3.3. Log-Verosimiglianza</strong></h3>\n<p>Per facilitare il calcolo, prendiamo il logaritmo della funzione di verosimiglianza (<strong>log-likelihood</strong>):</p>\n$$\n\\log L(\\mathbf{w}) = \\sum_{i=1}^{N} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2}{2\\sigma^2} \\right) \\right)\n$$\n<p>Separando i termini:</p>\n$$\n\\log L(\\mathbf{w}) = \\sum_{i=1}^{N} \\left[ -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2}{2\\sigma^2} \\right] = - \\frac{N}{2} \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2.\n$$\n<p>Prendere il logaritmo della funzione di verosimiglianza non modifica il problema di ottimizzazione perch√© il logaritmo √® una <strong>funzione monotona crescente</strong>. Questo significa che <strong>massimizzare la verosimiglianza √® equivalente a massimizzare la log-verosimiglianza</strong>:</p>\n$$\n\\arg\\max_{\\mathbf{w}} L(\\mathbf{w}) = \\arg\\max_{\\mathbf{w}} \\log L(\\mathbf{w})\n$$\n<p>I principali vantaggi del logaritmo sono:\n1. <strong>Trasforma il prodotto in somma</strong>, semplificando i calcoli:\n   $$\n   \\log L(\\mathbf{w}) = \\sum_{i=1}^{N} \\log p(y_i \\mid x_i, \\mathbf{w})\n   $$\n2. <strong>Evita problemi di precisione numerica</strong>, riducendo il rischio di underflow quando $N$ √® grande.\n3. <strong>Preserva la convessit√† della funzione obiettivo</strong>, facilitando l&rsquo;ottimizzazione.</p>\n<p>In sintesi, la log-verosimiglianza √® un&rsquo;utile trasformazione che rende il problema di stima pi√π semplice e numericamente stabile senza alterarne la soluzione</p>\n<h3 id=\"34-stima-dei-parametri-con-massima-verosimiglianza\"><strong>3.4. Stima dei Parametri con Massima Verosimiglianza</strong></h3>\n<p>Seguiamo ora questi passaggi: \n1. <strong>Identificazione dei termini dipendenti da $\\mathbf{w}$</strong> \n    - Il primo termine, $-\\frac{N}{2} \\log (2\\pi\\sigma^2)$, <strong>non dipende</strong> da $\\mathbf{w}$, quindi √® una costante e pu√≤ essere ignorato nell&rsquo;ottimizzazione. \n2. <strong>Massimizzazione della log-verosimiglianza equivale a minimizzare la penalit√† quadratica</strong> \n    - Il secondo termine, $- \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2$, <strong>dipende da $\\mathbf{w}$</strong> e deve essere massimizzato. \n    - Poich√© questo termine √® <strong>negativo</strong>, massimizzarlo significa <strong>minimizzare</strong> la somma dei quadrati degli errori: \n   $$ \\min_{\\mathbf{w}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2 $$</p>\n<p>Che in notazione matriciale generale diventa:\n   $$\n   \\min_{W} \\|Y -XW\\|^2_F\n   $$\n   Dove:</p>\n<ul>\n<li>$Y$ √® la matrice delle osservazioni $n \\times p$</li>\n<li>$X$ √® la matrice dei regressori $n \\times (m+1)$</li>\n<li>\n<p>$W$√® la matrice dei coefficienti $(m+1) \\times p$, e $w_i = \\begin{bmatrix} w_{0,i} \\\\ w_{1,i} \\\\ \\vdots \\\\ w_{m,i} \\end{bmatrix}$ con $i \\in [p]$</p>\n</li>\n<li>\n<p><strong>Il fattore $2\\sigma^2$ non influisce sull&rsquo;argomento del minimo</strong> </p>\n<ul>\n<li>Il termine √® diviso per $2\\sigma^2$, ma poich√© la varianza $\\sigma^2$ √® una costante positiva, rimuoverlo <strong>non cambia la posizione del minimo</strong>. Quindi, il problema di <strong>massima verosimiglianza</strong> si riduce esattamente alla minimizzazione della somma dei quadrati degli errori, che √® la funzione di costo dell&rsquo;<strong>errore quadratico medio (MSE)</strong> nella regressione lineare.</li>\n</ul>\n</li>\n</ul>\n<p>Dividendo per $N$ otteniamo la funzione di costo <strong>MSE</strong> (<strong>Mean Squared Error</strong>):</p>\n$$\nMSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n$$\n<p>dove:</p>\n$$\n\\hat{y}_i = \\mathbf{x}_i^\\top \\mathbf{w}\n$$\n<p>√® la previsione del modello. O equivalentemente:</p>\n$$\n\\hat{Y} = \\mathbf{X} \\mathbf{W}.\n$$\n<p>Che in notazione matriciale generale diventa:</p>\n$$\nMSE = \\frac{1}{N} \\|Y - \\hat{Y}\\|_F^2\n$$\n<p>Quindi, <strong>minimizzare il MSE √® equivalente alla stima di massima verosimiglianza quando si assume rumore gaussiano con varianza costante</strong>.</p>\n<h3 id=\"35-metodi-per-minimizzare-la-funzione-di-costo-mse\"><strong>3.5. Metodi per Minimizzare la Funzione di Costo (MSE)</strong></h3>\n<p>Poich√© l‚ÄôMSE deriva dalla <strong>Massima Verosimiglianza (MLE)</strong> sotto l‚Äôassunzione di rumore gaussiano, possiamo stimare i parametri della regressione lineare con diversi approcci:  </p>\n<h4 id=\"1-soluzione-analitica-minimi-quadrati-ordinari-ols-ordinary-least-squares\"><strong>1Ô∏è‚É£ Soluzione Analitica: Minimi Quadrati Ordinari (OLS - Ordinary Least Squares)</strong></h4>\n<ul>\n<li>Trova direttamente i coefficienti che minimizzano l‚ÄôMSE.  </li>\n<li>La soluzione chiusa √®:<br />\n  $$\n  \\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n  $$</li>\n<li><strong>Limiti:</strong>  </li>\n<li>Richiede l‚Äôinversione della matrice $(\\mathbf{X}^\\top \\mathbf{X})$, che pu√≤ essere numericamente instabile in presenza di <strong>multicollinearit√†</strong>.  </li>\n<li>Non √® scalabile per dataset molto grandi.  </li>\n</ul>\n<h4 id=\"2-approccio-bayesiano-massima-a-posteriori-map-maximum-a-posteriori\"><strong>2Ô∏è‚É£ Approccio Bayesiano: Massima A Posteriori (MAP - Maximum A Posteriori)</strong></h4>\n<ul>\n<li>Estende MLE introducendo una <strong>distribuzione a priori</strong> sui parametri $\\mathbf{w}$.  </li>\n<li>Se il prior √® <strong>gaussiano</strong> $\\mathcal{N}(0, \\lambda I)$, si ottiene la <strong>Regressione Ridge</strong>.  </li>\n</ul>\n<h4 id=\"3-minimi-quadrati-con-regolarizzazione-ridge-lasso-elastic-net\"><strong>3Ô∏è‚É£ Minimi Quadrati con <a href=\"/theory/introduction/Regolarizzazione\" class=\"text-blue-600 hover:underline\">Regolarizzazione</a> (Ridge, Lasso, Elastic Net)</strong></h4>\n<p>Aggiungono una penalizzazione ai coefficienti per migliorare la <strong>stabilit√†</strong> e il <strong>controllo della complessit√†</strong> del modello:  </p>\n<p>‚úÖ <strong>Ridge Regression</strong> (<em>L2-regularization</em>)<br />\n  $$\n  \\min_{\\mathbf{w}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda ||\\mathbf{w}||_2^2\n  $$\n  - Penalizza i coefficienti grandi, ma non li azzera.<br />\n  - <strong>Equivalente alla MAP con prior gaussiano.</strong>  </p>\n<p>‚úÖ <strong>Lasso Regression</strong> (<em>L1-regularization</em>)<br />\n  $$\n  \\min_{\\mathbf{w}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda ||\\mathbf{w}||_1\n  $$\n  - Impone <strong>sparsit√†</strong>, azzerando alcuni coefficienti.<br />\n  - Seleziona automaticamente le feature pi√π importanti.  </p>\n<p>‚úÖ <strong>Elastic Net</strong> (<em>combinazione di Ridge e Lasso</em>)<br />\n  $$\n  \\min_{\\mathbf{w}} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda_1 ||\\mathbf{w}||_1 + \\lambda_2 ||\\mathbf{w}||_2^2\n  $$\n  - Unisce i vantaggi di Ridge e Lasso.<br />\n  - Utile quando le feature sono <strong>correlate tra loro</strong>.  </p>\n<h4 id=\"4-soluzioni-iterative-discesa-del-gradiente-gradient-descent\"><strong>4Ô∏è‚É£ Soluzioni Iterative: Discesa del Gradiente (Gradient Descent)</strong></h4>\n<p>Metodo iterativo che aggiorna i coefficienti nella direzione del gradiente negativo:<br />\n  $$\n  \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha \\nabla MSE\n  $$\n- <strong>Vantaggi:</strong><br />\n  ‚úÖ Scalabile su dataset di grandi dimensioni.<br />\n  ‚úÖ Utile quando $(\\mathbf{X}^\\top \\mathbf{X})^{-1}$ √® difficile da calcolare.</p>\n<ul>\n<li><strong>Varianti:</strong><br />\n  üîπ <em>Batch Gradient Descent</em> (usa tutto il dataset a ogni iterazione).<br />\n  üîπ <em>Stochastic Gradient Descent (SGD)</em> (aggiorna i pesi a ogni singolo campione).<br />\n  üîπ <em>Mini-batch Gradient Descent</em> (compromesso tra batch e SGD).  </li>\n</ul>\n<h2 id=\"4-interpretazione-dei-coefficienti\"><strong>4. Interpretazione dei Coefficienti</strong></h2>\n<p>Nella regressione lineare, i coefficienti $\\mathbf{w}$ rappresentano l&rsquo;effetto che una variazione unitaria di una variabile indipendente ha sulla variabile dipendente, mantenendo le altre variabili costanti.</p>\n<h3 id=\"41-significato-dei-coefficienti\"><strong>4.1. Significato dei Coefficienti</strong></h3>\n<p>Consideriamo il modello di regressione lineare semplice con una sola variabile indipendente:</p>\n$$\ny = w_0 + w_1 x + \\epsilon\n$$\n<p>Dove:\n- $w_0$ √® <strong>l&rsquo;intercetta</strong> (bias), che rappresenta il valore previsto di $y$ quando $x = 0$.\n- $w_1$ √® <strong>il coefficiente angolare</strong>, che misura la variazione di $y$ per una variazione unitaria di $x$.</p>\n<p>Se $w_1 > 0$, significa che all&rsquo;aumentare di $x$, anche $y$ tende ad aumentare (relazione positiva).<br />\nSe $w_1 < 0$, significa che all&rsquo;aumentare di $x$, $y$ tende a diminuire (relazione negativa).  </p>\n<h3 id=\"42-interpretazione-nella-regressione-multipla\"><strong>4.2. Interpretazione nella Regressione Multipla</strong></h3>\n<p>Nel caso della regressione multipla:</p>\n$$\ny = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_m x_m + \\epsilon\n$$\n<p>Ogni coefficiente $w_j$ indica l&rsquo;effetto di una variazione unitaria di $x_j$ su $y$, <strong>tenendo tutte le altre variabili costanti</strong>.  </p>\n<p>Esempio:\n- Se $w_2 = 3$, significa che, <strong>a parit√† di tutte le altre variabili</strong>, un aumento di 1 unit√† in $x_2$ comporta un aumento medio di 3 unit√† in $y$.</p>\n<h3 id=\"43-punteggio-standardizzato-beta-coefficients\"><strong>4.3. Punteggio Standardizzato (Beta Coefficients)</strong></h3>\n<p>Nella regressione lineare multipla, i coefficienti standardizzati (noti anche come <strong>Beta Coefficients</strong>) permettono di confrontare l&rsquo;importanza relativa delle variabili indipendenti eliminando l&rsquo;effetto delle diverse unit√† di misura.  </p>\n<p>Quando una regressione utilizza coefficienti <strong>non standardizzati</strong>, i valori ottenuti dipendono dalle unit√† di misura delle variabili. Ci√≤ rende difficile confrontare l&rsquo;influenza relativa di ciascuna variabile indipendente sulla variabile dipendente.  </p>\n<p>Per risolvere questo problema, utilizziamo i <strong>coefficienti standardizzati</strong>, definiti dalla formula:  </p>\n$$\n\\beta_j = w_j \\cdot \\frac{\\sigma_{x_j}}{\\sigma_y}\n$$\n<p>Dove:<br />\n- $\\beta_j$ √® il coefficiente standardizzato della variabile $x_j$.<br />\n- $w_j$ √® il coefficiente non standardizzato ottenuto dalla regressione.<br />\n- $\\sigma_{x_j}$ √® la deviazione standard della variabile indipendente $x_j$.<br />\n- $\\sigma_y$ √® la deviazione standard della variabile dipendente $y$.  </p>\n<h4 id=\"interpretazione\"><strong>Interpretazione</strong></h4>\n<p>I coefficienti standardizzati indicano <strong>quanto cambia la variabile dipendente $y$, espressa in deviazioni standard</strong>, a seguito di una variazione di <strong>una deviazione standard</strong> nella variabile indipendente $x_j$.  </p>\n<p>In altre parole:<br />\n- Se $\\beta_j = 0.5$, significa che un aumento di una deviazione standard in $x_j$ comporta un aumento di <strong>0.5 deviazioni standard</strong> in $y$.<br />\n- Se $\\beta_j = -0.3$, significa che un aumento di una deviazione standard in $x_j$ comporta una <strong>diminuzione di 0.3 deviazioni standard</strong> in $y$.  </p>\n<h4 id=\"vantaggi-delluso-dei-coefficienti-standardizzati\"><strong>Vantaggi dell&rsquo;uso dei Coefficienti Standardizzati</strong></h4>\n<ul>\n<li><strong>Permettono di confrontare direttamente l&rsquo;impatto delle variabili</strong>: il valore assoluto di $\\beta_j$ indica l&rsquo;importanza relativa di $x_j$ rispetto alle altre variabili nel modello.  </li>\n<li><strong>Eliminano il problema delle diverse unit√† di misura</strong>, rendendo il confronto pi√π intuitivo.  </li>\n<li><strong>Facilitano l&rsquo;interpretazione pratica</strong> nei modelli con variabili di diversa scala (ad esempio, reddito in euro e et√† in anni).  </li>\n</ul>\n<p>In sintesi, l&rsquo;uso dei coefficienti standardizzati √® utile per comprendere <strong>quali variabili hanno un impatto maggiore sulla variabile dipendente</strong> e per confrontare la loro influenza in maniera oggettiva. </p>\n<h3 id=\"44-intervalli-di-confidenza\"><strong>4.4. Intervalli di Confidenza</strong></h3>\n<p>Poich√© i coefficienti sono stimati da un campione, √® utile calcolare il loro intervallo di confidenza per capire la loro precisione.</p>\n<p>L&rsquo;intervallo di confidenza al <strong>95%</strong> per un coefficiente $w_j$ √®:</p>\n$$\n[w_j - t_{\\alpha/2} \\cdot SE(w_j), \\quad w_j + t_{\\alpha/2} \\cdot SE(w_j)]\n$$\n<p>Dove:\n- $SE(w_j)$ √® l&rsquo;errore standard del coefficiente.\n- $t_{\\alpha/2}$ √® il valore della distribuzione $t$ di Student con $(n - m - 1)$ gradi di libert√†.</p>\n<p>Se l&rsquo;intervallo include <strong>zero</strong>, il coefficiente potrebbe non essere significativo.</p>\n<h3 id=\"45-p-value-e-significativita-statistica\"><strong>4.5. p-value e Significativit√† Statistica</strong></h3>\n<p>Per valutare se un coefficiente √® statisticamente significativo, si utilizza il <strong>test t</strong>:</p>\n$$\nt_j = \\frac{w_j}{SE(w_j)}\n$$\n<p>Il <strong>p-value</strong> associato a $t_j$ indica la probabilit√† di osservare un valore cos√¨ estremo sotto l&rsquo;ipotesi nulla $H_0: w_j = 0$.</p>\n<ul>\n<li>Se $p < 0.05$, il coefficiente √® <strong>statisticamente significativo</strong> al livello del 5%.</li>\n<li>Se $p > 0.05$, non abbiamo prove sufficienti per affermare che il coefficiente sia diverso da zero.</li>\n</ul>\n<h3 id=\"46-multicollinearita-e-interpretazione\"><strong>4.6. Multicollinearit√† e Interpretazione</strong></h3>\n<p>Se due o pi√π variabili indipendenti sono fortemente correlate, si verifica <strong>multicollinearit√†</strong>, che rende difficile interpretare i coefficienti.<br />\nUn alto <strong>Variance Inflation Factor (VIF)</strong> indica multicollinearit√†:</p>\n$$\nVIF_j = \\frac{1}{1 - R_j^2}\n$$\n<p>Dove $R_j^2$ √® il coefficiente di determinazione della regressione di $x_j$ sulle altre variabili indipendenti.</p>\n<ul>\n<li>Se $VIF > 10$, indica un problema di <strong>multicollinearit√† elevata</strong>.</li>\n</ul>\n<p>Per mitigare la multicollinearit√†, si possono usare:\n1. <strong>Ridge Regression</strong> o <strong>Lasso Regression</strong> (penalizzazione).\n2. <strong>Rimuovere variabili ridondanti</strong>.\n3. <strong>Utilizzare la PCA (Principal Component Analysis)</strong>.</p>\n<h3 id=\"conclusione\"><strong>Conclusione</strong></h3>\n<p>L&rsquo;interpretazione corretta dei coefficienti √® fondamentale per comprendere l&rsquo;effetto delle variabili indipendenti su $y$. √à importante considerare intervalli di confidenza, p-value e multicollinearit√† per trarre conclusioni valide dal modello.</p>\n<h2 id=\"5-valutazione-del-modello\"><strong>5. Valutazione del Modello</strong></h2>\n<p>Per determinare la qualit√† di un modello di regressione, utilizziamo diverse metriche e test statistici. Questi strumenti permettono di valutare <strong>quanto bene il modello spiega la variabilit√† dei dati</strong> e <strong>se i coefficienti stimati sono statisticamente significativi</strong>.  </p>\n<h3 id=\"51-errore-quadratico-medio-mse\"><strong>5.1. Errore Quadratico Medio (MSE)</strong></h3>\n<p>L&rsquo;<strong>Errore Quadratico Medio</strong> (<strong>Mean Squared Error</strong>, MSE) misura l&rsquo;accuratezza del modello calcolando la media dei quadrati degli errori di previsione.</p>\n<ul>\n<li>Un <strong>MSE pi√π basso</strong> indica un modello con <strong>migliore capacit√† predittiva</strong>.  </li>\n<li>Poich√© l&rsquo;MSE √® espresso nelle unit√† al quadrato di $y$, non √® sempre intuitivo da interpretare. Per questo motivo, spesso si usa la <strong>Radice dell&rsquo;Errore Quadratico Medio</strong> (<strong>RMSE</strong>):  </li>\n</ul>\n$$\nRMSE = \\sqrt{MSE}\n$$\n<hr />\n<h3 id=\"52-coefficiente-di-determinazione-math_inline_197\"><strong>5.2. Coefficiente di Determinazione ($R^2$)</strong></h3>\n<p>Il coefficiente di determinazione $R^2$ misura <strong>la proporzione della varianza della variabile dipendente spiegata dal modello</strong>:  </p>\n$$\nR^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n$$\n<p>Dove:<br />\n- $\\sum (y_i - \\hat{y}_i)^2$ rappresenta la <strong>devianza residua</strong> (errore del modello).<br />\n- $\\sum (y_i - \\bar{y})^2$ rappresenta la <strong>devianza totale</strong> (variabilit√† totale dei dati rispetto alla loro media).<br />\n- $\\bar{y}$ √® la media dei valori osservati di $y$.  </p>\n<p><strong>Interpretazione:</strong><br />\n- $R^2$ varia tra <strong>0 e 1</strong>:\n  - Un valore vicino a <strong>1</strong> indica che il modello spiega quasi tutta la variabilit√† dei dati.<br />\n  - Un valore vicino a <strong>0</strong> indica che il modello ha <strong>scarsa capacit√† esplicativa</strong>.<br />\n- Un $R^2$ elevato non implica necessariamente che il modello sia valido: pu√≤ essere influenzato dalla presenza di variabili irrilevanti.  </p>\n<p>Per modelli con molte variabili, si preferisce il <strong>$R^2$ aggiustato</strong>, che penalizza l&rsquo;aggiunta di variabili non significative:</p>\n$$\nR^2_{\\text{adj}} = 1 - \\left( \\frac{(1 - R^2)(N - 1)}{N - p - 1} \\right)\n$$\n<p>Dove $p$ √® il numero di variabili indipendenti nel modello.  </p>\n<hr />\n<h3 id=\"53-test-f-significativita-globale-del-modello\"><strong>5.3. Test F: Significativit√† Globale del Modello</strong></h3>\n<p>Il <strong>Test F</strong> valuta se il modello, nel suo complesso, √® statisticamente significativo, ovvero se almeno una delle variabili indipendenti ha un effetto su $y$. L&rsquo;ipotesi nulla ($H_0$) del test √®:  </p>\n$$\nH_0: \\quad w_1 = w_2 = ... = w_p = 0\n$$\n<p>Se il test F risulta significativo (p-value &lt; soglia, es. 0.05), possiamo rifiutare $H_0$, indicando che <strong>almeno una variabile indipendente ha un effetto significativo su $y$</strong>.  </p>\n<p>Il valore della statistica F √® calcolato come:  </p>\n$$\nF = \\frac{\\left( \\frac{R^2}{p} \\right)}{\\left( \\frac{1 - R^2}{N - p - 1} \\right)}\n$$\n<p>Dove:<br />\n- $p$ √® il numero di variabili indipendenti.<br />\n- $N$ √® il numero di osservazioni.  </p>\n<p>Un valore di <strong>F alto</strong> e un <strong>p-value basso</strong> indicano un modello globalmente significativo.</p>\n<hr />\n<h3 id=\"54-p-value-dei-coefficienti\"><strong>5.4. p-value dei Coefficienti</strong></h3>\n<p>Ogni coefficiente $w_j$ della regressione ha un <strong>p-value</strong>, che misura la probabilit√† di ottenere un effetto uguale o maggiore <strong>se l&rsquo;effetto reale fosse nullo</strong>.  </p>\n<p>L&rsquo;ipotesi nulla per ciascun coefficiente √®:  </p>\n$$\nH_0: \\quad w_j = 0\n$$\n<p>Se il <strong>p-value √® inferiore</strong> a una soglia prestabilita (tipicamente 0.05 o 0.01), possiamo rifiutare $H_0$, indicando che la variabile $x_j$ ha un effetto significativo su $y$.  </p>\n<p><strong>Interpretazione:</strong><br />\n- Un <strong>p-value &lt; 0.05</strong> suggerisce che la variabile $x_j$ √® statisticamente significativa.<br />\n- Un <strong>p-value alto</strong> indica che l&rsquo;effetto della variabile potrebbe essere dovuto al caso e che la variabile potrebbe non essere utile nel modello.  </p>\n<p>Se pi√π variabili hanno p-value alti, potrebbe essere necessario <strong>semplificare il modello</strong> eliminando quelle non significative.</p>\n<hr />\n<h3 id=\"55-considerazioni-finali-sulla-valutazione-del-modello\"><strong>5.5. Considerazioni Finali sulla Valutazione del Modello</strong></h3>\n<p>Un modello di regressione ideale dovrebbe:<br />\n‚úÖ Avere un <strong>MSE basso</strong> e, preferibilmente, un RMSE interpretabile.<br />\n‚úÖ Presentare un <strong>$R^2$ elevato</strong>, ma non eccessivamente vicino a 1 per evitare overfitting.<br />\n‚úÖ Superare il <strong>test F</strong>, indicando che almeno una variabile ha un effetto su $y$.<br />\n‚úÖ Avere <strong>coefficienti con p-value bassi</strong>, per garantire che le variabili siano significative.  </p>\n<p>L&rsquo;analisi dei residui (differenze tra $y_i$ e $\\hat{y}_i$) √® un altro strumento fondamentale per verificare la bont√† del modello e individuare eventuali problemi di eteroschedasticit√† o non linearit√†.</p>\n<h2 id=\"6-estensione-del-modello\">6. Estensione del Modello</h2>\n<p>La regressione lineare √® un modello potente e versatile, ma in alcuni casi la relazione tra le variabili indipendenti e la variabile dipendente non √® lineare. In queste situazioni, √® possibile estendere il modello di regressione lineare per catturare relazioni pi√π complesse. Di seguito esploriamo alcune delle principali estensioni del modello di regressione lineare.</p>\n<h3 id=\"61-regressione-polinomiale\">6.1. Regressione Polinomiale</h3>\n<p>La regressione polinomiale √® un&rsquo;estensione della regressione lineare che permette di modellare relazioni non lineari tra le variabili indipendenti e la variabile dipendente. Questo viene fatto introducendo termini polinomiali delle variabili indipendenti nel modello.</p>\n<h4 id=\"611-formulazione-del-modello\">6.1.1. Formulazione del Modello</h4>\n<p>Consideriamo il caso di una singola variabile indipendente $x$. Il modello di regressione polinomiale di grado $d$ √® dato da:</p>\n$$\ny = w_0 + w_1 x + w_2 x^2 + \\dots + w_d x^d + \\epsilon\n$$\n<p>Dove:</p>\n<ul>\n<li>$w_0, w_1, \\dots, w_d$ sono i coefficienti del modello.</li>\n<li>$x^2, x^3, \\dots, x^d$ sono i termini polinomiali della variabile indipendente $x$.</li>\n<li>$\\epsilon$ √® il termine di errore.</li>\n</ul>\n<p>In forma matriciale, il modello pu√≤ essere scritto come:</p>\n$$\ny = Xw + \\epsilon\n$$\n<p>Dove:</p>\n$$\nX = \\begin{bmatrix} \n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{bmatrix}\n$$\n<p>√® la matrice delle variabili indipendenti con i termini polinomiali.</p>\n$$\nw = \\begin{bmatrix} \nw_0 \\\\\nw_1 \\\\\n\\vdots \\\\\nw_d\n\\end{bmatrix}\n$$\n<p>√® il vettore dei coefficienti.</p>\n<h4 id=\"612-scelta-del-grado-del-polinomio\">6.1.2. Scelta del Grado del Polinomio</h4>\n<p>La scelta del grado $d$ del polinomio √® cruciale:</p>\n<ul>\n<li>Un grado troppo basso pu√≤ portare a <strong>underfitting</strong>, ovvero il modello non cattura la complessit√† dei dati.</li>\n<li>Un grado troppo alto pu√≤ portare a <strong>overfitting</strong>, ovvero il modello si adatta troppo ai dati di training e generalizza male su nuovi dati.</li>\n</ul>\n<p>Per scegliere il grado ottimale, si possono utilizzare tecniche come la cross-validation o l&rsquo;analisi dell&rsquo;errore di validazione.</p>\n<h4 id=\"613-esempio-di-regressione-polinomiale\">6.1.3. Esempio di Regressione Polinomiale</h4>\n<p>Supponiamo di avere un dataset con una relazione non lineare tra $x$ e $y$. Un modello di regressione polinomiale di grado 2 potrebbe essere:</p>\n$$\ny = w_0 + w_1 x + w_2 x^2 + \\epsilon\n$$\n<p>Questo modello pu√≤ catturare relazioni quadratiche tra $x$ e $y$, come ad esempio una parabola.</p>\n<h3 id=\"62-regressione-con-interazioni\">6.2. Regressione con Interazioni</h3>\n<p>La regressione con interazioni permette di modellare l&rsquo;effetto combinato di due o pi√π variabili indipendenti. Questo √® utile quando l&rsquo;effetto di una variabile dipendente su $y$ dipende dal valore di un&rsquo;altra variabile.</p>\n<h4 id=\"621-formulazione-del-modello\">6.2.1. Formulazione del Modello</h4>\n<p>Consideriamo due variabili indipendenti $x_1$ e $x_2$. Il modello di regressione con interazione √® dato da:</p>\n$$\ny = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + \\epsilon\n$$\n<p>Dove:</p>\n<ul>\n<li>$w_0$ √® l&rsquo;intercetta.</li>\n<li>$w_1$ e $w_2$ sono i coefficienti delle variabili $x_1$ e $x_2$.</li>\n<li>$w_3$ √® il coefficiente del termine di interazione $x_1 x_2$.</li>\n<li>$\\epsilon$ √® il termine di errore.</li>\n</ul>\n<h4 id=\"622-interpretazione-dei-coefficienti\">6.2.2. Interpretazione dei Coefficienti</h4>\n<ul>\n<li>Coefficiente di interazione ($w_3$): Misura l&rsquo;effetto combinato di $x_1$ e $x_2$ su $y$. Se $w_3$ √® positivo, l&rsquo;effetto di $x_1$ su $y$ aumenta all&rsquo;aumentare di $x_2$, e viceversa.</li>\n</ul>\n<h4 id=\"623-esempio-di-regressione-con-interazioni\">6.2.3. Esempio di Regressione con Interazioni</h4>\n<p>Supponiamo di voler modellare l&rsquo;effetto del prezzo ($x_1$) e della pubblicit√† ($x_2$) sulle vendite ($y$). Un modello con interazione potrebbe essere:</p>\n$$\ny = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + \\epsilon\n$$\n<p>Questo modello cattura l&rsquo;effetto combinato del prezzo e della pubblicit√† sulle vendite.</p>\n<h3 id=\"63-regressione-con-funzioni-di-base\">6.3. Regressione con Funzioni di Base</h3>\n<p>La regressione con funzioni di base √® un&rsquo;estensione della regressione lineare che permette di modellare relazioni non lineari utilizzando funzioni di base (basis functions) delle variabili indipendenti.</p>\n<h4 id=\"631-formulazione-del-modello\">6.3.1. Formulazione del Modello</h4>\n<p>Consideriamo una variabile indipendente $x$. Il modello di regressione con funzioni di base √® dato da:</p>\n$$\ny = w_0 + w_1 \\phi_1(x) + w_2 \\phi_2(x) + \\dots + w_d \\phi_d(x) + \\epsilon\n$$\n<p>Dove:</p>\n<ul>\n<li>$\\phi_1(x), \\phi_2(x), \\dots, \\phi_d(x)$ sono le funzioni di base.</li>\n<li>$w_0, w_1, \\dots, w_d$ sono i coefficienti del modello.</li>\n<li>$\\epsilon$ √® il termine di errore.</li>\n</ul>\n<h4 id=\"632-scelta-delle-funzioni-di-base\">6.3.2. Scelta delle Funzioni di Base</h4>\n<p>Le funzioni di base possono essere scelte in base alla natura dei dati e alla relazione attesa tra le variabili. Alcune scelte comuni includono:</p>\n<ul>\n<li>Funzioni polinomiali: $\\phi_j(x) = x^j$</li>\n<li>Funzioni trigonometriche: $\\phi_j(x) = \\sin(jx), \\phi_j(x) = \\cos(jx)$</li>\n<li>Funzioni radiali: $\\phi_j(x) = \\exp\\left(\\frac{-(x - \\mu_j)^2}{2\\sigma^2}\\right)$</li>\n</ul>\n<h4 id=\"633-esempio-di-regressione-con-funzioni-di-base\">6.3.3. Esempio di Regressione con Funzioni di Base</h4>\n<p>Supponiamo di voler modellare una relazione periodica tra $x$ e $y$. Un modello con funzioni trigonometriche potrebbe essere:</p>\n$$\ny = w_0 + w_1 \\sin(x) + w_2 \\cos(x) + \\epsilon\n$$\n<p>Questo modello pu√≤ catturare relazioni periodiche come quelle presenti in dati stagionali.</p>\n<h3 id=\"64-regressione-non-parametrica\">6.4. Regressione Non Parametrica</h3>\n<p>La regressione non parametrica √® un approccio che non assume una forma specifica per la relazione tra le variabili indipendenti e la variabile dipendente. Invece, il modello si adatta ai dati in modo flessibile.</p>\n<h4 id=\"641-metodi-comuni\">6.4.1. Metodi Comuni</h4>\n<p>Alcuni metodi comuni di regressione non parametrica includono:</p>\n<ul>\n<li><strong>Kernel Regression</strong>: Stima la relazione tra $x$ e $y$ utilizzando una funzione di kernel per pesare i dati vicini.</li>\n<li><strong>Spline Regression</strong>: Utilizza funzioni spline per modellare la relazione tra $x$ e $y$.</li>\n<li><strong>Local Regression (LOESS)</strong>: Adatta un modello di regressione lineare localmente ai dati.</li>\n</ul>\n<h4 id=\"642-vantaggi-e-svantaggi\">6.4.2. Vantaggi e Svantaggi</h4>\n<ul>\n<li><strong>Vantaggi</strong>:</li>\n<li>Flessibilit√† nel modellare relazioni complesse.</li>\n<li>\n<p>Non richiede assunzioni sulla forma della relazione.</p>\n</li>\n<li>\n<p><strong>Svantaggi</strong>:</p>\n</li>\n<li>Maggiore complessit√† computazionale.</li>\n<li>Rischio di overfitting se non si controlla la complessit√† del modello.</li>\n</ul>\n<h3 id=\"65-regressione-ponderata-weighted-regression\">6.5. Regressione Ponderata (Weighted Regression)</h3>\n<p>La regressione ponderata √® una variante della regressione lineare in cui ogni osservazione ha un peso specifico. Questo √® utile quando alcune osservazioni sono pi√π importanti o affidabili di altre.</p>\n<h4 id=\"651-formulazione-del-modello\">6.5.1. Formulazione del Modello</h4>\n<p>Nella regressione ponderata, l&rsquo;obiettivo √® minimizzare la somma degli errori quadratici ponderati:</p>\n$$\n\\min_{\\mathbf{w}} \\sum_{i=1}^{N} v_i \\left( y_i - \\mathbf{w}^T \\mathbf{x}_i \\right)^2\n$$\n<p>Dove:\n- $v_i$ √® il peso associato all&rsquo;osservazione $i$ (tipicamente costante per tutto il dataset)\n- $\\mathbf{w}$ √® il vettore dei coefficienti</p>\n<p>La soluzione in forma chiusa √®:\n$$\n\\mathbf{w}^* = (\\mathbf{X}^T \\mathbf{V} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{V} \\mathbf{y}\n$$</p>\n<p>dove $\\mathbf{V} = \\text{diag}(v_1, v_2, \\ldots, v_N)$ √® la matrice diagonale dei pesi.</p>\n<h4 id=\"652-estensione-locale-lwlr\">6.5.2. Estensione Locale: LWLR</h4>\n<p>Un&rsquo;importante estensione della regressione ponderata √® la <a href=\"/theory/supervised-learning/Non-Linear Models/Locally Weighted Linear Regression\" class=\"text-blue-600 hover:underline\">Locally Weighted Linear Regression</a> (LWLR), dove i pesi non sono fissi ma <strong>cambiano dinamicamente</strong> per ogni punto di query in base alla distanza:</p>\n$$\nw_i(\\mathbf{x}_q) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_q\\|^2}{2\\tau^2}\\right)\n$$\n<p>A differenza della regressione ponderata classica che usa pesi globali fissi, LWLR costruisce un modello lineare locale per ogni previsione, rendendo il metodo non-parametrico e capace di catturare relazioni non lineari.</p>\n<p>La differenza fondamentale √®:\n- <strong>Weighted Regression</strong>: Un singolo modello globale con pesi fissi\n- <strong>LWLR</strong>: Infiniti modelli locali con pesi adattivi per ogni query</p>\n<p>Per una trattazione completa di LWLR, incluse le tre implementazioni (forma chiusa, SGD, BGD) e tutti i dettagli matematici, si veda <a href=\"/theory/supervised-learning/Non-Linear Models/Locally Weighted Linear Regression\" class=\"text-blue-600 hover:underline\">Locally Weighted Linear Regression</a>.</p>\n<h3 id=\"66-conclusione\">6.6. Conclusione</h3>\n<p>Le estensioni del modello di regressione lineare permettono di catturare relazioni pi√π complesse tra le variabili indipendenti e la variabile dipendente. La scelta del modello dipende dalla natura dei dati e dalla relazione attesa. √à importante bilanciare la flessibilit√† del modello con il rischio di overfitting, utilizzando tecniche come la cross-validation e la regolarizzazione.</p>\n<h2 id=\"7-conclusioni\"><strong>7. Conclusioni</strong></h2>\n<ul>\n<li>La <strong>Regressione Lineare</strong> √® un modello semplice ma potente per analizzare relazioni tra variabili.</li>\n<li>Richiede l&rsquo;analisi delle <strong>assunzioni</strong> per evitare problemi di interpretabilit√†.</li>\n<li>Pu√≤ essere estesa con <strong>regolarizzazione</strong> e <strong>modelli polinomiali</strong> per migliorare le prestazioni</li>\n</ul>\n<p><strong>Risorse aggiuntive:</strong></p>\n<ul>\n<li><em>The Elements of Statistical Learning</em> - Hastie, Tibshirani, Friedman.</li>\n<li><em>Introduction to Statistical Learning</em> - James, Witten, Hastie, Tibshirani.</li>\n</ul>"
}