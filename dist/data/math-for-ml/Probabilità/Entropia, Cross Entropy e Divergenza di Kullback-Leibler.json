{
  "title": "Untitled",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<h2 id=\"entropia\">Entropia</h2>\n<p>L&rsquo;<strong>entropia</strong> è una misura dell&rsquo;incertezza o della quantità di informazione contenuta in una distribuzione di probabilità. In altre parole, indica quanto è &ldquo;disorganizzata&rdquo; o &ldquo;incerta&rdquo; una distribuzione di probabilità. Se una variabile casuale $X$ segue una distribuzione di probabilità $P(X)$, l&rsquo;entropia è definita come:</p>\n$$\nH(X) = -\\sum_{i} P(x_i) \\log P(x_i)\n$$\n<p>Dove:\n- $P(x_i)$ è la probabilità di ogni possibile esito $x_i$ della variabile casuale $X$.\n- La somma è fatta su tutti gli esiti possibili.</p>\n<h3 id=\"interpretazione-dellentropia\">Interpretazione dell&rsquo;Entropia</h3>\n<ul>\n<li><strong>Alta entropia</strong> indica una maggiore incertezza. Ciò significa che le probabilità sono distribuite in modo più uniforme tra gli esiti, suggerendo che non ci sono esiti dominanti.</li>\n<li><strong>Bassa entropia</strong> indica una distribuzione più concentrata, in cui uno o pochi esiti hanno probabilità più elevate, riducendo l&rsquo;incertezza.</li>\n</ul>\n<p>Ad esempio, se si lancia una moneta equa, la probabilità di ottenere testa o croce è $0.5$ ciascuna, e l&rsquo;entropia è massima ($H(X) = 1$). Se invece si lancia una moneta truccata, con probabilità $0.9$ per testa e $0.1$ per croce, l&rsquo;entropia sarà $\\approx 0.14$.</p>\n<hr />\n<h2 id=\"cross-entropy\">Cross Entropy</h2>\n<p>La <strong>cross-entropy</strong> misura la distanza tra due distribuzioni di probabilità. In particolare, misura quanto bene una distribuzione di probabilità predetta $Q(X)$ approssima una distribuzione di probabilità vera $P(X)$.</p>\n<p>La formula della <strong>cross-entropy</strong> tra due distribuzioni è:</p>\n$$\nH(P, Q) = -\\sum_{i} P(x_i) \\log Q(x_i)\n$$\n<p>Dove:\n- $P(x_i)$ è la distribuzione &ldquo;vera&rdquo; (ad esempio, la distribuzione dei dati reali).\n- $Q(x_i)$ è la distribuzione &ldquo;predetta&rdquo; (ad esempio, la distribuzione predetta da un modello).</p>\n<h3 id=\"interpretazione-della-cross-entropy\">Interpretazione della Cross-Entropy</h3>\n<ul>\n<li><strong>Alta cross-entropy</strong>: Se le due distribuzioni sono molto diverse, la cross-entropy sarà alta. Questo indica che la distribuzione predetta non è una buona approssimazione di quella vera.</li>\n<li><strong>Bassa cross-entropy</strong>: Se le distribuzioni sono simili, la cross-entropy sarà bassa. Questo indica che il modello ha predetto correttamente la distribuzione dei dati.</li>\n</ul>\n<p>La <strong>cross-entropy loss</strong> è una funzione di perdita comunemente utilizzata nell&rsquo;apprendimento automatico per misurare quanto bene un modello predice una distribuzione di probabilità rispetto ai dati reali. Durante l&rsquo;addestramento, l&rsquo;obiettivo è minimizzare la cross-entropy loss.</p>\n<hr />\n<h2 id=\"divergenza-di-kullback-leibler-kl-divergence\">Divergenza di Kullback-Leibler (KL Divergence)</h2>\n<p>La <strong>divergenza di Kullback-Leibler</strong> (KL Divergence) misura quanto una distribuzione di probabilità $Q(X)$ diverge dalla distribuzione di probabilità $P(X)$, ed è definita come:</p>\n$$\nD_{\\text{KL}}(P \\parallel Q) = \\sum_{i} P(x_i) \\log \\left( \\frac{P(x_i)}{Q(x_i)} \\right)\n$$\n<h3 id=\"interpretazione-della-kl-divergence\">Interpretazione della KL Divergence</h3>\n<ul>\n<li>La KL Divergence è una misura di &ldquo;distanza&rdquo; tra le due distribuzioni. Tuttavia, <strong>non è una metrica</strong>, poiché non soddisfa la proprietà di simmetria. Infatti, $D_{\\text{KL}}(P \\parallel Q)$ non è uguale a $D_{\\text{KL}}(Q \\parallel P)$.</li>\n<li>Intuitivamente, la KL Divergence rappresenta l&rsquo;<strong>aspettativa del numero di bit extra</strong> necessari per codificare campioni di $P$ utilizzando un codice ottimizzato per $Q$.</li>\n</ul>\n<h3 id=\"calcolo-della-kl-divergence\">Calcolo della KL Divergence</h3>\n<ul>\n<li>Se $Q$ è una buona approssimazione di $P$, la KL Divergence sarà bassa.</li>\n<li>Se $Q$ è molto diversa da $P$, la KL Divergence sarà alta.</li>\n</ul>\n<p>In altre parole, la KL Divergence ci dice quanto la distribuzione predetta $Q(X)$ è lontana dalla distribuzione vera $P(X)$ in termini di informazioni aggiuntive richieste.</p>\n<h3 id=\"formula-alternativa\">Formula Alternativa</h3>\n<p>La KL Divergence può anche essere espressa come:</p>\n$$\nD_{\\text{KL}}(P \\parallel Q) = H(P, Q) - H(P)\n$$\n<p>Dove:\n- $H(P, Q)$ è la cross-entropy tra le distribuzioni $P$ e $Q$.\n- $H(P)$ è l&rsquo;entropia della distribuzione vera $P$.</p>\n<h4 id=\"dimostrazione\">Dimostrazione</h4>\n$$\\begin{align} D_{\\text{KL}}(P \\parallel Q) &= \\sum_{i} P(x_i) \\log \\left( \\frac{P(x_i)}{Q(x_i)} \\right) \\\\ &= \\sum_{i} P(x_i) \\left[ \\log P(x_i) - \\log Q(x_i) \\right] \\\\ &= \\sum_{i} P(x_i) \\log P(x_i) - \\sum_{i} P(x_i) \\log Q(x_i) \\\\ &= - H(P) + H(P, Q) \\\\ &= H(P, Q) - H(P) \\end{align}\n$$\n<p><strong>Interpretazione</strong> \n- La <strong>cross-entropy</strong> $H(P, Q)$ rappresenta il numero medio di bit necessari per codificare i dati di $P$ usando $Q$. \n- L&rsquo;<strong>entropia</strong> $H(P)$ è il numero minimo teorico di bit necessari per codificare i dati di $P$. \n- La <strong>KL Divergence</strong> $D_{\\text{KL}}(P \\parallel Q)$ misura l&rsquo;aumento del numero medio di bit necessari a causa dell&rsquo;uso della distribuzione $Q$ invece di quella ottimale $P$. Se $Q$ è una buona approssimazione di $P$, allora $D_{\\text{KL}}(P \\parallel Q)$ sarà vicino a zero. Se invece $Q$ è molto diversa da $P$, la KL Divergence sarà alta.</p>\n<p>Questa formula evidenzia che la KL Divergence è la differenza tra l&rsquo;entropia della distribuzione vera e la cross-entropy.</p>\n<hr />\n<h2 id=\"in-apprendimento-automatico\">In Apprendimento Automatico</h2>\n<p>Nel contesto dell&rsquo;apprendimento automatico, la <strong>cross-entropy loss</strong> e la <strong>KL Divergence</strong> sono utilizzate per allenare modelli di classificazione e generativi.</p>\n<ul>\n<li><strong>Cross-Entropy Loss</strong>: Misura la qualità delle probabilità predette dal modello, e viene minimizzata durante il processo di addestramento.</li>\n<li><strong>KL Divergence</strong>: Viene usata per ottimizzare la distanza tra una distribuzione predetta e una distribuzione target, ed è comune in modelli probabilistici come i modelli di inferenza bayesiana e nei modelli generativi come le reti generative avversarie (GANs).</li>\n</ul>\n<hr />\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<ul>\n<li><strong>Entropia</strong> misura l&rsquo;incertezza di una distribuzione.</li>\n<li><strong>Cross-Entropy</strong> misura la distanza tra una distribuzione vera e una distribuzione predetta.</li>\n<li><strong>KL Divergence</strong> misura quanto una distribuzione predetta diverge dalla distribuzione vera, ed è spesso usata in apprendimento automatico per ottimizzare modelli probabilistici.</li>\n</ul>\n<p>L&rsquo;uso di queste misure permette di quantificare l&rsquo;incertezza e l&rsquo;accuratezza dei modelli predittivi, e gioca un ruolo cruciale in molte applicazioni di machine learning, specialmente in contesti probabilistici.</p>"
}