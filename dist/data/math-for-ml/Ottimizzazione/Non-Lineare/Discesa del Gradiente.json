{
  "title": "Discesa del Gradiente",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>La discesa del gradiente (<em>Gradient Descent</em>, GD) è un algoritmo iterativo di minimizzazione del primo ordine. Viene definito <strong>iterativo</strong> poiché esegue una sequenza di aggiornamenti successivi per determinare un minimo locale della funzione obiettivo, a partire da una condizione iniziale.</p>\n<p>Un aspetto cruciale della discesa del gradiente è che, nel caso di funzioni <strong>non convesse</strong>, non possiamo garantire che l&rsquo;algoritmo trovi il minimo globale. Infatti, tali funzioni possono presentare <strong>molteplici minimi locali</strong>, e il punto di convergenza dipenderà dalle condizioni iniziali del modello.</p>\n<p>È possibile che invece di un minimo locale (o globale), l&rsquo;algoritmo si interrompa su un punto di sella. Durante la discesa del gradiente, l’algoritmo cerca punti dove il valore della funzione diminuisce. Se si avvicina a un punto di sella, il gradiente (cioè l&rsquo;indicazione della direzione in cui scendere) può diventare molto piccolo, e questo può <strong>rallentare o bloccare</strong> temporaneamente l’ottimizzazione. Anche se i punti di sella esistono, è <strong>molto improbabile</strong> che la discesa del gradiente si fermi esattamente su uno di essi, per due motivi principali:</p>\n<ol>\n<li>\n<p><strong>Instabilità numerica</strong>: i punti di sella sono instabili — basta una piccola variazione (come un errore di arrotondamento o un passo leggermente diverso) per spingere l’algoritmo lontano dal punto di sella.</p>\n</li>\n<li>\n<p><strong>Alta dimensionalità</strong>: negli spazi ad alta dimensione, i punti di sella sono molto più frequenti dei minimi, ma anche molto più &ldquo;facili da evitare&rdquo;. È molto raro &ldquo;cadere&rdquo; perfettamente in un punto di sella, e ancor più raro restarci a lungo.</p>\n</li>\n</ol>\n<p>In pratica, anche se ci si può avvicinare a un punto di sella, la discesa del gradiente tende naturalmente a superarlo e continuare verso un minimo.</p>\n<p>Nel contesto del <em>model fitting</em>, l’obiettivo consiste nel minimizzare una <strong>funzione di perdita</strong> (<em>loss function</em>), definita come applicazione:  </p>\n$$\n\\ell: \\mathbb{R}^n \\; \\longrightarrow \\; \\mathbb{R}\n$$\n<p>dove:  </p>\n<ul>\n<li>il <strong>dominio</strong> è lo spazio dei parametri del modello, $\\mathbb{R}^n$,  </li>\n<li>il <strong>codominio</strong> è l’asse reale, $\\mathbb{R}$.  </li>\n</ul>\n<p>Il processo di ottimizzazione ha inizio a partire da una <strong>condizione iniziale</strong>, ossia da un punto $\\Theta^{(0)} \\in \\mathbb{R}^n$ che rappresenta un insieme di valori iniziali per i parametri del modello.<br />\nTale punto può essere determinato in maniera <strong>casuale</strong> oppure tramite una procedura di <strong>inizializzazione mirata</strong>, progettata per favorire la successiva convergenza dell’algoritmo di ottimizzazione.  </p>\n<p>L&rsquo;intuizione alla base della discesa del gradiente è piuttosto semplice:</p>\n<ol>\n<li>Si parte da un punto $\\Theta^{(0)}$ iniziale nello spazio dei parametri.  </li>\n<li>Si calcola il gradiente della funzione obiettivo in quel punto, il quale indica la direzione di massima crescita.  </li>\n<li>Per minimizzare la funzione, ci si sposta nella direzione opposta a quella del gradiente, effettuando un &ldquo;passo&rdquo; in quella direzione.  </li>\n<li>Questo processo viene ripetuto fino al raggiungimento di un criterio di arresto (convergenza).</li>\n</ol>\n<p><img src=\"/images/posts/gradient-descent.jpg\" alt=\"Gradient Descent\" style=\"display: block; margin-left: auto; margin-right: auto;\"></p>\n<p><em>Figura 1.0: Discesa del Gradiente su una funzione loss non convessa</em></p>\n<p>Formalmente, il processo di aggiornamento iterativo può essere espresso come:</p>\n$$\n\\Theta^{(t+1)} \\leftarrow \\Theta^{(t)} - \\alpha \\nabla \\ell_{\\Theta^{(t)}}\n$$\n<p>dove:</p>\n<ul>\n<li>$\\Theta^{(t)}$ rappresenta i parametri del modello all&rsquo;iterazione $t$,</li>\n<li>$\\alpha$ è il <strong>tasso di apprendimento</strong> (<em>learning rate</em>), un iperparametro che determina l&rsquo;ampiezza del passo nella direzione del gradiente,</li>\n<li>$\\nabla \\ell_{\\Theta^{(t)}}$ è il gradiente della funzione di perdita $\\ell$ rispetto ai parametri $\\Theta$.</li>\n</ul>\n<p>Il criterio di arresto può essere formalizzato come segue:</p>\n$$\n||\\nabla \\ell_{\\Theta^{(t)}}|| \\leq \\epsilon.\n$$\nper un certo $\\epsilon > 0$.</p>\n<p>Ovviamente, per poter calcolare correttamente il gradiente di una funzione, e quindi eseguire correttamente la discesa del gradiente, abbiamo bisogno che la funzione $\\ell$ sia differenziabile in ogni suo punto.</p>\n<p>Infatti, non basta che sia definita la derivata parziale di $\\ell$ rispetto a ogni singola variabile $\\theta_i$, ma è necessario che $\\ell$ abbia un gradiente continuo.</p>\n<h2 id=\"differenziabilita\">Differenziabilità</h2>\n<p>Come abbiamo visto, il gradiente è l’elemento chiave nel funzionamento di questo tipo di ottimizzazione non lineare. Ma ci si potrebbe chiedere: <strong>tutte le funzioni di perdita permettono il calcolo del gradiente?</strong></p>\n<p>La risposta è: <strong>non sempre</strong>.</p>\n<p>Non tutte le funzioni sono <strong>differenziabili</strong>, cioè non tutte ammettono un gradiente ben definito in ogni punto del dominio. Questo è un problema rilevante, perché <strong>la discesa del gradiente richiede che la funzione sia differenziabile</strong>, altrimenti il gradiente potrebbe non esistere in certi punti e l’algoritmo potrebbe bloccarsi o dare risultati errati.</p>\n<h3 id=\"derivate-parziali-definite-math_inline_64-differenziabilita\">Derivate parziali definite $≠$ Differenziabilità</h3>\n<p>In una funzione di più variabili, avere <strong>tutte le derivate parziali definite</strong> non è sufficiente per garantire la differenziabilità. Infatti, può succedere che tutte le derivate esistano, ma non siano continue — e questo è un segnale che la funzione <strong>non è veramente differenziabile</strong>.</p>\n<p>Un esempio classico è la seguente funzione:</p>\n<ul>\n<li>$f(x, y) = 0$ se $(x, y) = (0, 0)$</li>\n<li>$f(x, y) = \\frac{x^2 y}{x^2 + y^2}$ altrimenti</li>\n</ul>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">mpl_toolkits.mplot3d</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Axes3D</span>\n\n<span class=\"c1\"># Funzione definita a tratti</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">errstate</span><span class=\"p\">(</span><span class=\"n\">divide</span><span class=\"o\">=</span><span class=\"s1\">&#39;ignore&#39;</span><span class=\"p\">,</span> <span class=\"n\">invalid</span><span class=\"o\">=</span><span class=\"s1\">&#39;ignore&#39;</span><span class=\"p\">):</span>\n        <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">((</span><span class=\"n\">x</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">y</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">z</span>\n\n<span class=\"c1\"># Griglia</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Figura Matplotlib</span>\n<span class=\"n\">fig</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">))</span>  <span class=\"c1\"># circa 1920x1080</span>\n<span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_subplot</span><span class=\"p\">(</span><span class=\"mi\">111</span><span class=\"p\">,</span> <span class=\"n\">projection</span><span class=\"o\">=</span><span class=\"s1\">&#39;3d&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Superficie</span>\n<span class=\"n\">surf</span> <span class=\"o\">=</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot_surface</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">,</span> <span class=\"n\">edgecolor</span><span class=\"o\">=</span><span class=\"s1\">&#39;none&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Punto non differenziabile all&#39;origine</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;(0, 0)&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"n\">ha</span><span class=\"o\">=</span><span class=\"s1\">&#39;center&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Etichette</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Funzione non differenziabile in (0, 0)&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">14</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_zlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;f(x, y)&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Vista iniziale</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">view_init</span><span class=\"p\">(</span><span class=\"n\">elev</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"n\">azim</span><span class=\"o\">=</span><span class=\"mi\">45</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Salvataggio in HD</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s2\">&quot;gradient-non-differentiable.png&quot;</span><span class=\"p\">,</span> <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><img src=\"/images/posts/gradient-non-differentiable.png\" alt=\"Gradient Descent 3\" width=\"1000\" style=\"display: block; margin: 0 auto;\"></p>\n<p><em>Figura 1.3: Funzione non differenziabile</em></p>\n<p>Questa funzione ha derivate parziali definite ovunque, ma sono <strong>discontinue nell&rsquo;origine</strong>, e questo significa che $f$ <strong>non è differenziabile</strong> in $(0, 0)$.</p>\n<h4 id=\"1-derivate-parziali-in-math_inline_70\">1. Derivate parziali in $(0,0)$</h4>\n<p>Per definizione,\n$$\nf_x(0,0)\n=\\lim_{h\\to0}\\frac{f(h,0)-f(0,0)}{h}\n=\\lim_{h\\to0}\\frac{0-0}{h}=0,\n\\qquad\nf_y(0,0)\n=\\lim_{k\\to0}\\frac{f(0,k)-f(0,0)}{k}\n=\\lim_{k\\to0}\\frac{0-0}{k}=0.\n$$\nDove $f_x$ e $f_y$ sono le derivate parziali di $f$ rispetto a $x$ e a $y$, rispettivamente.</p>\n<h4 id=\"2-espressioni-di-math_inline_76-e-math_inline_77-per-math_inline_78\">2. Espressioni di $f_x$ e $f_y$ per $(x,y)\\neq(0,0)$</h4>\n<p>Usando la derivazione di un quoziente:\n$$\nf(x,y)=\\frac{u(x,y)}{v(x,y)},\n\\quad\nu=x^2y,\\quad v=x^2+y^2,\n$$\n$$\nf_x\n=\\frac{u_x\\,v - u\\,v_x}{v^2}\n=\\frac{(2x y)(x^2+y^2) - (x^2y)(2x)}{(x^2+y^2)^2}\n=\\frac{2x y^3}{(x^2+y^2)^2},\n$$\n$$\nf_y\n=\\frac{u_y\\,v - u\\,v_y}{v^2}\n=\\frac{(x^2)(x^2+y^2) - (x^2y)(2y)}{(x^2+y^2)^2}\n=\\frac{x^4 - x^2y^2}{(x^2+y^2)^2}.\n$$\n<h4 id=\"3-limiti-lungo-la-retta-math_inline_79\">3. Limiti lungo la retta $y = m x$</h4>\n<p>Sostituiamo $y=mx$ con $x\\to0$:\n$$\nf_x(x,mx)\n=\\frac{2x\\,(mx)^3}{\\bigl(x^2+(mx)^2\\bigr)^2}\n=\\frac{2m^3\\,x^4}{x^4\\,(1+m^2)^2}\n=\\frac{2m^3}{(1+m^2)^2},\n$$\n$$\nf_y(x,mx)\n=\\frac{x^4 - x^2\\,(mx)^2}{\\bigl(x^2+(mx)^2\\bigr)^2}\n=\\frac{x^4(1-m^2)}{x^4\\,(1+m^2)^2}\n=\\frac{1-m^2}{(1+m^2)^2}.\n$$\nQuesti valori dipendono dal parametro $m$. In particolare:\n- Se $m=0$, $f_x\\to0$ e $f_y\\to1$.<br />\n- Se $m=1$, $f_x\\to\\dfrac{2}{4}=\\tfrac12$ e $f_y\\to0$.</p>\n<h4 id=\"4-conclusione-sulla-discontinuita\">4. Conclusione sulla discontinuità</h4>\n<p>Poiché\n$$\n\\lim_{(x,y)\\to(0,0)}\\nabla f(x,y)\n$$\nassume valori diversi a seconda della retta di avvicinamento ($m$ diverso), il gradiente <strong>non è continuo</strong> in $(0,0)$, pur avendo entrambe le derivate parziali esistenti e finite.</p>\n<h3 id=\"implicazioni-pratiche\">Implicazioni pratiche</h3>\n<p>Fortunatamente, nella pratica si usano spesso funzioni di perdita ben progettate, che sono <strong>lisce e differenziabili</strong> quasi ovunque. Tuttavia, <strong>non è raro incontrare funzioni di perdita non differenziabili</strong>, ad esempio con funzioni <em>piecewise</em> o attivazioni come la <em>ReLU</em>.</p>\n<p>In questi casi, si adottano diverse strategie per rendere il problema trattabile:</p>\n<ul>\n<li><strong>Modifica o sostituzione della funzione</strong> con una variante liscia (es. ReLU → Softplus)</li>\n<li><strong>Tecniche come il &ldquo;reparametrization trick&rdquo;</strong> nei modelli generativi come le VAE, che permettono il passaggio del gradiente anche quando la funzione non è differenziabile nel senso classico</li>\n</ul>\n<p>In conclusione, <strong>la differenziabilità è un requisito fondamentale per l’applicazione diretta della discesa del gradiente</strong>, ma esistono metodi e tecniche per aggirare o gestire in modo efficace i casi in cui essa venga meno.</p>\n<h2 id=\"interpretazione-geometrica\">Interpretazione Geometrica</h2>\n<p>Dal punto di vista geometrico, la discesa del gradiente segue una traiettoria nello spazio dei parametri, cercando il punto in cui la funzione di perdita assume un valore minimo. Se la funzione è convessa, l&rsquo;algoritmo convergerà al minimo globale; altrimenti, si fermerà in un minimo locale.</p>\n<p>Un concetto importante in questo contesto è quello dei <strong>punti stazionari</strong>. Un punto stazionario di una funzione differenziabile è un punto in cui tutte le derivate parziali sono nulle, cioè il gradiente è zero. Dal punto di vista del <em>gradient descent</em>, questo significa che l’algoritmo può “bloccarsi” in questi punti:</p>\n$$\n\\Theta^{(t+1)} = \\Theta^{(t)} - \\bcancel{\\alpha \\nabla \\ell_{\\Theta^{(t)}}}.\n$$\n<p>Non tutti i punti stazionari sono minimi: esistono anche massimi locali e punti sella, ossia punti in cui la funzione aumenta lungo un asse e diminuisce lungo un altro. Il tipo di punto stazionario verso cui l’algoritmo converge dipende dall’inizializzazione; ad esempio, se si parte esattamente in un massimo locale o in un punto sella (evento molto improbabile in pratica), l’algoritmo si fermerà immediatamente, poiché il gradiente è zero, anche se tali punti non rappresentano minimi della funzione.</p>\n<p>È importante notare che, a causa della precisione finita delle macchine, difficilmente si raggiungerà un punto stazionario esatto, ma ci si fermerà quando la variazione della funzione di perdita diventa trascurabile.</p>\n<p>Volendo, possiamo anche, tramite l&rsquo;unrolling ricorsivo, riscrivere esplicitamente $\\Theta^{(t+1)}$ come:</p>\n$$\n\\begin{align*}\n\\Theta^{(1)}   &= \\Theta^{(0)} - \\alpha \\nabla \\ell_{\\Theta^{(0)}}\\\\\n\\Theta^{(2)}   &= \\Theta^{(1)} - \\alpha \\nabla \\ell_{\\Theta^{(1)}}\\\\\n               &= \\Theta^{(0)} - \\alpha \\nabla \\ell_{\\Theta^{(0)}} - \\alpha \\nabla \\ell_{\\Theta^{(1)}}\\\\\n\\vdots\\\\\n\\Theta^{(t+1)} &= \\Theta^{(0)} - \\alpha \\sum_{i=0}^{t} \\nabla \\ell_{\\Theta^{(i)}}.\n\\end{align*}\n$$\n<p>Il criterio di arresto più comune è la verifica della norma del gradiente:</p>\n$$\n\\|\\nabla \\ell_{\\Theta^{(t)}}\\| \\leq \\epsilon\n$$\n<p>dove $\\epsilon$ è una soglia positiva molto piccola che determina il livello di precisione desiderato.</p>\n<p>Altri criteri possibili sono:</p>\n<ul>\n<li><strong>Nessuna modifica dei parametri</strong>: se $\\Theta^{(t+1)} = \\Theta^{(t)}$, il criterio di arresto viene raggiunto.</li>\n<li><strong>Loss non migliorata</strong>: se $|\\ell_{\\Theta^{(t)}} - \\ell_{\\Theta^{(t+1)}}| \\leq \\epsilon$, il criterio di arresto viene raggiunto.</li>\n</ul>\n<h2 id=\"proprieta-del-gradiente\">Proprietà del Gradiente</h2>\n<h3 id=\"ortogonalita-e-massima-crescita\">Ortogonalità e Massima Crescita</h3>\n<p>Non abbiamo ancora fornito una giustificazione formale dell&rsquo;affermazione secondo cui il gradiente di una funzione in un dato punto rappresenta la direzione di massima crescita della funzione stessa.</p>\n<p>Per comprendere meglio questo concetto, introduciamo la <strong>derivata direzionale</strong>, una generalizzazione della derivata tradizionale nel dominio unidimensionale $\\mathbb{R}$. Mentre nella retta reale esiste un&rsquo;unica direzione lungo cui calcolare la derivata, in $\\mathbb{R}^n$ (per $n \\geq 2$) non esiste una direzione privilegiata per valutare la variazione di una funzione.</p>\n<p>La derivata direzionale di una funzione differenziabile $f: \\mathbb{R}^n \\to \\mathbb{R}$ lungo una direzione unitaria $\\mathbf{v}$ è definita come:</p>\n$$\nD_{\\mathbf{v}} f(\\mathbf{x}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{x} + h \\mathbf{v}) - f(\\mathbf{x})}{h}.\n$$\n<p>Questa definizione generalizza la derivata parziale $\\frac{\\partial f}{\\partial x}$, che assume che la direzione considerata sia allineata con uno degli assi canonici (ovvero, solo una variabile cambia alla volta mentre le altre restano fisse). Al contrario, la derivata direzionale consente una variazione simultanea di tutte le variabili lungo una direzione arbitraria $\\mathbf{v}$.</p>\n<blockquote>\n<p>Si consiglia la lettura di <a href=\"/theory/math-for-ml/Calcolo/Calcolo Matriciale/Relazione tra derivata direzionale e gradiente\" class=\"text-blue-600 hover:underline\">Relazione tra derivata direzionale e gradiente</a> prima di proseguire con questa nota.</p>\n</blockquote>\n<h3 id=\"relazione-tra-curve-di-livello-derivata-direzionale-e-gradiente\">Relazione tra Curve di Livello, Derivata Direzionale e Gradiente</h3>\n<p>Le <strong>curve di livello</strong> di una funzione sono le curve (o ipersuperfici) lungo le quali la funzione assume lo stesso valore. Ciò implica che la derivata direzionale della funzione in un punto appartenente alla curva, lungo una direzione tangente alla curva stessa, sia nulla, poiché la funzione non varia localmente in quella direzione.</p>\n<p>Formalmente, se $\\mathbf{v}$ è un vettore tangente alla curva di livello di $f$, allora</p>\n$$\n\\langle \\nabla f, \\mathbf{v} \\rangle = 0.\n$$\n<p>Questo risultato implica che il <strong>gradiente è ortogonale alla curva di livello</strong> e punta nella direzione di massima crescita della funzione. Inoltre, si può dimostrare che esso è orientato verso curve di livello con valori maggiori della funzione (ovvero, verso l&rsquo;incremento della funzione stessa).</p>\n<p>Questo concetto è riassunto nella seguente rappresentazione grafica:</p>\n<ul>\n<li>Le curve di livello rappresentano i punti di uguale valore della funzione.</li>\n<li>Il gradiente è sempre perpendicolare a tali curve.</li>\n<li>La discesa del gradiente segue la direzione opposta al gradiente stesso per minimizzare la funzione.</li>\n</ul>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">mpl_toolkits.mplot3d</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Axes3D</span>  <span class=\"c1\"># Necessario per i plot 3D</span>\n\n<span class=\"c1\"># Definizione della funzione f(x, y)</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Esempio di funzione con più minimi locali.</span>\n<span class=\"sd\">    Puoi modificarla come preferisci.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"k\">return</span> <span class=\"mf\">0.2</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mf\">2.0</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Definizione del gradiente di f(x, y)</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">grad_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Calcola il gradiente di f(x, y): restituisce (df/dx, df/dy).</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">dfdx</span> <span class=\"o\">=</span> <span class=\"mf\">0.4</span> <span class=\"o\">*</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mf\">2.0</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">cos</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"n\">dfdy</span> <span class=\"o\">=</span> <span class=\"mf\">0.4</span> <span class=\"o\">*</span> <span class=\"n\">y</span> <span class=\"o\">+</span> <span class=\"mf\">2.0</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">cos</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">dfdx</span><span class=\"p\">,</span> <span class=\"n\">dfdy</span>\n\n<span class=\"c1\"># Creazione della griglia di punti per i plot</span>\n<span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mi\">200</span>  <span class=\"c1\"># Numero di punti per dimensione</span>\n<span class=\"n\">x_vals</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n<span class=\"n\">y_vals</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">x_vals</span><span class=\"p\">,</span> <span class=\"n\">y_vals</span><span class=\"p\">)</span>\n<span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Calcolo del gradiente su una griglia più rada per la visualizzazione del campo vettoriale</span>\n<span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"mi\">15</span>  <span class=\"c1\"># Passo per la selezione dei punti per il campo vettoriale</span>\n<span class=\"n\">x_quiver</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[::</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"n\">step</span><span class=\"p\">]</span>\n<span class=\"n\">y_quiver</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"p\">[::</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"n\">step</span><span class=\"p\">]</span>\n<span class=\"n\">dfdx</span><span class=\"p\">,</span> <span class=\"n\">dfdy</span> <span class=\"o\">=</span> <span class=\"n\">grad_f</span><span class=\"p\">(</span><span class=\"n\">x_quiver</span><span class=\"p\">,</span> <span class=\"n\">y_quiver</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Creazione della figura con 2 righe e 2 colonne, usando width_ratios per modificare le dimensioni dei subplot in alto</span>\n<span class=\"n\">fig</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">gs</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_gridspec</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">width_ratios</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"c1\"># Pannello 1 (in alto a sinistra): Plot 3D della superficie (più grande)</span>\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"n\">ax1</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">projection</span><span class=\"o\">=</span><span class=\"s1\">&#39;3d&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">surf</span> <span class=\"o\">=</span> <span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot_surface</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">,</span> <span class=\"n\">edgecolor</span><span class=\"o\">=</span><span class=\"s1\">&#39;none&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Superficie 3D&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_zlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;f(x, y)&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"c1\"># Pannello 2 (in alto a destra): Mappa di colore e curve di livello (più quadrato)</span>\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"n\">ax2</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n<span class=\"n\">cont</span> <span class=\"o\">=</span> <span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">contourf</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">colors</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">,</span> <span class=\"n\">linewidths</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Mappa di colore e curve di livello&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_aspect</span><span class=\"p\">(</span><span class=\"s1\">&#39;equal&#39;</span><span class=\"p\">,</span> <span class=\"n\">adjustable</span><span class=\"o\">=</span><span class=\"s1\">&#39;box&#39;</span><span class=\"p\">)</span>  <span class=\"c1\"># Forza un aspetto quadrato</span>\n<span class=\"n\">cbar</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">colorbar</span><span class=\"p\">(</span><span class=\"n\">cont</span><span class=\"p\">,</span> <span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"n\">ax2</span><span class=\"p\">)</span>\n<span class=\"n\">cbar</span><span class=\"o\">.</span><span class=\"n\">set_label</span><span class=\"p\">(</span><span class=\"s1\">&#39;Valore di f(x, y)&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"c1\"># Pannello 3 (in basso a sinistra): Campo vettoriale del gradiente positivo</span>\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"n\">ax3</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">])</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">contourf</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">colors</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">,</span> <span class=\"n\">linewidths</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">Q</span> <span class=\"o\">=</span> <span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">quiver</span><span class=\"p\">(</span><span class=\"n\">x_quiver</span><span class=\"p\">,</span> <span class=\"n\">y_quiver</span><span class=\"p\">,</span> <span class=\"n\">dfdx</span><span class=\"p\">,</span> <span class=\"n\">dfdy</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Campo vettoriale del gradiente&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_aspect</span><span class=\"p\">(</span><span class=\"s1\">&#39;equal&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"c1\"># Pannello 4 (in basso a destra): Campo vettoriale del gradiente negativo</span>\n<span class=\"c1\"># -------------------------------------------------------------------------</span>\n<span class=\"n\">ax4</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">contourf</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">)</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">colors</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">,</span> <span class=\"n\">linewidths</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">Q2</span> <span class=\"o\">=</span> <span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">quiver</span><span class=\"p\">(</span><span class=\"n\">x_quiver</span><span class=\"p\">,</span> <span class=\"n\">y_quiver</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"n\">dfdx</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"n\">dfdy</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Campo vettoriale del gradiente negativo&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">set_aspect</span><span class=\"p\">(</span><span class=\"s1\">&#39;equal&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s1\">&#39;./images/gradient.jpg&#39;</span><span class=\"p\">,</span> \n           <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span> \n           <span class=\"n\">bbox_inches</span><span class=\"o\">=</span><span class=\"s1\">&#39;tight&#39;</span><span class=\"p\">,</span>\n           <span class=\"n\">pad_inches</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span>  <span class=\"c1\"># Aggiungere questo parametro</span>\n           <span class=\"c1\">#facecolor=fig.get_facecolor(),  # Mantenere il colore di sfondo</span>\n           <span class=\"n\">transparent</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>  <span class=\"c1\"># Disabilitare la trasparenza</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><img src=\"/images/posts/gradient.jpg\" alt=\"Gradient Descent\"></p>\n<p><em>Figura 1.1: Visualizzazione della funzione $f(x,y)$ con la sua superficie 3D, mappa di livello e campi vettoriali del gradiente positivo e negativo, evidenziando le direzioni di massima variazione.</em></p>\n<p>Questa proprietà è fondamentale nelle tecniche di ottimizzazione basate sul gradiente, in quanto garantisce che muovendosi nella direzione opposta al gradiente si riduce il valore della funzione obiettivo.</p>\n<h3 id=\"visualizzazione-di-vettori-tangenti-gradiente-e-campo-vettoriale\">Visualizzazione di vettori tangenti, gradiente e campo vettoriale</h3>\n<p>Vediamo ora un esempio di quanto detto prima.</p>\n<h4 id=\"1-definizione-della-funzione-e-del-gradiente\">1. Definizione della funzione e del gradiente</h4>\n<p>Consideriamo una funzione quadratica semplice:</p>\n$$\nf(x, y) = \\frac{1}{2} x^2 + y^2\n$$\n<p>Il gradiente di $f$ è dato da:</p>\n$$\n\\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (x, 2y)\n$$\n<h4 id=\"2-punto-di-interesse\">2. Punto di interesse</h4>\n<p>Scegliamo un punto specifico $P_0 = (1,1)$ e calcoliamo il gradiente in questo punto:</p>\n$$\n\\nabla f(P_0) = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n<h4 id=\"3-direzione-tangente\">3. Direzione tangente</h4>\n<p>Vogliamo trovare un vettore tangente $\\mathbf{v} = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$ tale che la derivata direzionale lungo $\\mathbf{v}$ sia nulla:</p>\n$$\n\\frac{df}{d\\mathbf{v}}(P_0) = \\langle \\nabla f(P_0), \\mathbf{v} \\rangle = g_x v_x + g_y v_y = 0.\n$$\nQuesto implica che il vettore $\\mathbf v$ ortogonale a $\\nabla f(P_0)$ è tangente alla curva di livello, perché la derivata direzionale della funzione nella direzione $\\mathbf v$ è nulla, e quindi la funzione è costante lungo quella direzione.</p>\n<p>La condizione di ortogonalità è <strong>una sola equazione lineare</strong> per le due componenti $v_x$ e $v_y$:</p>\n<p>$$\ng_x v_x + g_y v_y = 0.\n$$\n<p>Questa equazione vincola $v_x$ e $v_y$ a essere correlati, ma <strong>non determina univocamente</strong> un singolo vettore.  </p>\n<p>Infatti esistono infiniti vettori $\\mathbf{v}$ che soddisfano questa condizione: tutti i vettori lungo la retta definita da $v_y = -\\frac{g_x}{g_y} v_x$.</p>\n<blockquote>\n<p>Una sola equazione con due incognite: $v_x$ e $v_y$</p>\n</blockquote>\n<p>Possiamo scegliere arbitrariamente una delle due componenti di $\\mathbf{v}$. Ad esempio, fissiamo:</p>\n$$\nv_x = g_y\n$$\nDalla condizione di ortogonalità:</p>\n<p>$$\ng_x v_x + g_y v_y = 0 \\quad \\implies \\quad g_x g_y + g_y v_y = 0\n$$\n$$\nv_y = -g_x\n$$\n<p>Otteniamo quindi il vettore tangente non normalizzato:</p>\n$$\n\\mathbf{v} = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} g_y \\\\ -g_x \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\n<p><img src=\"/images/tikz/b31c02191ba705274ef7d7041b817dc1.svg\" style=\"display: block; width: 100%; height: auto; max-height: 600px;\" class=\"tikz-svg\" />\nIl vettore unitario tangente è:</p>\n$$\n\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|} = \\frac{1}{\\sqrt{v_x^2 + v_y^2}} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}\n$$\n$$\n\\mathbf{v} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad\n\\hat{\\mathbf{v}} = \\frac{1}{\\sqrt{2^2 + (-1)^2}} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\n<blockquote>\n<p>Questo garantisce che il vettore sia ortogonale al gradiente in $P_0$ e tangente alla curva di livello.</p>\n</blockquote>\n<h4 id=\"4-vettore-di-discesa-del-gradiente\">4. Vettore di discesa del gradiente</h4>\n<p>La direzione di discesa del gradiente è opposta al gradiente:</p>\n$$\n-\\nabla f(P_0) = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix}, \\quad\n\\text{unitario: } \\hat{g} = \\frac{-\\nabla f(P_0)}{\\|\\nabla f(P_0)\\|}\n$$\n<h4 id=\"5-campo-vettoriale-del-gradiente\">5. Campo vettoriale del gradiente</h4>\n<p>Il campo vettoriale dell&rsquo;intero gradiente è dato da:</p>\n$$\n\\text{campo vettoriale: } (x, y) \\mapsto \\nabla f(x, y) = (x, 2y)\n$$\n<blockquote>\n<p>Questo permette di visualizzare in ogni punto la direzione e la magnitudine del gradiente, utile per capire il comportamento della funzione.</p>\n</blockquote>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n\n<span class=\"c1\"># Definizione della funzione e del suo gradiente</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">grad_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">y</span>\n\n<span class=\"c1\"># Creazione della griglia di punti</span>\n<span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mi\">200</span>\n<span class=\"n\">x_vals</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n<span class=\"n\">y_vals</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">x_vals</span><span class=\"p\">,</span> <span class=\"n\">y_vals</span><span class=\"p\">)</span>\n<span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Punto di interesse P0</span>\n<span class=\"n\">P0</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">gx</span><span class=\"p\">,</span> <span class=\"n\">gy</span> <span class=\"o\">=</span> <span class=\"n\">grad_f</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>  <span class=\"c1\"># gradiente in P0</span>\n\n<span class=\"c1\"># Calcolo della direzione tangente (v) ortogonale al gradiente</span>\n<span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">gy</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"n\">gx</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">)</span>  <span class=\"c1\"># v = (2, -1)</span>\n<span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">v</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>  <span class=\"c1\"># normalizzo</span>\n\n<span class=\"c1\"># Calcolo del vettore -gradiente in P0 (direzione di discesa)</span>\n<span class=\"n\">neg_grad</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">gx</span><span class=\"p\">,</span> <span class=\"n\">gy</span><span class=\"p\">])</span>\n<span class=\"n\">neg_grad_unit</span> <span class=\"o\">=</span> <span class=\"n\">neg_grad</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">neg_grad</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Lunghezza delle frecce</span>\n<span class=\"n\">arrow_len</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>\n\n<span class=\"c1\"># Punti finali dei vettori</span>\n<span class=\"n\">P1_tangent</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">arrow_len</span> <span class=\"o\">*</span> <span class=\"n\">v</span>\n<span class=\"n\">P2_grad</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">arrow_len</span> <span class=\"o\">*</span> <span class=\"n\">neg_grad_unit</span>\n\n<span class=\"c1\"># Valore della funzione in P0 per assicurarsi che sia su una curva di livello</span>\n<span class=\"n\">f_P0</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Creazione dei livelli che includono esplicitamente f(P0)</span>\n<span class=\"n\">levels_base</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"p\">(),</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(),</span> <span class=\"mi\">14</span><span class=\"p\">)</span>\n<span class=\"n\">levels_with_P0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">levels_base</span><span class=\"p\">,</span> <span class=\"n\">f_P0</span><span class=\"p\">)</span>\n<span class=\"n\">levels_with_P0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">levels_with_P0</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Rimuoviamo la linea di livello appena sotto f(P0) per maggiore chiarezza</span>\n<span class=\"n\">levels_below_P0</span> <span class=\"o\">=</span> <span class=\"n\">levels_with_P0</span><span class=\"p\">[</span><span class=\"n\">levels_with_P0</span> <span class=\"o\">&lt;</span> <span class=\"n\">f_P0</span><span class=\"p\">]</span>\n<span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">levels_below_P0</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Rimuovi il livello più alto tra quelli sotto f(P0)</span>\n    <span class=\"n\">level_to_remove</span> <span class=\"o\">=</span> <span class=\"n\">levels_below_P0</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">levels_with_P0</span> <span class=\"o\">=</span> <span class=\"n\">levels_with_P0</span><span class=\"p\">[</span><span class=\"n\">levels_with_P0</span> <span class=\"o\">!=</span> <span class=\"n\">level_to_remove</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Creazione della figura con 3 pannelli affiancati</span>\n<span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">axes</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># ---------------------------</span>\n<span class=\"c1\"># Pannello 1: Curva di livello + vettore tangente</span>\n<span class=\"c1\"># ---------------------------</span>\n<span class=\"n\">ax1</span> <span class=\"o\">=</span> <span class=\"n\">axes</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">cont1</span> <span class=\"o\">=</span> <span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"n\">levels_with_P0</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;ko&#39;</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"c1\"># Freccia che parte da P0 e va verso P1_tangent</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">arrow</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> \n          <span class=\"n\">P1_tangent</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P1_tangent</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span>\n          <span class=\"n\">head_width</span><span class=\"o\">=</span><span class=\"mf\">0.08</span><span class=\"p\">,</span> <span class=\"n\">head_length</span><span class=\"o\">=</span><span class=\"mf\">0.08</span><span class=\"p\">,</span> <span class=\"n\">fc</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span><span class=\"p\">,</span> <span class=\"n\">ec</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span><span class=\"p\">,</span> <span class=\"n\">lw</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;Vettore tangente: $\\frac{df}{dv}(P_0)=0$&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_aspect</span><span class=\"p\">(</span><span class=\"s1\">&#39;equal&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ---------------------------</span>\n<span class=\"c1\"># Pannello 2: Curva di livello + vettore -gradiente</span>\n<span class=\"c1\"># ---------------------------</span>\n<span class=\"n\">ax2</span> <span class=\"o\">=</span> <span class=\"n\">axes</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"n\">cont2</span> <span class=\"o\">=</span> <span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"n\">levels_with_P0</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;ko&#39;</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"c1\"># Freccia che parte da P0 e va verso P2_grad</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">arrow</span><span class=\"p\">(</span><span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> \n          <span class=\"n\">P2_grad</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">P2_grad</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">P0</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span>\n          <span class=\"n\">head_width</span><span class=\"o\">=</span><span class=\"mf\">0.08</span><span class=\"p\">,</span> <span class=\"n\">head_length</span><span class=\"o\">=</span><span class=\"mf\">0.08</span><span class=\"p\">,</span> <span class=\"n\">fc</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span><span class=\"p\">,</span> <span class=\"n\">ec</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span><span class=\"p\">,</span> <span class=\"n\">lw</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;Vettore -$\\nabla f$ in $P_0$&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_aspect</span><span class=\"p\">(</span><span class=\"s1\">&#39;equal&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ---------------------------</span>\n<span class=\"c1\"># Pannello 3: Campo vettoriale dell&#39;intero gradiente</span>\n<span class=\"c1\"># ---------------------------</span>\n<span class=\"n\">ax3</span> <span class=\"o\">=</span> <span class=\"n\">axes</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n<span class=\"n\">cont3</span> <span class=\"o\">=</span> <span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">contourf</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;viridis&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">colors</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">,</span> <span class=\"n\">linewidths</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Campo vettoriale (quiver) per il gradiente</span>\n<span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">xq</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[::</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"n\">step</span><span class=\"p\">]</span>\n<span class=\"n\">yq</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"p\">[::</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"n\">step</span><span class=\"p\">]</span>\n<span class=\"n\">gxq</span><span class=\"p\">,</span> <span class=\"n\">gyq</span> <span class=\"o\">=</span> <span class=\"n\">grad_f</span><span class=\"p\">(</span><span class=\"n\">xq</span><span class=\"p\">,</span> <span class=\"n\">yq</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">quiver</span><span class=\"p\">(</span><span class=\"n\">xq</span><span class=\"p\">,</span> <span class=\"n\">yq</span><span class=\"p\">,</span> <span class=\"n\">gxq</span><span class=\"p\">,</span> <span class=\"n\">gyq</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;Campo vettoriale: $\\nabla f$&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax3</span><span class=\"o\">.</span><span class=\"n\">set_aspect</span><span class=\"p\">(</span><span class=\"s1\">&#39;equal&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s1\">&#39;gradient-orthogonal.jpg&#39;</span><span class=\"p\">,</span> \n           <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span> \n           <span class=\"n\">bbox_inches</span><span class=\"o\">=</span><span class=\"s1\">&#39;tight&#39;</span><span class=\"p\">,</span>\n           <span class=\"n\">pad_inches</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span>\n           <span class=\"n\">transparent</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><img src=\"/images/posts/gradient-orthogonal.jpg\" alt=\"Gradient Descent 2\" style=\"display: block; margin-left: auto; margin-right: auto;\"></p>\n<p><em>Figura 1.2: Ortogonalità tra il vettore tangente alla curva di livello e il vettore -gradiente</em></p>\n<h2 id=\"learning-rate\">Learning Rate</h2>\n<p>Nella legge di aggiornamento della discesa del gradiente:</p>\n$$\n\\Theta^{(t+1)} = \\Theta^{(t)} - \\alpha \\nabla \\ell_{\\Theta^{(t)}},\n$$\n<p>il parametro $\\alpha$ gioca un ruolo fondamentale. Questo parametro si chiama <strong>learning rate</strong> (tasso di apprendimento) ed è un <strong>iperparametro</strong>, cioè non viene appreso durante l’ottimizzazione, ma deve essere scelto manualmente (o tramite ricerca automatica).</p>\n<p>Il learning rate è <strong>sempre positivo</strong>: se fosse negativo, infatti, ci si muoverebbe nella direzione opposta a quella desiderata, <strong>massimizzando</strong> invece che minimizzando la funzione di perdita.</p>\n<h3 id=\"effetti-del-learning-rate\">Effetti del learning rate</h3>\n<p>Il valore di $\\alpha$ determina <strong>quanto grande è ogni passo</strong> che l’algoritmo compie nella direzione opposta al gradiente. Non coincide esattamente con la lunghezza del passo (che dipende anche dalla norma del gradiente), ma è <strong>proporzionale ad essa</strong>.</p>\n<p>Infatti, </p>\n$$\n\\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^{(t)}\\| = \\|\\alpha \\nabla f(\\mathbf{x}^{(t)})\\| = \\alpha \\|\\nabla f(\\mathbf{x}^{(t)})\\|.\n$$\n<p>A seconda della sua scelta, il comportamento dell’algoritmo può variare notevolmente:</p>\n<ul>\n<li>Se <strong>$\\alpha$ è troppo piccolo</strong>, l’algoritmo avanza molto lentamente e richiede molte iterazioni per convergere.</li>\n<li>Se <strong>$\\alpha$ è troppo grande</strong>, si rischia di <strong>superare il minimo</strong>, causando <strong>oscillazioni</strong> o addirittura <strong>divergenza</strong>.</li>\n<li>Esiste un valore &ldquo;ottimale&rdquo; $\\alpha^*$ per ogni punto, che minimizzerebbe la funzione lungo la direzione di discesa. Tuttavia, trovare questo valore è difficile perché richiederebbe una soluzione chiusa del problema, che <strong>non è disponibile in generale</strong> per funzioni non lineari.</li>\n</ul>\n<p>Questa situazione è illustrata nella seguente figura:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n\n<span class=\"c1\"># Dati sintetici</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.5</span>\n\n<span class=\"c1\"># Funzione di perdita</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">loss</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">y_pred</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Gradiente</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">gradients</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n    <span class=\"n\">error</span> <span class=\"o\">=</span> <span class=\"n\">y_pred</span> <span class=\"o\">-</span> <span class=\"n\">y</span>\n    <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">error</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>\n    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">error</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span>\n\n<span class=\"c1\"># Allenamento</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"n\">alpha</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span>\n    <span class=\"n\">trajectory</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)]</span>\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">):</span>\n        <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">gradients</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n        <span class=\"n\">w</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">dw</span>\n        <span class=\"n\">b</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">db</span>\n        <span class=\"n\">trajectory</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">trajectory</span>\n\n<span class=\"c1\"># Parametri per i plot</span>\n<span class=\"n\">alphas</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.95</span><span class=\"p\">]</span>\n<span class=\"n\">titles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;Small α&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Optimal α&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Large α&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;#1f77b4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;#2ca02c&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;#d62728&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">trajectories</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">alpha</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">alpha</span> <span class=\"ow\">in</span> <span class=\"n\">alphas</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Curve di livello</span>\n<span class=\"n\">w_range</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">b_range</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">w_range</span><span class=\"p\">,</span> <span class=\"n\">b_range</span><span class=\"p\">)</span>\n<span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">w_range</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">b</span> <span class=\"ow\">in</span> <span class=\"n\">b_range</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Plot</span>\n<span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">axs</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span> <span class=\"n\">sharey</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">ax</span><span class=\"p\">,</span> <span class=\"n\">traj</span><span class=\"p\">,</span> <span class=\"n\">title</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">axs</span><span class=\"p\">,</span> <span class=\"n\">trajectories</span><span class=\"p\">,</span> <span class=\"n\">titles</span><span class=\"p\">,</span> <span class=\"n\">colors</span><span class=\"p\">):</span>\n    <span class=\"n\">contours</span> <span class=\"o\">=</span> <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;cividis&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">w_vals</span><span class=\"p\">,</span> <span class=\"n\">b_vals</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">traj</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">w_vals</span><span class=\"p\">,</span> <span class=\"n\">b_vals</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">title</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;*&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Minimo&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">14</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;w&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s1\">&#39;--&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">()</span>\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;b&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">suptitle</span><span class=\"p\">(</span><span class=\"s1\">&#39;Confronto tra traiettorie di gradient descent con diversi learning rate&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">top</span><span class=\"o\">=</span><span class=\"mf\">0.85</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p align=\"center\">\n  <img src=\"/images/posts/learning-rate-comparison-sgd.png\" alt=\"Confronto tra diversi learning rate\" style=\"display: block; margin: 0 auto;\">\n</p>\n\n<p><em>Figura 2.0: Confronto tra learning rate troppo piccolo, troppo grande e ottimale</em></p>\n<h3 id=\"line-search\">Line Search</h3>\n<p>Una strategia per scegliere dinamicamente il valore di $\\alpha$ è il <strong>line search</strong>: una procedura che, una volta nota la direzione di discesa $-\\nabla \\ell_{\\Theta^{(t)}}$, cerca il valore di $\\alpha$ che <strong>massimizza la diminuzione</strong> della funzione di perdita lungo quella direzione. In pratica, si risolve un piccolo problema di ottimizzazione interno a ogni passo.</p>\n<p>Questa tecnica è più costosa, ma può migliorare la stabilità e l&rsquo;efficacia dell’ottimizzazione.</p>\n<h3 id=\"decadimento-del-learning-rate\">Decadimento del learning rate</h3>\n<p>In alternativa al line search, è comune utilizzare <strong>strategie di decadimento</strong> del learning rate, cioè farlo <strong>diminuire nel tempo</strong> secondo una certa regola:</p>\n<ul>\n<li><strong>Decadimento lineare</strong>:<br />\n  $$ \\alpha^{(t+1)} = \\alpha^{(0)} - \\rho t $$</li>\n<li><strong>Decadimento razionale</strong>:<br />\n  $$ \\alpha^{(t+1)} = \\frac{\\alpha^{(0)}}{1 + \\rho t} $$</li>\n<li><strong>Decadimento esponenziale</strong>:<br />\n  $$ \\alpha^{(t+1)} = \\alpha^{(0)} e^{-\\rho t} $$</li>\n</ul>\n<p>dove $\\rho$ è un parametro di decadimento.</p>\n<p>L’idea alla base è che all’inizio si vogliono fare <strong>passi ampi</strong> per esplorare rapidamente lo spazio dei parametri, mentre verso la fine servono <strong>passi piccoli</strong> per affinare la soluzione e garantire la convergenza ottimale.</p>\n<h3 id=\"considerazioni-pratiche\">Considerazioni pratiche</h3>\n<p>Non esiste una &ldquo;ricetta perfetta&rdquo; per scegliere il learning rate o la sua strategia di aggiornamento. Molto spesso, la scelta viene fatta tramite:</p>\n<ul>\n<li><strong>esperienza pratica</strong></li>\n<li><strong>grid search o random search</strong></li>\n<li><strong>ottimizzazione bayesiana o altri metodi automatici</strong></li>\n</ul>\n<p>Alcuni algoritmi, come <strong>Adam</strong>, includono meccanismi per <strong>adattare automaticamente il learning rate</strong> per ogni parametro, rendendo l&rsquo;ottimizzazione più robusta e spesso più veloce.</p>\n<h2 id=\"batch-mini-batch-e-stochastic-gradient-descent\">Batch, Mini-Batch e Stochastic Gradient Descent</h2>\n<p>La discesa del gradiente nella sua forma classica (chiamata <strong>Batch Gradient Descent</strong>) utilizza l&rsquo;intero dataset per calcolare il gradiente della funzione di perdita. Questo approccio fornisce una direzione precisa, ma può essere computazionalmente costoso, specialmente su dataset di grandi dimensioni.</p>\n<p>Per ovviare a questo problema, sono state sviluppate varianti più efficienti:</p>\n<h3 id=\"1-batch-gradient-descent\">1. <strong>Batch Gradient Descent</strong></h3>\n<p>In questo approccio, ad ogni iterazione viene utilizzato <strong>l&rsquo;intero dataset</strong> per calcolare il gradiente:</p>\n$$\n\\Theta^{(t+1)} \\leftarrow \\Theta^{(t)} - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell^{(i)}(\\Theta^{(t)}).\n$$\n<p>Si calcola quindi il gradiente della funzione di perdita per ogni esempio del dataset, e poi si effettua il passo di discesa con la media dei gradiente. Quindi un epoca in questo caso sonsiste in un solo passo di discesa.</p>\n<ul>\n<li>Vantaggi: direzione precisa della discesa.</li>\n<li>Svantaggi: lento per dataset molto grandi, non aggiornabile in tempo reale.</li>\n</ul>\n<h3 id=\"2-stochastic-gradient-descent-sgd\">2. <strong>Stochastic Gradient Descent (SGD)</strong></h3>\n<p>In questo caso, l&rsquo;aggiornamento dei parametri viene effettuato <strong>per ogni singolo esempio</strong> del dataset:</p>\n$$\n\\Theta^{(t+1)} \\leftarrow \\Theta^{(t)} - \\alpha \\cdot \\nabla \\ell^{(i)}(\\Theta^{(t)}).\n$$\n<p>Quindi un epoca in questo caso consiste in $n$ passi di discesa (iterazioni). Questo perché si calcola il gradiente per ogni esempio del dataset, quindi si effettua $n$ passi di discesa.</p>\n<ul>\n<li>Vantaggi: aggiornamenti molto rapidi, buona approssimazione della direzione di discesa.</li>\n<li>Svantaggi: il rumore introdotto da ogni esempio può causare oscillazioni e rendere difficile la convergenza stabile.</li>\n</ul>\n<h3 id=\"3-mini-batch-gradient-descent\">3. <strong>Mini-Batch Gradient Descent</strong></h3>\n<p>Rappresenta un compromesso tra le due precedenti. Si utilizza un <strong>sottoinsieme (mini-batch)</strong> di $m$ campioni (con $m \\ll n$) per calcolare il gradiente:</p>\n$$\n\\Theta^{(t+1)} \\leftarrow \\Theta^{(t)} - \\alpha \\cdot \\frac{1}{m} \\sum_{j=1}^{m} \\nabla \\ell^{(j)}(\\Theta^{(t)}).\n$$\n<p>Qui calcoliamo ogni volta il gradiente su $m$ esempi, quindi un epoca in questo caso consiste in $\\frac{n}{m}$ passi di discesa (iterazioni).</p>\n<ul>\n<li>Vantaggi: bilancia precisione e velocità, sfrutta l&rsquo;efficienza computazionale del calcolo vettoriale su GPU.</li>\n<li>È la scelta più comune nelle reti neurali moderne.</li>\n</ul>\n<h4 id=\"considerazioni-sulluso-dei-mini-batch\">Considerazioni sull&rsquo;uso dei Mini-Batch</h4>\n<ul>\n<li>\n<p>Ogni mini-batch può essere elaborato in <strong>parallelo</strong>, caratteristica che si sposa bene con l&rsquo;aumento di disponibilità e potenza delle <strong>architetture parallele</strong> come le <strong>GPGPU</strong> (General Purpose Graphic Processing Unit), sempre più usate nei compiti di deep learning. In questo caso, la dimensione massima del batch è limitata dall’hardware e dalla rappresentazione in memoria dei dati.</p>\n</li>\n<li>\n<p>Mini-batch di <strong>piccole dimensioni</strong> possono avere un <strong>effetto regolarizzante</strong>, introducendo <strong>varianza nella stima del gradiente</strong>. Questo può impedire all’algoritmo di raggiungere il minimo esatto, contribuendo così a <strong>ridurre l’overfitting</strong>. Tuttavia, batch troppo piccoli (nel limite, apprendimento online con un solo dato per volta) introducono <strong>una varianza troppo elevata</strong>, richiedendo l’uso di un <strong>learning rate piccolo</strong> (meglio se <strong>decrescente</strong>) per mantenere la stabilità dell’algoritmo.</p>\n</li>\n</ul>\n<h3 id=\"confronto-grafico\">Confronto Grafico</h3>\n<p>Il seguente esempio Python illustra la differenza tra Batch, Mini-Batch e Stochastic Gradient Descent, evidenziando le traiettorie nel piano dei parametri:</p>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n\n<span class=\"c1\"># Dati sintetici</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.5</span>\n\n<span class=\"c1\"># Funzione di perdita</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">loss</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">y_pred</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Gradiente</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">gradients</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n    <span class=\"n\">error</span> <span class=\"o\">=</span> <span class=\"n\">y_pred</span> <span class=\"o\">-</span> <span class=\"n\">y</span>\n    <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">error</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>\n    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">error</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span>\n\n<span class=\"c1\"># Addestramento con step uniformi</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"n\">method</span><span class=\"o\">=</span><span class=\"s1\">&#39;batch&#39;</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span>\n    <span class=\"n\">trajectory</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)]</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">method</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;batch&#39;</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">):</span>\n            <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">gradients</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n            <span class=\"n\">w</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">dw</span>\n            <span class=\"n\">b</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">db</span>\n            <span class=\"n\">trajectory</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n\n    <span class=\"k\">elif</span> <span class=\"n\">method</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;sgd&#39;</span><span class=\"p\">:</span>\n        <span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">permutation</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">))</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">):</span>\n            <span class=\"n\">idx</span> <span class=\"o\">=</span> <span class=\"n\">indices</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)]</span>\n            <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">gradients</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">:</span><span class=\"n\">idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">:</span><span class=\"n\">idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n            <span class=\"n\">w</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">dw</span>\n            <span class=\"n\">b</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">db</span>\n            <span class=\"n\">trajectory</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n\n    <span class=\"k\">elif</span> <span class=\"n\">method</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;minibatch&#39;</span><span class=\"p\">:</span>\n        <span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">steps</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">):</span>\n            <span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n            <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">indices</span><span class=\"p\">]</span>\n            <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">indices</span><span class=\"p\">]</span>\n            <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">gradients</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X_batch</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span>\n            <span class=\"n\">w</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">dw</span>\n            <span class=\"n\">b</span> <span class=\"o\">-=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">db</span>\n            <span class=\"n\">trajectory</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">trajectory</span>\n\n<span class=\"c1\"># Tracciamento traiettorie (30 step)</span>\n<span class=\"n\">traj_batch</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">method</span><span class=\"o\">=</span><span class=\"s1\">&#39;batch&#39;</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">)</span>\n<span class=\"n\">traj_sgd</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">method</span><span class=\"o\">=</span><span class=\"s1\">&#39;sgd&#39;</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">)</span>\n<span class=\"n\">traj_minibatch</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">method</span><span class=\"o\">=</span><span class=\"s1\">&#39;minibatch&#39;</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Curve di livello</span>\n<span class=\"n\">w_range</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">b_range</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">w_range</span><span class=\"p\">,</span> <span class=\"n\">b_range</span><span class=\"p\">)</span>\n<span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">w_range</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">b</span> <span class=\"ow\">in</span> <span class=\"n\">b_range</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Livelli coerenti e ordinati</span>\n<span class=\"n\">min_loss</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n<span class=\"n\">lower_limit</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">min_loss</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">all_levels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">lower_limit</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">),</span> <span class=\"mi\">50</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">))</span>\n<span class=\"n\">contours</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"n\">all_levels</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;cividis&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">clabel</span><span class=\"p\">(</span><span class=\"n\">contours</span><span class=\"p\">,</span> <span class=\"n\">inline</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">fmt</span><span class=\"o\">=</span><span class=\"s1\">&#39;</span><span class=\"si\">%.2f</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Colori desaturati</span>\n<span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;#3e8250&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;#567991&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;#b05541&#39;</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Traiettorie</span>\n<span class=\"k\">for</span> <span class=\"n\">traj</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">([</span><span class=\"n\">traj_batch</span><span class=\"p\">,</span> <span class=\"n\">traj_sgd</span><span class=\"p\">,</span> <span class=\"n\">traj_minibatch</span><span class=\"p\">],</span>\n                              <span class=\"p\">[</span><span class=\"s1\">&#39;Batch GD&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;SGD&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Mini-Batch GD&#39;</span><span class=\"p\">],</span>\n                              <span class=\"n\">colors</span><span class=\"p\">):</span>\n    <span class=\"n\">w_vals</span><span class=\"p\">,</span> <span class=\"n\">b_vals</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">traj</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">w_vals</span><span class=\"p\">,</span> <span class=\"n\">b_vals</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Minimo globale (approssimato analiticamente: w=3, b=2)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;*&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Minimo&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Zoom centrato ma visibile anche l&#39;origine</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlim</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">4.0</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylim</span><span class=\"p\">(</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">3.5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Stile</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;w&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;b&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Curve di livello della funzione di perdita con traiettorie&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">14</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s1\">&#39;--&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><img src=\"/images/posts/gradient-methods.png\" alt=\"Gradient Descent Methods\"></p>\n<p><em>Figura 1.3: Confronto visivo tra le traiettorie di Batch Gradient Descent, SGD e Mini-Batch Gradient Descent</em></p>\n<h3 id=\"conclusione\">Conclusione</h3>\n<p>Le varianti del Gradient Descent offrono una gamma di compromessi tra accuratezza, velocità e stabilità. In pratica:</p>\n<ul>\n<li><strong>Batch GD</strong> è utile per modelli piccoli e dataset contenuti.</li>\n<li><strong>SGD</strong> è adatto a scenari online o dataset giganteschi.</li>\n<li><strong>Mini-Batch GD</strong> è lo standard nell&rsquo;apprendimento profondo per la sua efficienza.</li>\n</ul>\n<p>Le tecniche moderne includono anche ottimizzatori avanzati (come Adam, RMSProp, Adagrad), che combinano il gradiente con meccanismi adattivi per un miglior controllo della discesa, che tratteremo proprio nella sezione successiva.</p>\n<h2 id=\"discesa-del-gradiente-con-momentum\">Discesa del Gradiente con Momentum</h2>\n<p>Uno dei principali limiti della discesa del gradiente standard è la sua <strong>lentezza di convergenza</strong> in presenza di <strong>vallate strette e profonde</strong> nella funzione di perdita, oppure in direzioni con <strong>curvature molto diverse</strong> (ad esempio funzioni “a sella” o “a banana”). In questi casi, l’algoritmo può oscillare lungo le direzioni di maggiore curvatura, rallentando notevolmente il percorso verso il minimo.</p>\n<p>Per mitigare questo problema, viene introdotto il concetto di <strong>momentum</strong>, ispirato alla fisica newtoniana: invece di aggiornare i parametri unicamente in base al gradiente attuale e al learning rate, si tiene conto anche della <strong>direzione e velocità del movimento passato</strong>, accumulando “inerzia” lungo le direzioni coerenti.</p>\n<h3 id=\"formula-dellaggiornamento-con-momentum\">Formula dell&rsquo;Aggiornamento con Momentum</h3>\n<p>L’algoritmo introduce una variabile ausiliaria $\\mathbf{v}^{(t)}$ che rappresenta la “velocità” del sistema, aggiornata iterativamente secondo:</p>\n$$\n\\begin{aligned}\n\\mathbf{v}^{(t+1)} &= \\lambda \\cdot \\mathbf{v}^{(t)} - \\alpha \\cdot \\nabla \\ell_{\\Theta^{(t)}}, \\\\\n\\Theta^{(t+1)} &= \\Theta^{(t)} + \\mathbf{v}^{(t+1)}.\n\\end{aligned}\n$$\n<p>dove:</p>\n<ul>\n<li>$\\alpha$ è il <strong>learning rate</strong>,</li>\n<li>$\\lambda \\in [0,1)$ è il <strong>coefficiente di momentum</strong>, che controlla il peso del termine di velocità accumulato (valori tipici: $\\lambda = 0.9$),</li>\n<li>$\\nabla \\ell_{\\Theta^{(t)}}$ è il gradiente della funzione di perdita all’iterazione $t$,</li>\n<li>$\\mathbf{v}^{(t)}$ è la velocità accumulata al passo precedente. Al tempo $t=0$, $\\mathbf{v}^{(0)} = 0$.</li>\n</ul>\n<h3 id=\"interpretazione-intuitiva\">Interpretazione Intuitiva</h3>\n<ul>\n<li>Quando i gradienti puntano nella <strong>stessa direzione</strong> in iterazioni successive, il termine $\\lambda \\cdot \\mathbf{v}^{(t)}$ <strong>rafforza</strong> la velocità in quella direzione, rendendo l’avanzamento più rapido.</li>\n<li>Quando la direzione del gradiente <strong>cambia spesso</strong> (es. oscillazioni), il momentum <strong>smorza le variazioni</strong>, stabilizzando l’andamento e migliorando la convergenza.</li>\n</ul>\n<details class=\"code-container\">\n<summary>Code</summary>\n<div class=\"code-wrapper\">\n<button class=\"copy-button\" onclick=\"\n                const code = this.parentElement.querySelector('pre');\n                if (code) {\n                    navigator.clipboard.writeText(code.innerText);\n                    this.textContent = 'Copied!';\n                    setTimeout(() => this.textContent = 'Copy', 2000);\n                }\n            \">Copy</button>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n\n<span class=\"c1\"># Funzione di costo</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">10</span> <span class=\"o\">*</span> <span class=\"n\">y</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Gradiente della funzione</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">grad_f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">10</span> <span class=\"o\">*</span> <span class=\"n\">y</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># GD semplice</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">gradient_descent</span><span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"p\">):</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">steps</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">start</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"p\">):</span>\n        <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">grad_f</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"n\">grad</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span>\n\n<span class=\"c1\"># GD con momentum</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">gradient_descent_momentum</span><span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">):</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">steps</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">start</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"p\">):</span>\n        <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">grad_f</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">gamma</span> <span class=\"o\">*</span> <span class=\"n\">v</span> <span class=\"o\">+</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"n\">grad</span>\n        <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">v</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span>\n\n<span class=\"c1\"># Parametri</span>\n<span class=\"n\">start</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">4.0</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">])</span>\n<span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"mi\">80</span>\n<span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">gamma</span> <span class=\"o\">=</span> <span class=\"mf\">0.75</span>\n<span class=\"n\">optimum</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">])</span>  <span class=\"c1\"># punto di minimo</span>\n\n<span class=\"c1\"># Percorsi</span>\n<span class=\"n\">path_gd</span> <span class=\"o\">=</span> <span class=\"n\">gradient_descent</span><span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"p\">)</span>\n<span class=\"n\">path_mom</span> <span class=\"o\">=</span> <span class=\"n\">gradient_descent_momentum</span><span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">steps</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Contorno della funzione</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">400</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">400</span><span class=\"p\">))</span>\n<span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n<span class=\"n\">levels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">logspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Setup figura allungata</span>\n<span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">axs</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span> <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">150</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">ax</span><span class=\"p\">,</span> <span class=\"n\">path</span><span class=\"p\">,</span> <span class=\"n\">title</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span>\n    <span class=\"n\">axs</span><span class=\"p\">,</span>\n    <span class=\"p\">[</span><span class=\"n\">path_gd</span><span class=\"p\">,</span> <span class=\"n\">path_mom</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">&#39;(a) Without momentum&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;(b) With momentum&#39;</span><span class=\"p\">]</span>\n<span class=\"p\">):</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">levels</span><span class=\"o\">=</span><span class=\"n\">levels</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;Greens_r&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">path</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;darkorange&#39;</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Starting point e Solution</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">annotate</span><span class=\"p\">(</span><span class=\"s1\">&#39;Starting Point&#39;</span><span class=\"p\">,</span> <span class=\"n\">xy</span><span class=\"o\">=</span><span class=\"n\">path</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">xytext</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">4.8</span><span class=\"p\">,</span> <span class=\"mf\">2.3</span><span class=\"p\">),</span> <span class=\"n\">arrowprops</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">arrowstyle</span><span class=\"o\">=</span><span class=\"s1\">&#39;-&gt;&#39;</span><span class=\"p\">))</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">annotate</span><span class=\"p\">(</span><span class=\"s1\">&#39;Solution&#39;</span><span class=\"p\">,</span> <span class=\"n\">xy</span><span class=\"o\">=</span><span class=\"n\">path</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">xytext</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">2.5</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.7</span><span class=\"p\">),</span> <span class=\"n\">arrowprops</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">arrowstyle</span><span class=\"o\">=</span><span class=\"s1\">&#39;-&gt;&#39;</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Ottimo</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">optimum</span><span class=\"p\">,</span> <span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;steelblue&#39;</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">annotate</span><span class=\"p\">(</span><span class=\"s1\">&#39;Optimum&#39;</span><span class=\"p\">,</span> <span class=\"n\">xy</span><span class=\"o\">=</span><span class=\"n\">optimum</span><span class=\"p\">,</span> <span class=\"n\">xytext</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">),</span> <span class=\"n\">textcoords</span><span class=\"o\">=</span><span class=\"s1\">&#39;data&#39;</span><span class=\"p\">,</span> <span class=\"n\">ha</span><span class=\"o\">=</span><span class=\"s1\">&#39;left&#39;</span><span class=\"p\">,</span>\n                <span class=\"n\">arrowprops</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">arrowstyle</span><span class=\"o\">=</span><span class=\"s1\">&#39;-&gt;&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;black&#39;</span><span class=\"p\">))</span>\n\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">(</span><span class=\"s1\">&#39;off&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div>\n</div>\n</details>\n\n<p><img src=\"/images/posts/momentum.png\" alt=\"Momentum Gradient Descent\"></p>\n<p><em>Figura 1.3: La discesa del gradiente con momentum permette una traiettoria più fluida e veloce verso il minimo, evitando oscillazioni e rallentamenti dovuti a curvature diverse nelle direzioni principali.</em></p>\n<h3 id=\"derivazione-della-forma-chiusa-per-gd-con-momentum\">Derivazione della forma chiusa per GD con Momentum</h3>\n<p>L’obiettivo è derivare una <strong>forma chiusa</strong> (non ricorsiva) dell’aggiornamento dei parametri al tempo $t+1$, in funzione di tutti i gradienti calcolati fino a quel momento. In questo modo possiamo analizzare in modo più chiaro <strong>l’effetto cumulativo del momentum</strong>, che combina i gradienti passati pesandoli secondo una <strong>decadimento geometrico</strong> controllato dall&rsquo; iperparametro $\\lambda$. Questo permette di evidenziare come il metodo favorisca le direzioni persistenti nel tempo e smorzi le oscillazioni dovute a cambiamenti locali nel paesaggio della funzione di perdita.</p>\n<p>Partiamo dalle <strong>equazioni ricorsive</strong> della discesa del gradiente con momentum:</p>\n$$\n\\begin{cases}\n\\mathbf{v}^{(t+1)} = \\lambda\\,\\mathbf{v}^{(t)} - \\alpha \\,\\nabla \\ell\\bigl(\\Theta^{(t)}\\bigr),\\\\\n\\Theta^{(t+1)} = \\Theta^{(t)} + \\mathbf{v}^{(t+1)}.\n\\end{cases}\n$$\n<p>Vogliamo <strong>unrollare</strong> queste relazioni fino all’iterazione iniziale $\\Theta^{(0)}$.</p>\n<h4 id=\"1-espressione-ricorsiva-di-math_inline_154\">1. Espressione ricorsiva di $\\mathbf{v}^{(t+1)}$</h4>\n<p>Applichiamo più volte la definizione di $\\mathbf{v}$:</p>\n$$\n\\begin{aligned}\n\\mathbf{v}^{(1)} &= \\lambda\\,\\mathbf{v}^{(0)} - \\alpha\\,\\nabla \\ell(\\Theta^{(0)}),\\\\ \n\\mathbf{v}^{(2)} &= \\lambda\\,\\mathbf{v}^{(1)} - \\alpha\\,\\nabla \\ell(\\Theta^{(1)})\\\\\n&= \\lambda \\bigl(\\lambda\\,\\mathbf{v}^{(0)} - \\alpha\\,\\nabla \\ell(\\Theta^{(0)})\\bigr)\n  - \\alpha\\,\\nabla \\ell(\\Theta^{(1)})\\\\\n&= \\lambda^2 \\mathbf{v}^{(0)}\n  - \\alpha \\bigl(\\lambda\\,\\nabla \\ell(\\Theta^{(0)}) + \\nabla \\ell(\\Theta^{(1)})\\bigr),\n\\end{aligned}\n$$\n<p>e in generale, per $0 \\le i \\le t$:</p>\n$$\n\\mathbf{v}^{(t+1)}\n= \\lambda^{\\,t+1}\\mathbf{v}^{(0)}\n- \\alpha \\sum_{i=0}^{t} \\lambda^{\\,t-i}\\,\\nabla \\ell\\bigl(\\Theta^{(i)}\\bigr).\n$$\n<p>Spesso si assume $\\mathbf{v}^{(0)}=\\mathbf{0}$, da cui:</p>\n$$\n\\mathbf{v}^{(t+1)}\n= -\\,\\alpha \\sum_{i=0}^{t} \\lambda^{\\,t-i}\\,\\nabla \\ell\\bigl(\\Theta^{(i)}\\bigr).\n$$\n<h4 id=\"2-unrolling-di-math_inline_158\">2. Unrolling di $\\Theta^{(t+1)}$</h4>\n<p>Ora inseriamo $\\mathbf{v}^{(t+1)}$ nell’aggiornamento di $\\Theta$:</p>\n$$\n\\begin{aligned}\n\\Theta^{(t+1)}\n&= \\Theta^{(t)} + \\mathbf{v}^{(t+1)}\\\\\n&= \\Theta^{(t)} \n  - \\alpha \\sum_{i=0}^{t} \\lambda^{\\,t-i}\\,\\nabla \\ell\\bigl(\\Theta^{(i)}\\bigr).\n\\end{aligned}\n$$\n<p>Ripetendo ricorsivamente l’aggiornamento su $\\Theta^{(t)}, \\Theta^{(t-1)}, \\dots, \\Theta^{(0)}$, otteniamo:</p>\n$$\n\\begin{aligned}\n\\Theta^{(t+1)}\n&= \\Theta^{(0)}\n  - \\alpha \\sum_{k=0}^{t} \\sum_{i=0}^{k} \\lambda^{\\,k-i}\\,\\nabla \\ell\\bigl(\\Theta^{(i)}\\bigr) \\\\\n&= \\Theta^{(0)}\n  - \\alpha \\sum_{i=0}^{t} \\Bigl(\\sum_{k=i}^{t} \\lambda^{\\,k-i}\\Bigr)\\,\\nabla \\ell\\bigl(\\Theta^{(i)}\\bigr).\n\\end{aligned}\n$$\n<h4 id=\"3-calcolo-della-somma-geometrica-interna\">3. Calcolo della somma geometrica interna</h4>\n<p>La somma interna $\\displaystyle\\sum_{k=i}^{t} \\lambda^{\\,k-i}$ è una <strong>serie geometrica</strong> di ragione $\\lambda$ e $t-i+1$ termini:</p>\n$$\n\\sum_{k=i}^{t} \\lambda^{\\,k-i}\n= \\sum_{h=0}^{t-i} \\lambda^{\\,h}\n= \\frac{1 - \\lambda^{\\,t-i+1}}{1 - \\lambda}.\n$$\n<h4 id=\"4-forma-finale\">4. Forma finale</h4>\n<p>Sostituendo nella formula di $\\Theta^{(t+1)}$, otteniamo la forma chiusa:</p>\n$$\n\\boxed{\n\\Theta^{(t+1)} \n= \\Theta^{(0)} \n- \\alpha \\sum_{i=0}^{t} \n      \\underbrace{\\frac{1 - \\lambda^{\\,t-i+1}}{1 - \\lambda}}_{\\Gamma_i^t}\n  \\,\\nabla \\ell\\bigl(\\Theta^{(i)}\\bigr).\n}\n$$\n<p>Qui $\\displaystyle\\Gamma_i^t = \\frac{1 - \\lambda^{\\,t+1-i}}{1 - \\lambda}$ è il <strong>fattore di accumulo</strong> che deriva dalla somma geometrica.</p>\n<p>Questa espansione chiarisce perché il momentum aiuta a <strong>smussare oscillazioni</strong> e a <strong>favorire direzioni stabili</strong>, facilitando la convergenza più rapida verso un minimo.</p>\n<h3 id=\"confronto-con-gradient-descent-standard\">Confronto con Gradient Descent Standard</h3>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Pro</th>\n<th>Contro</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Gradient Descent</strong></td>\n<td>Preciso, semplice da implementare</td>\n<td>Lento in presenza di vallate strette</td>\n</tr>\n<tr>\n<td><strong>Momentum Gradient Descent</strong></td>\n<td>Convergenza più rapida e fluida</td>\n<td>Richiede una variabile aggiuntiva ($\\mathbf{v}$) e tuning di $\\lambda$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"osservazioni-finali\">Osservazioni Finali</h3>\n<ul>\n<li>Il termine $\\lambda$ controlla <strong>quanto &ldquo;lontano&rdquo; nel passato</strong> guardiamo per l’accumulo di velocità. Valori troppo alti ($\\lambda \\approx 0.99$) possono causare overshooting, mentre valori bassi rendono il metodo simile al GD standard.</li>\n<li>Il metodo con momentum è la base di molte varianti moderne dell&rsquo;ottimizzazione, tra cui <strong>Nesterov Accelerated Gradient (NAG)</strong> e <strong>Adam</strong>.</li>\n</ul>\n<p>In sintesi, il momentum fornisce un <strong>bilanciamento intelligente tra memoria del passato e reattività al presente</strong>, migliorando l’efficienza di convergenza e la stabilità numerica della discesa del gradiente.</p>\n<h2 id=\"limiti-superiori-asintotici-convergenza-di-gd-e-sgd\">Limiti Superiori Asintotici: Convergenza di GD e SGD</h2>\n<p>Per problemi <strong>convessi</strong> (dove la funzione di loss ha un solo minimo globale), possiamo analizzare quanto velocemente i metodi di discesa del gradiente si avvicinano al minimo ottimo.</p>\n<p>Assumiamo di voler trovare un punto $\\Theta$ tale che la <strong>loss</strong> ottenuta sia entro una precisione $\\rho > 0$ dall</p>\n<p>dove:\n- $\\ell(f_\\Theta)$ è la loss del modello corrente,\n- $\\ell(f^*)$ è la loss ottima (raggiunta in teoria dal miglior modello),\n- $\\rho$ è l&rsquo;accuratezza desiderata.</p>\n<h3 id=\"notazione\">📌 Notazione</h3>\n<ul>\n<li>$n$ = numero di esempi nel dataset di training  </li>\n<li>$d$ = numero di parametri (dimensione di $\\Theta$)  </li>\n<li>$\\kappa$ = <strong>numero di condizionamento</strong>, ovvero $\\kappa = L/\\mu$, dove:</li>\n<li>$L$ è la <strong>costante di Lipschitz</strong> del gradiente: $\\|\\nabla \\ell(\\Theta_1) - \\nabla \\ell(\\Theta_2)\\| \\le L \\|\\Theta_1 - \\Theta_2\\|$</li>\n<li>$\\mu$ è la <strong>costante di forte convessità</strong>: $\\ell(\\Theta) \\ge \\ell(f^*) + \\frac{\\mu}{2}\\|\\Theta - \\Theta^*\\|^2$</li>\n<li>$\\nu$ = varianza del rumore stocastico nel gradiente, rilevante per SGD</li>\n</ul>\n<h3 id=\"costo-computazionale-per-iterazione\">⚙️ Costo Computazionale per Iterazione</h3>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Costo per iterazione</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>GD</strong></td>\n<td>$O(n\\,d)$</td>\n</tr>\n<tr>\n<td><strong>SGD</strong></td>\n<td>$O(d)$</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><strong>GD</strong>: calcola il gradiente <strong>esatto</strong>, sommando i contributi di tutti i $n$ esempi.</li>\n<li><strong>SGD</strong>: usa un <strong>solo</strong> esempio (o minibatch), abbattendo il costo computazionale per iterazione.</li>\n</ul>\n<h3 id=\"numero-di-iterazioni-per-raggiungere-precisione-math_inline_189\">📈 Numero di Iterazioni per Raggiungere Precisione $\\rho$</h3>\n<table>\n<thead>\n<tr>\n<th>Metodo</th>\n<th>Iterazioni necessarie</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>GD</strong></td>\n<td>$O\\left(\\kappa \\log \\frac{1}{\\rho}\\right)$</td>\n</tr>\n<tr>\n<td><strong>SGD</strong></td>\n<td>$O\\left(\\frac{\\nu \\kappa^2}{\\rho}\\right) + o\\left(\\frac{1}{\\rho}\\right)$</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"convergenza-di-gd-discesa-del-gradiente\">✳️ Convergenza di GD (Discesa del Gradiente)</h4>\n<p>Se $\\ell$ è fortemente convessa e ha gradiente Lipschitz, allora:</p>\n$$\n\\ell(f_{\\Theta^{(t)}}) - \\ell(f^*) \\le \\left(1 - \\frac{1}{\\kappa} \\right)^t \\cdot (\\ell(f_{\\Theta^{(0)}}) - \\ell(f^*)),\n$$\n<p>che converge <strong>esponenzialmente</strong> verso $\\ell(f^*)$. Invertendo questa relazione, bastano:</p>\n$$\nt = O\\left(\\kappa \\log \\frac{1}{\\rho} \\right)\n$$\n<p>iterazioni per raggiungere precisione $\\rho$.</p>\n<h4 id=\"convergenza-di-sgd\">✳️ Convergenza di SGD</h4>\n<p>Nel caso stocastico, ogni passo è più &ldquo;rumoroso&rdquo;, quindi la convergenza è più lenta. Si può dimostrare che:</p>\n$$\n\\mathbb{E}[\\ell(f_{\\Theta^{(t)}})] - \\ell(f^*) \\le O\\left( \\frac{\\nu \\kappa^2}{t} \\right),\n$$\n<p>dove $\\nu$ riflette la varianza del gradiente stocastico. Per ottenere precisione $\\rho$, servono:</p>\n$$\nt = O\\left( \\frac{\\nu \\kappa^2}{\\rho} \\right).\n$$\n<p>Quindi la <strong>convergenza è sublineare</strong>: più lenta, ma il costo per iterazione è molto inferiore.</p>\n<h3 id=\"confronto-finale\">✅ Confronto Finale</h3>\n<ul>\n<li><strong>GD</strong>: più costoso per iterazione, ma converge <strong>molto più velocemente</strong> (esponenzialmente in $\\rho$).</li>\n<li><strong>SGD</strong>: estremamente efficiente per iterazione, ma servono più passi per avvicinarsi all&rsquo;ottimo.</li>\n</ul>\n<p>In pratica, <strong>SGD</strong> è preferito nei grandi dataset (dove $n$ è molto grande), mentre <strong>GD</strong> è ideale per problemi più piccoli o ben condizionati.</p>\n<h2 id=\"criteri-di-arresto-per-la-discesa-del-gradiente\">Criteri di arresto per la discesa del gradiente</h2>\n<h2 id=\"conclusioni\">Conclusioni</h2>\n<p>La discesa del gradiente si conferma come uno degli algoritmi fondamentali nell&rsquo;ottimizzazione di modelli matematici e machine learning. Attraverso un&rsquo;analisi multidimensionale, emergono chiaramente diversi aspetti cruciali:</p>\n<ol>\n<li>\n<p><strong>Natura Iterativa e Sfide</strong>:</p>\n</li>\n<li>\n<p>La dipendenza dalle condizioni iniziali e la presenza di minimi locali in funzioni non convesse sottolineano l&rsquo;importanza di strategie di inizializzazione robuste.</p>\n</li>\n<li>\n<p>I punti di sella, sebbene teoricamente problematici, risultano meno critici in pratica grazie all&rsquo;instabilità numerica e all&rsquo;alta dimensionalità degli spazi di parametri.</p>\n</li>\n<li>\n<p><strong>Differenziabilità e Continuità</strong>:</p>\n</li>\n<li>\n<p>La differenziabilità della funzione obiettivo è un requisito essenziale per il calcolo del gradiente, con implicazioni pratiche nella scelta delle funzioni di attivazione e di loss.</p>\n</li>\n<li>\n<p>Casi patologici come funzioni con derivate parziali discontinue evidenziano la necessità di verifiche analitiche preliminari.</p>\n</li>\n<li>\n<p><strong>Aspetti Implementativi</strong>:</p>\n</li>\n<li>\n<p>Il <em>learning rate</em> si rivela un iperparametro critico, con strategie come il decadimento dinamico e il <em>line search</em> che mitigano rischi di divergenza o convergenza lenta.</p>\n</li>\n<li>\n<p>L&rsquo;eterogeneità delle curvature del terreno di ottimizzazione motiva l&rsquo;adozione di tecniche avanzate come il momentum, che accelerano la convergenza smorzando le oscillazioni.</p>\n</li>\n<li>\n<p><strong>Trade-off Computazionali</strong>:</p>\n</li>\n<li>\n<p>Il confronto tra Batch GD, SGD e Mini-Batch GD delinea un chiaro compromesso tra precisione, costo computazionale e rumore stocastico, con la variante Mini-Batch che rappresenta spesso il miglior bilanciamento per applicazioni su larga scala.</p>\n</li>\n<li>\n<p>I limiti superiori asintotici rivelano come SGD sia preferibile in scenari <em>big data</em> nonostante una convergenza teorica più lenta, grazie alla scalabilità indipendente dalla dimensione del dataset.</p>\n</li>\n<li>\n<p><strong>Prospettive Moderne</strong>:</p>\n</li>\n<li>\n<p>Estensioni come Nesterov Momentum e ottimizzatori adattativi (es. Adam) ereditano i principi della discesa del gradiente classica, integrando meccanismi di auto-regolazione per gestire paesaggi di loss complessi.</p>\n</li>\n</ol>\n<p>In sintesi, la discesa del gradiente non è solo un algoritmo ma un <em>framework concettuale</em> che unisce rigore matematico e pragmatismo computazionale. La sua efficacia deriva dall&rsquo;armonia tra teoria dell&rsquo;ottimizzazione, intuizione geometrica e adattamento alle sfide ingegneristiche, rendendolo uno strumento indispensabile nell&rsquo;era dei modelli ad alta dimensionalità.</p>"
}