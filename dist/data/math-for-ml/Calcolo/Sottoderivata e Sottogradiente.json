{
  "title": "Sottoderivata e Sottogradiente",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>La <strong>sottoderivata</strong> è un concetto matematico che estende la nozione di derivata a funzioni che non sono differenziabili in tutti i punti del loro dominio. Questo strumento è particolarmente utile quando si lavora con funzioni come la <strong>norma L1</strong>, che non è derivabile in $x = 0$. La sottoderivata permette di definire un insieme di &ldquo;derivate generalizzate&rdquo; in punti in cui la funzione non è regolare.</p>\n<h2 id=\"definizione-di-sottoderivata\">Definizione di Sottoderivata</h2>\n<p>Sia $f: \\mathbb{R} \\to \\mathbb{R}$ una funzione convessa. La <strong>sottoderivata</strong> di $f$ in un punto $x_0$ è definita come l&rsquo;insieme di tutti i valori $g$ tali che:</p>\n$$\nf(x) \\geq f(x_0) + g(x - x_0) \\quad \\text{per ogni } x \\in \\mathbb{R}.\n$$\n<p>In altre parole, la sottoderivata è l&rsquo;insieme di tutte le pendenze delle rette che toccano il grafico di $f$ in $x_0$ senza attraversarlo. Questo insieme è chiamato <strong>sottodifferenziale</strong> di $f$ in $x_0$ e si indica con $\\partial f(x_0)$.</p>\n<h2 id=\"esempi-di-sottoderivata\">Esempi di Sottoderivata</h2>\n<h3 id=\"1-funzione-valore-assoluto-norma-l1\">1. Funzione Valore Assoluto (Norma L1)</h3>\n<p>Consideriamo la funzione valore assoluto $f(x) = |x|$, che è un caso particolare della norma L1. Questa funzione non è derivabile in $x = 0$, ma possiamo calcolare il suo sottodifferenziale.</p>\n<ul>\n<li>Per $x > 0$, la derivata di $f(x)$ è $f'(x) = 1$.</li>\n<li>Per $x < 0$, la derivata di $f(x)$ è $f'(x) = -1$.</li>\n<li>In $x = 0$, la funzione non è derivabile, ma il sottodifferenziale è l&rsquo;intervallo $[-1, 1]$. Questo significa che ogni valore $g \\in [-1, 1]$ è una sottoderivata di $f$ in $x = 0$.</li>\n</ul>\n<p>Matematicamente:</p>\n$$\n\\partial f(0) = [-1, 1]\n$$\n<h3 id=\"2-funzione-convessa-generica\">2. Funzione Convessa Generica</h3>\n<p>Per una funzione convessa $f(x)$, il sottodifferenziale in un punto $x_0$ è un insieme chiuso e convesso. Se $f$ è differenziabile in $x_0$, allora il sottodifferenziale è un singleton contenente la derivata classica:</p>\n$$\n\\partial f(x_0) = \\{ f'(x_0) \\}\n$$\n<h2 id=\"proprieta-della-sottoderivata\">Proprietà della Sottoderivata</h2>\n<ol>\n<li><strong>Convessità</strong>: Se $f$ è una funzione convessa, il sottodifferenziale $\\partial f(x_0)$ è un insieme convesso e chiuso.</li>\n<li><strong>Monotonia</strong>: Se $f$ è una funzione convessa, il sottodifferenziale è una funzione monotona, cioè per ogni $x_1 < x_2$ e per ogni $g_1 \\in \\partial f(x_1)$, $g_2 \\in \\partial f(x_2)$, si ha $g_1 \\leq g_2$.</li>\n<li><strong>Ottimizzazione</strong>: La sottoderivata è utile nell&rsquo;ottimizzazione convessa, poiché fornisce una condizione necessaria e sufficiente per i minimi di una funzione convessa. Un punto $x_0$ è un minimo globale di $f$ se e solo se $0 \\in \\partial f(x_0)$.</li>\n</ol>\n<h2 id=\"applicazioni-della-sottoderivata\">Applicazioni della Sottoderivata</h2>\n<h3 id=\"1-regolarizzazione-l1\">1. Regolarizzazione L1</h3>\n<p>Nella regolarizzazione L1 (Lasso), la funzione di perdita include un termine di penalità basato sulla norma L1:</p>\n$$\n\\ell(\\Theta) = \\text{MSE}(\\Theta) + \\lambda \\|\\Theta\\|_1\n$$\n<p>Dove $\\|\\Theta\\|_1$ è la norma L1 dei parametri. Poiché la norma L1 non è differenziabile in $\\Theta = 0$, utilizziamo il concetto di sottoderivata per calcolare il gradiente generalizzato. In particolare, il sottodifferenziale della norma L1 in $\\Theta = 0$ è l&rsquo;insieme di tutti i vettori $g$ tali che:</p>\n$$\ng_i \\in \n\\begin{cases}\n\\{1\\}, & \\text{se } \\Theta_i > 0 \\\\\n\\{-1\\}, & \\text{se } \\Theta_i < 0 \\\\\n[-1, 1], & \\text{se } \\Theta_i = 0\n\\end{cases}\n$$\n<h3 id=\"2-ottimizzazione-convessa\">2. Ottimizzazione Convessa</h3>\n<p>La sottoderivata è ampiamente utilizzata nell&rsquo;ottimizzazione convessa, specialmente quando si lavora con funzioni non differenziabili. Ad esempio, nell&rsquo;algoritmo del <strong>subgradient descent</strong>, si utilizza un vettore del sottodifferenziale per aggiornare i parametri del modello.</p>\n<h2 id=\"sottogradiente\">Sottogradiente</h2>\n<p>Il <strong>sottogradiente</strong> è un concetto che estende la nozione di gradiente a funzioni non differenziabili. Mentre il gradiente classico è definito solo per funzioni differenziabili, il sottogradiente è definito per funzioni convesse (anche non differenziabili) e fornisce una generalizzazione utile per l&rsquo;ottimizzazione e l&rsquo;analisi di funzioni non lisce.</p>\n<h3 id=\"definizione-di-sottogradiente\">Definizione di Sottogradiente</h3>\n<p>Sia $f: \\mathbb{R}^n \\to \\mathbb{R}$ una funzione convessa. Un vettore $g \\in \\mathbb{R}^n$ è chiamato <strong>sottogradiente</strong> di $f$ in un punto $x_0$ se soddisfa la seguente disuguaglianza:</p>\n$$\nf(x) \\geq f(x_0) + g^\\top (x - x_0) \\quad \\text{per ogni } x \\in \\mathbb{R}^n.\n$$\n<p>L&rsquo;insieme di tutti i sottogradienti di $f$ in $x_0$ è chiamato <strong>sottodifferenziale</strong> di $f$ in $x_0$ e si indica con $\\partial f(x_0)$. Se $f$ è differenziabile in $x_0$, allora il sottodifferenziale è un singleton contenente il gradiente classico:</p>\n$$\n\\partial f(x_0) = \\{ \\nabla f(x_0) \\}.\n$$\n<h3 id=\"proprieta-del-sottogradiente\">Proprietà del Sottogradiente</h3>\n<ol>\n<li><strong>Convessità</strong>: Se $f$ è una funzione convessa, il sottodifferenziale $\\partial f(x_0)$ è un insieme convesso e chiuso.</li>\n<li><strong>Non vuoto</strong>: Per una funzione convessa, il sottodifferenziale è sempre non vuoto in ogni punto del dominio.</li>\n<li><strong>Monotonia</strong>: Se $f$ è una funzione convessa, il sottodifferenziale è una funzione monotona, cioè per ogni $x_1, x_2 \\in \\mathbb{R}^n$ e per ogni $g_1 \\in \\partial f(x_1)$, $g_2 \\in \\partial f(x_2)$, si ha:\n   $$\n   (g_1 - g_2)^\\top (x_1 - x_2) \\geq 0.\n   $$</li>\n</ol>\n<h3 id=\"esempi-di-sottogradiente\">Esempi di Sottogradiente</h3>\n<h4 id=\"1-norma-l1-funzione-valore-assoluto-generalizzata\">1. Norma L1 (Funzione Valore Assoluto Generalizzata)</h4>\n<p>Consideriamo la <strong>norma L1</strong> di un vettore $\\mathbf x \\in \\mathbb{R}^n$, definita come:</p>\n$$\nf(\\mathbf x) = \\|\\mathbf x\\|_1 = \\sum_{i=1}^n |\\mathbf x_i|.\n$$\n<p>Questa funzione non è differenziabile in tutti i punti in cui almeno una componente di $\\mathbf x$ è zero. Tuttavia, possiamo calcolare il suo sottodifferenziale.</p>\n<ul>\n<li>Per $x_i > 0$, la derivata parziale rispetto a $x_i$ è $\\frac{\\partial f}{\\partial x_i} = 1$.</li>\n<li>Per $x_i < 0$, la derivata parziale rispetto a $x_i$ è $\\frac{\\partial f}{\\partial x_i} = -1$.</li>\n<li>Per $x_i = 0$, la derivata parziale non è definita, ma il sottodifferenziale rispetto a $x_i$ è l&rsquo;intervallo $[-1, 1]$.</li>\n</ul>\n<p>Quindi, il sottodifferenziale della norma L1 in un punto $\\mathbf x \\in \\mathbb{R}^n$ è dato da:</p>\n$$\n\\partial f(\\mathbf x) = \\{ g \\in \\mathbb{R}^n \\mid g_i \\in \\partial |x_i| \\},\n$$\n<p>dove:</p>\n$$\n\\partial |x_i| = \n\\begin{cases}\n\\{1\\}, & \\text{se } x_i > 0 \\\\\n\\{-1\\}, & \\text{se } x_i < 0 \\\\\n[-1, 1], & \\text{se } x_i = 0\n\\end{cases}\n$$\n<h4 id=\"2-funzione-convessa-generica-in-math_inline_86\">2. Funzione Convessa Generica in $\\mathbb{R}^n$</h4>\n<p>Per una funzione convessa $f: \\mathbb{R}^n \\to \\mathbb{R}$, il sottodifferenziale in un punto $\\mathbf x_0 \\in \\mathbb{R}^n$ è un insieme convesso e chiuso. Se $f$ è differenziabile in $\\mathbf x_0$, allora il sottodifferenziale è un singleton contenente il gradiente classico:</p>\n$$\n\\partial f(\\mathbf x_0) = \\{ \\nabla f(\\mathbf x_0) \\}.\n$$\n<p>Se $f$ non è differenziabile in $\\mathbf x_0$, il sottodifferenziale è un insieme più grande. Ad esempio, per la funzione $f(x) = \\max(x_1, x_2, \\dots, x_n)$, il sottodifferenziale in un punto $\\mathbf x_0$ è l&rsquo;insieme di tutti i vettori $\\mathbf g \\in \\mathbb{R}^n$ tali che:</p>\n$$\ng_i = \n\\begin{cases}\n1, & \\text{se } x_i = \\max(x_1, x_2, \\dots, x_n) \\\\\n0, & \\text{altrimenti}\n\\end{cases}\n$$\n<h3 id=\"applicazioni-del-sottogradiente\">Applicazioni del Sottogradiente</h3>\n<h4 id=\"1-ottimizzazione-non-differenziabile\">1. Ottimizzazione Non Differenziabile</h4>\n<p>Il sottogradiente è ampiamente utilizzato nell&rsquo;ottimizzazione di funzioni non differenziabili. Ad esempio, nell&rsquo;algoritmo del <strong>subgradient descent</strong>, si utilizza un vettore del sottodifferenziale per aggiornare i parametri del modello. L&rsquo;aggiornamento è dato da:</p>\n$$\nx_{k+1} = x_k - \\alpha_k g_k\n$$\n<p>Dove:\n- $x_k$ è il vettore dei parametri al passo $k$.\n- $g_k$ è un sottogradiente di $f$ in $x_k$.\n- $\\alpha_k$ è il passo di apprendimento al passo $k$.</p>\n<h4 id=\"2-regolarizzazione-l1\">2. Regolarizzazione L1</h4>\n<p>Nella regolarizzazione L1 (Lasso), la funzione di perdita include un termine di penalità basato sulla norma L1:</p>\n$$\n\\ell(\\Theta) = \\text{MSE}(\\Theta) + \\lambda \\|\\Theta\\|_1\n$$\n<p>Dove $\\|\\Theta\\|_1$ è la norma L1 dei parametri. Poiché la norma L1 non è differenziabile in $\\Theta = 0$, utilizziamo il concetto di sottogradiente per calcolare il gradiente generalizzato. In particolare, il sottodifferenziale della norma L1 in $\\Theta = 0$ è l&rsquo;insieme di tutti i vettori $g$ tali che:</p>\n$$\ng_i \\in \n\\begin{cases}\n\\{1\\}, & \\text{se } \\Theta_i > 0 \\\\\n\\{-1\\}, & \\text{se } \\Theta_i < 0 \\\\\n[-1, 1], & \\text{se } \\Theta_i = 0\n\\end{cases}\n$$\n<h3 id=\"conclusione\">Conclusione</h3>\n<p>Il sottogradiente è uno strumento essenziale per estendere il concetto di gradiente a funzioni non differenziabili, come la norma L1. Questo concetto è particolarmente utile nell&rsquo;ottimizzazione convessa e nella regolarizzazione, dove funzioni non differenziabili sono comuni. La capacità di lavorare con sottogradienti permette di risolvere problemi complessi che altrimenti sarebbero intrattabili con gli strumenti classici del calcolo differenziale.</p>\n<h2 id=\"conclusione_1\">Conclusione</h2>\n<p>La sottoderivata è uno strumento potente per estendere il concetto di derivata a funzioni non differenziabili, come la norma L1. Questo concetto è particolarmente utile nell&rsquo;ottimizzazione convessa e nella regolarizzazione, dove funzioni non differenziabili sono comuni. La capacità di lavorare con sottoderivate permette di risolvere problemi complessi che altrimenti sarebbero intrattabili con gli strumenti classici del calcolo differenziale.</p>\n<h2 id=\"collegamenti-correlati\">Collegamenti Correlati</h2>\n<ul>\n<li><a href=\"/theory/introduction/Regolarizzazione\" class=\"text-blue-600 hover:underline\">Regolarizzazione</a></li>\n<li><span class=\"text-gray-600\">Norme L1 e L2</span></li>\n<li><span class=\"text-gray-600\">Ottimizzazione Convessa</span></li>\n<li><span class=\"text-gray-600\">Funzioni Convesse</span></li>\n</ul>"
}