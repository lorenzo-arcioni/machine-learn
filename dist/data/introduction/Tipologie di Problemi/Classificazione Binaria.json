{
  "title": "üìä Problema della Classificazione Binaria: Interpretazione Probabilistica",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>La <strong>classificazione binaria</strong> rappresenta uno dei problemi fondamentali dell‚Äôapprendimento supervisionato, in cui l‚Äôobiettivo √® assegnare un&rsquo;osservazione $\\mathbf{x} \\in \\mathbb{R}^d$ a una delle due classi possibili.<br />\nFormalmente, la variabile target $y$ assume valori in un insieme discreto a due elementi:<br />\n$$\ny \\in \\{0, 1\\}\n$$</p>\n<p>Il problema consiste quindi nel costruire una funzione di decisione, o pi√π in generale un modello predittivo, capace di mappare un vettore di caratteristiche (feature) $\\mathbf{x}$ in una stima della probabilit√† di appartenenza alla classe positiva $y = 1$. In particolare, l‚Äôapproccio probabilistico si propone di modellare esplicitamente la distribuzione condizionata:</p>\n$$\np(y = 1 \\mid \\mathbf{x}) = f(\\mathbf{x})\n$$\n<p>dove:\n- $f : \\mathbb{R}^d \\to [0,1]$ √® una funzione che restituisce una probabilit√†, cio√® un valore numerico interpretabile come il grado di confidenza con cui l‚Äôistanza $\\mathbf{x}$ viene attribuita alla classe positiva,\n- l&rsquo;uscita $f(\\mathbf{x})$ pu√≤ quindi essere interpretata come una <strong>stima del rischio condizionato</strong> o <strong>probabilit√† a posteriori</strong>, ottenuta sulla base delle caratteristiche osservate dell‚Äôinput.</p>\n<p>L‚Äôadozione di un modello probabilistico, rispetto a un approccio puramente deterministico, offre numerosi vantaggi:\n- consente di quantificare l‚Äôincertezza nelle predizioni,\n- permette di integrare agevolmente conoscenza a priori (tramite il teorema di Bayes),\n- rende possibili strategie decisionali ottimali rispetto a metriche di costo-asimmetrico (es. minimizzazione del rischio atteso).</p>\n<p>In questo contesto, il problema della classificazione binaria pu√≤ essere affrontato modellando opportunamente la relazione tra $\\mathbf{x}$ e $y$, al fine di apprendere $p(y \\mid \\mathbf{x})$ a partire da un insieme di dati osservati.<br />\nNei paragrafi successivi verr√† approfondito come tale distribuzione possa essere calcolata, interpretata, e utilizzata tramite strumenti teorici quali il <strong>logit</strong>, la <strong>funzione sigmoide</strong> e la <strong><a href=\"/theory/supervised-learning/Linear Models/Regressione Logistica\" class=\"text-blue-600 hover:underline\">regressione logistica</a></strong>.</p>\n<h2 id=\"likelihood-e-posteriori\"><strong>Likelihood e Posteriori</strong></h2>\n<p>Per stimare la probabilit√† della classe, possiamo applicare il <strong>Teorema di Bayes</strong>:\n$$\np(y \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid y) p(y)}{p(\\mathbf{x})}\n$$</p>\n<ul>\n<li>$p(y)$ √® la probabilit√† a priori della classe $y$.</li>\n<li>$p(\\mathbf{x} \\mid y)$ √® la <strong>verosimiglianza</strong> (<em>likelihood</em>), che modella come le caratteristiche $\\mathbf{x}$ sono distribuite all&rsquo;interno di ciascuna classe.</li>\n<li>$p(\\mathbf{x}) = \\sum_{y \\in \\{0,1\\}} p(\\mathbf{x} \\mid y) p(y)$ √® la probabilit√† marginale dei dati.</li>\n</ul>\n<p>In pratica, possiamo stimare $p(y \\mid \\mathbf{x})$ attraverso un modello parametrico che approssima la distribuzione dei dati.</p>\n<h2 id=\"il-concetto-di-logit-e-la-funzione-sigmoide\"><strong>Il Concetto di Logit e la Funzione Sigmoide</strong></h2>\n<p>Come abbiamo visto, nella classificazione binaria, possiamo esprimere la probabilit√† che un&rsquo;osservazione $\\mathbf x \\in \\mathbb R^d$ appartenga alla classe 1 come: $p(y = 1 \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid y) p(y)}{p(\\mathbf{x})}$. Introduciamo ora il concetto di logit: una misura logaritmica di quanto verosimile sia la classe 1 rispetto alla classe 0 (logaritmicamente):</p>\n$$\\begin{align*}\nlogit(p(y = 1 \\mid \\mathbf{x})) = a &= \\log \\frac{p(y = 1 \\mid \\mathbf{x})}{\\underbrace{p(y = 0 \\mid \\mathbf{x})}_{= 1 - p(y = 1 \\mid \\mathbf{x})}}\\\\\ne^a &= \\frac{p(y = 1 \\mid \\mathbf{x})}{\\underbrace{p(y = 0 \\mid \\mathbf{x})}_{= 1 - p(y = 1 \\mid \\mathbf{x})}}\\\\\np(y = 1 \\mid \\mathbf{x}) &= e^a p(y = 0 \\mid \\mathbf{x})\\\\\n\\end{align*}\n$$\n<p>E dato che $p(y = 0 \\mid \\mathbf{x}) + p(y = 1 \\mid \\mathbf{x}) = 1$,</p>\n$$\\begin{align*}\ne^a p(y = 0 \\mid \\mathbf{x}) + p(y = 0 \\mid \\mathbf{x}) &= 1\\\\\np(y = 0 \\mid \\mathbf{x}) (e^a + 1) &= 1\\\\\np(y = 0 \\mid \\mathbf{x}) &= \\frac{1}{e^a + 1}\\\\\n\\end{align*}\n$$\n<p>E ora, usando il fatto che $p(y=1 \\mid \\mathbf x) = e^a \\cdot p(y=0\\mid\\mathbf x)$, otteniamo:</p>\n$$\\begin{align*}\n    p(y=1 \\mid\\mathbf x) &= e^a \\cdot \\frac{1}{1 + e^a}\\\\\n    p(y=1 \\mid\\mathbf x) &= \\frac{e^a}{1 + e^a}\\\\\n    p(y=1 \\mid\\mathbf x) &= \\frac{\\frac{e^a}{e^a}}{\\frac{1}{e^a} + \\frac{e^a}{e^a}}\\\\\n    p(y=1 \\mid\\mathbf x) &= \\frac{1}{1 + e^{-a}} = \\sigma(a) \\ \\text{La funzione sigmoide.}\\\\\n\\end{align*}\n$$\n<p>Questa funzione si chiama <strong>funzione sigmoide</strong> e viene utilizzata per ottenere la probabilit√† di una classe dato un vettore di caratteristiche. Sarebbe quindi l&rsquo;inverso della funzione logit, in quanto abbiamo ricavato la $x$ (che nel nostro caso era $p(y = 1 \\mid \\mathbf{x})$) dalla funzione $logit(x)$. Infatti,</p>\n$$\n\\begin{align*}\n\\text{Partendo da } p &= \\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\\n\\frac{1}{p} &= 1 + e^{-x} \\quad \\text{(Reciproco di entrambi i lati)} \\\\\n\\frac{1}{p} - 1 &= e^{-x} \\quad \\text{(Isolare } e^{-x}) \\\\\n\\frac{1 - p}{p} &= e^{-x} \\quad \\text{(Semplificare)} \\\\\n\\ln\\left(\\frac{1 - p}{p}\\right) &= -x \\quad \\text{(Applicare il logaritmo naturale)} \\\\\nx &= -\\ln\\left(\\frac{1 - p}{p}\\right) = \\ln\\left(\\frac{p}{1 - p}\\right) \\quad \\text{(Risolvere per } x) \\\\\n\\text{Quindi otteniamo alla fine: }\\\\\n\\sigma^{-1}(p) &= \\ln\\left(\\frac{p}{1 - p}\\right).\n\\end{align*}\n$$\n<p>Questo perch√© il logit trasforma un rapporto di probabilit√† $\\in [0, 1]$ in un valore $\\in (-\\infty, +\\infty)$. Mentre la funzione sigmoide trasforma un valore $\\in (-\\infty, +\\infty)$ in un rapporto di probabilit√† $\\in [0, 1]$. In altre parole, il logit controlla quanto le features in input influenzano la probabilit√† di appartenenza alla classe 1 rispetto alla classe 0.</p>\n<h3 id=\"qual-e-la-classe-migliore\">Qual √® la classe migliore?</h3>\n<p>Nel caso di classificazione binaria, la classe con la maggiore probabilit√† di appartenenza viene chiamata <strong>classe migliore</strong>. Assumiamo ad esempio: $\\mathbb P(y = 1 \\mid \\mathbf x) > \\mathbb P(y = 0 \\mid \\mathbf x) = 1 - \\mathbb P(y = 1 \\mid \\mathbf x)$, allora la classe migliore sar√† quella 1. Quindi:</p>\n$$\\begin{align*}\n\\frac{\\mathbb P(y=1 \\mid \\mathbf x)}{1 - \\mathbb P(y=1 \\mid \\mathbf x)} &> 1\\\\\ne^a &> 1\\\\\na &> 0\\\\\n\\end{align*}\n$$\n<h2 id=\"collegamento-con-la-regressione-logistica\">üîó Collegamento con la Regressione Logistica</h2>\n<p>Tutto quanto discusso finora riguardo al logit, alla funzione sigmoide e all&rsquo;interpretazione probabilistica della classificazione binaria, trova una formalizzazione diretta nel modello noto come <strong><a href=\"/theory/supervised-learning/Linear Models/Regressione Logistica\" class=\"text-blue-600 hover:underline\">regressione logistica</a></strong>.</p>\n<p>Nel contesto della regressione logistica, l&rsquo;argomento del logit ‚Äî che abbiamo indicato con $a$ ‚Äî √® espresso come una <strong>combinazione lineare</strong> delle caratteristiche $\\mathbf{x}$, pesate da un vettore di parametri $\\mathbf{w}$, pi√π un termine di bias $b$. Formalmente:</p>\n$$\na = \\mathbf{w}^\\top \\mathbf{x} + b\n$$\n<p>Dove:\n- $\\mathbf{x} \\in \\mathbb{R}^d$ √® il vettore delle feature (caratteristiche dell&rsquo;input),\n- $\\mathbf{w} \\in \\mathbb{R}^d$ √® il vettore dei pesi associati a ciascuna feature,\n- $b \\in \\mathbb{R}$ √® un termine di bias (intercetta).</p>\n<p>Quindi, la <strong>probabilit√† che un&rsquo;osservazione appartenga alla classe 1</strong>, secondo la regressione logistica, √®:</p>\n$$\np(y = 1 \\mid \\mathbf{x}) = \\sigma(a) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n$$\n<h3 id=\"connessione-con-il-framework-probabilistico-visto-prima\">üß† Connessione con il framework probabilistico visto prima</h3>\n<p>Nel paragrafo precedente abbiamo derivato la forma generale della <strong>funzione sigmoide</strong> come:</p>\n$$\np(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-a}} \\quad \\text{dove } a = \\log \\left( \\frac{p(y=1 \\mid \\mathbf{x})}{p(y=0 \\mid \\mathbf{x})} \\right)\n$$\n<p>Ora, nella regressione logistica, si assume <strong>esplicitamente</strong> che questo logit sia <strong>modellato come funzione lineare delle feature</strong>:</p>\n$$\n\\log \\left( \\frac{p(y=1 \\mid \\mathbf{x})}{p(y=0 \\mid \\mathbf{x})} \\right) = \\mathbf{w}^\\top \\mathbf{x} + b\n$$\n<p>In altre parole: il logit, che abbiamo introdotto come misura della tendenza verso la classe positiva, viene parametrizzato linearmente nel modello logistico.</p>\n<p>Questo <strong>collega in modo diretto</strong> la regressione logistica con il framework probabilistico della classificazione binaria:<br />\n<strong>la regressione logistica √® un caso specifico</strong> in cui si assume che il log-odds sia una funzione lineare delle variabili osservabili.</p>\n<p>Quindi:\n- il valore $a = \\mathbf{w}^\\top \\mathbf{x} + b$ rappresenta l&rsquo;evidenza (in scala logaritmica) a favore della classe 1,\n- la funzione sigmoide serve a convertire questa evidenza in una probabilit√† interpretabile in senso bayesiano,\n- la soglia di classificazione (tipicamente 0.5) corrisponde a $a = 0$, cio√® a un logit neutro.</p>\n<p>Questa formulazione consente un&rsquo;integrazione naturale tra modelli statistici, inferenza bayesiana e ottimizzazione numerica, rendendo la regressione logistica un ponte formale tra:\n- il principio del massimo di verosimiglianza,\n- il principio di massima entropia (nel quale la regressione logistica pu√≤ essere reinterpretata),\n- e l‚Äôinterpretazione geometrica dei classificatori lineari.</p>\n<h2 id=\"conclusioni\">üßæ Conclusioni</h2>\n<p>La classificazione binaria, nel contesto probabilistico, fornisce una struttura interpretativa rigorosa per modellare l&rsquo;incertezza e le decisioni in presenza di dati etichettati. Attraverso il teorema di Bayes, la funzione logit e la funzione sigmoide, √® possibile tradurre una relazione tra dati e classi in termini di probabilit√† interpretabili, consentendo predizioni robuste e controllabili.</p>\n<p>L&rsquo;introduzione della <strong>regressione logistica</strong> come specificazione parametrica del modello binario completa elegantemente questo framework:<br />\n- La trasformazione logit consente di modellare i <strong>log-odds</strong> in funzione delle feature osservate.<br />\n- La funzione sigmoide traduce questi log-odds in <strong>probabilit√† ben calibrate</strong>, comprese tra 0 e 1.<br />\n- L&rsquo;intera struttura consente non solo una previsione binaria, ma anche una misura del <strong>grado di confidenza</strong> associato ad essa.</p>\n<p>Inoltre, l&rsquo;approccio probabilistico:\n- rende il modello interpretabile a livello statistico e decisionale,\n- fornisce un criterio naturale di classificazione basato sul <strong>massimo a posteriori</strong>,\n- √® estensibile a contesti pi√π complessi (classificazione multiclasse, sequenziale, o strutturata) tramite generalizzazioni come:\n  - modelli discriminativi (es. <strong><a href=\"/theory/supervised-learning/Non-Linear Models/Maximum Entropy Models\" class=\"text-blue-600 hover:underline\">Maximum Entropy Models</a></strong>),\n  - modelli generativi (es. <strong><span class=\"text-gray-600\">Naive Bayes</span></strong>),\n  - o metodi bayesiani pi√π avanzati.</p>\n<p>In sintesi, comprendere il legame tra probabilit√†, logit e regressione logistica fornisce una base solida non solo per costruire classificatori binari efficaci, ma anche per <strong>interpretare e giustificare le decisioni predittive</strong> nel contesto reale.<br />\nQuesto ponte tra teoria dell&rsquo;informazione, statistica e apprendimento automatico √® ci√≤ che rende questo paradigma fondamentale per tutta la moderna modellazione predittiva.</p>"
}