{
  "title": "Parole, Corpora e Normalizzazione: Concetti Base",
  "content": "<style>pre { line-height: 125%; }\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n.codehilite .hll { background-color: #ffffcc }\n.codehilite { background: #f8f8f8; }\n.codehilite .c { color: #3D7B7B; font-style: italic } /* Comment */\n.codehilite .err { border: 1px solid #F00 } /* Error */\n.codehilite .k { color: #008000; font-weight: bold } /* Keyword */\n.codehilite .o { color: #666 } /* Operator */\n.codehilite .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n.codehilite .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n.codehilite .cp { color: #9C6500 } /* Comment.Preproc */\n.codehilite .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n.codehilite .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n.codehilite .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n.codehilite .gd { color: #A00000 } /* Generic.Deleted */\n.codehilite .ge { font-style: italic } /* Generic.Emph */\n.codehilite .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n.codehilite .gr { color: #E40000 } /* Generic.Error */\n.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n.codehilite .gi { color: #008400 } /* Generic.Inserted */\n.codehilite .go { color: #717171 } /* Generic.Output */\n.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n.codehilite .gs { font-weight: bold } /* Generic.Strong */\n.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n.codehilite .gt { color: #04D } /* Generic.Traceback */\n.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n.codehilite .kp { color: #008000 } /* Keyword.Pseudo */\n.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n.codehilite .kt { color: #B00040 } /* Keyword.Type */\n.codehilite .m { color: #666 } /* Literal.Number */\n.codehilite .s { color: #BA2121 } /* Literal.String */\n.codehilite .na { color: #687822 } /* Name.Attribute */\n.codehilite .nb { color: #008000 } /* Name.Builtin */\n.codehilite .nc { color: #00F; font-weight: bold } /* Name.Class */\n.codehilite .no { color: #800 } /* Name.Constant */\n.codehilite .nd { color: #A2F } /* Name.Decorator */\n.codehilite .ni { color: #717171; font-weight: bold } /* Name.Entity */\n.codehilite .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n.codehilite .nf { color: #00F } /* Name.Function */\n.codehilite .nl { color: #767600 } /* Name.Label */\n.codehilite .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */\n.codehilite .nv { color: #19177C } /* Name.Variable */\n.codehilite .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n.codehilite .w { color: #BBB } /* Text.Whitespace */\n.codehilite .mb { color: #666 } /* Literal.Number.Bin */\n.codehilite .mf { color: #666 } /* Literal.Number.Float */\n.codehilite .mh { color: #666 } /* Literal.Number.Hex */\n.codehilite .mi { color: #666 } /* Literal.Number.Integer */\n.codehilite .mo { color: #666 } /* Literal.Number.Oct */\n.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */\n.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */\n.codehilite .sc { color: #BA2121 } /* Literal.String.Char */\n.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */\n.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */\n.codehilite .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */\n.codehilite .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n.codehilite .sx { color: #008000 } /* Literal.String.Other */\n.codehilite .sr { color: #A45A77 } /* Literal.String.Regex */\n.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */\n.codehilite .ss { color: #19177C } /* Literal.String.Symbol */\n.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */\n.codehilite .fm { color: #00F } /* Name.Function.Magic */\n.codehilite .vc { color: #19177C } /* Name.Variable.Class */\n.codehilite .vg { color: #19177C } /* Name.Variable.Global */\n.codehilite .vi { color: #19177C } /* Name.Variable.Instance */\n.codehilite .vm { color: #19177C } /* Name.Variable.Magic */\n.codehilite .il { color: #666 } /* Literal.Number.Integer.Long */\n\n/* Styling per blocchi di codice */\n.codehilite {\n    background: transparent !important;\n    border-radius: 8px;\n    overflow: hidden;\n}\n.codehilite pre {\n    background: transparent !important;\n    margin: 0 !important;\n    padding: 20px !important;\n    font-family: 'Monaco', 'Menlo', 'Consolas', monospace !important;\n    font-size: 14px !important;\n    line-height: 1.5 !important;\n    white-space: pre !important;\n    overflow-x: auto !important;\n    color: inherit !important;\n}\n.codehilite code {\n    background: transparent !important;\n    padding: 0 !important;\n    font-family: inherit !important;\n}\n\n\n.code-wrapper { \n    position: relative; \n}\n.copy-button {\n    position: absolute; \n    top: 12px; \n    right: 12px; \n    padding: 6px 12px; \n    font-size: 12px;\n    cursor: pointer; \n    border: none; \n    border-radius: 4px; \n    background: rgba(255,255,255,0.9);\n    color: #374151; \n    transition: all 0.2s ease;\n    font-weight: 500;\n}\n.copy-button:hover { \n    background: rgba(255,255,255,1);\n    transform: translateY(-1px);\n}\n\n\ndetails.code-container {\n    border: 1px solid #e5e7eb; \n    border-radius: 12px; \n    background: #f9fafb;\n    margin: 16px 0;\n    transition: all 0.3s ease;\n}\ndetails.code-container summary {\n    padding: 12px 16px;\n    font-size: 14px; \n    color: #6b7280; \n    cursor: pointer; \n    outline: none; \n    user-select: none;\n    font-weight: 500;\n}\ndetails.code-container[open] summary::after { \n    content: \" (Hide Code)\"; \n    color: #9ca3af; \n}\ndetails.code-container:not([open]) summary::after { \n    content: \" (Show Code)\"; \n    color: #d1d5db; \n}\ndetails.code-container .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n/* Blocchi di codice sempre visibili */\n.code-visible {\n    border: 1px solid #e5e7eb;\n    border-radius: 12px;\n    background: #f9fafb;\n    margin: 16px 0;\n}\n.code-visible .code-wrapper {\n    padding: 0;\n    margin: 0;\n}\n</style>\n<p>L&rsquo;elaborazione del linguaggio naturale (NLP) si basa su concetti fondamentali come <strong>corpora</strong>, <strong>parole</strong> e <strong>normalizzazione</strong>. Comprendere queste nozioni è essenziale per pre-elaborare i dati e costruire modelli linguistici efficaci. Questi concetti caratterizzano sia i <strong>rule-based systems</strong> sia i modelli che si basano sulla <strong>machine/deep learning</strong>.</p>\n<h2 id=\"1-corpus-pl-corpora\">1. Corpus (pl. Corpora)</h2>\n<p>Un <strong>corpus</strong> è una raccolta strutturata e digitale di testi o discorsi, spesso utilizzata per analisi linguistiche o per addestrare modelli NLP.  </p>\n<h3 id=\"caratteristiche-principali\">Caratteristiche principali</h3>\n<ul>\n<li><strong>Strutturato</strong>: Organizzato secondo criteri specifici (es. testi scritti vs. parlati).  </li>\n<li><strong>Digitale</strong>: Formato leggibile da un computer per l&rsquo;elaborazione automatica.  </li>\n<li><strong>Annotato (opzionale)</strong>: Alcuni corpora contengono metadati come parti del discorso (<em>POS tagging</em>), analisi sintattica o entità nominate (<em>NER</em>).  </li>\n</ul>\n<h3 id=\"esempi-di-corpora-noti\">Esempi di corpora noti</h3>\n<ol>\n<li><strong>British National Corpus (BNC)</strong>  </li>\n<li>Contiene circa <strong>100 milioni di parole</strong> tratte da giornali, testi accademici e conversazioni.  </li>\n<li>\n<p>Usato per analisi lessicali, studio delle collocazioni e modellazione linguistica.  </p>\n</li>\n<li>\n<p><strong>Corpus of Contemporary American English (COCA)</strong>  </p>\n</li>\n<li>Include testi dal <strong>1990 a oggi</strong> da fonti come TV, riviste e siti web.  </li>\n<li>\n<p>Consente di analizzare come il linguaggio evolve nel tempo.  </p>\n</li>\n<li>\n<p><strong>Penn Treebank</strong>  </p>\n</li>\n<li>\n<p>Corpus annotato con strutture sintattiche usato per il training di modelli NLP avanzati.  </p>\n</li>\n<li>\n<p><strong>Google Books Ngram</strong>  </p>\n</li>\n<li>Raccolta di milioni di libri, utile per studiare trend linguistici su scala storica.  </li>\n</ol>\n<h3 id=\"utilizzo-dei-corpora\">Utilizzo dei corpora</h3>\n<ul>\n<li><strong>Addestramento di modelli linguistici</strong> (es. Word2Vec, BERT).  </li>\n<li><strong>Studio della frequenza delle parole</strong> per identificare termini comuni e rari.  </li>\n<li><strong>Analisi del contesto d&rsquo;uso</strong> di parole e frasi in lingue diverse.  </li>\n</ul>\n<h2 id=\"2-utterance-enunciato\">2. Utterance (Enunciato)</h2>\n<p>Un <strong>utterance</strong> è un&rsquo;unità di discorso parlato, spesso diversa dal testo scritto perché riflette le caratteristiche spontanee del linguaggio orale.  </p>\n<h3 id=\"caratteristiche-del-linguaggio-parlato\">Caratteristiche del linguaggio parlato</h3>\n<ul>\n<li><strong>Disfluenze</strong>: Interruzioni naturali del discorso come pause e esitazioni.  </li>\n<li><em>Esempio</em>: &ldquo;I do <strong>uh</strong> mainly business data processing.&rdquo;  </li>\n<li><strong>Ripetizioni</strong>: Riformulazioni di parole per correggersi o enfatizzare un punto.  </li>\n<li><em>Esempio</em>: &ldquo;I do <strong>main- mainly</strong> business data processing.&rdquo;  </li>\n<li><strong>Elisioni</strong>: Omessa articolazione di alcune parole o sillabe.  </li>\n<li><em>Esempio</em>: &ldquo;Gonna&rdquo; invece di &ldquo;Going to&rdquo;.  </li>\n</ul>\n<h3 id=\"esempio-di-differenza-tra-testo-scritto-e-parlato\">Esempio di differenza tra testo scritto e parlato</h3>\n<table>\n<thead>\n<tr>\n<th>Tipo di testo</th>\n<th>Esempio</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Testo scritto</strong></td>\n<td>&ldquo;I do mainly business data processing.&rdquo;</td>\n</tr>\n<tr>\n<td><strong>Discorso reale (utterance)</strong></td>\n<td>&ldquo;I do uh main- mainly business data processing.&rdquo;</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"applicazioni-dellanalisi-degli-enunciati\">Applicazioni dell&rsquo;analisi degli enunciati</h3>\n<ul>\n<li><strong>Riconoscimento vocale</strong>: Modelli NLP per il riconoscimento automatico del parlato devono gestire disfluenze e variazioni fonetiche.  </li>\n<li><strong>Analisi della spontaneità nel linguaggio</strong>: Utile in studi di linguistica computazionale.  </li>\n</ul>\n<h2 id=\"3-parola-definizione-contestuale\">3. Parola: Definizione Contestuale</h2>\n<p>Il concetto di <strong>parola</strong> in NLP non è sempre univoco e dipende dal contesto di analisi.  </p>\n<h3 id=\"definizioni-fondamentali\">Definizioni Fondamentali</h3>\n<ul>\n<li><strong>Lemma</strong>: Insieme di forme lessicali con la stessa radice, stessa categoria grammaticale principale e stesso significato.  </li>\n<li><strong>Wordform (Forma Parola)</strong>: Forma completa di una parola, inclusi flessioni e derivazioni (es. &ldquo;corre&rdquo;, &ldquo;correrà&rdquo;).  </li>\n<li><strong>Word Types (Tipi di Parola)</strong>: Insieme delle parole distinte in un corpus. La dimensione del vocabolario ( |V| ) rappresenta il numero di tipi.  </li>\n<li><strong>Word Tokens (Token di Parola)</strong>: Occorrenze effettive delle parole nel testo. ( N ) indica il numero totale di token.  </li>\n</ul>\n<h3 id=\"sfide-nella-definizione-di-una-parola\">Sfide nella definizione di una parola</h3>\n<ol>\n<li><strong>Punteggiatura</strong>  </li>\n<li>&ldquo;gatto.&rdquo; e &ldquo;gatto&rdquo; sono lo stesso token?  </li>\n<li>Alcuni modelli considerano il punto un token separato, altri lo uniscono alla parola.  </li>\n<li><strong>Maiuscole/minuscole</strong>  </li>\n<li>&ldquo;Roma&rdquo; (nome proprio) vs. &ldquo;roma&rdquo; (nome comune per un tipo di fiore).  </li>\n<li>La distinzione può essere fondamentale nel riconoscimento delle entità nominate (NER).  </li>\n<li><strong>Contrazioni</strong>  </li>\n<li>&ldquo;can&rsquo;t&rdquo; può essere considerato:  <ul>\n<li>Un <strong>unico token</strong>.  </li>\n<li>Due <strong>token distinti</strong>: &ldquo;can&rdquo; e &ldquo;not&rdquo;.  </li>\n</ul>\n</li>\n<li>La scelta dipende dal metodo di tokenizzazione usato.  </li>\n<li><strong>Ambiguità nelle Forme</strong>  </li>\n<li><strong>Parole composte</strong>: &ldquo;Hewlett-Packard&rdquo; → due token o uno?  </li>\n<li><strong>Clitici</strong>: &ldquo;what’re&rdquo; → &ldquo;what are&rdquo;, &ldquo;L&rsquo;ensemble&rdquo; → &ldquo;Le&rdquo; + &ldquo;ensemble&rdquo;?  </li>\n<li><strong>Numeri e date</strong>: Formati multipli (es. &ldquo;01/02/2024&rdquo; vs. &ldquo;1 febbraio 2024&rdquo;).  </li>\n</ol>\n<h2 id=\"4-tokenizzazione-lessicale\">4. Tokenizzazione lessicale</h2>\n<p>La segmentazione del testo in unità significative (token) è un passo fondamentale della pre-elaborazione NLP.  </p>\n<h3 id=\"metodi-comuni\">Metodi Comuni</h3>\n<ul>\n<li><strong>Tokenizzazione basata su spazi</strong> → &ldquo;Il cane corre&rdquo; → <code>[\"Il\", \"cane\", \"corre\"]</code>.  </li>\n<li><strong>Rimozione della punteggiatura</strong> → &ldquo;Ciao, come stai?&rdquo; → <code>[\"Ciao\", \"come\", \"stai\"]</code>.  </li>\n<li><strong>Standard Penn Treebank</strong>:  </li>\n<li>Separa i clitici (&ldquo;doesn’t&rdquo; → &ldquo;does&rdquo; + &ldquo;n’t&rdquo;).  </li>\n<li>Mantiene le parole hyphenate unite (es. &ldquo;state-of-the-art&rdquo;).  </li>\n<li>Separa tutta la punteggiatura (es. &ldquo;,&rdquo; e &ldquo;?&rdquo; come token singoli).  </li>\n</ul>\n<h3 id=\"lingue-con-spaziatura-complessa\">Lingue con Spaziatura Complessa</h3>\n<ul>\n<li><strong>Cinese e Giapponese</strong>:  </li>\n<li>Non utilizzano spazi tra le parole (es. &ldquo;莎拉波娃现在居住在美国东南部的佛罗里达。&rdquo;).  </li>\n<li>La tokenizzazione richiede modelli specifici per identificare i confini delle parole.  </li>\n</ul>\n<h3 id=\"tokenizzazione-avanzata\">Tokenizzazione Avanzata</h3>\n<ul>\n<li><strong><a href=\"/theory/nlp/Byte-pair Encoding\" class=\"text-blue-600 hover:underline\">Byte-Pair Encoding</a> (BPE)</strong>:  </li>\n<li>Usato in GPT e BERT per gestire sottoparole.  </li>\n<li><strong>Fasi</strong>:  <ol>\n<li>Parte da un vocabolario di caratteri singoli.  </li>\n<li>Unisce gradualmente le coppie di simboli più frequenti (es. &ldquo;A&rdquo; + &ldquo;B&rdquo; → &ldquo;AB&rdquo;).  </li>\n<li>Ripete il processo per ( k ) volte, creando token complessi (es. &ldquo;ri-cor-da-re&rdquo;).  </li>\n</ol>\n</li>\n</ul>\n<h3 id=\"sfide-specifiche\">Sfide Specifiche</h3>\n<ul>\n<li><strong>Espressioni multi-parola</strong>: &ldquo;San Francisco&rdquo; → uno o due token?  </li>\n<li><strong>Varianti ortografiche</strong>: &ldquo;lowercase&rdquo; vs. &ldquo;lower-case&rdquo; vs. &ldquo;lower case&rdquo;.  </li>\n<li><strong>Riconoscimento di entità nominate</strong>: Influenza la scelta di unire o separare token (es. &ldquo;New York&rdquo;).  </li>\n</ul>\n<h3 id=\"tokenizzazione-ulteriori-problemi\">Tokenizzazione: Ulteriori Problemi</h3>\n<ul>\n<li><strong>Parole con apostrofo</strong>: &ldquo;Finland’s capital&rdquo; → &ldquo;Finland&rdquo;, &ldquo;Finlands&rdquo;, o &ldquo;Finland’s&rdquo;?  </li>\n<li><strong>Decisioni contestuali</strong>:  </li>\n<li>&ldquo;co-education&rdquo; → un singolo token.  </li>\n<li>&ldquo;State of the art&rdquo; → segmentazione in base al contesto.  </li>\n<li><strong>Clitici complessi</strong>:  </li>\n<li>Francese: &ldquo;L&rsquo;ensemble&rdquo; → &ldquo;L’&rdquo; + &ldquo;ensemble&rdquo; o token unico?  </li>\n<li>Tedesco: parole composte lunghe (es. &ldquo;Lebensversicherungsgesellschaftsangestellter&rdquo;).  </li>\n</ul>\n<h3 id=\"implementazione-pratica\">Implementazione Pratica</h3>\n<ul>\n<li><strong>Espressioni regolari</strong>: Metodo comune per tokenizzazione deterministica.  </li>\n<li><strong>Strumenti avanzati</strong>:  </li>\n<li><strong>Token Learner</strong>: Identifica pattern ricorrenti nel testo.  </li>\n<li><strong>Token Parser</strong>: Applica regole apprese per suddividere il testo.</li>\n</ul>\n<h2 id=\"5-word-normalization\">5. Word Normalization</h2>\n<p>La <strong>normalizzazione delle parole/token</strong> è il processo di standardizzazione di termini con forme multiple (es. &ldquo;USA&rdquo; vs &ldquo;US&rdquo; o &ldquo;uh-huh&rdquo; vs &ldquo;uhhuh&rdquo;) in un formato coerente.  </p>\n<h3 id=\"tipi-di-normalizzazione\">Tipi di Normalizzazione</h3>\n<ol>\n<li><strong>Case Folding</strong>:  </li>\n<li>Conversione di tutto il testo in minuscolo.  </li>\n<li><em>Esempio</em>: &ldquo;Woodchuck&rdquo; → &ldquo;woodchuck&rdquo;.  </li>\n<li><strong>Vantaggi</strong>: Utile per compiti come information retrieval e riconoscimento vocale.  </li>\n<li>\n<p><strong>Limitazioni</strong>: Perdita di informazioni contestuali (es. nomi propri vs. nomi comuni).  </p>\n</li>\n<li>\n<p><strong>Lemmatization</strong>:  </p>\n</li>\n<li>Identificazione del <strong>lemma</strong> (forma base) di una parola.  </li>\n<li><em>Esempio</em>: &ldquo;am&rdquo;, &ldquo;are&rdquo;, &ldquo;is&rdquo; → lemma &ldquo;be&rdquo;.  Oppure &ldquo;dinner&rdquo; e &ldquo;dinners&rdquo; → lemma &ldquo;dinner&rdquo;.</li>\n<li><strong>Metodo</strong>: Utilizza un <strong>parser morfologico</strong> per scomporre le parole in morfemi (radici e affissi).</li>\n<li><em>Esempio</em>: Un esempio di frase lemmatizzata &ldquo;he is reading detective stories&rdquo; → &ldquo;he be read detective story&rdquo;. Con lemmi &ldquo;be&rdquo; e &ldquo;read&rdquo;.</li>\n<li><strong>Vantaggi</strong>: Utile per compiti come information retrieval e riconoscimento vocale.  </li>\n</ol>\n<h2 id=\"6-porter-stemmer\">6. Porter Stemmer</h2>\n<p>Un approccio &ldquo;brutale&rdquo; alla lemmatizzazione, che rimuove gli affissi per ottenere la <strong>radice</strong> (stem) delle parole.  </p>\n<h3 id=\"funzionamento\">Funzionamento</h3>\n<ul>\n<li><strong>Regole a cascata</strong>: Applica una serie di trasformazioni sequenziali.  </li>\n<li><em>Esempio di regole</em>:  <ul>\n<li>&ldquo;sses&rdquo; → &ldquo;ss&rdquo; (es. &ldquo;caresses&rdquo; → &ldquo;caress&rdquo;).  </li>\n<li>&ldquo;ies&rdquo; → &ldquo;i&rdquo; (es. &ldquo;ponies&rdquo; → &ldquo;poni&rdquo;).</li>\n<li>&ldquo;ing&rdquo; → &ldquo;&rdquo; (es. &ldquo;running&rdquo; → &ldquo;run&rdquo;).  </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"esempio-pratico\">Esempio Pratico</h3>\n<p><strong>Testo originale</strong>:<br />\n<em>This was not the map we found in Billy Bones&rsquo;s chest,\nbut an accurate copy, complete in all things-names\nand heights and soundings-with the single exception\nof the red crosses and the written notes.</em>  </p>\n<p><strong>Testo dopo Porter Stemmer</strong>:<br />\n<em>Thi wa not the map we found in Billi Bone s chest\nbut an accur copi complet in all thing name and\nheight and sound with the singl except of the red\ncross and the written note</em> </p>\n<h3 id=\"limiti-e-dettagli\">Limiti e Dettagli</h3>\n<ul>\n<li>Produce spesso <strong>stemi non lessicali</strong> (es. &ldquo;thi&rdquo; invece di &ldquo;this&rdquo;).  </li>\n<li><a href=\"https://tartarus.org/martin/\">Dettagli e implementazioni</a>.</li>\n<li><a href=\"https://textanalysisonline.com/nltk-porter-stemmer\">Un esempio di implementazione</a>.</li>\n</ul>\n<h2 id=\"7-tokenizzazione-frasale\">7. Tokenizzazione Frasale</h2>\n<p>La <strong>tokenizzazione frasale</strong> o <strong>sentence segmentation</strong> divide il testo in frasi. In particolare, è il processo di divisione di un testo in frasi, tipicamente identificando i segni di punteggio come marcatori di fine frase.</p>\n<p>Può sembrare banale, ma non lo è sempre.</p>\n<p>Il carattere del punto &ldquo;.&rdquo;, ad esempio, è ambiguo tra:\n- <strong>Marcatore di confine frasale</strong> (fine frase)\n- <strong>Marcatore di abbreviazioni</strong> come &ldquo;Mr.&rdquo; o &ldquo;Inc.&rdquo;</p>\n<p>Per questo motivo, la <strong>tokenizzazione frasale</strong> e la <strong>tokenizzazione lessicale</strong> vengono spesso affrontate congiuntamente.</p>\n<h3 id=\"metodi-di-segmentazione-frasale\">Metodi di Segmentazione Frasale</h3>\n<p>I metodi di tokenizzazione frasale generalmente:</p>\n<ol>\n<li><strong>Decidono</strong> (tramite regole o machine learning) se un punto:</li>\n<li>Fa parte della parola (es. abbreviazione)</li>\n<li>\n<p>È un marcatore di fine frase</p>\n</li>\n<li>\n<p><strong>Utilizzano dizionari di abbreviazioni</strong> per identificare:</p>\n</li>\n<li>Abbreviazioni comuni (es. &ldquo;Dr.&rdquo;, &ldquo;Prof.&rdquo;)</li>\n<li>I dizionari possono essere:<ul>\n<li>Costruiti manualmente</li>\n<li>Appresi automaticamente (machine learning)</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"implementazione-in-stanford-corenlp\">Implementazione in Stanford CoreNLP</h3>\n<p>Nel toolkit <a href=\"https://stanfordnlp.github.io/CoreNLP/\">Stanford CoreNLP</a> [Manning et al., 2014]:\n- La segmentazione frasale è <strong>basata su regole</strong>\n- Una frase termina quando incontra:\n  - Punteggiatura finale (., !, ?) \n  - Che <strong>non</strong> fa parte di un token esistente (es. abbreviazioni o numeri)\n  - Opzionalmente seguita da virgolette o parentesi finali</p>\n<p><strong>Esempio pratico (toolkit)</strong>:\n&ldquo;Dr. Smith arrived at 5 p.m. He was late.&rdquo; → Viene correttamente segmentato in 2 frasi nonostante i punti in &ldquo;Dr.&rdquo; e &ldquo;p.m.&rdquo;</p>\n<p><strong>Etichette</strong>: #NLP #Tokenizzazione #Corpora<br />\n<strong>Collegamenti</strong>: <span class=\"text-gray-600\">Elaborazione del Testo</span>, <a href=\"/theory/nlp/Espressioni Regolari/Espressioni Regolari\" class=\"text-blue-600 hover:underline\">Espressioni Regolari</a>, <span class=\"text-gray-600\">Modelli Linguistici</span></p>"
}